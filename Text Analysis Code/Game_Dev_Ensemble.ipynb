{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Game_Dev_Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "r_qCc6Wc-Ddf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vn7LgUYv8Rxz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "0gir5MFM83pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN4KhuCI8hjv",
        "outputId": "0448a677-6c1a-471c-8b7e-910014e953a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Classifiers"
      ],
      "metadata": {
        "id": "0Clgp4JK-GAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizedata(X_train):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "tNO8fboU8wcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model8(trdata,tract,tsdata):\n",
        "    model = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5).fit(trdata,tract)\n",
        "    pred= model.predict(tsdata)\n",
        "    pred1=model.predict_proba(tsdata)\n",
        "    return pred,pred1\n",
        "def model10(trdata,tract,tsdata):\n",
        "    model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0).fit(trdata,tract)\n",
        "    model.fit(trdata,tract)\n",
        "    pred= model.predict(tsdata)\n",
        "    pred1=model.predict_proba(tsdata)\n",
        "    return pred,pred1\n",
        "def model11(trdata,tract,tsdata):\n",
        "    model = AdaBoostClassifier(n_estimators=10).fit(trdata,tract)\n",
        "    pred= model.predict(tsdata)\n",
        "    pred1=model.predict_proba(tsdata)\n",
        "    return pred,pred1\n",
        "def model12(trdata,tract,tsdata):\n",
        "    model = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0,max_depth=1, random_state=0).fit(trdata,tract)\n",
        "    pred= model.predict(tsdata)\n",
        "    pred1=model.predict_proba(tsdata)\n",
        "    return pred,pred1"
      ],
      "metadata": {
        "id": "RI5pXbF59Hj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(10)"
      ],
      "metadata": {
        "id": "Ps8FEiwh9hZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Game_Dev_DataSet/\" "
      ],
      "metadata": {
        "id": "13y5m5iy8ZtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i in range(1,57):\n",
        "    print(i)\n",
        "    fname=path+str(i)+'.csv'\n",
        "    data=np.genfromtxt(fname,delimiter=',')\n",
        "    predvalue=np.zeros((np.shape(data)[0],6))\n",
        "    predvalue1=np.zeros((np.shape(data)[0],25))\n",
        "    data[:,0:-1]=normalizedata(data[:,0:-1])\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        trdata=data[train_index,0:-1]\n",
        "        tsdata=data[test_index,0:-1]\n",
        "        tract=data[train_index,-1]\n",
        "        tsact=data[test_index,-1]\n",
        "        predvalue[test_index,0],predvalue1[test_index,0:4]=model8(trdata,tract,tsdata) #Bagging\n",
        "        predvalue[test_index,1],predvalue1[test_index,4:8]=model10(trdata,tract,tsdata) #ExtraTrees\n",
        "        predvalue[test_index,2],predvalue1[test_index,8:12]=model11(trdata,tract,tsdata) #AdaBoost\n",
        "        predvalue[test_index,3],predvalue1[test_index,12:16]=model12(trdata,tract,tsdata) #GradientBoost\n",
        "        #predvalue[test_index,4],predvalue1[test_index,16:20]=model13(trdata,tract,tsdata); #Multi layer perceptron \n",
        "        predvalue[test_index,4]=tsact\n",
        "        predvalue1[test_index,16:20]=to_categorical(tsact,4);\n",
        "    fname=str(i)+'two.csv'    \n",
        "    np.savetxt(fname, predvalue, delimiter=',', fmt='%f')  \n",
        "    #files.download(fname)\n",
        "    fname=str(i)+'twop.csv'    \n",
        "    np.savetxt(fname,predvalue1, delimiter=',', fmt='%f')  \n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "FulXAs1p9yD5",
        "outputId": "e024fd52-0d14-4c7b-cb39-4376640b0218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i in range(1,57):\\n    print(i)\\n    fname=path+str(i)+'.csv'\\n    data=np.genfromtxt(fname,delimiter=',')\\n    predvalue=np.zeros((np.shape(data)[0],6))\\n    predvalue1=np.zeros((np.shape(data)[0],25))\\n    data[:,0:-1]=normalizedata(data[:,0:-1])\\n    for train_index, test_index in kf.split(data):\\n        trdata=data[train_index,0:-1]\\n        tsdata=data[test_index,0:-1]\\n        tract=data[train_index,-1]\\n        tsact=data[test_index,-1]\\n        predvalue[test_index,0],predvalue1[test_index,0:4]=model8(trdata,tract,tsdata) #Bagging\\n        predvalue[test_index,1],predvalue1[test_index,4:8]=model10(trdata,tract,tsdata) #ExtraTrees\\n        predvalue[test_index,2],predvalue1[test_index,8:12]=model11(trdata,tract,tsdata) #AdaBoost\\n        predvalue[test_index,3],predvalue1[test_index,12:16]=model12(trdata,tract,tsdata) #GradientBoost\\n        #predvalue[test_index,4],predvalue1[test_index,16:20]=model13(trdata,tract,tsdata); #Multi layer perceptron \\n        predvalue[test_index,4]=tsact\\n        predvalue1[test_index,16:20]=to_categorical(tsact,4);\\n    fname=str(i)+'two.csv'    \\n    np.savetxt(fname, predvalue, delimiter=',', fmt='%f')  \\n    #files.download(fname)\\n    fname=str(i)+'twop.csv'    \\n    np.savetxt(fname,predvalue1, delimiter=',', fmt='%f')  \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "plFnWqU-ms1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141\n",
        "def model13(trdata, tract, tsdata):\n",
        "  #Relu activation since multi class classification. \n",
        "   model = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=100, activation = \"relu\", solver=\"adam\", verbose=10, random_state=1, learning_rate=\"invscaling\")\n",
        "   model.fit(trdata,tract)\n",
        "   pred = model.predict(tsdata)\n",
        "   pred1 = model.predict_proba(tsdata)\n",
        "   return pred,pred1"
      ],
      "metadata": {
        "id": "wLFQDvhqCg_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,57):\n",
        "    print(i)\n",
        "    fname=path+str(i)+'.csv'\n",
        "    data=np.genfromtxt(fname,delimiter=',')\n",
        "    predvalue=np.zeros((np.shape(data)[0],6))\n",
        "    predvalue1=np.zeros((np.shape(data)[0],25))\n",
        "    data[:,0:-1]=normalizedata(data[:,0:-1])\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        trdata=data[train_index,0:-1]\n",
        "        tsdata=data[test_index,0:-1]\n",
        "        tract=data[train_index,-1]\n",
        "        tsact=data[test_index,-1]\n",
        "        predvalue[test_index,0],predvalue1[test_index,0:4]=model8(trdata,tract,tsdata) #Bagging\n",
        "        predvalue[test_index,1],predvalue1[test_index,4:8]=model10(trdata,tract,tsdata) #ExtraTrees\n",
        "        predvalue[test_index,2],predvalue1[test_index,8:12]=model11(trdata,tract,tsdata) #AdaBoost\n",
        "        predvalue[test_index,3],predvalue1[test_index,12:16]=model12(trdata,tract,tsdata) #GradientBoost\n",
        "        predvalue[test_index,4],predvalue1[test_index,16:20]=model13(trdata,tract,tsdata); #Multi layer perceptron \n",
        "        predvalue[test_index,5]=tsact\n",
        "        predvalue1[test_index,20:24]=to_categorical(tsact,4);\n",
        "    fname=str(i)+'two.csv'    \n",
        "    np.savetxt(fname, predvalue, delimiter=',', fmt='%f')  \n",
        "    #files.download(fname)\n",
        "    fname=str(i)+'twop.csv'    \n",
        "    np.savetxt(fname,predvalue1, delimiter=',', fmt='%f')  "
      ],
      "metadata": {
        "id": "sYOu19fGFFmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f597efcb-731f-46dc-fa13-0b35875a410e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Iteration 1, loss = 1.31394992\n",
            "Iteration 2, loss = 1.17907843\n",
            "Iteration 3, loss = 1.02403513\n",
            "Iteration 4, loss = 0.82693330\n",
            "Iteration 5, loss = 0.59012141\n",
            "Iteration 6, loss = 0.35465452\n",
            "Iteration 7, loss = 0.15906807\n",
            "Iteration 8, loss = 0.05833391\n",
            "Iteration 9, loss = 0.01958859\n",
            "Iteration 10, loss = 0.00762010\n",
            "Iteration 11, loss = 0.00358832\n",
            "Iteration 12, loss = 0.00203766\n",
            "Iteration 13, loss = 0.00139395\n",
            "Iteration 14, loss = 0.00105822\n",
            "Iteration 15, loss = 0.00088441\n",
            "Iteration 16, loss = 0.00077143\n",
            "Iteration 17, loss = 0.00070011\n",
            "Iteration 18, loss = 0.00065123\n",
            "Iteration 19, loss = 0.00060917\n",
            "Iteration 20, loss = 0.00057934\n",
            "Iteration 21, loss = 0.00055386\n",
            "Iteration 22, loss = 0.00053198\n",
            "Iteration 23, loss = 0.00051235\n",
            "Iteration 24, loss = 0.00049299\n",
            "Iteration 25, loss = 0.00047741\n",
            "Iteration 26, loss = 0.00046155\n",
            "Iteration 27, loss = 0.00044754\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31395427\n",
            "Iteration 2, loss = 1.17810998\n",
            "Iteration 3, loss = 1.02722884\n",
            "Iteration 4, loss = 0.83144673\n",
            "Iteration 5, loss = 0.59735416\n",
            "Iteration 6, loss = 0.36102094\n",
            "Iteration 7, loss = 0.16109946\n",
            "Iteration 8, loss = 0.05781486\n",
            "Iteration 9, loss = 0.01889293\n",
            "Iteration 10, loss = 0.00724129\n",
            "Iteration 11, loss = 0.00338161\n",
            "Iteration 12, loss = 0.00191252\n",
            "Iteration 13, loss = 0.00132369\n",
            "Iteration 14, loss = 0.00100857\n",
            "Iteration 15, loss = 0.00084572\n",
            "Iteration 16, loss = 0.00073961\n",
            "Iteration 17, loss = 0.00067317\n",
            "Iteration 18, loss = 0.00062636\n",
            "Iteration 19, loss = 0.00058721\n",
            "Iteration 20, loss = 0.00055925\n",
            "Iteration 21, loss = 0.00053533\n",
            "Iteration 22, loss = 0.00051508\n",
            "Iteration 23, loss = 0.00049655\n",
            "Iteration 24, loss = 0.00047831\n",
            "Iteration 25, loss = 0.00046406\n",
            "Iteration 26, loss = 0.00044983\n",
            "Iteration 27, loss = 0.00043712\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31607991\n",
            "Iteration 2, loss = 1.18174006\n",
            "Iteration 3, loss = 1.03249888\n",
            "Iteration 4, loss = 0.83169380\n",
            "Iteration 5, loss = 0.59270097\n",
            "Iteration 6, loss = 0.35238553\n",
            "Iteration 7, loss = 0.15571173\n",
            "Iteration 8, loss = 0.05632349\n",
            "Iteration 9, loss = 0.01870329\n",
            "Iteration 10, loss = 0.00720400\n",
            "Iteration 11, loss = 0.00338231\n",
            "Iteration 12, loss = 0.00192474\n",
            "Iteration 13, loss = 0.00132847\n",
            "Iteration 14, loss = 0.00100865\n",
            "Iteration 15, loss = 0.00084383\n",
            "Iteration 16, loss = 0.00073399\n",
            "Iteration 17, loss = 0.00066552\n",
            "Iteration 18, loss = 0.00061744\n",
            "Iteration 19, loss = 0.00057648\n",
            "Iteration 20, loss = 0.00054743\n",
            "Iteration 21, loss = 0.00052239\n",
            "Iteration 22, loss = 0.00050078\n",
            "Iteration 23, loss = 0.00048214\n",
            "Iteration 24, loss = 0.00046351\n",
            "Iteration 25, loss = 0.00044868\n",
            "Iteration 26, loss = 0.00043400\n",
            "Iteration 27, loss = 0.00042072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31771463\n",
            "Iteration 2, loss = 1.18657322\n",
            "Iteration 3, loss = 1.04093132\n",
            "Iteration 4, loss = 0.83898217\n",
            "Iteration 5, loss = 0.59243782\n",
            "Iteration 6, loss = 0.34679490\n",
            "Iteration 7, loss = 0.15196331\n",
            "Iteration 8, loss = 0.05509681\n",
            "Iteration 9, loss = 0.01840430\n",
            "Iteration 10, loss = 0.00712096\n",
            "Iteration 11, loss = 0.00336971\n",
            "Iteration 12, loss = 0.00190753\n",
            "Iteration 13, loss = 0.00131464\n",
            "Iteration 14, loss = 0.00099796\n",
            "Iteration 15, loss = 0.00083387\n",
            "Iteration 16, loss = 0.00072558\n",
            "Iteration 17, loss = 0.00065872\n",
            "Iteration 18, loss = 0.00061038\n",
            "Iteration 19, loss = 0.00056946\n",
            "Iteration 20, loss = 0.00054004\n",
            "Iteration 21, loss = 0.00051512\n",
            "Iteration 22, loss = 0.00049345\n",
            "Iteration 23, loss = 0.00047431\n",
            "Iteration 24, loss = 0.00045522\n",
            "Iteration 25, loss = 0.00044064\n",
            "Iteration 26, loss = 0.00042579\n",
            "Iteration 27, loss = 0.00041224\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31490565\n",
            "Iteration 2, loss = 1.17658953\n",
            "Iteration 3, loss = 1.02855574\n",
            "Iteration 4, loss = 0.82652193\n",
            "Iteration 5, loss = 0.58680034\n",
            "Iteration 6, loss = 0.34886447\n",
            "Iteration 7, loss = 0.15644635\n",
            "Iteration 8, loss = 0.05698344\n",
            "Iteration 9, loss = 0.01908021\n",
            "Iteration 10, loss = 0.00733236\n",
            "Iteration 11, loss = 0.00344527\n",
            "Iteration 12, loss = 0.00194758\n",
            "Iteration 13, loss = 0.00135068\n",
            "Iteration 14, loss = 0.00102690\n",
            "Iteration 15, loss = 0.00085997\n",
            "Iteration 16, loss = 0.00074854\n",
            "Iteration 17, loss = 0.00068160\n",
            "Iteration 18, loss = 0.00063372\n",
            "Iteration 19, loss = 0.00059482\n",
            "Iteration 20, loss = 0.00056701\n",
            "Iteration 21, loss = 0.00054368\n",
            "Iteration 22, loss = 0.00052360\n",
            "Iteration 23, loss = 0.00050603\n",
            "Iteration 24, loss = 0.00048829\n",
            "Iteration 25, loss = 0.00047430\n",
            "Iteration 26, loss = 0.00045998\n",
            "Iteration 27, loss = 0.00044667\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31086699\n",
            "Iteration 2, loss = 1.17013665\n",
            "Iteration 3, loss = 1.02046570\n",
            "Iteration 4, loss = 0.82229590\n",
            "Iteration 5, loss = 0.58845527\n",
            "Iteration 6, loss = 0.35671721\n",
            "Iteration 7, loss = 0.16357456\n",
            "Iteration 8, loss = 0.06000914\n",
            "Iteration 9, loss = 0.02019010\n",
            "Iteration 10, loss = 0.00779723\n",
            "Iteration 11, loss = 0.00364926\n",
            "Iteration 12, loss = 0.00205572\n",
            "Iteration 13, loss = 0.00141127\n",
            "Iteration 14, loss = 0.00107303\n",
            "Iteration 15, loss = 0.00089988\n",
            "Iteration 16, loss = 0.00078278\n",
            "Iteration 17, loss = 0.00071130\n",
            "Iteration 18, loss = 0.00066082\n",
            "Iteration 19, loss = 0.00061889\n",
            "Iteration 20, loss = 0.00058957\n",
            "Iteration 21, loss = 0.00056386\n",
            "Iteration 22, loss = 0.00054096\n",
            "Iteration 23, loss = 0.00052076\n",
            "Iteration 24, loss = 0.00050120\n",
            "Iteration 25, loss = 0.00048548\n",
            "Iteration 26, loss = 0.00046919\n",
            "Iteration 27, loss = 0.00045466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31204696\n",
            "Iteration 2, loss = 1.17119420\n",
            "Iteration 3, loss = 1.02146023\n",
            "Iteration 4, loss = 0.81421431\n",
            "Iteration 5, loss = 0.57142990\n",
            "Iteration 6, loss = 0.33462368\n",
            "Iteration 7, loss = 0.15216661\n",
            "Iteration 8, loss = 0.05575425\n",
            "Iteration 9, loss = 0.01910686\n",
            "Iteration 10, loss = 0.00724176\n",
            "Iteration 11, loss = 0.00337996\n",
            "Iteration 12, loss = 0.00186517\n",
            "Iteration 13, loss = 0.00129135\n",
            "Iteration 14, loss = 0.00098057\n",
            "Iteration 15, loss = 0.00082098\n",
            "Iteration 16, loss = 0.00071849\n",
            "Iteration 17, loss = 0.00065435\n",
            "Iteration 18, loss = 0.00061075\n",
            "Iteration 19, loss = 0.00057385\n",
            "Iteration 20, loss = 0.00054844\n",
            "Iteration 21, loss = 0.00052665\n",
            "Iteration 22, loss = 0.00050746\n",
            "Iteration 23, loss = 0.00049152\n",
            "Iteration 24, loss = 0.00047597\n",
            "Iteration 25, loss = 0.00046328\n",
            "Iteration 26, loss = 0.00045034\n",
            "Iteration 27, loss = 0.00043822\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31016762\n",
            "Iteration 2, loss = 1.16930383\n",
            "Iteration 3, loss = 1.01722474\n",
            "Iteration 4, loss = 0.80825968\n",
            "Iteration 5, loss = 0.57027942\n",
            "Iteration 6, loss = 0.33796999\n",
            "Iteration 7, loss = 0.15624504\n",
            "Iteration 8, loss = 0.05607096\n",
            "Iteration 9, loss = 0.01903515\n",
            "Iteration 10, loss = 0.00711610\n",
            "Iteration 11, loss = 0.00329275\n",
            "Iteration 12, loss = 0.00186617\n",
            "Iteration 13, loss = 0.00126385\n",
            "Iteration 14, loss = 0.00096518\n",
            "Iteration 15, loss = 0.00079976\n",
            "Iteration 16, loss = 0.00069827\n",
            "Iteration 17, loss = 0.00063636\n",
            "Iteration 18, loss = 0.00058790\n",
            "Iteration 19, loss = 0.00055169\n",
            "Iteration 20, loss = 0.00052339\n",
            "Iteration 21, loss = 0.00049949\n",
            "Iteration 22, loss = 0.00048019\n",
            "Iteration 23, loss = 0.00046240\n",
            "Iteration 24, loss = 0.00044699\n",
            "Iteration 25, loss = 0.00043351\n",
            "Iteration 26, loss = 0.00042106\n",
            "Iteration 27, loss = 0.00040907\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30795241\n",
            "Iteration 2, loss = 1.16274136\n",
            "Iteration 3, loss = 1.00957881\n",
            "Iteration 4, loss = 0.80099459\n",
            "Iteration 5, loss = 0.56343671\n",
            "Iteration 6, loss = 0.33124297\n",
            "Iteration 7, loss = 0.14897456\n",
            "Iteration 8, loss = 0.05090894\n",
            "Iteration 9, loss = 0.01721918\n",
            "Iteration 10, loss = 0.00643738\n",
            "Iteration 11, loss = 0.00297947\n",
            "Iteration 12, loss = 0.00169815\n",
            "Iteration 13, loss = 0.00115921\n",
            "Iteration 14, loss = 0.00090465\n",
            "Iteration 15, loss = 0.00075523\n",
            "Iteration 16, loss = 0.00066630\n",
            "Iteration 17, loss = 0.00060871\n",
            "Iteration 18, loss = 0.00056571\n",
            "Iteration 19, loss = 0.00053375\n",
            "Iteration 20, loss = 0.00050855\n",
            "Iteration 21, loss = 0.00048649\n",
            "Iteration 22, loss = 0.00046873\n",
            "Iteration 23, loss = 0.00045301\n",
            "Iteration 24, loss = 0.00043896\n",
            "Iteration 25, loss = 0.00042662\n",
            "Iteration 26, loss = 0.00041634\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30312781\n",
            "Iteration 2, loss = 1.15966799\n",
            "Iteration 3, loss = 1.00671859\n",
            "Iteration 4, loss = 0.79405685\n",
            "Iteration 5, loss = 0.55483126\n",
            "Iteration 6, loss = 0.32872296\n",
            "Iteration 7, loss = 0.15642427\n",
            "Iteration 8, loss = 0.05862448\n",
            "Iteration 9, loss = 0.02079611\n",
            "Iteration 10, loss = 0.00817569\n",
            "Iteration 11, loss = 0.00381943\n",
            "Iteration 12, loss = 0.00214570\n",
            "Iteration 13, loss = 0.00143350\n",
            "Iteration 14, loss = 0.00109434\n",
            "Iteration 15, loss = 0.00089150\n",
            "Iteration 16, loss = 0.00078002\n",
            "Iteration 17, loss = 0.00069530\n",
            "Iteration 18, loss = 0.00063651\n",
            "Iteration 19, loss = 0.00059408\n",
            "Iteration 20, loss = 0.00056125\n",
            "Iteration 21, loss = 0.00053249\n",
            "Iteration 22, loss = 0.00051040\n",
            "Iteration 23, loss = 0.00048923\n",
            "Iteration 24, loss = 0.00047148\n",
            "Iteration 25, loss = 0.00045518\n",
            "Iteration 26, loss = 0.00044217\n",
            "Iteration 27, loss = 0.00042837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "2\n",
            "Iteration 1, loss = 1.30929674\n",
            "Iteration 2, loss = 1.24160951\n",
            "Iteration 3, loss = 1.21880967\n",
            "Iteration 4, loss = 1.21735524\n",
            "Iteration 5, loss = 1.22283469\n",
            "Iteration 6, loss = 1.19850129\n",
            "Iteration 7, loss = 1.18734137\n",
            "Iteration 8, loss = 1.17701319\n",
            "Iteration 9, loss = 1.17746928\n",
            "Iteration 10, loss = 1.17073889\n",
            "Iteration 11, loss = 1.16889198\n",
            "Iteration 12, loss = 1.15493999\n",
            "Iteration 13, loss = 1.14840888\n",
            "Iteration 14, loss = 1.17402080\n",
            "Iteration 15, loss = 1.14794424\n",
            "Iteration 16, loss = 1.15335969\n",
            "Iteration 17, loss = 1.15144656\n",
            "Iteration 18, loss = 1.13559623\n",
            "Iteration 19, loss = 1.12804579\n",
            "Iteration 20, loss = 1.12267557\n",
            "Iteration 21, loss = 1.14788193\n",
            "Iteration 22, loss = 1.12414191\n",
            "Iteration 23, loss = 1.11555566\n",
            "Iteration 24, loss = 1.12092079\n",
            "Iteration 25, loss = 1.12632401\n",
            "Iteration 26, loss = 1.10440024\n",
            "Iteration 27, loss = 1.10933013\n",
            "Iteration 28, loss = 1.10297990\n",
            "Iteration 29, loss = 1.09637768\n",
            "Iteration 30, loss = 1.10032344\n",
            "Iteration 31, loss = 1.09879406\n",
            "Iteration 32, loss = 1.09141987\n",
            "Iteration 33, loss = 1.09281082\n",
            "Iteration 34, loss = 1.08172848\n",
            "Iteration 35, loss = 1.09640706\n",
            "Iteration 36, loss = 1.08311094\n",
            "Iteration 37, loss = 1.07817975\n",
            "Iteration 38, loss = 1.07123582\n",
            "Iteration 39, loss = 1.07202539\n",
            "Iteration 40, loss = 1.09556550\n",
            "Iteration 41, loss = 1.11477685\n",
            "Iteration 42, loss = 1.09986815\n",
            "Iteration 43, loss = 1.06891827\n",
            "Iteration 44, loss = 1.08576252\n",
            "Iteration 45, loss = 1.07409245\n",
            "Iteration 46, loss = 1.06157377\n",
            "Iteration 47, loss = 1.06319134\n",
            "Iteration 48, loss = 1.05647737\n",
            "Iteration 49, loss = 1.05296115\n",
            "Iteration 50, loss = 1.06229528\n",
            "Iteration 51, loss = 1.05580530\n",
            "Iteration 52, loss = 1.05681834\n",
            "Iteration 53, loss = 1.04964439\n",
            "Iteration 54, loss = 1.05263342\n",
            "Iteration 55, loss = 1.05461217\n",
            "Iteration 56, loss = 1.06132278\n",
            "Iteration 57, loss = 1.08083088\n",
            "Iteration 58, loss = 1.05664138\n",
            "Iteration 59, loss = 1.03895887\n",
            "Iteration 60, loss = 1.02981682\n",
            "Iteration 61, loss = 1.03548981\n",
            "Iteration 62, loss = 1.03655357\n",
            "Iteration 63, loss = 1.08198966\n",
            "Iteration 64, loss = 1.10738239\n",
            "Iteration 65, loss = 1.05710865\n",
            "Iteration 66, loss = 1.03967088\n",
            "Iteration 67, loss = 1.02425557\n",
            "Iteration 68, loss = 1.01255550\n",
            "Iteration 69, loss = 1.01180546\n",
            "Iteration 70, loss = 1.03385626\n",
            "Iteration 71, loss = 1.05006471\n",
            "Iteration 72, loss = 1.02563776\n",
            "Iteration 73, loss = 1.02407338\n",
            "Iteration 74, loss = 1.02292810\n",
            "Iteration 75, loss = 1.00853918\n",
            "Iteration 76, loss = 1.01016714\n",
            "Iteration 77, loss = 1.01374640\n",
            "Iteration 78, loss = 1.00992555\n",
            "Iteration 79, loss = 1.00125677\n",
            "Iteration 80, loss = 0.99153035\n",
            "Iteration 81, loss = 0.98341288\n",
            "Iteration 82, loss = 0.98316293\n",
            "Iteration 83, loss = 0.98106794\n",
            "Iteration 84, loss = 0.98838217\n",
            "Iteration 85, loss = 1.00309451\n",
            "Iteration 86, loss = 1.01216837\n",
            "Iteration 87, loss = 1.02736119\n",
            "Iteration 88, loss = 0.99046236\n",
            "Iteration 89, loss = 0.98972384\n",
            "Iteration 90, loss = 1.00161861\n",
            "Iteration 91, loss = 1.00026218\n",
            "Iteration 92, loss = 1.00211187\n",
            "Iteration 93, loss = 0.98786432\n",
            "Iteration 94, loss = 0.99571579\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30777821\n",
            "Iteration 2, loss = 1.23849587\n",
            "Iteration 3, loss = 1.21343112\n",
            "Iteration 4, loss = 1.21693853\n",
            "Iteration 5, loss = 1.22113578\n",
            "Iteration 6, loss = 1.19597335\n",
            "Iteration 7, loss = 1.17289589\n",
            "Iteration 8, loss = 1.16870355\n",
            "Iteration 9, loss = 1.15976153\n",
            "Iteration 10, loss = 1.15787322\n",
            "Iteration 11, loss = 1.15719464\n",
            "Iteration 12, loss = 1.13925195\n",
            "Iteration 13, loss = 1.14755637\n",
            "Iteration 14, loss = 1.18651170\n",
            "Iteration 15, loss = 1.14792705\n",
            "Iteration 16, loss = 1.13065654\n",
            "Iteration 17, loss = 1.15143641\n",
            "Iteration 18, loss = 1.12537394\n",
            "Iteration 19, loss = 1.12406509\n",
            "Iteration 20, loss = 1.11581028\n",
            "Iteration 21, loss = 1.12823892\n",
            "Iteration 22, loss = 1.11695531\n",
            "Iteration 23, loss = 1.10781032\n",
            "Iteration 24, loss = 1.11313900\n",
            "Iteration 25, loss = 1.10687727\n",
            "Iteration 26, loss = 1.10579582\n",
            "Iteration 27, loss = 1.10143341\n",
            "Iteration 28, loss = 1.10402821\n",
            "Iteration 29, loss = 1.09957210\n",
            "Iteration 30, loss = 1.11011585\n",
            "Iteration 31, loss = 1.10859801\n",
            "Iteration 32, loss = 1.10032408\n",
            "Iteration 33, loss = 1.08065569\n",
            "Iteration 34, loss = 1.08587158\n",
            "Iteration 35, loss = 1.08756450\n",
            "Iteration 36, loss = 1.07827603\n",
            "Iteration 37, loss = 1.07710063\n",
            "Iteration 38, loss = 1.06982699\n",
            "Iteration 39, loss = 1.06588346\n",
            "Iteration 40, loss = 1.08178937\n",
            "Iteration 41, loss = 1.08932425\n",
            "Iteration 42, loss = 1.07873257\n",
            "Iteration 43, loss = 1.07223066\n",
            "Iteration 44, loss = 1.07050949\n",
            "Iteration 45, loss = 1.06084940\n",
            "Iteration 46, loss = 1.05776994\n",
            "Iteration 47, loss = 1.05443193\n",
            "Iteration 48, loss = 1.05413761\n",
            "Iteration 49, loss = 1.04617550\n",
            "Iteration 50, loss = 1.06126753\n",
            "Iteration 51, loss = 1.05769142\n",
            "Iteration 52, loss = 1.04876679\n",
            "Iteration 53, loss = 1.03683290\n",
            "Iteration 54, loss = 1.03985214\n",
            "Iteration 55, loss = 1.04118462\n",
            "Iteration 56, loss = 1.05606714\n",
            "Iteration 57, loss = 1.06877236\n",
            "Iteration 58, loss = 1.06036373\n",
            "Iteration 59, loss = 1.03936577\n",
            "Iteration 60, loss = 1.03560495\n",
            "Iteration 61, loss = 1.05530808\n",
            "Iteration 62, loss = 1.04446314\n",
            "Iteration 63, loss = 1.09964538\n",
            "Iteration 64, loss = 1.05173613\n",
            "Iteration 65, loss = 1.04979230\n",
            "Iteration 66, loss = 1.04183355\n",
            "Iteration 67, loss = 1.02387865\n",
            "Iteration 68, loss = 1.01918475\n",
            "Iteration 69, loss = 1.02784017\n",
            "Iteration 70, loss = 1.04274732\n",
            "Iteration 71, loss = 1.05660759\n",
            "Iteration 72, loss = 1.05213341\n",
            "Iteration 73, loss = 1.05032531\n",
            "Iteration 74, loss = 1.02613042\n",
            "Iteration 75, loss = 1.01083815\n",
            "Iteration 76, loss = 1.01367598\n",
            "Iteration 77, loss = 1.03195894\n",
            "Iteration 78, loss = 1.02073059\n",
            "Iteration 79, loss = 1.01604307\n",
            "Iteration 80, loss = 1.01006355\n",
            "Iteration 81, loss = 0.99851199\n",
            "Iteration 82, loss = 1.00252882\n",
            "Iteration 83, loss = 0.99283883\n",
            "Iteration 84, loss = 0.99107828\n",
            "Iteration 85, loss = 1.01735103\n",
            "Iteration 86, loss = 1.01511778\n",
            "Iteration 87, loss = 1.00075873\n",
            "Iteration 88, loss = 0.99181505\n",
            "Iteration 89, loss = 0.99397142\n",
            "Iteration 90, loss = 1.02151214\n",
            "Iteration 91, loss = 1.03281103\n",
            "Iteration 92, loss = 1.01972553\n",
            "Iteration 93, loss = 1.01672987\n",
            "Iteration 94, loss = 1.01261295\n",
            "Iteration 95, loss = 0.99540962\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31502272\n",
            "Iteration 2, loss = 1.24325519\n",
            "Iteration 3, loss = 1.22232516\n",
            "Iteration 4, loss = 1.20682116\n",
            "Iteration 5, loss = 1.21083316\n",
            "Iteration 6, loss = 1.20785249\n",
            "Iteration 7, loss = 1.17450666\n",
            "Iteration 8, loss = 1.17534807\n",
            "Iteration 9, loss = 1.16677788\n",
            "Iteration 10, loss = 1.17446986\n",
            "Iteration 11, loss = 1.16801386\n",
            "Iteration 12, loss = 1.15337249\n",
            "Iteration 13, loss = 1.15466993\n",
            "Iteration 14, loss = 1.16106640\n",
            "Iteration 15, loss = 1.13846619\n",
            "Iteration 16, loss = 1.12885346\n",
            "Iteration 17, loss = 1.14272572\n",
            "Iteration 18, loss = 1.12476350\n",
            "Iteration 19, loss = 1.12729230\n",
            "Iteration 20, loss = 1.12554043\n",
            "Iteration 21, loss = 1.13550183\n",
            "Iteration 22, loss = 1.13296231\n",
            "Iteration 23, loss = 1.12894299\n",
            "Iteration 24, loss = 1.11631638\n",
            "Iteration 25, loss = 1.10728160\n",
            "Iteration 26, loss = 1.11985996\n",
            "Iteration 27, loss = 1.11097864\n",
            "Iteration 28, loss = 1.11798172\n",
            "Iteration 29, loss = 1.10848858\n",
            "Iteration 30, loss = 1.13030738\n",
            "Iteration 31, loss = 1.13135633\n",
            "Iteration 32, loss = 1.11259943\n",
            "Iteration 33, loss = 1.09418885\n",
            "Iteration 34, loss = 1.09578162\n",
            "Iteration 35, loss = 1.10285556\n",
            "Iteration 36, loss = 1.08992325\n",
            "Iteration 37, loss = 1.08393627\n",
            "Iteration 38, loss = 1.08789756\n",
            "Iteration 39, loss = 1.08275061\n",
            "Iteration 40, loss = 1.08873072\n",
            "Iteration 41, loss = 1.08696748\n",
            "Iteration 42, loss = 1.07533438\n",
            "Iteration 43, loss = 1.08246785\n",
            "Iteration 44, loss = 1.07189489\n",
            "Iteration 45, loss = 1.07611123\n",
            "Iteration 46, loss = 1.07990147\n",
            "Iteration 47, loss = 1.08044398\n",
            "Iteration 48, loss = 1.06008096\n",
            "Iteration 49, loss = 1.06204827\n",
            "Iteration 50, loss = 1.06404872\n",
            "Iteration 51, loss = 1.06787677\n",
            "Iteration 52, loss = 1.05831352\n",
            "Iteration 53, loss = 1.05307869\n",
            "Iteration 54, loss = 1.05909486\n",
            "Iteration 55, loss = 1.05522566\n",
            "Iteration 56, loss = 1.07096020\n",
            "Iteration 57, loss = 1.06231844\n",
            "Iteration 58, loss = 1.05060929\n",
            "Iteration 59, loss = 1.04157933\n",
            "Iteration 60, loss = 1.03634200\n",
            "Iteration 61, loss = 1.04592655\n",
            "Iteration 62, loss = 1.04213739\n",
            "Iteration 63, loss = 1.08294765\n",
            "Iteration 64, loss = 1.06651177\n",
            "Iteration 65, loss = 1.08457035\n",
            "Iteration 66, loss = 1.06516702\n",
            "Iteration 67, loss = 1.05912428\n",
            "Iteration 68, loss = 1.03062681\n",
            "Iteration 69, loss = 1.04260828\n",
            "Iteration 70, loss = 1.04543731\n",
            "Iteration 71, loss = 1.07352349\n",
            "Iteration 72, loss = 1.05169622\n",
            "Iteration 73, loss = 1.04122179\n",
            "Iteration 74, loss = 1.03665257\n",
            "Iteration 75, loss = 1.02986291\n",
            "Iteration 76, loss = 1.01744350\n",
            "Iteration 77, loss = 1.04671072\n",
            "Iteration 78, loss = 1.02341725\n",
            "Iteration 79, loss = 1.02437485\n",
            "Iteration 80, loss = 1.03828014\n",
            "Iteration 81, loss = 1.03217792\n",
            "Iteration 82, loss = 1.03811111\n",
            "Iteration 83, loss = 1.02375715\n",
            "Iteration 84, loss = 1.00563311\n",
            "Iteration 85, loss = 1.02377757\n",
            "Iteration 86, loss = 1.02054301\n",
            "Iteration 87, loss = 1.02376358\n",
            "Iteration 88, loss = 1.03720398\n",
            "Iteration 89, loss = 1.01793236\n",
            "Iteration 90, loss = 1.00710692\n",
            "Iteration 91, loss = 1.03136288\n",
            "Iteration 92, loss = 1.01272253\n",
            "Iteration 93, loss = 1.00717409\n",
            "Iteration 94, loss = 1.02107949\n",
            "Iteration 95, loss = 0.99539292\n",
            "Iteration 96, loss = 1.00356522\n",
            "Iteration 97, loss = 1.00485498\n",
            "Iteration 98, loss = 1.00383328\n",
            "Iteration 99, loss = 0.99712499\n",
            "Iteration 100, loss = 0.98827258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31516384\n",
            "Iteration 2, loss = 1.24681253\n",
            "Iteration 3, loss = 1.22362578\n",
            "Iteration 4, loss = 1.20993149\n",
            "Iteration 5, loss = 1.22797084\n",
            "Iteration 6, loss = 1.21486060\n",
            "Iteration 7, loss = 1.18586969\n",
            "Iteration 8, loss = 1.17592510\n",
            "Iteration 9, loss = 1.18337534\n",
            "Iteration 10, loss = 1.18192556\n",
            "Iteration 11, loss = 1.17215344\n",
            "Iteration 12, loss = 1.15610373\n",
            "Iteration 13, loss = 1.16012331\n",
            "Iteration 14, loss = 1.16729924\n",
            "Iteration 15, loss = 1.14298446\n",
            "Iteration 16, loss = 1.13253602\n",
            "Iteration 17, loss = 1.14270716\n",
            "Iteration 18, loss = 1.13041644\n",
            "Iteration 19, loss = 1.12569555\n",
            "Iteration 20, loss = 1.14507028\n",
            "Iteration 21, loss = 1.14642910\n",
            "Iteration 22, loss = 1.13633438\n",
            "Iteration 23, loss = 1.12622895\n",
            "Iteration 24, loss = 1.11099226\n",
            "Iteration 25, loss = 1.11372253\n",
            "Iteration 26, loss = 1.10356570\n",
            "Iteration 27, loss = 1.10639744\n",
            "Iteration 28, loss = 1.13195586\n",
            "Iteration 29, loss = 1.13049937\n",
            "Iteration 30, loss = 1.12605232\n",
            "Iteration 31, loss = 1.13917217\n",
            "Iteration 32, loss = 1.12060773\n",
            "Iteration 33, loss = 1.10157476\n",
            "Iteration 34, loss = 1.10158081\n",
            "Iteration 35, loss = 1.09851813\n",
            "Iteration 36, loss = 1.08683523\n",
            "Iteration 37, loss = 1.08770285\n",
            "Iteration 38, loss = 1.07914327\n",
            "Iteration 39, loss = 1.07690614\n",
            "Iteration 40, loss = 1.09420770\n",
            "Iteration 41, loss = 1.08692100\n",
            "Iteration 42, loss = 1.07816887\n",
            "Iteration 43, loss = 1.08980448\n",
            "Iteration 44, loss = 1.07505048\n",
            "Iteration 45, loss = 1.07424119\n",
            "Iteration 46, loss = 1.07896287\n",
            "Iteration 47, loss = 1.07633221\n",
            "Iteration 48, loss = 1.05489157\n",
            "Iteration 49, loss = 1.05859894\n",
            "Iteration 50, loss = 1.06698867\n",
            "Iteration 51, loss = 1.04800399\n",
            "Iteration 52, loss = 1.04634189\n",
            "Iteration 53, loss = 1.05770222\n",
            "Iteration 54, loss = 1.05004333\n",
            "Iteration 55, loss = 1.05418575\n",
            "Iteration 56, loss = 1.06772116\n",
            "Iteration 57, loss = 1.06639086\n",
            "Iteration 58, loss = 1.03407417\n",
            "Iteration 59, loss = 1.03401949\n",
            "Iteration 60, loss = 1.02856906\n",
            "Iteration 61, loss = 1.03496705\n",
            "Iteration 62, loss = 1.02656477\n",
            "Iteration 63, loss = 1.05966383\n",
            "Iteration 64, loss = 1.04319655\n",
            "Iteration 65, loss = 1.03886300\n",
            "Iteration 66, loss = 1.03693304\n",
            "Iteration 67, loss = 1.02819470\n",
            "Iteration 68, loss = 1.02239865\n",
            "Iteration 69, loss = 1.01256363\n",
            "Iteration 70, loss = 1.04521329\n",
            "Iteration 71, loss = 1.02827617\n",
            "Iteration 72, loss = 1.01430205\n",
            "Iteration 73, loss = 1.01816517\n",
            "Iteration 74, loss = 1.02931776\n",
            "Iteration 75, loss = 1.00003603\n",
            "Iteration 76, loss = 0.99877922\n",
            "Iteration 77, loss = 0.99957352\n",
            "Iteration 78, loss = 0.99470981\n",
            "Iteration 79, loss = 0.99539657\n",
            "Iteration 80, loss = 1.02248658\n",
            "Iteration 81, loss = 1.01203655\n",
            "Iteration 82, loss = 1.02049528\n",
            "Iteration 83, loss = 1.01304358\n",
            "Iteration 84, loss = 1.00852642\n",
            "Iteration 85, loss = 0.98732007\n",
            "Iteration 86, loss = 0.98176211\n",
            "Iteration 87, loss = 0.99188376\n",
            "Iteration 88, loss = 1.00262128\n",
            "Iteration 89, loss = 0.99280508\n",
            "Iteration 90, loss = 1.00764550\n",
            "Iteration 91, loss = 0.98838781\n",
            "Iteration 92, loss = 0.99568274\n",
            "Iteration 93, loss = 0.99168515\n",
            "Iteration 94, loss = 0.98941373\n",
            "Iteration 95, loss = 0.98632882\n",
            "Iteration 96, loss = 0.98267686\n",
            "Iteration 97, loss = 0.97757880\n",
            "Iteration 98, loss = 0.98333987\n",
            "Iteration 99, loss = 0.97019421\n",
            "Iteration 100, loss = 0.96325235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31532516\n",
            "Iteration 2, loss = 1.24435968\n",
            "Iteration 3, loss = 1.21977481\n",
            "Iteration 4, loss = 1.21085164\n",
            "Iteration 5, loss = 1.23768129\n",
            "Iteration 6, loss = 1.19054017\n",
            "Iteration 7, loss = 1.18861257\n",
            "Iteration 8, loss = 1.16941269\n",
            "Iteration 9, loss = 1.17929842\n",
            "Iteration 10, loss = 1.17019963\n",
            "Iteration 11, loss = 1.19852231\n",
            "Iteration 12, loss = 1.16819057\n",
            "Iteration 13, loss = 1.16583831\n",
            "Iteration 14, loss = 1.16212935\n",
            "Iteration 15, loss = 1.14397148\n",
            "Iteration 16, loss = 1.13098812\n",
            "Iteration 17, loss = 1.12658842\n",
            "Iteration 18, loss = 1.12175454\n",
            "Iteration 19, loss = 1.12036115\n",
            "Iteration 20, loss = 1.13632368\n",
            "Iteration 21, loss = 1.13892679\n",
            "Iteration 22, loss = 1.12829373\n",
            "Iteration 23, loss = 1.11671780\n",
            "Iteration 24, loss = 1.11572318\n",
            "Iteration 25, loss = 1.10380184\n",
            "Iteration 26, loss = 1.10620963\n",
            "Iteration 27, loss = 1.10090193\n",
            "Iteration 28, loss = 1.11072489\n",
            "Iteration 29, loss = 1.09692308\n",
            "Iteration 30, loss = 1.10732802\n",
            "Iteration 31, loss = 1.11260300\n",
            "Iteration 32, loss = 1.11268590\n",
            "Iteration 33, loss = 1.09028052\n",
            "Iteration 34, loss = 1.08867908\n",
            "Iteration 35, loss = 1.09836132\n",
            "Iteration 36, loss = 1.08572370\n",
            "Iteration 37, loss = 1.08412349\n",
            "Iteration 38, loss = 1.06776788\n",
            "Iteration 39, loss = 1.07213209\n",
            "Iteration 40, loss = 1.07886321\n",
            "Iteration 41, loss = 1.07431414\n",
            "Iteration 42, loss = 1.08500670\n",
            "Iteration 43, loss = 1.07804237\n",
            "Iteration 44, loss = 1.07894034\n",
            "Iteration 45, loss = 1.07332388\n",
            "Iteration 46, loss = 1.07131601\n",
            "Iteration 47, loss = 1.08477795\n",
            "Iteration 48, loss = 1.06706377\n",
            "Iteration 49, loss = 1.07033202\n",
            "Iteration 50, loss = 1.08152969\n",
            "Iteration 51, loss = 1.05712522\n",
            "Iteration 52, loss = 1.05146172\n",
            "Iteration 53, loss = 1.06996765\n",
            "Iteration 54, loss = 1.04945312\n",
            "Iteration 55, loss = 1.04607716\n",
            "Iteration 56, loss = 1.07901723\n",
            "Iteration 57, loss = 1.06515377\n",
            "Iteration 58, loss = 1.04833448\n",
            "Iteration 59, loss = 1.06813316\n",
            "Iteration 60, loss = 1.04590534\n",
            "Iteration 61, loss = 1.04956959\n",
            "Iteration 62, loss = 1.04851160\n",
            "Iteration 63, loss = 1.07278684\n",
            "Iteration 64, loss = 1.04292572\n",
            "Iteration 65, loss = 1.04715835\n",
            "Iteration 66, loss = 1.03664183\n",
            "Iteration 67, loss = 1.02717024\n",
            "Iteration 68, loss = 1.02811968\n",
            "Iteration 69, loss = 1.02973325\n",
            "Iteration 70, loss = 1.02792959\n",
            "Iteration 71, loss = 1.03002198\n",
            "Iteration 72, loss = 1.01937667\n",
            "Iteration 73, loss = 1.02544865\n",
            "Iteration 74, loss = 1.02421779\n",
            "Iteration 75, loss = 1.01405420\n",
            "Iteration 76, loss = 1.00929395\n",
            "Iteration 77, loss = 1.00695970\n",
            "Iteration 78, loss = 1.00576962\n",
            "Iteration 79, loss = 1.00252538\n",
            "Iteration 80, loss = 1.01322746\n",
            "Iteration 81, loss = 1.00993140\n",
            "Iteration 82, loss = 1.03682208\n",
            "Iteration 83, loss = 1.05673115\n",
            "Iteration 84, loss = 1.02870987\n",
            "Iteration 85, loss = 1.00579613\n",
            "Iteration 86, loss = 0.99747972\n",
            "Iteration 87, loss = 1.00464394\n",
            "Iteration 88, loss = 1.00870287\n",
            "Iteration 89, loss = 1.00281343\n",
            "Iteration 90, loss = 1.02297458\n",
            "Iteration 91, loss = 1.02021978\n",
            "Iteration 92, loss = 1.02032475\n",
            "Iteration 93, loss = 1.00706050\n",
            "Iteration 94, loss = 0.99495257\n",
            "Iteration 95, loss = 1.00517780\n",
            "Iteration 96, loss = 0.99598108\n",
            "Iteration 97, loss = 0.98095663\n",
            "Iteration 98, loss = 0.99168398\n",
            "Iteration 99, loss = 1.00206486\n",
            "Iteration 100, loss = 0.97286752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30912235\n",
            "Iteration 2, loss = 1.23204737\n",
            "Iteration 3, loss = 1.20967422\n",
            "Iteration 4, loss = 1.19770983\n",
            "Iteration 5, loss = 1.21951807\n",
            "Iteration 6, loss = 1.17728841\n",
            "Iteration 7, loss = 1.17286093\n",
            "Iteration 8, loss = 1.15669198\n",
            "Iteration 9, loss = 1.15811289\n",
            "Iteration 10, loss = 1.14524956\n",
            "Iteration 11, loss = 1.18538872\n",
            "Iteration 12, loss = 1.15042914\n",
            "Iteration 13, loss = 1.16548371\n",
            "Iteration 14, loss = 1.15990208\n",
            "Iteration 15, loss = 1.13659163\n",
            "Iteration 16, loss = 1.13336747\n",
            "Iteration 17, loss = 1.12372211\n",
            "Iteration 18, loss = 1.12160335\n",
            "Iteration 19, loss = 1.11666669\n",
            "Iteration 20, loss = 1.12394397\n",
            "Iteration 21, loss = 1.12498130\n",
            "Iteration 22, loss = 1.11954081\n",
            "Iteration 23, loss = 1.11183396\n",
            "Iteration 24, loss = 1.11236187\n",
            "Iteration 25, loss = 1.10097683\n",
            "Iteration 26, loss = 1.09312584\n",
            "Iteration 27, loss = 1.09557337\n",
            "Iteration 28, loss = 1.11224951\n",
            "Iteration 29, loss = 1.09585570\n",
            "Iteration 30, loss = 1.09014842\n",
            "Iteration 31, loss = 1.10995664\n",
            "Iteration 32, loss = 1.11179707\n",
            "Iteration 33, loss = 1.10828622\n",
            "Iteration 34, loss = 1.09360382\n",
            "Iteration 35, loss = 1.10297493\n",
            "Iteration 36, loss = 1.09456659\n",
            "Iteration 37, loss = 1.09983936\n",
            "Iteration 38, loss = 1.07720179\n",
            "Iteration 39, loss = 1.07695932\n",
            "Iteration 40, loss = 1.08602574\n",
            "Iteration 41, loss = 1.06452846\n",
            "Iteration 42, loss = 1.05947785\n",
            "Iteration 43, loss = 1.06964443\n",
            "Iteration 44, loss = 1.05745579\n",
            "Iteration 45, loss = 1.06438463\n",
            "Iteration 46, loss = 1.06483219\n",
            "Iteration 47, loss = 1.05869063\n",
            "Iteration 48, loss = 1.05036844\n",
            "Iteration 49, loss = 1.05198305\n",
            "Iteration 50, loss = 1.06233330\n",
            "Iteration 51, loss = 1.04101347\n",
            "Iteration 52, loss = 1.03195297\n",
            "Iteration 53, loss = 1.07339526\n",
            "Iteration 54, loss = 1.05359909\n",
            "Iteration 55, loss = 1.05491533\n",
            "Iteration 56, loss = 1.04869353\n",
            "Iteration 57, loss = 1.04185827\n",
            "Iteration 58, loss = 1.02881147\n",
            "Iteration 59, loss = 1.09180355\n",
            "Iteration 60, loss = 1.05390515\n",
            "Iteration 61, loss = 1.04705975\n",
            "Iteration 62, loss = 1.05948119\n",
            "Iteration 63, loss = 1.06329152\n",
            "Iteration 64, loss = 1.04230023\n",
            "Iteration 65, loss = 1.03610003\n",
            "Iteration 66, loss = 1.02086727\n",
            "Iteration 67, loss = 1.00505198\n",
            "Iteration 68, loss = 1.00902402\n",
            "Iteration 69, loss = 1.01084101\n",
            "Iteration 70, loss = 1.02012075\n",
            "Iteration 71, loss = 1.01862850\n",
            "Iteration 72, loss = 1.00597519\n",
            "Iteration 73, loss = 1.00299873\n",
            "Iteration 74, loss = 1.00786958\n",
            "Iteration 75, loss = 1.00536865\n",
            "Iteration 76, loss = 1.00460344\n",
            "Iteration 77, loss = 0.99564524\n",
            "Iteration 78, loss = 1.00631208\n",
            "Iteration 79, loss = 0.98590545\n",
            "Iteration 80, loss = 0.98140559\n",
            "Iteration 81, loss = 1.00447027\n",
            "Iteration 82, loss = 1.01256607\n",
            "Iteration 83, loss = 1.03368507\n",
            "Iteration 84, loss = 1.00700788\n",
            "Iteration 85, loss = 1.00917450\n",
            "Iteration 86, loss = 0.98845757\n",
            "Iteration 87, loss = 1.00608012\n",
            "Iteration 88, loss = 0.99504506\n",
            "Iteration 89, loss = 0.99163833\n",
            "Iteration 90, loss = 0.98618146\n",
            "Iteration 91, loss = 1.01877925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31997025\n",
            "Iteration 2, loss = 1.24163524\n",
            "Iteration 3, loss = 1.21850917\n",
            "Iteration 4, loss = 1.20533654\n",
            "Iteration 5, loss = 1.20933924\n",
            "Iteration 6, loss = 1.18270069\n",
            "Iteration 7, loss = 1.17336711\n",
            "Iteration 8, loss = 1.15911679\n",
            "Iteration 9, loss = 1.15943095\n",
            "Iteration 10, loss = 1.15374409\n",
            "Iteration 11, loss = 1.18115865\n",
            "Iteration 12, loss = 1.15246550\n",
            "Iteration 13, loss = 1.13805398\n",
            "Iteration 14, loss = 1.13073324\n",
            "Iteration 15, loss = 1.12135759\n",
            "Iteration 16, loss = 1.13880229\n",
            "Iteration 17, loss = 1.12918124\n",
            "Iteration 18, loss = 1.12574507\n",
            "Iteration 19, loss = 1.11533531\n",
            "Iteration 20, loss = 1.11620119\n",
            "Iteration 21, loss = 1.13919255\n",
            "Iteration 22, loss = 1.15062468\n",
            "Iteration 23, loss = 1.12216662\n",
            "Iteration 24, loss = 1.11312317\n",
            "Iteration 25, loss = 1.10642629\n",
            "Iteration 26, loss = 1.09882726\n",
            "Iteration 27, loss = 1.09795397\n",
            "Iteration 28, loss = 1.10424755\n",
            "Iteration 29, loss = 1.09211520\n",
            "Iteration 30, loss = 1.09201746\n",
            "Iteration 31, loss = 1.10918647\n",
            "Iteration 32, loss = 1.10784768\n",
            "Iteration 33, loss = 1.11427368\n",
            "Iteration 34, loss = 1.10597907\n",
            "Iteration 35, loss = 1.12318767\n",
            "Iteration 36, loss = 1.10829943\n",
            "Iteration 37, loss = 1.10227344\n",
            "Iteration 38, loss = 1.07884620\n",
            "Iteration 39, loss = 1.07046106\n",
            "Iteration 40, loss = 1.07086179\n",
            "Iteration 41, loss = 1.06682370\n",
            "Iteration 42, loss = 1.08070159\n",
            "Iteration 43, loss = 1.07107419\n",
            "Iteration 44, loss = 1.06578852\n",
            "Iteration 45, loss = 1.07126268\n",
            "Iteration 46, loss = 1.06307661\n",
            "Iteration 47, loss = 1.06117022\n",
            "Iteration 48, loss = 1.05183464\n",
            "Iteration 49, loss = 1.05727890\n",
            "Iteration 50, loss = 1.06045991\n",
            "Iteration 51, loss = 1.04632895\n",
            "Iteration 52, loss = 1.04236208\n",
            "Iteration 53, loss = 1.09984812\n",
            "Iteration 54, loss = 1.07350994\n",
            "Iteration 55, loss = 1.05306721\n",
            "Iteration 56, loss = 1.06108641\n",
            "Iteration 57, loss = 1.04034441\n",
            "Iteration 58, loss = 1.02941760\n",
            "Iteration 59, loss = 1.06445918\n",
            "Iteration 60, loss = 1.04250820\n",
            "Iteration 61, loss = 1.02445837\n",
            "Iteration 62, loss = 1.03074614\n",
            "Iteration 63, loss = 1.03472966\n",
            "Iteration 64, loss = 1.02461753\n",
            "Iteration 65, loss = 1.02229863\n",
            "Iteration 66, loss = 1.02748294\n",
            "Iteration 67, loss = 1.01057913\n",
            "Iteration 68, loss = 1.00929881\n",
            "Iteration 69, loss = 1.02768301\n",
            "Iteration 70, loss = 1.00496622\n",
            "Iteration 71, loss = 1.02337480\n",
            "Iteration 72, loss = 1.00918603\n",
            "Iteration 73, loss = 1.00250546\n",
            "Iteration 74, loss = 1.00460569\n",
            "Iteration 75, loss = 1.00424945\n",
            "Iteration 76, loss = 0.99793422\n",
            "Iteration 77, loss = 0.99556035\n",
            "Iteration 78, loss = 1.02127358\n",
            "Iteration 79, loss = 0.99069585\n",
            "Iteration 80, loss = 0.98228875\n",
            "Iteration 81, loss = 0.98867860\n",
            "Iteration 82, loss = 0.99445949\n",
            "Iteration 83, loss = 1.03748380\n",
            "Iteration 84, loss = 0.99825130\n",
            "Iteration 85, loss = 0.99143088\n",
            "Iteration 86, loss = 1.00092508\n",
            "Iteration 87, loss = 1.00127224\n",
            "Iteration 88, loss = 0.96738032\n",
            "Iteration 89, loss = 0.96733156\n",
            "Iteration 90, loss = 0.96111172\n",
            "Iteration 91, loss = 0.96061873\n",
            "Iteration 92, loss = 0.96293495\n",
            "Iteration 93, loss = 0.95647473\n",
            "Iteration 94, loss = 0.96957225\n",
            "Iteration 95, loss = 0.95836134\n",
            "Iteration 96, loss = 0.97396259\n",
            "Iteration 97, loss = 1.00216473\n",
            "Iteration 98, loss = 1.03026782\n",
            "Iteration 99, loss = 1.00349534\n",
            "Iteration 100, loss = 0.95713783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32014949\n",
            "Iteration 2, loss = 1.24120533\n",
            "Iteration 3, loss = 1.23955792\n",
            "Iteration 4, loss = 1.20770580\n",
            "Iteration 5, loss = 1.19315359\n",
            "Iteration 6, loss = 1.18015263\n",
            "Iteration 7, loss = 1.17348176\n",
            "Iteration 8, loss = 1.17047425\n",
            "Iteration 9, loss = 1.15892816\n",
            "Iteration 10, loss = 1.15273655\n",
            "Iteration 11, loss = 1.15421768\n",
            "Iteration 12, loss = 1.14754419\n",
            "Iteration 13, loss = 1.14446157\n",
            "Iteration 14, loss = 1.14277586\n",
            "Iteration 15, loss = 1.13176264\n",
            "Iteration 16, loss = 1.13589010\n",
            "Iteration 17, loss = 1.13437513\n",
            "Iteration 18, loss = 1.13083545\n",
            "Iteration 19, loss = 1.13060502\n",
            "Iteration 20, loss = 1.11249063\n",
            "Iteration 21, loss = 1.12267369\n",
            "Iteration 22, loss = 1.13242336\n",
            "Iteration 23, loss = 1.14212267\n",
            "Iteration 24, loss = 1.13987569\n",
            "Iteration 25, loss = 1.13426760\n",
            "Iteration 26, loss = 1.11314107\n",
            "Iteration 27, loss = 1.09768153\n",
            "Iteration 28, loss = 1.10830139\n",
            "Iteration 29, loss = 1.09898226\n",
            "Iteration 30, loss = 1.11664486\n",
            "Iteration 31, loss = 1.11228993\n",
            "Iteration 32, loss = 1.09222846\n",
            "Iteration 33, loss = 1.08989350\n",
            "Iteration 34, loss = 1.08527914\n",
            "Iteration 35, loss = 1.09056977\n",
            "Iteration 36, loss = 1.09426105\n",
            "Iteration 37, loss = 1.09556735\n",
            "Iteration 38, loss = 1.09045383\n",
            "Iteration 39, loss = 1.08681025\n",
            "Iteration 40, loss = 1.07908113\n",
            "Iteration 41, loss = 1.07763745\n",
            "Iteration 42, loss = 1.10111871\n",
            "Iteration 43, loss = 1.07161770\n",
            "Iteration 44, loss = 1.07597395\n",
            "Iteration 45, loss = 1.07292870\n",
            "Iteration 46, loss = 1.07124132\n",
            "Iteration 47, loss = 1.05747938\n",
            "Iteration 48, loss = 1.05925038\n",
            "Iteration 49, loss = 1.06848924\n",
            "Iteration 50, loss = 1.07477587\n",
            "Iteration 51, loss = 1.06035530\n",
            "Iteration 52, loss = 1.04865287\n",
            "Iteration 53, loss = 1.05419045\n",
            "Iteration 54, loss = 1.04597921\n",
            "Iteration 55, loss = 1.04427400\n",
            "Iteration 56, loss = 1.04775010\n",
            "Iteration 57, loss = 1.04495846\n",
            "Iteration 58, loss = 1.06326866\n",
            "Iteration 59, loss = 1.06808434\n",
            "Iteration 60, loss = 1.06351251\n",
            "Iteration 61, loss = 1.05175903\n",
            "Iteration 62, loss = 1.03578258\n",
            "Iteration 63, loss = 1.05653595\n",
            "Iteration 64, loss = 1.06128134\n",
            "Iteration 65, loss = 1.05075301\n",
            "Iteration 66, loss = 1.03823018\n",
            "Iteration 67, loss = 1.02430641\n",
            "Iteration 68, loss = 1.02901043\n",
            "Iteration 69, loss = 1.01613121\n",
            "Iteration 70, loss = 1.03068209\n",
            "Iteration 71, loss = 1.03439890\n",
            "Iteration 72, loss = 1.01626595\n",
            "Iteration 73, loss = 1.00939517\n",
            "Iteration 74, loss = 1.01839643\n",
            "Iteration 75, loss = 1.02279537\n",
            "Iteration 76, loss = 1.02033753\n",
            "Iteration 77, loss = 1.00802707\n",
            "Iteration 78, loss = 0.99932436\n",
            "Iteration 79, loss = 1.01497528\n",
            "Iteration 80, loss = 1.03129158\n",
            "Iteration 81, loss = 1.02614180\n",
            "Iteration 82, loss = 1.03352129\n",
            "Iteration 83, loss = 1.00053906\n",
            "Iteration 84, loss = 1.00096198\n",
            "Iteration 85, loss = 1.01108659\n",
            "Iteration 86, loss = 0.99809569\n",
            "Iteration 87, loss = 0.98584832\n",
            "Iteration 88, loss = 1.00149555\n",
            "Iteration 89, loss = 0.99595761\n",
            "Iteration 90, loss = 0.97983086\n",
            "Iteration 91, loss = 1.00835068\n",
            "Iteration 92, loss = 0.98849238\n",
            "Iteration 93, loss = 0.97290799\n",
            "Iteration 94, loss = 0.97839170\n",
            "Iteration 95, loss = 0.98033828\n",
            "Iteration 96, loss = 0.99815394\n",
            "Iteration 97, loss = 0.96613066\n",
            "Iteration 98, loss = 0.97458276\n",
            "Iteration 99, loss = 0.98858766\n",
            "Iteration 100, loss = 0.98049645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31571382\n",
            "Iteration 2, loss = 1.22577196\n",
            "Iteration 3, loss = 1.22410553\n",
            "Iteration 4, loss = 1.19134667\n",
            "Iteration 5, loss = 1.18100821\n",
            "Iteration 6, loss = 1.16623135\n",
            "Iteration 7, loss = 1.16508806\n",
            "Iteration 8, loss = 1.16454287\n",
            "Iteration 9, loss = 1.14994347\n",
            "Iteration 10, loss = 1.15328900\n",
            "Iteration 11, loss = 1.14714971\n",
            "Iteration 12, loss = 1.13403875\n",
            "Iteration 13, loss = 1.12714116\n",
            "Iteration 14, loss = 1.13608944\n",
            "Iteration 15, loss = 1.12623818\n",
            "Iteration 16, loss = 1.11871659\n",
            "Iteration 17, loss = 1.12672562\n",
            "Iteration 18, loss = 1.11537533\n",
            "Iteration 19, loss = 1.10517914\n",
            "Iteration 20, loss = 1.10060404\n",
            "Iteration 21, loss = 1.12899995\n",
            "Iteration 22, loss = 1.12718107\n",
            "Iteration 23, loss = 1.12017473\n",
            "Iteration 24, loss = 1.11424722\n",
            "Iteration 25, loss = 1.10169214\n",
            "Iteration 26, loss = 1.09288278\n",
            "Iteration 27, loss = 1.08322980\n",
            "Iteration 28, loss = 1.09424717\n",
            "Iteration 29, loss = 1.07536768\n",
            "Iteration 30, loss = 1.08286985\n",
            "Iteration 31, loss = 1.08358731\n",
            "Iteration 32, loss = 1.07628835\n",
            "Iteration 33, loss = 1.07803762\n",
            "Iteration 34, loss = 1.06538208\n",
            "Iteration 35, loss = 1.07318470\n",
            "Iteration 36, loss = 1.08352948\n",
            "Iteration 37, loss = 1.07623436\n",
            "Iteration 38, loss = 1.07688660\n",
            "Iteration 39, loss = 1.06383846\n",
            "Iteration 40, loss = 1.06766690\n",
            "Iteration 41, loss = 1.05848271\n",
            "Iteration 42, loss = 1.07904081\n",
            "Iteration 43, loss = 1.06165222\n",
            "Iteration 44, loss = 1.08648850\n",
            "Iteration 45, loss = 1.05634368\n",
            "Iteration 46, loss = 1.06795084\n",
            "Iteration 47, loss = 1.04246431\n",
            "Iteration 48, loss = 1.04770674\n",
            "Iteration 49, loss = 1.04576775\n",
            "Iteration 50, loss = 1.04792850\n",
            "Iteration 51, loss = 1.03393369\n",
            "Iteration 52, loss = 1.03230287\n",
            "Iteration 53, loss = 1.03348471\n",
            "Iteration 54, loss = 1.02820008\n",
            "Iteration 55, loss = 1.02400283\n",
            "Iteration 56, loss = 1.02674392\n",
            "Iteration 57, loss = 1.03781218\n",
            "Iteration 58, loss = 1.04173657\n",
            "Iteration 59, loss = 1.04351405\n",
            "Iteration 60, loss = 1.05274227\n",
            "Iteration 61, loss = 1.03068202\n",
            "Iteration 62, loss = 1.01181156\n",
            "Iteration 63, loss = 1.03383828\n",
            "Iteration 64, loss = 1.06733175\n",
            "Iteration 65, loss = 1.05304607\n",
            "Iteration 66, loss = 1.03452958\n",
            "Iteration 67, loss = 1.02293704\n",
            "Iteration 68, loss = 1.01884300\n",
            "Iteration 69, loss = 1.01098854\n",
            "Iteration 70, loss = 1.01705067\n",
            "Iteration 71, loss = 1.01398006\n",
            "Iteration 72, loss = 1.00180237\n",
            "Iteration 73, loss = 1.00844128\n",
            "Iteration 74, loss = 1.00258837\n",
            "Iteration 75, loss = 1.00030086\n",
            "Iteration 76, loss = 1.00669687\n",
            "Iteration 77, loss = 1.01828211\n",
            "Iteration 78, loss = 1.00738067\n",
            "Iteration 79, loss = 0.99536823\n",
            "Iteration 80, loss = 1.00024416\n",
            "Iteration 81, loss = 1.05196403\n",
            "Iteration 82, loss = 1.06337191\n",
            "Iteration 83, loss = 1.02756749\n",
            "Iteration 84, loss = 1.03086994\n",
            "Iteration 85, loss = 1.02061304\n",
            "Iteration 86, loss = 1.00698484\n",
            "Iteration 87, loss = 0.98856358\n",
            "Iteration 88, loss = 0.99148504\n",
            "Iteration 89, loss = 0.98905463\n",
            "Iteration 90, loss = 0.99076628\n",
            "Iteration 91, loss = 1.01020967\n",
            "Iteration 92, loss = 0.98360791\n",
            "Iteration 93, loss = 0.97216825\n",
            "Iteration 94, loss = 0.96569287\n",
            "Iteration 95, loss = 0.96790487\n",
            "Iteration 96, loss = 0.97055480\n",
            "Iteration 97, loss = 0.96627909\n",
            "Iteration 98, loss = 0.97555305\n",
            "Iteration 99, loss = 0.97326690\n",
            "Iteration 100, loss = 0.96355226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30332700\n",
            "Iteration 2, loss = 1.21523139\n",
            "Iteration 3, loss = 1.21599742\n",
            "Iteration 4, loss = 1.18861174\n",
            "Iteration 5, loss = 1.18136948\n",
            "Iteration 6, loss = 1.16429334\n",
            "Iteration 7, loss = 1.16590725\n",
            "Iteration 8, loss = 1.15598053\n",
            "Iteration 9, loss = 1.14345501\n",
            "Iteration 10, loss = 1.14399145\n",
            "Iteration 11, loss = 1.14299728\n",
            "Iteration 12, loss = 1.13719626\n",
            "Iteration 13, loss = 1.13711243\n",
            "Iteration 14, loss = 1.14379896\n",
            "Iteration 15, loss = 1.12375324\n",
            "Iteration 16, loss = 1.11725273\n",
            "Iteration 17, loss = 1.12568023\n",
            "Iteration 18, loss = 1.11964265\n",
            "Iteration 19, loss = 1.11547269\n",
            "Iteration 20, loss = 1.10457703\n",
            "Iteration 21, loss = 1.12488078\n",
            "Iteration 22, loss = 1.11862191\n",
            "Iteration 23, loss = 1.12702584\n",
            "Iteration 24, loss = 1.12256646\n",
            "Iteration 25, loss = 1.11332248\n",
            "Iteration 26, loss = 1.09999978\n",
            "Iteration 27, loss = 1.09591000\n",
            "Iteration 28, loss = 1.09889947\n",
            "Iteration 29, loss = 1.08269249\n",
            "Iteration 30, loss = 1.09187874\n",
            "Iteration 31, loss = 1.08566023\n",
            "Iteration 32, loss = 1.07213374\n",
            "Iteration 33, loss = 1.08377340\n",
            "Iteration 34, loss = 1.07338918\n",
            "Iteration 35, loss = 1.08150785\n",
            "Iteration 36, loss = 1.10262569\n",
            "Iteration 37, loss = 1.07846099\n",
            "Iteration 38, loss = 1.07286985\n",
            "Iteration 39, loss = 1.06311927\n",
            "Iteration 40, loss = 1.06495193\n",
            "Iteration 41, loss = 1.07598177\n",
            "Iteration 42, loss = 1.06558327\n",
            "Iteration 43, loss = 1.05652169\n",
            "Iteration 44, loss = 1.07997175\n",
            "Iteration 45, loss = 1.04817337\n",
            "Iteration 46, loss = 1.05505210\n",
            "Iteration 47, loss = 1.04948589\n",
            "Iteration 48, loss = 1.04316369\n",
            "Iteration 49, loss = 1.05439576\n",
            "Iteration 50, loss = 1.05347798\n",
            "Iteration 51, loss = 1.04481971\n",
            "Iteration 52, loss = 1.03419866\n",
            "Iteration 53, loss = 1.03465564\n",
            "Iteration 54, loss = 1.03623369\n",
            "Iteration 55, loss = 1.03016080\n",
            "Iteration 56, loss = 1.03601926\n",
            "Iteration 57, loss = 1.03181301\n",
            "Iteration 58, loss = 1.03621769\n",
            "Iteration 59, loss = 1.03109101\n",
            "Iteration 60, loss = 1.03188745\n",
            "Iteration 61, loss = 1.01638239\n",
            "Iteration 62, loss = 1.01152635\n",
            "Iteration 63, loss = 1.03727346\n",
            "Iteration 64, loss = 1.06357106\n",
            "Iteration 65, loss = 1.03331158\n",
            "Iteration 66, loss = 1.03502942\n",
            "Iteration 67, loss = 1.02343891\n",
            "Iteration 68, loss = 1.00535302\n",
            "Iteration 69, loss = 1.00917156\n",
            "Iteration 70, loss = 1.00887417\n",
            "Iteration 71, loss = 0.99569241\n",
            "Iteration 72, loss = 1.00052503\n",
            "Iteration 73, loss = 0.99139959\n",
            "Iteration 74, loss = 1.02117685\n",
            "Iteration 75, loss = 1.03238783\n",
            "Iteration 76, loss = 1.00959376\n",
            "Iteration 77, loss = 1.00243967\n",
            "Iteration 78, loss = 0.99438620\n",
            "Iteration 79, loss = 0.99908259\n",
            "Iteration 80, loss = 0.98988935\n",
            "Iteration 81, loss = 1.01557677\n",
            "Iteration 82, loss = 1.04975269\n",
            "Iteration 83, loss = 0.98303591\n",
            "Iteration 84, loss = 0.98693551\n",
            "Iteration 85, loss = 0.97216466\n",
            "Iteration 86, loss = 0.97220774\n",
            "Iteration 87, loss = 0.97728244\n",
            "Iteration 88, loss = 0.97142961\n",
            "Iteration 89, loss = 0.97653134\n",
            "Iteration 90, loss = 0.96589540\n",
            "Iteration 91, loss = 0.98439601\n",
            "Iteration 92, loss = 0.97573200\n",
            "Iteration 93, loss = 0.98377678\n",
            "Iteration 94, loss = 0.95753399\n",
            "Iteration 95, loss = 0.95134398\n",
            "Iteration 96, loss = 0.95500979\n",
            "Iteration 97, loss = 0.94910080\n",
            "Iteration 98, loss = 0.95598267\n",
            "Iteration 99, loss = 0.95068000\n",
            "Iteration 100, loss = 0.94233126\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30573798\n",
            "Iteration 2, loss = 1.25714671\n",
            "Iteration 3, loss = 1.24041831\n",
            "Iteration 4, loss = 1.25640918\n",
            "Iteration 5, loss = 1.25979979\n",
            "Iteration 6, loss = 1.23722430\n",
            "Iteration 7, loss = 1.23623615\n",
            "Iteration 8, loss = 1.22941225\n",
            "Iteration 9, loss = 1.23238553\n",
            "Iteration 10, loss = 1.22873612\n",
            "Iteration 11, loss = 1.22826920\n",
            "Iteration 12, loss = 1.22832270\n",
            "Iteration 13, loss = 1.22299197\n",
            "Iteration 14, loss = 1.22130722\n",
            "Iteration 15, loss = 1.22220005\n",
            "Iteration 16, loss = 1.21888760\n",
            "Iteration 17, loss = 1.21323133\n",
            "Iteration 18, loss = 1.21910935\n",
            "Iteration 19, loss = 1.21760490\n",
            "Iteration 20, loss = 1.21394194\n",
            "Iteration 21, loss = 1.21213029\n",
            "Iteration 22, loss = 1.20773387\n",
            "Iteration 23, loss = 1.20453344\n",
            "Iteration 24, loss = 1.20850650\n",
            "Iteration 25, loss = 1.20510753\n",
            "Iteration 26, loss = 1.20514080\n",
            "Iteration 27, loss = 1.20365696\n",
            "Iteration 28, loss = 1.19425251\n",
            "Iteration 29, loss = 1.19318947\n",
            "Iteration 30, loss = 1.19425856\n",
            "Iteration 31, loss = 1.19090401\n",
            "Iteration 32, loss = 1.19171611\n",
            "Iteration 33, loss = 1.18773496\n",
            "Iteration 34, loss = 1.18338483\n",
            "Iteration 35, loss = 1.20049531\n",
            "Iteration 36, loss = 1.18870013\n",
            "Iteration 37, loss = 1.18073243\n",
            "Iteration 38, loss = 1.18176574\n",
            "Iteration 39, loss = 1.18035324\n",
            "Iteration 40, loss = 1.18627662\n",
            "Iteration 41, loss = 1.18364739\n",
            "Iteration 42, loss = 1.18200103\n",
            "Iteration 43, loss = 1.18057834\n",
            "Iteration 44, loss = 1.17362240\n",
            "Iteration 45, loss = 1.17743668\n",
            "Iteration 46, loss = 1.19218687\n",
            "Iteration 47, loss = 1.19365129\n",
            "Iteration 48, loss = 1.16183824\n",
            "Iteration 49, loss = 1.17162299\n",
            "Iteration 50, loss = 1.15227622\n",
            "Iteration 51, loss = 1.15811905\n",
            "Iteration 52, loss = 1.16347005\n",
            "Iteration 53, loss = 1.15948271\n",
            "Iteration 54, loss = 1.16175205\n",
            "Iteration 55, loss = 1.17147089\n",
            "Iteration 56, loss = 1.17953074\n",
            "Iteration 57, loss = 1.18930350\n",
            "Iteration 58, loss = 1.17845361\n",
            "Iteration 59, loss = 1.17380727\n",
            "Iteration 60, loss = 1.15189016\n",
            "Iteration 61, loss = 1.14504158\n",
            "Iteration 62, loss = 1.14161042\n",
            "Iteration 63, loss = 1.14474263\n",
            "Iteration 64, loss = 1.15480759\n",
            "Iteration 65, loss = 1.13530684\n",
            "Iteration 66, loss = 1.12975583\n",
            "Iteration 67, loss = 1.13439463\n",
            "Iteration 68, loss = 1.13383331\n",
            "Iteration 69, loss = 1.12522663\n",
            "Iteration 70, loss = 1.14527993\n",
            "Iteration 71, loss = 1.15048077\n",
            "Iteration 72, loss = 1.12885196\n",
            "Iteration 73, loss = 1.12878274\n",
            "Iteration 74, loss = 1.13519111\n",
            "Iteration 75, loss = 1.11601239\n",
            "Iteration 76, loss = 1.11436939\n",
            "Iteration 77, loss = 1.11184863\n",
            "Iteration 78, loss = 1.11515182\n",
            "Iteration 79, loss = 1.12128380\n",
            "Iteration 80, loss = 1.10816649\n",
            "Iteration 81, loss = 1.11841732\n",
            "Iteration 82, loss = 1.10345926\n",
            "Iteration 83, loss = 1.10874006\n",
            "Iteration 84, loss = 1.10152872\n",
            "Iteration 85, loss = 1.12807865\n",
            "Iteration 86, loss = 1.11894098\n",
            "Iteration 87, loss = 1.14027344\n",
            "Iteration 88, loss = 1.10868978\n",
            "Iteration 89, loss = 1.14632772\n",
            "Iteration 90, loss = 1.18205587\n",
            "Iteration 91, loss = 1.14047311\n",
            "Iteration 92, loss = 1.13921557\n",
            "Iteration 93, loss = 1.12563750\n",
            "Iteration 94, loss = 1.11464254\n",
            "Iteration 95, loss = 1.09880253\n",
            "Iteration 96, loss = 1.10666535\n",
            "Iteration 97, loss = 1.09691586\n",
            "Iteration 98, loss = 1.09779182\n",
            "Iteration 99, loss = 1.10740886\n",
            "Iteration 100, loss = 1.10412118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30378323\n",
            "Iteration 2, loss = 1.25476222\n",
            "Iteration 3, loss = 1.23924111\n",
            "Iteration 4, loss = 1.25042099\n",
            "Iteration 5, loss = 1.25784908\n",
            "Iteration 6, loss = 1.23631207\n",
            "Iteration 7, loss = 1.23744235\n",
            "Iteration 8, loss = 1.23237866\n",
            "Iteration 9, loss = 1.23139082\n",
            "Iteration 10, loss = 1.23007181\n",
            "Iteration 11, loss = 1.22839096\n",
            "Iteration 12, loss = 1.22198987\n",
            "Iteration 13, loss = 1.22791422\n",
            "Iteration 14, loss = 1.23002865\n",
            "Iteration 15, loss = 1.22275609\n",
            "Iteration 16, loss = 1.21504190\n",
            "Iteration 17, loss = 1.21642051\n",
            "Iteration 18, loss = 1.21473661\n",
            "Iteration 19, loss = 1.21402467\n",
            "Iteration 20, loss = 1.20806227\n",
            "Iteration 21, loss = 1.21325608\n",
            "Iteration 22, loss = 1.20710717\n",
            "Iteration 23, loss = 1.20634814\n",
            "Iteration 24, loss = 1.21338848\n",
            "Iteration 25, loss = 1.20418529\n",
            "Iteration 26, loss = 1.21225685\n",
            "Iteration 27, loss = 1.20086954\n",
            "Iteration 28, loss = 1.19855877\n",
            "Iteration 29, loss = 1.19257700\n",
            "Iteration 30, loss = 1.20021840\n",
            "Iteration 31, loss = 1.20534614\n",
            "Iteration 32, loss = 1.20169327\n",
            "Iteration 33, loss = 1.19063236\n",
            "Iteration 34, loss = 1.18715420\n",
            "Iteration 35, loss = 1.18581532\n",
            "Iteration 36, loss = 1.18288495\n",
            "Iteration 37, loss = 1.18534184\n",
            "Iteration 38, loss = 1.18619523\n",
            "Iteration 39, loss = 1.17928177\n",
            "Iteration 40, loss = 1.18064461\n",
            "Iteration 41, loss = 1.19544855\n",
            "Iteration 42, loss = 1.18735751\n",
            "Iteration 43, loss = 1.18291088\n",
            "Iteration 44, loss = 1.17823365\n",
            "Iteration 45, loss = 1.17669902\n",
            "Iteration 46, loss = 1.18883874\n",
            "Iteration 47, loss = 1.18643005\n",
            "Iteration 48, loss = 1.16496508\n",
            "Iteration 49, loss = 1.17289534\n",
            "Iteration 50, loss = 1.15528309\n",
            "Iteration 51, loss = 1.16275834\n",
            "Iteration 52, loss = 1.16378652\n",
            "Iteration 53, loss = 1.15037385\n",
            "Iteration 54, loss = 1.15571200\n",
            "Iteration 55, loss = 1.16506655\n",
            "Iteration 56, loss = 1.18241583\n",
            "Iteration 57, loss = 1.20695236\n",
            "Iteration 58, loss = 1.16820063\n",
            "Iteration 59, loss = 1.17681985\n",
            "Iteration 60, loss = 1.15606993\n",
            "Iteration 61, loss = 1.15672713\n",
            "Iteration 62, loss = 1.15308970\n",
            "Iteration 63, loss = 1.16513990\n",
            "Iteration 64, loss = 1.14322312\n",
            "Iteration 65, loss = 1.13910865\n",
            "Iteration 66, loss = 1.13306319\n",
            "Iteration 67, loss = 1.13146562\n",
            "Iteration 68, loss = 1.14698567\n",
            "Iteration 69, loss = 1.12579341\n",
            "Iteration 70, loss = 1.14607062\n",
            "Iteration 71, loss = 1.14541620\n",
            "Iteration 72, loss = 1.13916038\n",
            "Iteration 73, loss = 1.14989001\n",
            "Iteration 74, loss = 1.15191444\n",
            "Iteration 75, loss = 1.12677304\n",
            "Iteration 76, loss = 1.11106131\n",
            "Iteration 77, loss = 1.11884254\n",
            "Iteration 78, loss = 1.11582475\n",
            "Iteration 79, loss = 1.11137244\n",
            "Iteration 80, loss = 1.10939784\n",
            "Iteration 81, loss = 1.10945612\n",
            "Iteration 82, loss = 1.09803032\n",
            "Iteration 83, loss = 1.09843393\n",
            "Iteration 84, loss = 1.10100731\n",
            "Iteration 85, loss = 1.14210498\n",
            "Iteration 86, loss = 1.13354983\n",
            "Iteration 87, loss = 1.13885127\n",
            "Iteration 88, loss = 1.10981530\n",
            "Iteration 89, loss = 1.11498282\n",
            "Iteration 90, loss = 1.16962611\n",
            "Iteration 91, loss = 1.12059936\n",
            "Iteration 92, loss = 1.12983563\n",
            "Iteration 93, loss = 1.11334766\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31069962\n",
            "Iteration 2, loss = 1.25954872\n",
            "Iteration 3, loss = 1.24527222\n",
            "Iteration 4, loss = 1.24251483\n",
            "Iteration 5, loss = 1.25353112\n",
            "Iteration 6, loss = 1.24343280\n",
            "Iteration 7, loss = 1.23664279\n",
            "Iteration 8, loss = 1.23007510\n",
            "Iteration 9, loss = 1.23279179\n",
            "Iteration 10, loss = 1.23181725\n",
            "Iteration 11, loss = 1.24602510\n",
            "Iteration 12, loss = 1.23001913\n",
            "Iteration 13, loss = 1.22951375\n",
            "Iteration 14, loss = 1.22496336\n",
            "Iteration 15, loss = 1.22761809\n",
            "Iteration 16, loss = 1.21261957\n",
            "Iteration 17, loss = 1.21603840\n",
            "Iteration 18, loss = 1.21205226\n",
            "Iteration 19, loss = 1.21292908\n",
            "Iteration 20, loss = 1.21099258\n",
            "Iteration 21, loss = 1.22040434\n",
            "Iteration 22, loss = 1.20914465\n",
            "Iteration 23, loss = 1.20947568\n",
            "Iteration 24, loss = 1.20730216\n",
            "Iteration 25, loss = 1.20468977\n",
            "Iteration 26, loss = 1.20696161\n",
            "Iteration 27, loss = 1.19753294\n",
            "Iteration 28, loss = 1.20393095\n",
            "Iteration 29, loss = 1.20508440\n",
            "Iteration 30, loss = 1.20825331\n",
            "Iteration 31, loss = 1.20274451\n",
            "Iteration 32, loss = 1.20207319\n",
            "Iteration 33, loss = 1.20153108\n",
            "Iteration 34, loss = 1.19099324\n",
            "Iteration 35, loss = 1.18920046\n",
            "Iteration 36, loss = 1.18592065\n",
            "Iteration 37, loss = 1.18495924\n",
            "Iteration 38, loss = 1.18969498\n",
            "Iteration 39, loss = 1.17627717\n",
            "Iteration 40, loss = 1.18032768\n",
            "Iteration 41, loss = 1.18503633\n",
            "Iteration 42, loss = 1.17544578\n",
            "Iteration 43, loss = 1.18140448\n",
            "Iteration 44, loss = 1.17759457\n",
            "Iteration 45, loss = 1.17359181\n",
            "Iteration 46, loss = 1.18333807\n",
            "Iteration 47, loss = 1.20251399\n",
            "Iteration 48, loss = 1.18782524\n",
            "Iteration 49, loss = 1.17457229\n",
            "Iteration 50, loss = 1.16968612\n",
            "Iteration 51, loss = 1.16896976\n",
            "Iteration 52, loss = 1.17326180\n",
            "Iteration 53, loss = 1.16242851\n",
            "Iteration 54, loss = 1.16354552\n",
            "Iteration 55, loss = 1.16591352\n",
            "Iteration 56, loss = 1.17045258\n",
            "Iteration 57, loss = 1.17465309\n",
            "Iteration 58, loss = 1.16275866\n",
            "Iteration 59, loss = 1.17494579\n",
            "Iteration 60, loss = 1.15530609\n",
            "Iteration 61, loss = 1.15037208\n",
            "Iteration 62, loss = 1.15539526\n",
            "Iteration 63, loss = 1.16865491\n",
            "Iteration 64, loss = 1.16175023\n",
            "Iteration 65, loss = 1.15885037\n",
            "Iteration 66, loss = 1.14080187\n",
            "Iteration 67, loss = 1.14183815\n",
            "Iteration 68, loss = 1.13688210\n",
            "Iteration 69, loss = 1.13367497\n",
            "Iteration 70, loss = 1.14322814\n",
            "Iteration 71, loss = 1.15799367\n",
            "Iteration 72, loss = 1.13544464\n",
            "Iteration 73, loss = 1.12507406\n",
            "Iteration 74, loss = 1.13803705\n",
            "Iteration 75, loss = 1.12379533\n",
            "Iteration 76, loss = 1.11680331\n",
            "Iteration 77, loss = 1.13083182\n",
            "Iteration 78, loss = 1.12836660\n",
            "Iteration 79, loss = 1.11834023\n",
            "Iteration 80, loss = 1.11217702\n",
            "Iteration 81, loss = 1.10835871\n",
            "Iteration 82, loss = 1.10345975\n",
            "Iteration 83, loss = 1.10595916\n",
            "Iteration 84, loss = 1.09955282\n",
            "Iteration 85, loss = 1.13290892\n",
            "Iteration 86, loss = 1.11966504\n",
            "Iteration 87, loss = 1.12501897\n",
            "Iteration 88, loss = 1.10581520\n",
            "Iteration 89, loss = 1.11911707\n",
            "Iteration 90, loss = 1.16462194\n",
            "Iteration 91, loss = 1.14062025\n",
            "Iteration 92, loss = 1.15583443\n",
            "Iteration 93, loss = 1.12339959\n",
            "Iteration 94, loss = 1.12383043\n",
            "Iteration 95, loss = 1.10908091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31141741\n",
            "Iteration 2, loss = 1.25991522\n",
            "Iteration 3, loss = 1.24833390\n",
            "Iteration 4, loss = 1.24826788\n",
            "Iteration 5, loss = 1.26766375\n",
            "Iteration 6, loss = 1.25715061\n",
            "Iteration 7, loss = 1.24610888\n",
            "Iteration 8, loss = 1.24197824\n",
            "Iteration 9, loss = 1.24586903\n",
            "Iteration 10, loss = 1.23884142\n",
            "Iteration 11, loss = 1.24142302\n",
            "Iteration 12, loss = 1.23512720\n",
            "Iteration 13, loss = 1.23427147\n",
            "Iteration 14, loss = 1.23419733\n",
            "Iteration 15, loss = 1.23048696\n",
            "Iteration 16, loss = 1.22151995\n",
            "Iteration 17, loss = 1.22669152\n",
            "Iteration 18, loss = 1.22322528\n",
            "Iteration 19, loss = 1.22201617\n",
            "Iteration 20, loss = 1.21702768\n",
            "Iteration 21, loss = 1.23533473\n",
            "Iteration 22, loss = 1.21944016\n",
            "Iteration 23, loss = 1.22036675\n",
            "Iteration 24, loss = 1.21698383\n",
            "Iteration 25, loss = 1.22119509\n",
            "Iteration 26, loss = 1.21203895\n",
            "Iteration 27, loss = 1.20820098\n",
            "Iteration 28, loss = 1.20959308\n",
            "Iteration 29, loss = 1.21999765\n",
            "Iteration 30, loss = 1.21939024\n",
            "Iteration 31, loss = 1.21499279\n",
            "Iteration 32, loss = 1.21437564\n",
            "Iteration 33, loss = 1.20888114\n",
            "Iteration 34, loss = 1.20450277\n",
            "Iteration 35, loss = 1.20398885\n",
            "Iteration 36, loss = 1.19332717\n",
            "Iteration 37, loss = 1.19897918\n",
            "Iteration 38, loss = 1.19543365\n",
            "Iteration 39, loss = 1.18602970\n",
            "Iteration 40, loss = 1.19327178\n",
            "Iteration 41, loss = 1.18576975\n",
            "Iteration 42, loss = 1.18938494\n",
            "Iteration 43, loss = 1.20179508\n",
            "Iteration 44, loss = 1.18822534\n",
            "Iteration 45, loss = 1.19749175\n",
            "Iteration 46, loss = 1.19492229\n",
            "Iteration 47, loss = 1.19504256\n",
            "Iteration 48, loss = 1.18403336\n",
            "Iteration 49, loss = 1.17083351\n",
            "Iteration 50, loss = 1.17379988\n",
            "Iteration 51, loss = 1.16910263\n",
            "Iteration 52, loss = 1.17063290\n",
            "Iteration 53, loss = 1.16821560\n",
            "Iteration 54, loss = 1.16807402\n",
            "Iteration 55, loss = 1.17325152\n",
            "Iteration 56, loss = 1.18157569\n",
            "Iteration 57, loss = 1.17787707\n",
            "Iteration 58, loss = 1.15160966\n",
            "Iteration 59, loss = 1.18191864\n",
            "Iteration 60, loss = 1.16756615\n",
            "Iteration 61, loss = 1.19401820\n",
            "Iteration 62, loss = 1.18101837\n",
            "Iteration 63, loss = 1.18384476\n",
            "Iteration 64, loss = 1.15468193\n",
            "Iteration 65, loss = 1.15572304\n",
            "Iteration 66, loss = 1.14678270\n",
            "Iteration 67, loss = 1.13783818\n",
            "Iteration 68, loss = 1.14519411\n",
            "Iteration 69, loss = 1.13660547\n",
            "Iteration 70, loss = 1.15070569\n",
            "Iteration 71, loss = 1.15820150\n",
            "Iteration 72, loss = 1.13947239\n",
            "Iteration 73, loss = 1.14312701\n",
            "Iteration 74, loss = 1.14239443\n",
            "Iteration 75, loss = 1.13038206\n",
            "Iteration 76, loss = 1.12506920\n",
            "Iteration 77, loss = 1.12662179\n",
            "Iteration 78, loss = 1.12390314\n",
            "Iteration 79, loss = 1.11544584\n",
            "Iteration 80, loss = 1.12607397\n",
            "Iteration 81, loss = 1.11498430\n",
            "Iteration 82, loss = 1.12305083\n",
            "Iteration 83, loss = 1.12030563\n",
            "Iteration 84, loss = 1.12594045\n",
            "Iteration 85, loss = 1.13120801\n",
            "Iteration 86, loss = 1.12431641\n",
            "Iteration 87, loss = 1.11398784\n",
            "Iteration 88, loss = 1.11686643\n",
            "Iteration 89, loss = 1.11320398\n",
            "Iteration 90, loss = 1.17673089\n",
            "Iteration 91, loss = 1.16209434\n",
            "Iteration 92, loss = 1.13789635\n",
            "Iteration 93, loss = 1.11393346\n",
            "Iteration 94, loss = 1.11734444\n",
            "Iteration 95, loss = 1.10476846\n",
            "Iteration 96, loss = 1.11093281\n",
            "Iteration 97, loss = 1.10129506\n",
            "Iteration 98, loss = 1.08442664\n",
            "Iteration 99, loss = 1.10346289\n",
            "Iteration 100, loss = 1.10011528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30745911\n",
            "Iteration 2, loss = 1.25159331\n",
            "Iteration 3, loss = 1.23891330\n",
            "Iteration 4, loss = 1.24159084\n",
            "Iteration 5, loss = 1.25921665\n",
            "Iteration 6, loss = 1.23374890\n",
            "Iteration 7, loss = 1.23878580\n",
            "Iteration 8, loss = 1.22940122\n",
            "Iteration 9, loss = 1.23784975\n",
            "Iteration 10, loss = 1.23351814\n",
            "Iteration 11, loss = 1.24005320\n",
            "Iteration 12, loss = 1.23109942\n",
            "Iteration 13, loss = 1.23107204\n",
            "Iteration 14, loss = 1.22407049\n",
            "Iteration 15, loss = 1.22378622\n",
            "Iteration 16, loss = 1.21636787\n",
            "Iteration 17, loss = 1.21470281\n",
            "Iteration 18, loss = 1.21930012\n",
            "Iteration 19, loss = 1.21629675\n",
            "Iteration 20, loss = 1.21535473\n",
            "Iteration 21, loss = 1.21885361\n",
            "Iteration 22, loss = 1.21788201\n",
            "Iteration 23, loss = 1.21148865\n",
            "Iteration 24, loss = 1.21555030\n",
            "Iteration 25, loss = 1.20994868\n",
            "Iteration 26, loss = 1.21082815\n",
            "Iteration 27, loss = 1.20402879\n",
            "Iteration 28, loss = 1.21593122\n",
            "Iteration 29, loss = 1.22304623\n",
            "Iteration 30, loss = 1.20886535\n",
            "Iteration 31, loss = 1.20333830\n",
            "Iteration 32, loss = 1.21989978\n",
            "Iteration 33, loss = 1.20670936\n",
            "Iteration 34, loss = 1.20292186\n",
            "Iteration 35, loss = 1.20147821\n",
            "Iteration 36, loss = 1.19683249\n",
            "Iteration 37, loss = 1.19975296\n",
            "Iteration 38, loss = 1.19081781\n",
            "Iteration 39, loss = 1.18684404\n",
            "Iteration 40, loss = 1.19111781\n",
            "Iteration 41, loss = 1.18113963\n",
            "Iteration 42, loss = 1.18135754\n",
            "Iteration 43, loss = 1.19917911\n",
            "Iteration 44, loss = 1.18175113\n",
            "Iteration 45, loss = 1.18021961\n",
            "Iteration 46, loss = 1.19089586\n",
            "Iteration 47, loss = 1.19924580\n",
            "Iteration 48, loss = 1.17497135\n",
            "Iteration 49, loss = 1.17437036\n",
            "Iteration 50, loss = 1.16948441\n",
            "Iteration 51, loss = 1.18504372\n",
            "Iteration 52, loss = 1.17160529\n",
            "Iteration 53, loss = 1.17259596\n",
            "Iteration 54, loss = 1.16412091\n",
            "Iteration 55, loss = 1.16673786\n",
            "Iteration 56, loss = 1.18363854\n",
            "Iteration 57, loss = 1.17488212\n",
            "Iteration 58, loss = 1.14931755\n",
            "Iteration 59, loss = 1.16486351\n",
            "Iteration 60, loss = 1.15976212\n",
            "Iteration 61, loss = 1.16895140\n",
            "Iteration 62, loss = 1.16442737\n",
            "Iteration 63, loss = 1.16748528\n",
            "Iteration 64, loss = 1.13690473\n",
            "Iteration 65, loss = 1.14600509\n",
            "Iteration 66, loss = 1.13589063\n",
            "Iteration 67, loss = 1.14070394\n",
            "Iteration 68, loss = 1.13801652\n",
            "Iteration 69, loss = 1.12576300\n",
            "Iteration 70, loss = 1.16884971\n",
            "Iteration 71, loss = 1.13259256\n",
            "Iteration 72, loss = 1.12462390\n",
            "Iteration 73, loss = 1.12477841\n",
            "Iteration 74, loss = 1.12620681\n",
            "Iteration 75, loss = 1.11934166\n",
            "Iteration 76, loss = 1.12494680\n",
            "Iteration 77, loss = 1.11988044\n",
            "Iteration 78, loss = 1.10952347\n",
            "Iteration 79, loss = 1.10376879\n",
            "Iteration 80, loss = 1.12695088\n",
            "Iteration 81, loss = 1.12159853\n",
            "Iteration 82, loss = 1.10987693\n",
            "Iteration 83, loss = 1.11136047\n",
            "Iteration 84, loss = 1.10758867\n",
            "Iteration 85, loss = 1.11273351\n",
            "Iteration 86, loss = 1.09879808\n",
            "Iteration 87, loss = 1.11176085\n",
            "Iteration 88, loss = 1.10045007\n",
            "Iteration 89, loss = 1.09083139\n",
            "Iteration 90, loss = 1.15669308\n",
            "Iteration 91, loss = 1.18133971\n",
            "Iteration 92, loss = 1.14017568\n",
            "Iteration 93, loss = 1.12389999\n",
            "Iteration 94, loss = 1.12032151\n",
            "Iteration 95, loss = 1.10781867\n",
            "Iteration 96, loss = 1.09996668\n",
            "Iteration 97, loss = 1.09964658\n",
            "Iteration 98, loss = 1.09669203\n",
            "Iteration 99, loss = 1.11381965\n",
            "Iteration 100, loss = 1.10471872\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30011725\n",
            "Iteration 2, loss = 1.23696625\n",
            "Iteration 3, loss = 1.22843054\n",
            "Iteration 4, loss = 1.22563684\n",
            "Iteration 5, loss = 1.24219180\n",
            "Iteration 6, loss = 1.21920434\n",
            "Iteration 7, loss = 1.22245359\n",
            "Iteration 8, loss = 1.21555772\n",
            "Iteration 9, loss = 1.21467602\n",
            "Iteration 10, loss = 1.21609277\n",
            "Iteration 11, loss = 1.22278299\n",
            "Iteration 12, loss = 1.21119819\n",
            "Iteration 13, loss = 1.21441628\n",
            "Iteration 14, loss = 1.21192479\n",
            "Iteration 15, loss = 1.20476150\n",
            "Iteration 16, loss = 1.20281568\n",
            "Iteration 17, loss = 1.19503361\n",
            "Iteration 18, loss = 1.19562427\n",
            "Iteration 19, loss = 1.20014641\n",
            "Iteration 20, loss = 1.19186135\n",
            "Iteration 21, loss = 1.21199527\n",
            "Iteration 22, loss = 1.20247003\n",
            "Iteration 23, loss = 1.19442106\n",
            "Iteration 24, loss = 1.19923038\n",
            "Iteration 25, loss = 1.19145722\n",
            "Iteration 26, loss = 1.18964301\n",
            "Iteration 27, loss = 1.18593263\n",
            "Iteration 28, loss = 1.18978511\n",
            "Iteration 29, loss = 1.19282460\n",
            "Iteration 30, loss = 1.18796053\n",
            "Iteration 31, loss = 1.18490599\n",
            "Iteration 32, loss = 1.19343626\n",
            "Iteration 33, loss = 1.19049044\n",
            "Iteration 34, loss = 1.18817087\n",
            "Iteration 35, loss = 1.18458814\n",
            "Iteration 36, loss = 1.18159976\n",
            "Iteration 37, loss = 1.18336603\n",
            "Iteration 38, loss = 1.17389362\n",
            "Iteration 39, loss = 1.17078827\n",
            "Iteration 40, loss = 1.17078045\n",
            "Iteration 41, loss = 1.16829169\n",
            "Iteration 42, loss = 1.17320106\n",
            "Iteration 43, loss = 1.17301699\n",
            "Iteration 44, loss = 1.16614374\n",
            "Iteration 45, loss = 1.17505070\n",
            "Iteration 46, loss = 1.16078709\n",
            "Iteration 47, loss = 1.16036124\n",
            "Iteration 48, loss = 1.16301419\n",
            "Iteration 49, loss = 1.15778475\n",
            "Iteration 50, loss = 1.16324803\n",
            "Iteration 51, loss = 1.16195591\n",
            "Iteration 52, loss = 1.15138748\n",
            "Iteration 53, loss = 1.16192797\n",
            "Iteration 54, loss = 1.14989989\n",
            "Iteration 55, loss = 1.14162918\n",
            "Iteration 56, loss = 1.15799913\n",
            "Iteration 57, loss = 1.14748355\n",
            "Iteration 58, loss = 1.13491261\n",
            "Iteration 59, loss = 1.15969894\n",
            "Iteration 60, loss = 1.15453553\n",
            "Iteration 61, loss = 1.17798236\n",
            "Iteration 62, loss = 1.16741586\n",
            "Iteration 63, loss = 1.16529120\n",
            "Iteration 64, loss = 1.14431609\n",
            "Iteration 65, loss = 1.13954366\n",
            "Iteration 66, loss = 1.12855772\n",
            "Iteration 67, loss = 1.12296635\n",
            "Iteration 68, loss = 1.12522759\n",
            "Iteration 69, loss = 1.13171453\n",
            "Iteration 70, loss = 1.17342260\n",
            "Iteration 71, loss = 1.12701559\n",
            "Iteration 72, loss = 1.14763066\n",
            "Iteration 73, loss = 1.12943952\n",
            "Iteration 74, loss = 1.11268493\n",
            "Iteration 75, loss = 1.11583412\n",
            "Iteration 76, loss = 1.11797037\n",
            "Iteration 77, loss = 1.11580548\n",
            "Iteration 78, loss = 1.11274741\n",
            "Iteration 79, loss = 1.10172111\n",
            "Iteration 80, loss = 1.11504124\n",
            "Iteration 81, loss = 1.12080049\n",
            "Iteration 82, loss = 1.12320844\n",
            "Iteration 83, loss = 1.12686133\n",
            "Iteration 84, loss = 1.11476920\n",
            "Iteration 85, loss = 1.11101807\n",
            "Iteration 86, loss = 1.10727985\n",
            "Iteration 87, loss = 1.12141016\n",
            "Iteration 88, loss = 1.11488747\n",
            "Iteration 89, loss = 1.09013856\n",
            "Iteration 90, loss = 1.10728482\n",
            "Iteration 91, loss = 1.15379494\n",
            "Iteration 92, loss = 1.12558607\n",
            "Iteration 93, loss = 1.12990086\n",
            "Iteration 94, loss = 1.11271842\n",
            "Iteration 95, loss = 1.11117127\n",
            "Iteration 96, loss = 1.11152483\n",
            "Iteration 97, loss = 1.10391145\n",
            "Iteration 98, loss = 1.15542820\n",
            "Iteration 99, loss = 1.13264338\n",
            "Iteration 100, loss = 1.10174003\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31177907\n",
            "Iteration 2, loss = 1.24635358\n",
            "Iteration 3, loss = 1.23619605\n",
            "Iteration 4, loss = 1.23314802\n",
            "Iteration 5, loss = 1.23488279\n",
            "Iteration 6, loss = 1.22097916\n",
            "Iteration 7, loss = 1.22611680\n",
            "Iteration 8, loss = 1.21945987\n",
            "Iteration 9, loss = 1.21792418\n",
            "Iteration 10, loss = 1.21685014\n",
            "Iteration 11, loss = 1.23226066\n",
            "Iteration 12, loss = 1.22039556\n",
            "Iteration 13, loss = 1.21788212\n",
            "Iteration 14, loss = 1.20591474\n",
            "Iteration 15, loss = 1.20742972\n",
            "Iteration 16, loss = 1.21422753\n",
            "Iteration 17, loss = 1.20452495\n",
            "Iteration 18, loss = 1.20131077\n",
            "Iteration 19, loss = 1.20475522\n",
            "Iteration 20, loss = 1.19668651\n",
            "Iteration 21, loss = 1.22327788\n",
            "Iteration 22, loss = 1.20848980\n",
            "Iteration 23, loss = 1.20395189\n",
            "Iteration 24, loss = 1.20542447\n",
            "Iteration 25, loss = 1.19816964\n",
            "Iteration 26, loss = 1.19632935\n",
            "Iteration 27, loss = 1.19066656\n",
            "Iteration 28, loss = 1.19972301\n",
            "Iteration 29, loss = 1.19555573\n",
            "Iteration 30, loss = 1.21883941\n",
            "Iteration 31, loss = 1.22941869\n",
            "Iteration 32, loss = 1.21232015\n",
            "Iteration 33, loss = 1.19767186\n",
            "Iteration 34, loss = 1.19845160\n",
            "Iteration 35, loss = 1.19156230\n",
            "Iteration 36, loss = 1.18965402\n",
            "Iteration 37, loss = 1.18512520\n",
            "Iteration 38, loss = 1.17901709\n",
            "Iteration 39, loss = 1.17246535\n",
            "Iteration 40, loss = 1.17012933\n",
            "Iteration 41, loss = 1.17273304\n",
            "Iteration 42, loss = 1.17864703\n",
            "Iteration 43, loss = 1.16737693\n",
            "Iteration 44, loss = 1.16972233\n",
            "Iteration 45, loss = 1.17242890\n",
            "Iteration 46, loss = 1.17385468\n",
            "Iteration 47, loss = 1.17385793\n",
            "Iteration 48, loss = 1.15964485\n",
            "Iteration 49, loss = 1.15786661\n",
            "Iteration 50, loss = 1.15342335\n",
            "Iteration 51, loss = 1.15450455\n",
            "Iteration 52, loss = 1.15194806\n",
            "Iteration 53, loss = 1.16527307\n",
            "Iteration 54, loss = 1.15603290\n",
            "Iteration 55, loss = 1.15019633\n",
            "Iteration 56, loss = 1.15529709\n",
            "Iteration 57, loss = 1.14954469\n",
            "Iteration 58, loss = 1.14628045\n",
            "Iteration 59, loss = 1.16217783\n",
            "Iteration 60, loss = 1.14420327\n",
            "Iteration 61, loss = 1.13969338\n",
            "Iteration 62, loss = 1.13673114\n",
            "Iteration 63, loss = 1.12906542\n",
            "Iteration 64, loss = 1.12104338\n",
            "Iteration 65, loss = 1.12214382\n",
            "Iteration 66, loss = 1.13323743\n",
            "Iteration 67, loss = 1.13389027\n",
            "Iteration 68, loss = 1.11919587\n",
            "Iteration 69, loss = 1.13256828\n",
            "Iteration 70, loss = 1.14142266\n",
            "Iteration 71, loss = 1.13050661\n",
            "Iteration 72, loss = 1.13261601\n",
            "Iteration 73, loss = 1.11601193\n",
            "Iteration 74, loss = 1.12217261\n",
            "Iteration 75, loss = 1.11046290\n",
            "Iteration 76, loss = 1.10103503\n",
            "Iteration 77, loss = 1.11236926\n",
            "Iteration 78, loss = 1.10945900\n",
            "Iteration 79, loss = 1.14667127\n",
            "Iteration 80, loss = 1.13506839\n",
            "Iteration 81, loss = 1.11843187\n",
            "Iteration 82, loss = 1.10660076\n",
            "Iteration 83, loss = 1.12385069\n",
            "Iteration 84, loss = 1.10533603\n",
            "Iteration 85, loss = 1.09720417\n",
            "Iteration 86, loss = 1.11447186\n",
            "Iteration 87, loss = 1.12107098\n",
            "Iteration 88, loss = 1.09558387\n",
            "Iteration 89, loss = 1.08529533\n",
            "Iteration 90, loss = 1.09804793\n",
            "Iteration 91, loss = 1.11534515\n",
            "Iteration 92, loss = 1.10441132\n",
            "Iteration 93, loss = 1.08177347\n",
            "Iteration 94, loss = 1.09142431\n",
            "Iteration 95, loss = 1.08488207\n",
            "Iteration 96, loss = 1.08442166\n",
            "Iteration 97, loss = 1.08338680\n",
            "Iteration 98, loss = 1.08386569\n",
            "Iteration 99, loss = 1.10987577\n",
            "Iteration 100, loss = 1.07830438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31237772\n",
            "Iteration 2, loss = 1.24467254\n",
            "Iteration 3, loss = 1.24745876\n",
            "Iteration 4, loss = 1.23363920\n",
            "Iteration 5, loss = 1.22929009\n",
            "Iteration 6, loss = 1.22822326\n",
            "Iteration 7, loss = 1.23063904\n",
            "Iteration 8, loss = 1.22277027\n",
            "Iteration 9, loss = 1.22396193\n",
            "Iteration 10, loss = 1.21903535\n",
            "Iteration 11, loss = 1.21644623\n",
            "Iteration 12, loss = 1.21671925\n",
            "Iteration 13, loss = 1.20715246\n",
            "Iteration 14, loss = 1.21029092\n",
            "Iteration 15, loss = 1.20967889\n",
            "Iteration 16, loss = 1.21938781\n",
            "Iteration 17, loss = 1.21846911\n",
            "Iteration 18, loss = 1.21145550\n",
            "Iteration 19, loss = 1.20358115\n",
            "Iteration 20, loss = 1.20268011\n",
            "Iteration 21, loss = 1.19601446\n",
            "Iteration 22, loss = 1.20117277\n",
            "Iteration 23, loss = 1.20641919\n",
            "Iteration 24, loss = 1.20935554\n",
            "Iteration 25, loss = 1.19377732\n",
            "Iteration 26, loss = 1.19706391\n",
            "Iteration 27, loss = 1.18789412\n",
            "Iteration 28, loss = 1.19909867\n",
            "Iteration 29, loss = 1.18890043\n",
            "Iteration 30, loss = 1.19133433\n",
            "Iteration 31, loss = 1.18910482\n",
            "Iteration 32, loss = 1.19283371\n",
            "Iteration 33, loss = 1.18228720\n",
            "Iteration 34, loss = 1.18183293\n",
            "Iteration 35, loss = 1.18317494\n",
            "Iteration 36, loss = 1.19093867\n",
            "Iteration 37, loss = 1.18270520\n",
            "Iteration 38, loss = 1.18311677\n",
            "Iteration 39, loss = 1.18049337\n",
            "Iteration 40, loss = 1.17550808\n",
            "Iteration 41, loss = 1.17536108\n",
            "Iteration 42, loss = 1.19169903\n",
            "Iteration 43, loss = 1.17448127\n",
            "Iteration 44, loss = 1.18266017\n",
            "Iteration 45, loss = 1.18544112\n",
            "Iteration 46, loss = 1.18872428\n",
            "Iteration 47, loss = 1.17136544\n",
            "Iteration 48, loss = 1.17169879\n",
            "Iteration 49, loss = 1.20794635\n",
            "Iteration 50, loss = 1.17312886\n",
            "Iteration 51, loss = 1.16487138\n",
            "Iteration 52, loss = 1.15805075\n",
            "Iteration 53, loss = 1.15831634\n",
            "Iteration 54, loss = 1.14958528\n",
            "Iteration 55, loss = 1.17688646\n",
            "Iteration 56, loss = 1.16042252\n",
            "Iteration 57, loss = 1.17964743\n",
            "Iteration 58, loss = 1.17875264\n",
            "Iteration 59, loss = 1.17091311\n",
            "Iteration 60, loss = 1.14650662\n",
            "Iteration 61, loss = 1.14266409\n",
            "Iteration 62, loss = 1.15353589\n",
            "Iteration 63, loss = 1.15483530\n",
            "Iteration 64, loss = 1.15359231\n",
            "Iteration 65, loss = 1.13953624\n",
            "Iteration 66, loss = 1.13658521\n",
            "Iteration 67, loss = 1.13952481\n",
            "Iteration 68, loss = 1.13703093\n",
            "Iteration 69, loss = 1.12414474\n",
            "Iteration 70, loss = 1.15852221\n",
            "Iteration 71, loss = 1.15784549\n",
            "Iteration 72, loss = 1.12456371\n",
            "Iteration 73, loss = 1.13104926\n",
            "Iteration 74, loss = 1.13112388\n",
            "Iteration 75, loss = 1.11935108\n",
            "Iteration 76, loss = 1.13231929\n",
            "Iteration 77, loss = 1.15267675\n",
            "Iteration 78, loss = 1.13464495\n",
            "Iteration 79, loss = 1.13386854\n",
            "Iteration 80, loss = 1.14520696\n",
            "Iteration 81, loss = 1.13244564\n",
            "Iteration 82, loss = 1.12504134\n",
            "Iteration 83, loss = 1.11010709\n",
            "Iteration 84, loss = 1.11892664\n",
            "Iteration 85, loss = 1.13802971\n",
            "Iteration 86, loss = 1.13151617\n",
            "Iteration 87, loss = 1.11696810\n",
            "Iteration 88, loss = 1.13553062\n",
            "Iteration 89, loss = 1.12011914\n",
            "Iteration 90, loss = 1.09742241\n",
            "Iteration 91, loss = 1.10687447\n",
            "Iteration 92, loss = 1.10698549\n",
            "Iteration 93, loss = 1.09563507\n",
            "Iteration 94, loss = 1.09594386\n",
            "Iteration 95, loss = 1.10549918\n",
            "Iteration 96, loss = 1.09499948\n",
            "Iteration 97, loss = 1.10026625\n",
            "Iteration 98, loss = 1.10980626\n",
            "Iteration 99, loss = 1.10591328\n",
            "Iteration 100, loss = 1.12209860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31058882\n",
            "Iteration 2, loss = 1.24002839\n",
            "Iteration 3, loss = 1.24554979\n",
            "Iteration 4, loss = 1.22999100\n",
            "Iteration 5, loss = 1.22755965\n",
            "Iteration 6, loss = 1.22001917\n",
            "Iteration 7, loss = 1.22319821\n",
            "Iteration 8, loss = 1.22131121\n",
            "Iteration 9, loss = 1.21695360\n",
            "Iteration 10, loss = 1.21831892\n",
            "Iteration 11, loss = 1.20607664\n",
            "Iteration 12, loss = 1.22379784\n",
            "Iteration 13, loss = 1.20478999\n",
            "Iteration 14, loss = 1.20999683\n",
            "Iteration 15, loss = 1.20271061\n",
            "Iteration 16, loss = 1.20992049\n",
            "Iteration 17, loss = 1.21683406\n",
            "Iteration 18, loss = 1.20705886\n",
            "Iteration 19, loss = 1.19585108\n",
            "Iteration 20, loss = 1.20171298\n",
            "Iteration 21, loss = 1.19719898\n",
            "Iteration 22, loss = 1.19653404\n",
            "Iteration 23, loss = 1.19899899\n",
            "Iteration 24, loss = 1.20415343\n",
            "Iteration 25, loss = 1.19123867\n",
            "Iteration 26, loss = 1.19055474\n",
            "Iteration 27, loss = 1.18394026\n",
            "Iteration 28, loss = 1.19838664\n",
            "Iteration 29, loss = 1.18551958\n",
            "Iteration 30, loss = 1.18878615\n",
            "Iteration 31, loss = 1.18143065\n",
            "Iteration 32, loss = 1.18618965\n",
            "Iteration 33, loss = 1.17523312\n",
            "Iteration 34, loss = 1.17402831\n",
            "Iteration 35, loss = 1.17602783\n",
            "Iteration 36, loss = 1.18847116\n",
            "Iteration 37, loss = 1.17191654\n",
            "Iteration 38, loss = 1.18298417\n",
            "Iteration 39, loss = 1.16823433\n",
            "Iteration 40, loss = 1.16914156\n",
            "Iteration 41, loss = 1.17309866\n",
            "Iteration 42, loss = 1.17958143\n",
            "Iteration 43, loss = 1.16889985\n",
            "Iteration 44, loss = 1.17048240\n",
            "Iteration 45, loss = 1.17275140\n",
            "Iteration 46, loss = 1.16227841\n",
            "Iteration 47, loss = 1.15082721\n",
            "Iteration 48, loss = 1.15580245\n",
            "Iteration 49, loss = 1.17411922\n",
            "Iteration 50, loss = 1.16409622\n",
            "Iteration 51, loss = 1.14746855\n",
            "Iteration 52, loss = 1.14791369\n",
            "Iteration 53, loss = 1.14312938\n",
            "Iteration 54, loss = 1.13860106\n",
            "Iteration 55, loss = 1.14967286\n",
            "Iteration 56, loss = 1.14805481\n",
            "Iteration 57, loss = 1.16751698\n",
            "Iteration 58, loss = 1.18846965\n",
            "Iteration 59, loss = 1.19347132\n",
            "Iteration 60, loss = 1.15179283\n",
            "Iteration 61, loss = 1.15259792\n",
            "Iteration 62, loss = 1.14364967\n",
            "Iteration 63, loss = 1.14811433\n",
            "Iteration 64, loss = 1.16805740\n",
            "Iteration 65, loss = 1.14783962\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29361765\n",
            "Iteration 2, loss = 1.22453432\n",
            "Iteration 3, loss = 1.22992376\n",
            "Iteration 4, loss = 1.21282242\n",
            "Iteration 5, loss = 1.21161991\n",
            "Iteration 6, loss = 1.20765883\n",
            "Iteration 7, loss = 1.20978389\n",
            "Iteration 8, loss = 1.20219853\n",
            "Iteration 9, loss = 1.20634677\n",
            "Iteration 10, loss = 1.20111801\n",
            "Iteration 11, loss = 1.20100967\n",
            "Iteration 12, loss = 1.20214808\n",
            "Iteration 13, loss = 1.19866925\n",
            "Iteration 14, loss = 1.19846629\n",
            "Iteration 15, loss = 1.18956080\n",
            "Iteration 16, loss = 1.19376110\n",
            "Iteration 17, loss = 1.19576935\n",
            "Iteration 18, loss = 1.18805593\n",
            "Iteration 19, loss = 1.18311720\n",
            "Iteration 20, loss = 1.18543548\n",
            "Iteration 21, loss = 1.18005587\n",
            "Iteration 22, loss = 1.18544000\n",
            "Iteration 23, loss = 1.19132076\n",
            "Iteration 24, loss = 1.19108567\n",
            "Iteration 25, loss = 1.17866760\n",
            "Iteration 26, loss = 1.19055885\n",
            "Iteration 27, loss = 1.17781998\n",
            "Iteration 28, loss = 1.18143722\n",
            "Iteration 29, loss = 1.17758510\n",
            "Iteration 30, loss = 1.18473380\n",
            "Iteration 31, loss = 1.18025125\n",
            "Iteration 32, loss = 1.17189699\n",
            "Iteration 33, loss = 1.17424731\n",
            "Iteration 34, loss = 1.17221080\n",
            "Iteration 35, loss = 1.17343906\n",
            "Iteration 36, loss = 1.19316955\n",
            "Iteration 37, loss = 1.17459052\n",
            "Iteration 38, loss = 1.17408644\n",
            "Iteration 39, loss = 1.17018259\n",
            "Iteration 40, loss = 1.16875662\n",
            "Iteration 41, loss = 1.16532078\n",
            "Iteration 42, loss = 1.17423899\n",
            "Iteration 43, loss = 1.16052535\n",
            "Iteration 44, loss = 1.17035219\n",
            "Iteration 45, loss = 1.15223713\n",
            "Iteration 46, loss = 1.15765355\n",
            "Iteration 47, loss = 1.15506302\n",
            "Iteration 48, loss = 1.15297778\n",
            "Iteration 49, loss = 1.15788238\n",
            "Iteration 50, loss = 1.14375036\n",
            "Iteration 51, loss = 1.14926845\n",
            "Iteration 52, loss = 1.14025869\n",
            "Iteration 53, loss = 1.13993646\n",
            "Iteration 54, loss = 1.12927234\n",
            "Iteration 55, loss = 1.14159721\n",
            "Iteration 56, loss = 1.13779392\n",
            "Iteration 57, loss = 1.13702887\n",
            "Iteration 58, loss = 1.14848903\n",
            "Iteration 59, loss = 1.15165823\n",
            "Iteration 60, loss = 1.13517353\n",
            "Iteration 61, loss = 1.12364296\n",
            "Iteration 62, loss = 1.12893994\n",
            "Iteration 63, loss = 1.13307819\n",
            "Iteration 64, loss = 1.13530813\n",
            "Iteration 65, loss = 1.13192419\n",
            "Iteration 66, loss = 1.12571785\n",
            "Iteration 67, loss = 1.11734952\n",
            "Iteration 68, loss = 1.11008700\n",
            "Iteration 69, loss = 1.10976681\n",
            "Iteration 70, loss = 1.11558617\n",
            "Iteration 71, loss = 1.11935748\n",
            "Iteration 72, loss = 1.10562765\n",
            "Iteration 73, loss = 1.10694351\n",
            "Iteration 74, loss = 1.11537023\n",
            "Iteration 75, loss = 1.10818745\n",
            "Iteration 76, loss = 1.09936314\n",
            "Iteration 77, loss = 1.10576585\n",
            "Iteration 78, loss = 1.11963408\n",
            "Iteration 79, loss = 1.10797887\n",
            "Iteration 80, loss = 1.11367465\n",
            "Iteration 81, loss = 1.15110780\n",
            "Iteration 82, loss = 1.14777793\n",
            "Iteration 83, loss = 1.10983605\n",
            "Iteration 84, loss = 1.09976135\n",
            "Iteration 85, loss = 1.10422809\n",
            "Iteration 86, loss = 1.11770216\n",
            "Iteration 87, loss = 1.09369525\n",
            "Iteration 88, loss = 1.08809265\n",
            "Iteration 89, loss = 1.07573891\n",
            "Iteration 90, loss = 1.07422050\n",
            "Iteration 91, loss = 1.06671435\n",
            "Iteration 92, loss = 1.07085493\n",
            "Iteration 93, loss = 1.07825935\n",
            "Iteration 94, loss = 1.06980915\n",
            "Iteration 95, loss = 1.06251334\n",
            "Iteration 96, loss = 1.06702174\n",
            "Iteration 97, loss = 1.05719365\n",
            "Iteration 98, loss = 1.05941316\n",
            "Iteration 99, loss = 1.06152314\n",
            "Iteration 100, loss = 1.07521421\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30665230\n",
            "Iteration 2, loss = 1.24368226\n",
            "Iteration 3, loss = 1.22394865\n",
            "Iteration 4, loss = 1.23219732\n",
            "Iteration 5, loss = 1.22408720\n",
            "Iteration 6, loss = 1.18682939\n",
            "Iteration 7, loss = 1.18275013\n",
            "Iteration 8, loss = 1.15629461\n",
            "Iteration 9, loss = 1.12853635\n",
            "Iteration 10, loss = 1.10629869\n",
            "Iteration 11, loss = 1.09889750\n",
            "Iteration 12, loss = 1.05376227\n",
            "Iteration 13, loss = 1.02507491\n",
            "Iteration 14, loss = 1.01510156\n",
            "Iteration 15, loss = 0.97435716\n",
            "Iteration 16, loss = 0.95523011\n",
            "Iteration 17, loss = 0.92581794\n",
            "Iteration 18, loss = 0.89578363\n",
            "Iteration 19, loss = 0.86734396\n",
            "Iteration 20, loss = 0.89838162\n",
            "Iteration 21, loss = 0.98020856\n",
            "Iteration 22, loss = 0.89779978\n",
            "Iteration 23, loss = 0.84213653\n",
            "Iteration 24, loss = 0.79320780\n",
            "Iteration 25, loss = 0.79156028\n",
            "Iteration 26, loss = 0.81941216\n",
            "Iteration 27, loss = 0.81177049\n",
            "Iteration 28, loss = 0.77982659\n",
            "Iteration 29, loss = 0.79437259\n",
            "Iteration 30, loss = 0.76362031\n",
            "Iteration 31, loss = 0.73875540\n",
            "Iteration 32, loss = 0.73580912\n",
            "Iteration 33, loss = 0.75040073\n",
            "Iteration 34, loss = 0.71831775\n",
            "Iteration 35, loss = 0.74728699\n",
            "Iteration 36, loss = 0.72147327\n",
            "Iteration 37, loss = 0.70736479\n",
            "Iteration 38, loss = 0.70347089\n",
            "Iteration 39, loss = 0.67152954\n",
            "Iteration 40, loss = 0.68380324\n",
            "Iteration 41, loss = 0.70957151\n",
            "Iteration 42, loss = 0.69174069\n",
            "Iteration 43, loss = 0.69714032\n",
            "Iteration 44, loss = 0.65276524\n",
            "Iteration 45, loss = 0.64796878\n",
            "Iteration 46, loss = 0.64654354\n",
            "Iteration 47, loss = 0.66215880\n",
            "Iteration 48, loss = 0.63706198\n",
            "Iteration 49, loss = 0.64085473\n",
            "Iteration 50, loss = 0.65439760\n",
            "Iteration 51, loss = 0.66783073\n",
            "Iteration 52, loss = 0.65284038\n",
            "Iteration 53, loss = 0.63395336\n",
            "Iteration 54, loss = 0.61247881\n",
            "Iteration 55, loss = 0.61827862\n",
            "Iteration 56, loss = 0.69828239\n",
            "Iteration 57, loss = 0.64203602\n",
            "Iteration 58, loss = 0.67298288\n",
            "Iteration 59, loss = 0.65078975\n",
            "Iteration 60, loss = 0.62603905\n",
            "Iteration 61, loss = 0.61162838\n",
            "Iteration 62, loss = 0.61779267\n",
            "Iteration 63, loss = 0.61348522\n",
            "Iteration 64, loss = 0.60983546\n",
            "Iteration 65, loss = 0.58522013\n",
            "Iteration 66, loss = 0.57070120\n",
            "Iteration 67, loss = 0.57990219\n",
            "Iteration 68, loss = 0.56870670\n",
            "Iteration 69, loss = 0.58965859\n",
            "Iteration 70, loss = 0.56636899\n",
            "Iteration 71, loss = 0.54874758\n",
            "Iteration 72, loss = 0.55465877\n",
            "Iteration 73, loss = 0.55965835\n",
            "Iteration 74, loss = 0.54065310\n",
            "Iteration 75, loss = 0.55893042\n",
            "Iteration 76, loss = 0.56151535\n",
            "Iteration 77, loss = 0.60393607\n",
            "Iteration 78, loss = 0.57798340\n",
            "Iteration 79, loss = 0.55032472\n",
            "Iteration 80, loss = 0.53882448\n",
            "Iteration 81, loss = 0.52118645\n",
            "Iteration 82, loss = 0.51381976\n",
            "Iteration 83, loss = 0.51368866\n",
            "Iteration 84, loss = 0.50606678\n",
            "Iteration 85, loss = 0.52192823\n",
            "Iteration 86, loss = 0.54201528\n",
            "Iteration 87, loss = 0.55363790\n",
            "Iteration 88, loss = 0.62606522\n",
            "Iteration 89, loss = 0.55796104\n",
            "Iteration 90, loss = 0.56993955\n",
            "Iteration 91, loss = 0.57697144\n",
            "Iteration 92, loss = 0.56451501\n",
            "Iteration 93, loss = 0.53792496\n",
            "Iteration 94, loss = 0.53555363\n",
            "Iteration 95, loss = 0.49711449\n",
            "Iteration 96, loss = 0.49179824\n",
            "Iteration 97, loss = 0.47817497\n",
            "Iteration 98, loss = 0.47097056\n",
            "Iteration 99, loss = 0.47100537\n",
            "Iteration 100, loss = 0.47623637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30504981\n",
            "Iteration 2, loss = 1.24387942\n",
            "Iteration 3, loss = 1.21981203\n",
            "Iteration 4, loss = 1.22776254\n",
            "Iteration 5, loss = 1.22759268\n",
            "Iteration 6, loss = 1.18810375\n",
            "Iteration 7, loss = 1.17890350\n",
            "Iteration 8, loss = 1.16096946\n",
            "Iteration 9, loss = 1.12964908\n",
            "Iteration 10, loss = 1.10570069\n",
            "Iteration 11, loss = 1.10003698\n",
            "Iteration 12, loss = 1.06943872\n",
            "Iteration 13, loss = 1.02442317\n",
            "Iteration 14, loss = 1.00728773\n",
            "Iteration 15, loss = 0.97743888\n",
            "Iteration 16, loss = 0.95113801\n",
            "Iteration 17, loss = 0.92255446\n",
            "Iteration 18, loss = 0.90343825\n",
            "Iteration 19, loss = 0.87587419\n",
            "Iteration 20, loss = 0.94305439\n",
            "Iteration 21, loss = 0.97303821\n",
            "Iteration 22, loss = 0.88740122\n",
            "Iteration 23, loss = 0.83418209\n",
            "Iteration 24, loss = 0.83544219\n",
            "Iteration 25, loss = 0.83530348\n",
            "Iteration 26, loss = 0.80505158\n",
            "Iteration 27, loss = 0.79982757\n",
            "Iteration 28, loss = 0.75248981\n",
            "Iteration 29, loss = 0.74612169\n",
            "Iteration 30, loss = 0.73705068\n",
            "Iteration 31, loss = 0.72934385\n",
            "Iteration 32, loss = 0.74018790\n",
            "Iteration 33, loss = 0.72900255\n",
            "Iteration 34, loss = 0.71219972\n",
            "Iteration 35, loss = 0.72377810\n",
            "Iteration 36, loss = 0.73304369\n",
            "Iteration 37, loss = 0.71749485\n",
            "Iteration 38, loss = 0.69203008\n",
            "Iteration 39, loss = 0.68370437\n",
            "Iteration 40, loss = 0.67600072\n",
            "Iteration 41, loss = 0.68794821\n",
            "Iteration 42, loss = 0.69846267\n",
            "Iteration 43, loss = 0.69827077\n",
            "Iteration 44, loss = 0.71786018\n",
            "Iteration 45, loss = 0.67670704\n",
            "Iteration 46, loss = 0.68458312\n",
            "Iteration 47, loss = 0.67399252\n",
            "Iteration 48, loss = 0.66120133\n",
            "Iteration 49, loss = 0.64366873\n",
            "Iteration 50, loss = 0.65340002\n",
            "Iteration 51, loss = 0.65729308\n",
            "Iteration 52, loss = 0.65324871\n",
            "Iteration 53, loss = 0.64841550\n",
            "Iteration 54, loss = 0.61994352\n",
            "Iteration 55, loss = 0.62139172\n",
            "Iteration 56, loss = 0.69056659\n",
            "Iteration 57, loss = 0.64412571\n",
            "Iteration 58, loss = 0.62855122\n",
            "Iteration 59, loss = 0.64344909\n",
            "Iteration 60, loss = 0.64009098\n",
            "Iteration 61, loss = 0.62063795\n",
            "Iteration 62, loss = 0.61869114\n",
            "Iteration 63, loss = 0.60805184\n",
            "Iteration 64, loss = 0.59974153\n",
            "Iteration 65, loss = 0.58684267\n",
            "Iteration 66, loss = 0.59399223\n",
            "Iteration 67, loss = 0.57851504\n",
            "Iteration 68, loss = 0.58440901\n",
            "Iteration 69, loss = 0.61768619\n",
            "Iteration 70, loss = 0.59845419\n",
            "Iteration 71, loss = 0.59708405\n",
            "Iteration 72, loss = 0.57700760\n",
            "Iteration 73, loss = 0.56454075\n",
            "Iteration 74, loss = 0.55386960\n",
            "Iteration 75, loss = 0.55118379\n",
            "Iteration 76, loss = 0.56275980\n",
            "Iteration 77, loss = 0.61022772\n",
            "Iteration 78, loss = 0.59108201\n",
            "Iteration 79, loss = 0.56727671\n",
            "Iteration 80, loss = 0.55721909\n",
            "Iteration 81, loss = 0.54064153\n",
            "Iteration 82, loss = 0.52612368\n",
            "Iteration 83, loss = 0.53082107\n",
            "Iteration 84, loss = 0.52051109\n",
            "Iteration 85, loss = 0.52407952\n",
            "Iteration 86, loss = 0.55205616\n",
            "Iteration 87, loss = 0.56243018\n",
            "Iteration 88, loss = 0.61275183\n",
            "Iteration 89, loss = 0.55815727\n",
            "Iteration 90, loss = 0.56381335\n",
            "Iteration 91, loss = 0.55261202\n",
            "Iteration 92, loss = 0.57834446\n",
            "Iteration 93, loss = 0.54647095\n",
            "Iteration 94, loss = 0.51039270\n",
            "Iteration 95, loss = 0.51402990\n",
            "Iteration 96, loss = 0.48711234\n",
            "Iteration 97, loss = 0.48869246\n",
            "Iteration 98, loss = 0.48635642\n",
            "Iteration 99, loss = 0.48945662\n",
            "Iteration 100, loss = 0.48556437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31406760\n",
            "Iteration 2, loss = 1.25075693\n",
            "Iteration 3, loss = 1.22884911\n",
            "Iteration 4, loss = 1.21802508\n",
            "Iteration 5, loss = 1.22432804\n",
            "Iteration 6, loss = 1.18844599\n",
            "Iteration 7, loss = 1.17488008\n",
            "Iteration 8, loss = 1.14915628\n",
            "Iteration 9, loss = 1.12887979\n",
            "Iteration 10, loss = 1.10577719\n",
            "Iteration 11, loss = 1.09433972\n",
            "Iteration 12, loss = 1.06369090\n",
            "Iteration 13, loss = 1.03322758\n",
            "Iteration 14, loss = 1.03045502\n",
            "Iteration 15, loss = 0.97731425\n",
            "Iteration 16, loss = 0.95462196\n",
            "Iteration 17, loss = 0.93380270\n",
            "Iteration 18, loss = 0.89934722\n",
            "Iteration 19, loss = 0.87464310\n",
            "Iteration 20, loss = 0.94859317\n",
            "Iteration 21, loss = 0.97136430\n",
            "Iteration 22, loss = 0.88624065\n",
            "Iteration 23, loss = 0.83081862\n",
            "Iteration 24, loss = 0.82576688\n",
            "Iteration 25, loss = 0.79701750\n",
            "Iteration 26, loss = 0.79622229\n",
            "Iteration 27, loss = 0.80159540\n",
            "Iteration 28, loss = 0.76549323\n",
            "Iteration 29, loss = 0.78140511\n",
            "Iteration 30, loss = 0.77165561\n",
            "Iteration 31, loss = 0.73812269\n",
            "Iteration 32, loss = 0.79646734\n",
            "Iteration 33, loss = 0.76287405\n",
            "Iteration 34, loss = 0.74455165\n",
            "Iteration 35, loss = 0.75712781\n",
            "Iteration 36, loss = 0.75074148\n",
            "Iteration 37, loss = 0.73091118\n",
            "Iteration 38, loss = 0.71366373\n",
            "Iteration 39, loss = 0.68749289\n",
            "Iteration 40, loss = 0.69191102\n",
            "Iteration 41, loss = 0.71677212\n",
            "Iteration 42, loss = 0.71066730\n",
            "Iteration 43, loss = 0.70677888\n",
            "Iteration 44, loss = 0.66973553\n",
            "Iteration 45, loss = 0.67035498\n",
            "Iteration 46, loss = 0.67121750\n",
            "Iteration 47, loss = 0.66302661\n",
            "Iteration 48, loss = 0.64630939\n",
            "Iteration 49, loss = 0.64676613\n",
            "Iteration 50, loss = 0.67664932\n",
            "Iteration 51, loss = 0.67403836\n",
            "Iteration 52, loss = 0.67659624\n",
            "Iteration 53, loss = 0.66313635\n",
            "Iteration 54, loss = 0.65338303\n",
            "Iteration 55, loss = 0.64240129\n",
            "Iteration 56, loss = 0.68187788\n",
            "Iteration 57, loss = 0.64937231\n",
            "Iteration 58, loss = 0.64302947\n",
            "Iteration 59, loss = 0.64098603\n",
            "Iteration 60, loss = 0.62674414\n",
            "Iteration 61, loss = 0.59962165\n",
            "Iteration 62, loss = 0.61352476\n",
            "Iteration 63, loss = 0.60131153\n",
            "Iteration 64, loss = 0.58493619\n",
            "Iteration 65, loss = 0.59092345\n",
            "Iteration 66, loss = 0.59662318\n",
            "Iteration 67, loss = 0.57480971\n",
            "Iteration 68, loss = 0.60196638\n",
            "Iteration 69, loss = 0.64793335\n",
            "Iteration 70, loss = 0.62723960\n",
            "Iteration 71, loss = 0.58652752\n",
            "Iteration 72, loss = 0.57464661\n",
            "Iteration 73, loss = 0.59024746\n",
            "Iteration 74, loss = 0.57211139\n",
            "Iteration 75, loss = 0.55527869\n",
            "Iteration 76, loss = 0.55307239\n",
            "Iteration 77, loss = 0.58491664\n",
            "Iteration 78, loss = 0.56797789\n",
            "Iteration 79, loss = 0.55354669\n",
            "Iteration 80, loss = 0.57323060\n",
            "Iteration 81, loss = 0.54430271\n",
            "Iteration 82, loss = 0.53138346\n",
            "Iteration 83, loss = 0.53883664\n",
            "Iteration 84, loss = 0.52238489\n",
            "Iteration 85, loss = 0.52178240\n",
            "Iteration 86, loss = 0.52965310\n",
            "Iteration 87, loss = 0.54346713\n",
            "Iteration 88, loss = 0.56902696\n",
            "Iteration 89, loss = 0.57224320\n",
            "Iteration 90, loss = 0.55490510\n",
            "Iteration 91, loss = 0.68181936\n",
            "Iteration 92, loss = 0.62164099\n",
            "Iteration 93, loss = 0.57024858\n",
            "Iteration 94, loss = 0.54835999\n",
            "Iteration 95, loss = 0.53717149\n",
            "Iteration 96, loss = 0.49544649\n",
            "Iteration 97, loss = 0.49281669\n",
            "Iteration 98, loss = 0.51446673\n",
            "Iteration 99, loss = 0.49065641\n",
            "Iteration 100, loss = 0.49077713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31260237\n",
            "Iteration 2, loss = 1.24667186\n",
            "Iteration 3, loss = 1.22563123\n",
            "Iteration 4, loss = 1.21233194\n",
            "Iteration 5, loss = 1.22240241\n",
            "Iteration 6, loss = 1.18135568\n",
            "Iteration 7, loss = 1.16185648\n",
            "Iteration 8, loss = 1.13651868\n",
            "Iteration 9, loss = 1.11851105\n",
            "Iteration 10, loss = 1.11023046\n",
            "Iteration 11, loss = 1.06939924\n",
            "Iteration 12, loss = 1.04083922\n",
            "Iteration 13, loss = 1.01218540\n",
            "Iteration 14, loss = 0.99222425\n",
            "Iteration 15, loss = 0.95272849\n",
            "Iteration 16, loss = 0.93202029\n",
            "Iteration 17, loss = 0.91927815\n",
            "Iteration 18, loss = 0.88294362\n",
            "Iteration 19, loss = 0.86238777\n",
            "Iteration 20, loss = 0.87799732\n",
            "Iteration 21, loss = 0.91882270\n",
            "Iteration 22, loss = 0.89063120\n",
            "Iteration 23, loss = 0.82484901\n",
            "Iteration 24, loss = 0.81734080\n",
            "Iteration 25, loss = 0.83979459\n",
            "Iteration 26, loss = 0.79957320\n",
            "Iteration 27, loss = 0.76814355\n",
            "Iteration 28, loss = 0.76606838\n",
            "Iteration 29, loss = 0.78157285\n",
            "Iteration 30, loss = 0.76952027\n",
            "Iteration 31, loss = 0.73852578\n",
            "Iteration 32, loss = 0.79142834\n",
            "Iteration 33, loss = 0.73044368\n",
            "Iteration 34, loss = 0.72419665\n",
            "Iteration 35, loss = 0.74455893\n",
            "Iteration 36, loss = 0.76252201\n",
            "Iteration 37, loss = 0.75059324\n",
            "Iteration 38, loss = 0.71689006\n",
            "Iteration 39, loss = 0.68852823\n",
            "Iteration 40, loss = 0.69106062\n",
            "Iteration 41, loss = 0.72564671\n",
            "Iteration 42, loss = 0.71237176\n",
            "Iteration 43, loss = 0.70304297\n",
            "Iteration 44, loss = 0.68951281\n",
            "Iteration 45, loss = 0.67486872\n",
            "Iteration 46, loss = 0.68903376\n",
            "Iteration 47, loss = 0.69281896\n",
            "Iteration 48, loss = 0.66674442\n",
            "Iteration 49, loss = 0.66752945\n",
            "Iteration 50, loss = 0.69778688\n",
            "Iteration 51, loss = 0.66846874\n",
            "Iteration 52, loss = 0.67070733\n",
            "Iteration 53, loss = 0.65473039\n",
            "Iteration 54, loss = 0.63806797\n",
            "Iteration 55, loss = 0.63610766\n",
            "Iteration 56, loss = 0.69447257\n",
            "Iteration 57, loss = 0.67279016\n",
            "Iteration 58, loss = 0.67100961\n",
            "Iteration 59, loss = 0.65181759\n",
            "Iteration 60, loss = 0.63497442\n",
            "Iteration 61, loss = 0.61710736\n",
            "Iteration 62, loss = 0.59936723\n",
            "Iteration 63, loss = 0.60277787\n",
            "Iteration 64, loss = 0.60530851\n",
            "Iteration 65, loss = 0.59846590\n",
            "Iteration 66, loss = 0.60495358\n",
            "Iteration 67, loss = 0.60218196\n",
            "Iteration 68, loss = 0.59502755\n",
            "Iteration 69, loss = 0.63574876\n",
            "Iteration 70, loss = 0.60691808\n",
            "Iteration 71, loss = 0.61731005\n",
            "Iteration 72, loss = 0.59819204\n",
            "Iteration 73, loss = 0.60765063\n",
            "Iteration 74, loss = 0.59590728\n",
            "Iteration 75, loss = 0.58282559\n",
            "Iteration 76, loss = 0.57680964\n",
            "Iteration 77, loss = 0.59649788\n",
            "Iteration 78, loss = 0.61154345\n",
            "Iteration 79, loss = 0.58391840\n",
            "Iteration 80, loss = 0.62577487\n",
            "Iteration 81, loss = 0.58269996\n",
            "Iteration 82, loss = 0.58156532\n",
            "Iteration 83, loss = 0.58421672\n",
            "Iteration 84, loss = 0.58243097\n",
            "Iteration 85, loss = 0.55240942\n",
            "Iteration 86, loss = 0.54214389\n",
            "Iteration 87, loss = 0.58536628\n",
            "Iteration 88, loss = 0.55492476\n",
            "Iteration 89, loss = 0.54908372\n",
            "Iteration 90, loss = 0.58125143\n",
            "Iteration 91, loss = 0.64990105\n",
            "Iteration 92, loss = 0.64102365\n",
            "Iteration 93, loss = 0.58263304\n",
            "Iteration 94, loss = 0.53080853\n",
            "Iteration 95, loss = 0.52450048\n",
            "Iteration 96, loss = 0.51575044\n",
            "Iteration 97, loss = 0.50370461\n",
            "Iteration 98, loss = 0.52359923\n",
            "Iteration 99, loss = 0.50161812\n",
            "Iteration 100, loss = 0.49520766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31288693\n",
            "Iteration 2, loss = 1.24205684\n",
            "Iteration 3, loss = 1.21832423\n",
            "Iteration 4, loss = 1.20283341\n",
            "Iteration 5, loss = 1.22830029\n",
            "Iteration 6, loss = 1.18005988\n",
            "Iteration 7, loss = 1.16165650\n",
            "Iteration 8, loss = 1.13857195\n",
            "Iteration 9, loss = 1.12406288\n",
            "Iteration 10, loss = 1.09783066\n",
            "Iteration 11, loss = 1.08527949\n",
            "Iteration 12, loss = 1.05078852\n",
            "Iteration 13, loss = 1.02554154\n",
            "Iteration 14, loss = 1.00290238\n",
            "Iteration 15, loss = 0.96573673\n",
            "Iteration 16, loss = 0.94625540\n",
            "Iteration 17, loss = 0.92146146\n",
            "Iteration 18, loss = 0.90035127\n",
            "Iteration 19, loss = 0.87389383\n",
            "Iteration 20, loss = 0.92669949\n",
            "Iteration 21, loss = 0.90861896\n",
            "Iteration 22, loss = 0.94235404\n",
            "Iteration 23, loss = 0.84256367\n",
            "Iteration 24, loss = 0.83504871\n",
            "Iteration 25, loss = 0.81758396\n",
            "Iteration 26, loss = 0.77084147\n",
            "Iteration 27, loss = 0.77739400\n",
            "Iteration 28, loss = 0.78485359\n",
            "Iteration 29, loss = 0.79694680\n",
            "Iteration 30, loss = 0.77618010\n",
            "Iteration 31, loss = 0.75711319\n",
            "Iteration 32, loss = 0.80269312\n",
            "Iteration 33, loss = 0.73742390\n",
            "Iteration 34, loss = 0.74585137\n",
            "Iteration 35, loss = 0.73807995\n",
            "Iteration 36, loss = 0.79502083\n",
            "Iteration 37, loss = 0.75342888\n",
            "Iteration 38, loss = 0.70317842\n",
            "Iteration 39, loss = 0.70329039\n",
            "Iteration 40, loss = 0.69591289\n",
            "Iteration 41, loss = 0.72370189\n",
            "Iteration 42, loss = 0.70254652\n",
            "Iteration 43, loss = 0.68927919\n",
            "Iteration 44, loss = 0.67243290\n",
            "Iteration 45, loss = 0.66118793\n",
            "Iteration 46, loss = 0.68322968\n",
            "Iteration 47, loss = 0.67006135\n",
            "Iteration 48, loss = 0.65401695\n",
            "Iteration 49, loss = 0.67796051\n",
            "Iteration 50, loss = 0.69741911\n",
            "Iteration 51, loss = 0.68607187\n",
            "Iteration 52, loss = 0.67089407\n",
            "Iteration 53, loss = 0.65935419\n",
            "Iteration 54, loss = 0.63393852\n",
            "Iteration 55, loss = 0.62014434\n",
            "Iteration 56, loss = 0.64129008\n",
            "Iteration 57, loss = 0.64145022\n",
            "Iteration 58, loss = 0.63811093\n",
            "Iteration 59, loss = 0.61979028\n",
            "Iteration 60, loss = 0.61121801\n",
            "Iteration 61, loss = 0.60400390\n",
            "Iteration 62, loss = 0.61277654\n",
            "Iteration 63, loss = 0.59477730\n",
            "Iteration 64, loss = 0.58196205\n",
            "Iteration 65, loss = 0.58066301\n",
            "Iteration 66, loss = 0.58424972\n",
            "Iteration 67, loss = 0.58616057\n",
            "Iteration 68, loss = 0.59708885\n",
            "Iteration 69, loss = 0.65770115\n",
            "Iteration 70, loss = 0.59865947\n",
            "Iteration 71, loss = 0.61549636\n",
            "Iteration 72, loss = 0.58623746\n",
            "Iteration 73, loss = 0.63550284\n",
            "Iteration 74, loss = 0.56182156\n",
            "Iteration 75, loss = 0.56497918\n",
            "Iteration 76, loss = 0.56192763\n",
            "Iteration 77, loss = 0.56012306\n",
            "Iteration 78, loss = 0.55194628\n",
            "Iteration 79, loss = 0.54894846\n",
            "Iteration 80, loss = 0.56059932\n",
            "Iteration 81, loss = 0.53602029\n",
            "Iteration 82, loss = 0.54041033\n",
            "Iteration 83, loss = 0.55628853\n",
            "Iteration 84, loss = 0.56006038\n",
            "Iteration 85, loss = 0.51327813\n",
            "Iteration 86, loss = 0.52965285\n",
            "Iteration 87, loss = 0.52436655\n",
            "Iteration 88, loss = 0.54181698\n",
            "Iteration 89, loss = 0.53923009\n",
            "Iteration 90, loss = 0.54812672\n",
            "Iteration 91, loss = 0.67613909\n",
            "Iteration 92, loss = 0.68367321\n",
            "Iteration 93, loss = 0.57968747\n",
            "Iteration 94, loss = 0.54645467\n",
            "Iteration 95, loss = 0.51931173\n",
            "Iteration 96, loss = 0.51646008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30723723\n",
            "Iteration 2, loss = 1.22941893\n",
            "Iteration 3, loss = 1.20758082\n",
            "Iteration 4, loss = 1.19472920\n",
            "Iteration 5, loss = 1.20798185\n",
            "Iteration 6, loss = 1.16321282\n",
            "Iteration 7, loss = 1.14440281\n",
            "Iteration 8, loss = 1.11985264\n",
            "Iteration 9, loss = 1.08764034\n",
            "Iteration 10, loss = 1.07438655\n",
            "Iteration 11, loss = 1.08020141\n",
            "Iteration 12, loss = 1.02888675\n",
            "Iteration 13, loss = 0.98322885\n",
            "Iteration 14, loss = 0.95499301\n",
            "Iteration 15, loss = 0.93394300\n",
            "Iteration 16, loss = 0.95324728\n",
            "Iteration 17, loss = 0.92600328\n",
            "Iteration 18, loss = 0.90931115\n",
            "Iteration 19, loss = 0.87911454\n",
            "Iteration 20, loss = 0.86268451\n",
            "Iteration 21, loss = 0.88428313\n",
            "Iteration 22, loss = 0.86283453\n",
            "Iteration 23, loss = 0.81827546\n",
            "Iteration 24, loss = 0.80854429\n",
            "Iteration 25, loss = 0.81127931\n",
            "Iteration 26, loss = 0.79456877\n",
            "Iteration 27, loss = 0.77534994\n",
            "Iteration 28, loss = 0.77152239\n",
            "Iteration 29, loss = 0.76557102\n",
            "Iteration 30, loss = 0.73569748\n",
            "Iteration 31, loss = 0.74910171\n",
            "Iteration 32, loss = 0.73583503\n",
            "Iteration 33, loss = 0.71745816\n",
            "Iteration 34, loss = 0.71626251\n",
            "Iteration 35, loss = 0.71476043\n",
            "Iteration 36, loss = 0.75551998\n",
            "Iteration 37, loss = 0.74343823\n",
            "Iteration 38, loss = 0.70721562\n",
            "Iteration 39, loss = 0.69117543\n",
            "Iteration 40, loss = 0.68513450\n",
            "Iteration 41, loss = 0.68269065\n",
            "Iteration 42, loss = 0.69908187\n",
            "Iteration 43, loss = 0.69359498\n",
            "Iteration 44, loss = 0.68141912\n",
            "Iteration 45, loss = 0.66581849\n",
            "Iteration 46, loss = 0.65262741\n",
            "Iteration 47, loss = 0.64549657\n",
            "Iteration 48, loss = 0.64193625\n",
            "Iteration 49, loss = 0.64424658\n",
            "Iteration 50, loss = 0.68608169\n",
            "Iteration 51, loss = 0.65316955\n",
            "Iteration 52, loss = 0.62803941\n",
            "Iteration 53, loss = 0.63372462\n",
            "Iteration 54, loss = 0.63092136\n",
            "Iteration 55, loss = 0.61206058\n",
            "Iteration 56, loss = 0.61918261\n",
            "Iteration 57, loss = 0.60696389\n",
            "Iteration 58, loss = 0.60336343\n",
            "Iteration 59, loss = 0.64265984\n",
            "Iteration 60, loss = 0.60261910\n",
            "Iteration 61, loss = 0.59059827\n",
            "Iteration 62, loss = 0.59747613\n",
            "Iteration 63, loss = 0.59401374\n",
            "Iteration 64, loss = 0.59091191\n",
            "Iteration 65, loss = 0.57595454\n",
            "Iteration 66, loss = 0.59321352\n",
            "Iteration 67, loss = 0.58205220\n",
            "Iteration 68, loss = 0.59316315\n",
            "Iteration 69, loss = 0.60144138\n",
            "Iteration 70, loss = 0.63557150\n",
            "Iteration 71, loss = 0.60311650\n",
            "Iteration 72, loss = 0.61405277\n",
            "Iteration 73, loss = 0.57239019\n",
            "Iteration 74, loss = 0.55090328\n",
            "Iteration 75, loss = 0.56146605\n",
            "Iteration 76, loss = 0.55293016\n",
            "Iteration 77, loss = 0.55058344\n",
            "Iteration 78, loss = 0.58557960\n",
            "Iteration 79, loss = 0.60023399\n",
            "Iteration 80, loss = 0.60598518\n",
            "Iteration 81, loss = 0.59525067\n",
            "Iteration 82, loss = 0.62685553\n",
            "Iteration 83, loss = 0.62106889\n",
            "Iteration 84, loss = 0.57263575\n",
            "Iteration 85, loss = 0.53686388\n",
            "Iteration 86, loss = 0.51616664\n",
            "Iteration 87, loss = 0.52689838\n",
            "Iteration 88, loss = 0.52514570\n",
            "Iteration 89, loss = 0.50741953\n",
            "Iteration 90, loss = 0.52643106\n",
            "Iteration 91, loss = 0.55625188\n",
            "Iteration 92, loss = 0.56519920\n",
            "Iteration 93, loss = 0.53536966\n",
            "Iteration 94, loss = 0.55365154\n",
            "Iteration 95, loss = 0.53164219\n",
            "Iteration 96, loss = 0.51117732\n",
            "Iteration 97, loss = 0.51009308\n",
            "Iteration 98, loss = 0.50973325\n",
            "Iteration 99, loss = 0.47829072\n",
            "Iteration 100, loss = 0.50106197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31629927\n",
            "Iteration 2, loss = 1.23757604\n",
            "Iteration 3, loss = 1.21448863\n",
            "Iteration 4, loss = 1.19398581\n",
            "Iteration 5, loss = 1.19317974\n",
            "Iteration 6, loss = 1.15966103\n",
            "Iteration 7, loss = 1.12867822\n",
            "Iteration 8, loss = 1.10390875\n",
            "Iteration 9, loss = 1.07577857\n",
            "Iteration 10, loss = 1.05523129\n",
            "Iteration 11, loss = 1.06889368\n",
            "Iteration 12, loss = 1.02489477\n",
            "Iteration 13, loss = 0.99530344\n",
            "Iteration 14, loss = 0.96149949\n",
            "Iteration 15, loss = 0.93548503\n",
            "Iteration 16, loss = 0.94344098\n",
            "Iteration 17, loss = 0.92706721\n",
            "Iteration 18, loss = 0.92554550\n",
            "Iteration 19, loss = 0.88327009\n",
            "Iteration 20, loss = 0.85208263\n",
            "Iteration 21, loss = 0.84585721\n",
            "Iteration 22, loss = 0.85280051\n",
            "Iteration 23, loss = 0.81609509\n",
            "Iteration 24, loss = 0.79914385\n",
            "Iteration 25, loss = 0.80331115\n",
            "Iteration 26, loss = 0.78175683\n",
            "Iteration 27, loss = 0.78520710\n",
            "Iteration 28, loss = 0.77194814\n",
            "Iteration 29, loss = 0.81012867\n",
            "Iteration 30, loss = 0.78422974\n",
            "Iteration 31, loss = 0.77614593\n",
            "Iteration 32, loss = 0.75307470\n",
            "Iteration 33, loss = 0.73556046\n",
            "Iteration 34, loss = 0.74229436\n",
            "Iteration 35, loss = 0.72206844\n",
            "Iteration 36, loss = 0.75026844\n",
            "Iteration 37, loss = 0.80093309\n",
            "Iteration 38, loss = 0.78667546\n",
            "Iteration 39, loss = 0.73187954\n",
            "Iteration 40, loss = 0.72169265\n",
            "Iteration 41, loss = 0.74152815\n",
            "Iteration 42, loss = 0.73739319\n",
            "Iteration 43, loss = 0.69412214\n",
            "Iteration 44, loss = 0.67756345\n",
            "Iteration 45, loss = 0.67520712\n",
            "Iteration 46, loss = 0.66543885\n",
            "Iteration 47, loss = 0.66946204\n",
            "Iteration 48, loss = 0.66151149\n",
            "Iteration 49, loss = 0.66493962\n",
            "Iteration 50, loss = 0.68373225\n",
            "Iteration 51, loss = 0.68217221\n",
            "Iteration 52, loss = 0.63674798\n",
            "Iteration 53, loss = 0.64697863\n",
            "Iteration 54, loss = 0.63452704\n",
            "Iteration 55, loss = 0.62167081\n",
            "Iteration 56, loss = 0.62933015\n",
            "Iteration 57, loss = 0.60992197\n",
            "Iteration 58, loss = 0.61455249\n",
            "Iteration 59, loss = 0.62576734\n",
            "Iteration 60, loss = 0.60488735\n",
            "Iteration 61, loss = 0.61286947\n",
            "Iteration 62, loss = 0.64136810\n",
            "Iteration 63, loss = 0.62683332\n",
            "Iteration 64, loss = 0.60912290\n",
            "Iteration 65, loss = 0.60424222\n",
            "Iteration 66, loss = 0.62444283\n",
            "Iteration 67, loss = 0.59242808\n",
            "Iteration 68, loss = 0.58830365\n",
            "Iteration 69, loss = 0.58517118\n",
            "Iteration 70, loss = 0.60364266\n",
            "Iteration 71, loss = 0.57281665\n",
            "Iteration 72, loss = 0.59618023\n",
            "Iteration 73, loss = 0.57407691\n",
            "Iteration 74, loss = 0.55363558\n",
            "Iteration 75, loss = 0.56449172\n",
            "Iteration 76, loss = 0.58623018\n",
            "Iteration 77, loss = 0.55349408\n",
            "Iteration 78, loss = 0.58692414\n",
            "Iteration 79, loss = 0.58740978\n",
            "Iteration 80, loss = 0.62292897\n",
            "Iteration 81, loss = 0.57776400\n",
            "Iteration 82, loss = 0.56912101\n",
            "Iteration 83, loss = 0.56689341\n",
            "Iteration 84, loss = 0.53494806\n",
            "Iteration 85, loss = 0.51508084\n",
            "Iteration 86, loss = 0.52514047\n",
            "Iteration 87, loss = 0.52379810\n",
            "Iteration 88, loss = 0.53318788\n",
            "Iteration 89, loss = 0.52833650\n",
            "Iteration 90, loss = 0.55831628\n",
            "Iteration 91, loss = 0.60029242\n",
            "Iteration 92, loss = 0.54996891\n",
            "Iteration 93, loss = 0.51141816\n",
            "Iteration 94, loss = 0.49698136\n",
            "Iteration 95, loss = 0.53149971\n",
            "Iteration 96, loss = 0.53322783\n",
            "Iteration 97, loss = 0.53331962\n",
            "Iteration 98, loss = 0.50199526\n",
            "Iteration 99, loss = 0.51329316\n",
            "Iteration 100, loss = 0.51610871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32005593\n",
            "Iteration 2, loss = 1.23433069\n",
            "Iteration 3, loss = 1.22590545\n",
            "Iteration 4, loss = 1.19409823\n",
            "Iteration 5, loss = 1.16965122\n",
            "Iteration 6, loss = 1.14268147\n",
            "Iteration 7, loss = 1.15154479\n",
            "Iteration 8, loss = 1.12015933\n",
            "Iteration 9, loss = 1.09392906\n",
            "Iteration 10, loss = 1.05918657\n",
            "Iteration 11, loss = 1.04902474\n",
            "Iteration 12, loss = 1.05724932\n",
            "Iteration 13, loss = 1.01913170\n",
            "Iteration 14, loss = 1.00198689\n",
            "Iteration 15, loss = 0.97514339\n",
            "Iteration 16, loss = 0.96077300\n",
            "Iteration 17, loss = 0.93229957\n",
            "Iteration 18, loss = 0.91786900\n",
            "Iteration 19, loss = 0.90176326\n",
            "Iteration 20, loss = 0.88122391\n",
            "Iteration 21, loss = 0.89560164\n",
            "Iteration 22, loss = 0.86070880\n",
            "Iteration 23, loss = 0.84074089\n",
            "Iteration 24, loss = 0.82171959\n",
            "Iteration 25, loss = 0.82291134\n",
            "Iteration 26, loss = 0.80002213\n",
            "Iteration 27, loss = 0.77970596\n",
            "Iteration 28, loss = 0.79487441\n",
            "Iteration 29, loss = 0.77093348\n",
            "Iteration 30, loss = 0.75386853\n",
            "Iteration 31, loss = 0.73996088\n",
            "Iteration 32, loss = 0.73202517\n",
            "Iteration 33, loss = 0.72317763\n",
            "Iteration 34, loss = 0.71152218\n",
            "Iteration 35, loss = 0.73030326\n",
            "Iteration 36, loss = 0.70770815\n",
            "Iteration 37, loss = 0.69467033\n",
            "Iteration 38, loss = 0.69713385\n",
            "Iteration 39, loss = 0.69808200\n",
            "Iteration 40, loss = 0.68650509\n",
            "Iteration 41, loss = 0.68875712\n",
            "Iteration 42, loss = 0.73794142\n",
            "Iteration 43, loss = 0.74195405\n",
            "Iteration 44, loss = 0.77255422\n",
            "Iteration 45, loss = 0.76385968\n",
            "Iteration 46, loss = 0.78271856\n",
            "Iteration 47, loss = 0.70853376\n",
            "Iteration 48, loss = 0.68732034\n",
            "Iteration 49, loss = 0.71603241\n",
            "Iteration 50, loss = 0.71802766\n",
            "Iteration 51, loss = 0.74922856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31565343\n",
            "Iteration 2, loss = 1.23003596\n",
            "Iteration 3, loss = 1.22344192\n",
            "Iteration 4, loss = 1.18749865\n",
            "Iteration 5, loss = 1.16507175\n",
            "Iteration 6, loss = 1.14770237\n",
            "Iteration 7, loss = 1.15750051\n",
            "Iteration 8, loss = 1.11322887\n",
            "Iteration 9, loss = 1.08201715\n",
            "Iteration 10, loss = 1.04425157\n",
            "Iteration 11, loss = 1.03415104\n",
            "Iteration 12, loss = 1.07584021\n",
            "Iteration 13, loss = 1.00390341\n",
            "Iteration 14, loss = 0.99497332\n",
            "Iteration 15, loss = 0.94845701\n",
            "Iteration 16, loss = 0.92795283\n",
            "Iteration 17, loss = 0.90544659\n",
            "Iteration 18, loss = 0.88558954\n",
            "Iteration 19, loss = 0.86427730\n",
            "Iteration 20, loss = 0.84598641\n",
            "Iteration 21, loss = 0.86034155\n",
            "Iteration 22, loss = 0.82545357\n",
            "Iteration 23, loss = 0.81226981\n",
            "Iteration 24, loss = 0.78752563\n",
            "Iteration 25, loss = 0.79899591\n",
            "Iteration 26, loss = 0.78918090\n",
            "Iteration 27, loss = 0.76845693\n",
            "Iteration 28, loss = 0.77804973\n",
            "Iteration 29, loss = 0.72912057\n",
            "Iteration 30, loss = 0.74032554\n",
            "Iteration 31, loss = 0.74451860\n",
            "Iteration 32, loss = 0.72407843\n",
            "Iteration 33, loss = 0.70221480\n",
            "Iteration 34, loss = 0.69220286\n",
            "Iteration 35, loss = 0.69974252\n",
            "Iteration 36, loss = 0.70604418\n",
            "Iteration 37, loss = 0.69555415\n",
            "Iteration 38, loss = 0.72510255\n",
            "Iteration 39, loss = 0.71162437\n",
            "Iteration 40, loss = 0.68833455\n",
            "Iteration 41, loss = 0.68666509\n",
            "Iteration 42, loss = 0.70245463\n",
            "Iteration 43, loss = 0.68396651\n",
            "Iteration 44, loss = 0.70073741\n",
            "Iteration 45, loss = 0.69608452\n",
            "Iteration 46, loss = 0.71139605\n",
            "Iteration 47, loss = 0.64697472\n",
            "Iteration 48, loss = 0.63661311\n",
            "Iteration 49, loss = 0.64437658\n",
            "Iteration 50, loss = 0.64914571\n",
            "Iteration 51, loss = 0.65455298\n",
            "Iteration 52, loss = 0.65212382\n",
            "Iteration 53, loss = 0.70613383\n",
            "Iteration 54, loss = 0.63363224\n",
            "Iteration 55, loss = 0.62788473\n",
            "Iteration 56, loss = 0.61696798\n",
            "Iteration 57, loss = 0.60583082\n",
            "Iteration 58, loss = 0.59004935\n",
            "Iteration 59, loss = 0.59453145\n",
            "Iteration 60, loss = 0.58977209\n",
            "Iteration 61, loss = 0.58707555\n",
            "Iteration 62, loss = 0.58116437\n",
            "Iteration 63, loss = 0.58550605\n",
            "Iteration 64, loss = 0.65572006\n",
            "Iteration 65, loss = 0.59677082\n",
            "Iteration 66, loss = 0.56172216\n",
            "Iteration 67, loss = 0.60806577\n",
            "Iteration 68, loss = 0.58206122\n",
            "Iteration 69, loss = 0.57542901\n",
            "Iteration 70, loss = 0.57195181\n",
            "Iteration 71, loss = 0.54368592\n",
            "Iteration 72, loss = 0.54671291\n",
            "Iteration 73, loss = 0.57177163\n",
            "Iteration 74, loss = 0.55952757\n",
            "Iteration 75, loss = 0.56329851\n",
            "Iteration 76, loss = 0.54507481\n",
            "Iteration 77, loss = 0.55032678\n",
            "Iteration 78, loss = 0.53022863\n",
            "Iteration 79, loss = 0.53052718\n",
            "Iteration 80, loss = 0.55462481\n",
            "Iteration 81, loss = 0.58906817\n",
            "Iteration 82, loss = 0.61154098\n",
            "Iteration 83, loss = 0.54914639\n",
            "Iteration 84, loss = 0.54174842\n",
            "Iteration 85, loss = 0.52436524\n",
            "Iteration 86, loss = 0.53507921\n",
            "Iteration 87, loss = 0.52066121\n",
            "Iteration 88, loss = 0.49349818\n",
            "Iteration 89, loss = 0.48617516\n",
            "Iteration 90, loss = 0.49468235\n",
            "Iteration 91, loss = 0.51154731\n",
            "Iteration 92, loss = 0.50639379\n",
            "Iteration 93, loss = 0.50126668\n",
            "Iteration 94, loss = 0.47644923\n",
            "Iteration 95, loss = 0.51013159\n",
            "Iteration 96, loss = 0.59099224\n",
            "Iteration 97, loss = 0.54114234\n",
            "Iteration 98, loss = 0.49712774\n",
            "Iteration 99, loss = 0.48485135\n",
            "Iteration 100, loss = 0.48536691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29950235\n",
            "Iteration 2, loss = 1.21992682\n",
            "Iteration 3, loss = 1.21539729\n",
            "Iteration 4, loss = 1.18738969\n",
            "Iteration 5, loss = 1.17149821\n",
            "Iteration 6, loss = 1.14865653\n",
            "Iteration 7, loss = 1.13930911\n",
            "Iteration 8, loss = 1.10910308\n",
            "Iteration 9, loss = 1.09202120\n",
            "Iteration 10, loss = 1.05098932\n",
            "Iteration 11, loss = 1.04129734\n",
            "Iteration 12, loss = 1.03906393\n",
            "Iteration 13, loss = 1.00867679\n",
            "Iteration 14, loss = 0.97529032\n",
            "Iteration 15, loss = 0.94703837\n",
            "Iteration 16, loss = 0.93105352\n",
            "Iteration 17, loss = 0.90739336\n",
            "Iteration 18, loss = 0.88500526\n",
            "Iteration 19, loss = 0.86597425\n",
            "Iteration 20, loss = 0.84881830\n",
            "Iteration 21, loss = 0.85037795\n",
            "Iteration 22, loss = 0.84659228\n",
            "Iteration 23, loss = 0.83027391\n",
            "Iteration 24, loss = 0.80485391\n",
            "Iteration 25, loss = 0.80828979\n",
            "Iteration 26, loss = 0.77714481\n",
            "Iteration 27, loss = 0.77536062\n",
            "Iteration 28, loss = 0.78503029\n",
            "Iteration 29, loss = 0.74094332\n",
            "Iteration 30, loss = 0.78357581\n",
            "Iteration 31, loss = 0.77237434\n",
            "Iteration 32, loss = 0.74997499\n",
            "Iteration 33, loss = 0.73371377\n",
            "Iteration 34, loss = 0.72840567\n",
            "Iteration 35, loss = 0.74807459\n",
            "Iteration 36, loss = 0.73059635\n",
            "Iteration 37, loss = 0.74183483\n",
            "Iteration 38, loss = 0.74355787\n",
            "Iteration 39, loss = 0.72217566\n",
            "Iteration 40, loss = 0.69184452\n",
            "Iteration 41, loss = 0.73633940\n",
            "Iteration 42, loss = 0.73178626\n",
            "Iteration 43, loss = 0.70237791\n",
            "Iteration 44, loss = 0.74550702\n",
            "Iteration 45, loss = 0.69490885\n",
            "Iteration 46, loss = 0.70755637\n",
            "Iteration 47, loss = 0.65507812\n",
            "Iteration 48, loss = 0.65555919\n",
            "Iteration 49, loss = 0.66241437\n",
            "Iteration 50, loss = 0.65708098\n",
            "Iteration 51, loss = 0.64843629\n",
            "Iteration 52, loss = 0.64781370\n",
            "Iteration 53, loss = 0.65260039\n",
            "Iteration 54, loss = 0.64009952\n",
            "Iteration 55, loss = 0.63788259\n",
            "Iteration 56, loss = 0.64630023\n",
            "Iteration 57, loss = 0.63067929\n",
            "Iteration 58, loss = 0.62943739\n",
            "Iteration 59, loss = 0.61304496\n",
            "Iteration 60, loss = 0.60462111\n",
            "Iteration 61, loss = 0.59289766\n",
            "Iteration 62, loss = 0.60150253\n",
            "Iteration 63, loss = 0.64985729\n",
            "Iteration 64, loss = 0.70942280\n",
            "Iteration 65, loss = 0.62188608\n",
            "Iteration 66, loss = 0.58545226\n",
            "Iteration 67, loss = 0.58488956\n",
            "Iteration 68, loss = 0.59036981\n",
            "Iteration 69, loss = 0.58879745\n",
            "Iteration 70, loss = 0.59198473\n",
            "Iteration 71, loss = 0.57322051\n",
            "Iteration 72, loss = 0.55401033\n",
            "Iteration 73, loss = 0.56274560\n",
            "Iteration 74, loss = 0.58145372\n",
            "Iteration 75, loss = 0.60463869\n",
            "Iteration 76, loss = 0.58387156\n",
            "Iteration 77, loss = 0.56044834\n",
            "Iteration 78, loss = 0.54376918\n",
            "Iteration 79, loss = 0.55417280\n",
            "Iteration 80, loss = 0.59758591\n",
            "Iteration 81, loss = 0.64160950\n",
            "Iteration 82, loss = 0.65931504\n",
            "Iteration 83, loss = 0.62746793\n",
            "Iteration 84, loss = 0.65280161\n",
            "Iteration 85, loss = 0.60181795\n",
            "Iteration 86, loss = 0.56472742\n",
            "Iteration 87, loss = 0.55493097\n",
            "Iteration 88, loss = 0.54343163\n",
            "Iteration 89, loss = 0.54061758\n",
            "Iteration 90, loss = 0.51655206\n",
            "Iteration 91, loss = 0.52751120\n",
            "Iteration 92, loss = 0.52440601\n",
            "Iteration 93, loss = 0.54118207\n",
            "Iteration 94, loss = 0.51466122\n",
            "Iteration 95, loss = 0.53783925\n",
            "Iteration 96, loss = 0.57463946\n",
            "Iteration 97, loss = 0.55284087\n",
            "Iteration 98, loss = 0.53086921\n",
            "Iteration 99, loss = 0.51663154\n",
            "Iteration 100, loss = 0.50452041\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32214700\n",
            "Iteration 2, loss = 1.26916725\n",
            "Iteration 3, loss = 1.24920711\n",
            "Iteration 4, loss = 1.26577222\n",
            "Iteration 5, loss = 1.26438790\n",
            "Iteration 6, loss = 1.25229532\n",
            "Iteration 7, loss = 1.24584274\n",
            "Iteration 8, loss = 1.24392466\n",
            "Iteration 9, loss = 1.24344493\n",
            "Iteration 10, loss = 1.24085850\n",
            "Iteration 11, loss = 1.23628982\n",
            "Iteration 12, loss = 1.23867797\n",
            "Iteration 13, loss = 1.23040446\n",
            "Iteration 14, loss = 1.22820725\n",
            "Iteration 15, loss = 1.22289523\n",
            "Iteration 16, loss = 1.22309771\n",
            "Iteration 17, loss = 1.21808067\n",
            "Iteration 18, loss = 1.22408480\n",
            "Iteration 19, loss = 1.21786775\n",
            "Iteration 20, loss = 1.22013616\n",
            "Iteration 21, loss = 1.20975919\n",
            "Iteration 22, loss = 1.20926413\n",
            "Iteration 23, loss = 1.20947468\n",
            "Iteration 24, loss = 1.20597802\n",
            "Iteration 25, loss = 1.20405027\n",
            "Iteration 26, loss = 1.20553209\n",
            "Iteration 27, loss = 1.20616718\n",
            "Iteration 28, loss = 1.19248226\n",
            "Iteration 29, loss = 1.18531499\n",
            "Iteration 30, loss = 1.18539633\n",
            "Iteration 31, loss = 1.18746114\n",
            "Iteration 32, loss = 1.18482849\n",
            "Iteration 33, loss = 1.18366769\n",
            "Iteration 34, loss = 1.17162180\n",
            "Iteration 35, loss = 1.17567490\n",
            "Iteration 36, loss = 1.16854634\n",
            "Iteration 37, loss = 1.16345657\n",
            "Iteration 38, loss = 1.16692772\n",
            "Iteration 39, loss = 1.16308402\n",
            "Iteration 40, loss = 1.16047187\n",
            "Iteration 41, loss = 1.16316010\n",
            "Iteration 42, loss = 1.16088370\n",
            "Iteration 43, loss = 1.14928211\n",
            "Iteration 44, loss = 1.15643746\n",
            "Iteration 45, loss = 1.15948400\n",
            "Iteration 46, loss = 1.15703352\n",
            "Iteration 47, loss = 1.15327339\n",
            "Iteration 48, loss = 1.15783570\n",
            "Iteration 49, loss = 1.15353584\n",
            "Iteration 50, loss = 1.14985583\n",
            "Iteration 51, loss = 1.16685975\n",
            "Iteration 52, loss = 1.17857371\n",
            "Iteration 53, loss = 1.17777032\n",
            "Iteration 54, loss = 1.14714511\n",
            "Iteration 55, loss = 1.16433743\n",
            "Iteration 56, loss = 1.14675775\n",
            "Iteration 57, loss = 1.15518937\n",
            "Iteration 58, loss = 1.15735065\n",
            "Iteration 59, loss = 1.15409581\n",
            "Iteration 60, loss = 1.14008542\n",
            "Iteration 61, loss = 1.13249936\n",
            "Iteration 62, loss = 1.14560994\n",
            "Iteration 63, loss = 1.15218783\n",
            "Iteration 64, loss = 1.14230430\n",
            "Iteration 65, loss = 1.13237747\n",
            "Iteration 66, loss = 1.13275367\n",
            "Iteration 67, loss = 1.13869485\n",
            "Iteration 68, loss = 1.12612016\n",
            "Iteration 69, loss = 1.11887816\n",
            "Iteration 70, loss = 1.13330677\n",
            "Iteration 71, loss = 1.12519268\n",
            "Iteration 72, loss = 1.11491519\n",
            "Iteration 73, loss = 1.11100814\n",
            "Iteration 74, loss = 1.11646219\n",
            "Iteration 75, loss = 1.10354071\n",
            "Iteration 76, loss = 1.10634756\n",
            "Iteration 77, loss = 1.10859083\n",
            "Iteration 78, loss = 1.10716255\n",
            "Iteration 79, loss = 1.09795542\n",
            "Iteration 80, loss = 1.09406130\n",
            "Iteration 81, loss = 1.08981414\n",
            "Iteration 82, loss = 1.09685686\n",
            "Iteration 83, loss = 1.10329377\n",
            "Iteration 84, loss = 1.09440213\n",
            "Iteration 85, loss = 1.12297406\n",
            "Iteration 86, loss = 1.11086763\n",
            "Iteration 87, loss = 1.11335348\n",
            "Iteration 88, loss = 1.09624875\n",
            "Iteration 89, loss = 1.09681693\n",
            "Iteration 90, loss = 1.13521857\n",
            "Iteration 91, loss = 1.15357652\n",
            "Iteration 92, loss = 1.14452067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32017637\n",
            "Iteration 2, loss = 1.26596328\n",
            "Iteration 3, loss = 1.24413197\n",
            "Iteration 4, loss = 1.26027096\n",
            "Iteration 5, loss = 1.26265189\n",
            "Iteration 6, loss = 1.24485917\n",
            "Iteration 7, loss = 1.24197294\n",
            "Iteration 8, loss = 1.23804748\n",
            "Iteration 9, loss = 1.24221645\n",
            "Iteration 10, loss = 1.23876665\n",
            "Iteration 11, loss = 1.23488810\n",
            "Iteration 12, loss = 1.23236181\n",
            "Iteration 13, loss = 1.23051279\n",
            "Iteration 14, loss = 1.22956783\n",
            "Iteration 15, loss = 1.22716669\n",
            "Iteration 16, loss = 1.21977973\n",
            "Iteration 17, loss = 1.21754992\n",
            "Iteration 18, loss = 1.21386569\n",
            "Iteration 19, loss = 1.21534392\n",
            "Iteration 20, loss = 1.22799910\n",
            "Iteration 21, loss = 1.20553407\n",
            "Iteration 22, loss = 1.20460839\n",
            "Iteration 23, loss = 1.19961323\n",
            "Iteration 24, loss = 1.19751577\n",
            "Iteration 25, loss = 1.19924665\n",
            "Iteration 26, loss = 1.19824779\n",
            "Iteration 27, loss = 1.18705430\n",
            "Iteration 28, loss = 1.18647887\n",
            "Iteration 29, loss = 1.18404242\n",
            "Iteration 30, loss = 1.18165172\n",
            "Iteration 31, loss = 1.18394063\n",
            "Iteration 32, loss = 1.17085323\n",
            "Iteration 33, loss = 1.17165330\n",
            "Iteration 34, loss = 1.16535008\n",
            "Iteration 35, loss = 1.17089904\n",
            "Iteration 36, loss = 1.16251936\n",
            "Iteration 37, loss = 1.16940003\n",
            "Iteration 38, loss = 1.17365815\n",
            "Iteration 39, loss = 1.16389783\n",
            "Iteration 40, loss = 1.17551174\n",
            "Iteration 41, loss = 1.18446807\n",
            "Iteration 42, loss = 1.17047364\n",
            "Iteration 43, loss = 1.16032501\n",
            "Iteration 44, loss = 1.16237186\n",
            "Iteration 45, loss = 1.15389314\n",
            "Iteration 46, loss = 1.16485505\n",
            "Iteration 47, loss = 1.15582050\n",
            "Iteration 48, loss = 1.15695586\n",
            "Iteration 49, loss = 1.14599160\n",
            "Iteration 50, loss = 1.15123896\n",
            "Iteration 51, loss = 1.15583463\n",
            "Iteration 52, loss = 1.15691395\n",
            "Iteration 53, loss = 1.13733985\n",
            "Iteration 54, loss = 1.14259626\n",
            "Iteration 55, loss = 1.16125079\n",
            "Iteration 56, loss = 1.16291203\n",
            "Iteration 57, loss = 1.14591118\n",
            "Iteration 58, loss = 1.18252599\n",
            "Iteration 59, loss = 1.15407774\n",
            "Iteration 60, loss = 1.13524853\n",
            "Iteration 61, loss = 1.12835704\n",
            "Iteration 62, loss = 1.13612682\n",
            "Iteration 63, loss = 1.15472813\n",
            "Iteration 64, loss = 1.15096985\n",
            "Iteration 65, loss = 1.12901175\n",
            "Iteration 66, loss = 1.12320215\n",
            "Iteration 67, loss = 1.12914706\n",
            "Iteration 68, loss = 1.13199528\n",
            "Iteration 69, loss = 1.12056495\n",
            "Iteration 70, loss = 1.13501890\n",
            "Iteration 71, loss = 1.13577243\n",
            "Iteration 72, loss = 1.11839255\n",
            "Iteration 73, loss = 1.11539216\n",
            "Iteration 74, loss = 1.11545642\n",
            "Iteration 75, loss = 1.10062793\n",
            "Iteration 76, loss = 1.10676619\n",
            "Iteration 77, loss = 1.13530840\n",
            "Iteration 78, loss = 1.12992195\n",
            "Iteration 79, loss = 1.11162392\n",
            "Iteration 80, loss = 1.10980820\n",
            "Iteration 81, loss = 1.09217900\n",
            "Iteration 82, loss = 1.09251204\n",
            "Iteration 83, loss = 1.08903376\n",
            "Iteration 84, loss = 1.08741079\n",
            "Iteration 85, loss = 1.14324732\n",
            "Iteration 86, loss = 1.12064098\n",
            "Iteration 87, loss = 1.09893674\n",
            "Iteration 88, loss = 1.08429517\n",
            "Iteration 89, loss = 1.08449959\n",
            "Iteration 90, loss = 1.09585504\n",
            "Iteration 91, loss = 1.13255477\n",
            "Iteration 92, loss = 1.12848100\n",
            "Iteration 93, loss = 1.11471098\n",
            "Iteration 94, loss = 1.10139590\n",
            "Iteration 95, loss = 1.08631128\n",
            "Iteration 96, loss = 1.08296314\n",
            "Iteration 97, loss = 1.09376041\n",
            "Iteration 98, loss = 1.07722841\n",
            "Iteration 99, loss = 1.07481107\n",
            "Iteration 100, loss = 1.05098670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32780279\n",
            "Iteration 2, loss = 1.26730404\n",
            "Iteration 3, loss = 1.25144811\n",
            "Iteration 4, loss = 1.25477990\n",
            "Iteration 5, loss = 1.27054548\n",
            "Iteration 6, loss = 1.24574335\n",
            "Iteration 7, loss = 1.25107361\n",
            "Iteration 8, loss = 1.24601067\n",
            "Iteration 9, loss = 1.24727692\n",
            "Iteration 10, loss = 1.24435140\n",
            "Iteration 11, loss = 1.24225032\n",
            "Iteration 12, loss = 1.24167590\n",
            "Iteration 13, loss = 1.23701631\n",
            "Iteration 14, loss = 1.23134645\n",
            "Iteration 15, loss = 1.23503951\n",
            "Iteration 16, loss = 1.22664711\n",
            "Iteration 17, loss = 1.22255397\n",
            "Iteration 18, loss = 1.21966201\n",
            "Iteration 19, loss = 1.21913011\n",
            "Iteration 20, loss = 1.24099029\n",
            "Iteration 21, loss = 1.21557690\n",
            "Iteration 22, loss = 1.21375737\n",
            "Iteration 23, loss = 1.21350948\n",
            "Iteration 24, loss = 1.20360699\n",
            "Iteration 25, loss = 1.20359450\n",
            "Iteration 26, loss = 1.20573511\n",
            "Iteration 27, loss = 1.19490139\n",
            "Iteration 28, loss = 1.20464010\n",
            "Iteration 29, loss = 1.20968307\n",
            "Iteration 30, loss = 1.19971062\n",
            "Iteration 31, loss = 1.20235113\n",
            "Iteration 32, loss = 1.19106711\n",
            "Iteration 33, loss = 1.19868812\n",
            "Iteration 34, loss = 1.18578560\n",
            "Iteration 35, loss = 1.18697672\n",
            "Iteration 36, loss = 1.18542423\n",
            "Iteration 37, loss = 1.17681978\n",
            "Iteration 38, loss = 1.17961080\n",
            "Iteration 39, loss = 1.17050776\n",
            "Iteration 40, loss = 1.17519806\n",
            "Iteration 41, loss = 1.18508359\n",
            "Iteration 42, loss = 1.16924255\n",
            "Iteration 43, loss = 1.15839373\n",
            "Iteration 44, loss = 1.15755987\n",
            "Iteration 45, loss = 1.15758954\n",
            "Iteration 46, loss = 1.16416217\n",
            "Iteration 47, loss = 1.15291787\n",
            "Iteration 48, loss = 1.14867407\n",
            "Iteration 49, loss = 1.14790541\n",
            "Iteration 50, loss = 1.15024102\n",
            "Iteration 51, loss = 1.15071790\n",
            "Iteration 52, loss = 1.15457965\n",
            "Iteration 53, loss = 1.15260357\n",
            "Iteration 54, loss = 1.15068750\n",
            "Iteration 55, loss = 1.17448957\n",
            "Iteration 56, loss = 1.18204105\n",
            "Iteration 57, loss = 1.15485824\n",
            "Iteration 58, loss = 1.16120791\n",
            "Iteration 59, loss = 1.16299944\n",
            "Iteration 60, loss = 1.14364418\n",
            "Iteration 61, loss = 1.15077184\n",
            "Iteration 62, loss = 1.15008050\n",
            "Iteration 63, loss = 1.14894792\n",
            "Iteration 64, loss = 1.13208003\n",
            "Iteration 65, loss = 1.15755309\n",
            "Iteration 66, loss = 1.13435587\n",
            "Iteration 67, loss = 1.13171587\n",
            "Iteration 68, loss = 1.12314969\n",
            "Iteration 69, loss = 1.12082987\n",
            "Iteration 70, loss = 1.13553420\n",
            "Iteration 71, loss = 1.14677480\n",
            "Iteration 72, loss = 1.12426961\n",
            "Iteration 73, loss = 1.12074378\n",
            "Iteration 74, loss = 1.13818484\n",
            "Iteration 75, loss = 1.12422124\n",
            "Iteration 76, loss = 1.12223331\n",
            "Iteration 77, loss = 1.13925544\n",
            "Iteration 78, loss = 1.12095351\n",
            "Iteration 79, loss = 1.11703989\n",
            "Iteration 80, loss = 1.10852903\n",
            "Iteration 81, loss = 1.10006596\n",
            "Iteration 82, loss = 1.09765073\n",
            "Iteration 83, loss = 1.08997874\n",
            "Iteration 84, loss = 1.09187480\n",
            "Iteration 85, loss = 1.10885843\n",
            "Iteration 86, loss = 1.10706631\n",
            "Iteration 87, loss = 1.11324158\n",
            "Iteration 88, loss = 1.10909419\n",
            "Iteration 89, loss = 1.10155124\n",
            "Iteration 90, loss = 1.12145955\n",
            "Iteration 91, loss = 1.11092835\n",
            "Iteration 92, loss = 1.10013910\n",
            "Iteration 93, loss = 1.11963060\n",
            "Iteration 94, loss = 1.09862940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32828408\n",
            "Iteration 2, loss = 1.26892891\n",
            "Iteration 3, loss = 1.25460077\n",
            "Iteration 4, loss = 1.25671651\n",
            "Iteration 5, loss = 1.27188916\n",
            "Iteration 6, loss = 1.24901503\n",
            "Iteration 7, loss = 1.25495298\n",
            "Iteration 8, loss = 1.24519826\n",
            "Iteration 9, loss = 1.24850305\n",
            "Iteration 10, loss = 1.24511630\n",
            "Iteration 11, loss = 1.24794910\n",
            "Iteration 12, loss = 1.24547834\n",
            "Iteration 13, loss = 1.24443261\n",
            "Iteration 14, loss = 1.23608677\n",
            "Iteration 15, loss = 1.24513838\n",
            "Iteration 16, loss = 1.23359972\n",
            "Iteration 17, loss = 1.23127360\n",
            "Iteration 18, loss = 1.22631149\n",
            "Iteration 19, loss = 1.22249115\n",
            "Iteration 20, loss = 1.24003831\n",
            "Iteration 21, loss = 1.22158084\n",
            "Iteration 22, loss = 1.21854984\n",
            "Iteration 23, loss = 1.21359820\n",
            "Iteration 24, loss = 1.20680218\n",
            "Iteration 25, loss = 1.20963979\n",
            "Iteration 26, loss = 1.20586202\n",
            "Iteration 27, loss = 1.20397482\n",
            "Iteration 28, loss = 1.20763302\n",
            "Iteration 29, loss = 1.21632947\n",
            "Iteration 30, loss = 1.20097238\n",
            "Iteration 31, loss = 1.19849207\n",
            "Iteration 32, loss = 1.19844171\n",
            "Iteration 33, loss = 1.20214195\n",
            "Iteration 34, loss = 1.18538276\n",
            "Iteration 35, loss = 1.19221817\n",
            "Iteration 36, loss = 1.19772065\n",
            "Iteration 37, loss = 1.18560154\n",
            "Iteration 38, loss = 1.18732074\n",
            "Iteration 39, loss = 1.18009366\n",
            "Iteration 40, loss = 1.18306138\n",
            "Iteration 41, loss = 1.19001344\n",
            "Iteration 42, loss = 1.17559642\n",
            "Iteration 43, loss = 1.17072215\n",
            "Iteration 44, loss = 1.16600017\n",
            "Iteration 45, loss = 1.17480097\n",
            "Iteration 46, loss = 1.17772210\n",
            "Iteration 47, loss = 1.16640100\n",
            "Iteration 48, loss = 1.16439253\n",
            "Iteration 49, loss = 1.15818470\n",
            "Iteration 50, loss = 1.15937111\n",
            "Iteration 51, loss = 1.16113084\n",
            "Iteration 52, loss = 1.16135504\n",
            "Iteration 53, loss = 1.15976352\n",
            "Iteration 54, loss = 1.14947547\n",
            "Iteration 55, loss = 1.16664087\n",
            "Iteration 56, loss = 1.16523814\n",
            "Iteration 57, loss = 1.15535234\n",
            "Iteration 58, loss = 1.16225231\n",
            "Iteration 59, loss = 1.17186820\n",
            "Iteration 60, loss = 1.16034294\n",
            "Iteration 61, loss = 1.15294533\n",
            "Iteration 62, loss = 1.14710086\n",
            "Iteration 63, loss = 1.15204180\n",
            "Iteration 64, loss = 1.13487437\n",
            "Iteration 65, loss = 1.13218996\n",
            "Iteration 66, loss = 1.12515876\n",
            "Iteration 67, loss = 1.12444240\n",
            "Iteration 68, loss = 1.13591376\n",
            "Iteration 69, loss = 1.12963242\n",
            "Iteration 70, loss = 1.13865723\n",
            "Iteration 71, loss = 1.14417768\n",
            "Iteration 72, loss = 1.11236985\n",
            "Iteration 73, loss = 1.12438556\n",
            "Iteration 74, loss = 1.12445683\n",
            "Iteration 75, loss = 1.11062447\n",
            "Iteration 76, loss = 1.12510515\n",
            "Iteration 77, loss = 1.12443273\n",
            "Iteration 78, loss = 1.11845731\n",
            "Iteration 79, loss = 1.10178638\n",
            "Iteration 80, loss = 1.11072917\n",
            "Iteration 81, loss = 1.11164348\n",
            "Iteration 82, loss = 1.09749564\n",
            "Iteration 83, loss = 1.09223792\n",
            "Iteration 84, loss = 1.09346632\n",
            "Iteration 85, loss = 1.10790472\n",
            "Iteration 86, loss = 1.09240855\n",
            "Iteration 87, loss = 1.12503704\n",
            "Iteration 88, loss = 1.11292888\n",
            "Iteration 89, loss = 1.09360741\n",
            "Iteration 90, loss = 1.11441927\n",
            "Iteration 91, loss = 1.17391462\n",
            "Iteration 92, loss = 1.14242399\n",
            "Iteration 93, loss = 1.14593540\n",
            "Iteration 94, loss = 1.11983672\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32612519\n",
            "Iteration 2, loss = 1.26150857\n",
            "Iteration 3, loss = 1.24453507\n",
            "Iteration 4, loss = 1.24737208\n",
            "Iteration 5, loss = 1.26311933\n",
            "Iteration 6, loss = 1.24116729\n",
            "Iteration 7, loss = 1.24311249\n",
            "Iteration 8, loss = 1.23295884\n",
            "Iteration 9, loss = 1.23732243\n",
            "Iteration 10, loss = 1.23270440\n",
            "Iteration 11, loss = 1.27230564\n",
            "Iteration 12, loss = 1.23492564\n",
            "Iteration 13, loss = 1.23853331\n",
            "Iteration 14, loss = 1.23243166\n",
            "Iteration 15, loss = 1.22767785\n",
            "Iteration 16, loss = 1.21842335\n",
            "Iteration 17, loss = 1.21136070\n",
            "Iteration 18, loss = 1.21049310\n",
            "Iteration 19, loss = 1.20551410\n",
            "Iteration 20, loss = 1.20748185\n",
            "Iteration 21, loss = 1.20560240\n",
            "Iteration 22, loss = 1.20680745\n",
            "Iteration 23, loss = 1.19554020\n",
            "Iteration 24, loss = 1.18620373\n",
            "Iteration 25, loss = 1.19201041\n",
            "Iteration 26, loss = 1.19043374\n",
            "Iteration 27, loss = 1.18356552\n",
            "Iteration 28, loss = 1.18767461\n",
            "Iteration 29, loss = 1.19585520\n",
            "Iteration 30, loss = 1.18514972\n",
            "Iteration 31, loss = 1.18908110\n",
            "Iteration 32, loss = 1.18102121\n",
            "Iteration 33, loss = 1.18569617\n",
            "Iteration 34, loss = 1.16927907\n",
            "Iteration 35, loss = 1.16657441\n",
            "Iteration 36, loss = 1.17899738\n",
            "Iteration 37, loss = 1.16486727\n",
            "Iteration 38, loss = 1.16391380\n",
            "Iteration 39, loss = 1.15914590\n",
            "Iteration 40, loss = 1.16581379\n",
            "Iteration 41, loss = 1.16596554\n",
            "Iteration 42, loss = 1.16040841\n",
            "Iteration 43, loss = 1.16004117\n",
            "Iteration 44, loss = 1.15900566\n",
            "Iteration 45, loss = 1.16764458\n",
            "Iteration 46, loss = 1.15451870\n",
            "Iteration 47, loss = 1.15048306\n",
            "Iteration 48, loss = 1.14400749\n",
            "Iteration 49, loss = 1.14498257\n",
            "Iteration 50, loss = 1.16214703\n",
            "Iteration 51, loss = 1.16343126\n",
            "Iteration 52, loss = 1.15496402\n",
            "Iteration 53, loss = 1.15149730\n",
            "Iteration 54, loss = 1.14772363\n",
            "Iteration 55, loss = 1.14935323\n",
            "Iteration 56, loss = 1.14842316\n",
            "Iteration 57, loss = 1.14195104\n",
            "Iteration 58, loss = 1.14332628\n",
            "Iteration 59, loss = 1.14826461\n",
            "Iteration 60, loss = 1.14213052\n",
            "Iteration 61, loss = 1.15464605\n",
            "Iteration 62, loss = 1.14927856\n",
            "Iteration 63, loss = 1.14257531\n",
            "Iteration 64, loss = 1.12043372\n",
            "Iteration 65, loss = 1.12188934\n",
            "Iteration 66, loss = 1.11732711\n",
            "Iteration 67, loss = 1.10780505\n",
            "Iteration 68, loss = 1.10800955\n",
            "Iteration 69, loss = 1.10929149\n",
            "Iteration 70, loss = 1.11815025\n",
            "Iteration 71, loss = 1.11433873\n",
            "Iteration 72, loss = 1.10576315\n",
            "Iteration 73, loss = 1.10579123\n",
            "Iteration 74, loss = 1.11896857\n",
            "Iteration 75, loss = 1.10677257\n",
            "Iteration 76, loss = 1.11502700\n",
            "Iteration 77, loss = 1.10845701\n",
            "Iteration 78, loss = 1.09427703\n",
            "Iteration 79, loss = 1.09246992\n",
            "Iteration 80, loss = 1.09037808\n",
            "Iteration 81, loss = 1.11976374\n",
            "Iteration 82, loss = 1.12131055\n",
            "Iteration 83, loss = 1.10957501\n",
            "Iteration 84, loss = 1.09117986\n",
            "Iteration 85, loss = 1.09699370\n",
            "Iteration 86, loss = 1.08687507\n",
            "Iteration 87, loss = 1.09318217\n",
            "Iteration 88, loss = 1.11011876\n",
            "Iteration 89, loss = 1.09627662\n",
            "Iteration 90, loss = 1.10064418\n",
            "Iteration 91, loss = 1.10545786\n",
            "Iteration 92, loss = 1.09052448\n",
            "Iteration 93, loss = 1.09772006\n",
            "Iteration 94, loss = 1.08692160\n",
            "Iteration 95, loss = 1.08438127\n",
            "Iteration 96, loss = 1.07659729\n",
            "Iteration 97, loss = 1.07337229\n",
            "Iteration 98, loss = 1.06169272\n",
            "Iteration 99, loss = 1.07562916\n",
            "Iteration 100, loss = 1.06814317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32172145\n",
            "Iteration 2, loss = 1.25059897\n",
            "Iteration 3, loss = 1.23653218\n",
            "Iteration 4, loss = 1.24070145\n",
            "Iteration 5, loss = 1.24707778\n",
            "Iteration 6, loss = 1.23130354\n",
            "Iteration 7, loss = 1.23587169\n",
            "Iteration 8, loss = 1.22682939\n",
            "Iteration 9, loss = 1.22718886\n",
            "Iteration 10, loss = 1.22365276\n",
            "Iteration 11, loss = 1.24221253\n",
            "Iteration 12, loss = 1.22457567\n",
            "Iteration 13, loss = 1.21851781\n",
            "Iteration 14, loss = 1.22399699\n",
            "Iteration 15, loss = 1.21697314\n",
            "Iteration 16, loss = 1.20945968\n",
            "Iteration 17, loss = 1.20985457\n",
            "Iteration 18, loss = 1.21429804\n",
            "Iteration 19, loss = 1.20740307\n",
            "Iteration 20, loss = 1.20672212\n",
            "Iteration 21, loss = 1.20624942\n",
            "Iteration 22, loss = 1.19869010\n",
            "Iteration 23, loss = 1.19481956\n",
            "Iteration 24, loss = 1.20251830\n",
            "Iteration 25, loss = 1.19394187\n",
            "Iteration 26, loss = 1.18624563\n",
            "Iteration 27, loss = 1.18895606\n",
            "Iteration 28, loss = 1.18914586\n",
            "Iteration 29, loss = 1.18951609\n",
            "Iteration 30, loss = 1.18332854\n",
            "Iteration 31, loss = 1.18578494\n",
            "Iteration 32, loss = 1.18335856\n",
            "Iteration 33, loss = 1.17854406\n",
            "Iteration 34, loss = 1.17748325\n",
            "Iteration 35, loss = 1.18205263\n",
            "Iteration 36, loss = 1.18742074\n",
            "Iteration 37, loss = 1.16669218\n",
            "Iteration 38, loss = 1.16701835\n",
            "Iteration 39, loss = 1.16215128\n",
            "Iteration 40, loss = 1.17840520\n",
            "Iteration 41, loss = 1.17266669\n",
            "Iteration 42, loss = 1.15968518\n",
            "Iteration 43, loss = 1.16010234\n",
            "Iteration 44, loss = 1.15520676\n",
            "Iteration 45, loss = 1.16484798\n",
            "Iteration 46, loss = 1.17696163\n",
            "Iteration 47, loss = 1.16061314\n",
            "Iteration 48, loss = 1.16965884\n",
            "Iteration 49, loss = 1.15648260\n",
            "Iteration 50, loss = 1.15543821\n",
            "Iteration 51, loss = 1.15410513\n",
            "Iteration 52, loss = 1.14948172\n",
            "Iteration 53, loss = 1.17263325\n",
            "Iteration 54, loss = 1.17341126\n",
            "Iteration 55, loss = 1.15411336\n",
            "Iteration 56, loss = 1.14926763\n",
            "Iteration 57, loss = 1.15320145\n",
            "Iteration 58, loss = 1.15479757\n",
            "Iteration 59, loss = 1.18834369\n",
            "Iteration 60, loss = 1.16088896\n",
            "Iteration 61, loss = 1.14645383\n",
            "Iteration 62, loss = 1.14613673\n",
            "Iteration 63, loss = 1.15129092\n",
            "Iteration 64, loss = 1.13579075\n",
            "Iteration 65, loss = 1.13069027\n",
            "Iteration 66, loss = 1.12608858\n",
            "Iteration 67, loss = 1.11892861\n",
            "Iteration 68, loss = 1.12336286\n",
            "Iteration 69, loss = 1.11922338\n",
            "Iteration 70, loss = 1.14407777\n",
            "Iteration 71, loss = 1.12996980\n",
            "Iteration 72, loss = 1.11996538\n",
            "Iteration 73, loss = 1.11240210\n",
            "Iteration 74, loss = 1.11048291\n",
            "Iteration 75, loss = 1.11358354\n",
            "Iteration 76, loss = 1.10679871\n",
            "Iteration 77, loss = 1.10909368\n",
            "Iteration 78, loss = 1.10594144\n",
            "Iteration 79, loss = 1.09043356\n",
            "Iteration 80, loss = 1.09486436\n",
            "Iteration 81, loss = 1.11196828\n",
            "Iteration 82, loss = 1.13444305\n",
            "Iteration 83, loss = 1.13341176\n",
            "Iteration 84, loss = 1.11059177\n",
            "Iteration 85, loss = 1.10894143\n",
            "Iteration 86, loss = 1.10156356\n",
            "Iteration 87, loss = 1.09771301\n",
            "Iteration 88, loss = 1.09765066\n",
            "Iteration 89, loss = 1.09581717\n",
            "Iteration 90, loss = 1.09452649\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33064904\n",
            "Iteration 2, loss = 1.26026426\n",
            "Iteration 3, loss = 1.24282242\n",
            "Iteration 4, loss = 1.23948293\n",
            "Iteration 5, loss = 1.24891179\n",
            "Iteration 6, loss = 1.23216524\n",
            "Iteration 7, loss = 1.23471173\n",
            "Iteration 8, loss = 1.22723445\n",
            "Iteration 9, loss = 1.22869919\n",
            "Iteration 10, loss = 1.22543619\n",
            "Iteration 11, loss = 1.22829812\n",
            "Iteration 12, loss = 1.22328094\n",
            "Iteration 13, loss = 1.22962674\n",
            "Iteration 14, loss = 1.21758161\n",
            "Iteration 15, loss = 1.21172312\n",
            "Iteration 16, loss = 1.22300429\n",
            "Iteration 17, loss = 1.20519667\n",
            "Iteration 18, loss = 1.20185308\n",
            "Iteration 19, loss = 1.20113374\n",
            "Iteration 20, loss = 1.19683108\n",
            "Iteration 21, loss = 1.22119214\n",
            "Iteration 22, loss = 1.22520770\n",
            "Iteration 23, loss = 1.20853264\n",
            "Iteration 24, loss = 1.21740944\n",
            "Iteration 25, loss = 1.19881354\n",
            "Iteration 26, loss = 1.19288427\n",
            "Iteration 27, loss = 1.19092705\n",
            "Iteration 28, loss = 1.18903240\n",
            "Iteration 29, loss = 1.20025433\n",
            "Iteration 30, loss = 1.19973806\n",
            "Iteration 31, loss = 1.20904230\n",
            "Iteration 32, loss = 1.18799818\n",
            "Iteration 33, loss = 1.17880742\n",
            "Iteration 34, loss = 1.17887490\n",
            "Iteration 35, loss = 1.17341628\n",
            "Iteration 36, loss = 1.17600336\n",
            "Iteration 37, loss = 1.16560199\n",
            "Iteration 38, loss = 1.16441449\n",
            "Iteration 39, loss = 1.15421536\n",
            "Iteration 40, loss = 1.16593211\n",
            "Iteration 41, loss = 1.16609308\n",
            "Iteration 42, loss = 1.15173598\n",
            "Iteration 43, loss = 1.16333592\n",
            "Iteration 44, loss = 1.15627362\n",
            "Iteration 45, loss = 1.16491650\n",
            "Iteration 46, loss = 1.15395001\n",
            "Iteration 47, loss = 1.15350181\n",
            "Iteration 48, loss = 1.14276679\n",
            "Iteration 49, loss = 1.14488733\n",
            "Iteration 50, loss = 1.15848454\n",
            "Iteration 51, loss = 1.15520872\n",
            "Iteration 52, loss = 1.14669806\n",
            "Iteration 53, loss = 1.14829035\n",
            "Iteration 54, loss = 1.15201320\n",
            "Iteration 55, loss = 1.13793549\n",
            "Iteration 56, loss = 1.14952997\n",
            "Iteration 57, loss = 1.14088928\n",
            "Iteration 58, loss = 1.13629272\n",
            "Iteration 59, loss = 1.17724871\n",
            "Iteration 60, loss = 1.16062021\n",
            "Iteration 61, loss = 1.13034700\n",
            "Iteration 62, loss = 1.13903947\n",
            "Iteration 63, loss = 1.13114198\n",
            "Iteration 64, loss = 1.12174057\n",
            "Iteration 65, loss = 1.12330782\n",
            "Iteration 66, loss = 1.13684721\n",
            "Iteration 67, loss = 1.12026001\n",
            "Iteration 68, loss = 1.12095806\n",
            "Iteration 69, loss = 1.12777239\n",
            "Iteration 70, loss = 1.10961059\n",
            "Iteration 71, loss = 1.12371760\n",
            "Iteration 72, loss = 1.11110221\n",
            "Iteration 73, loss = 1.10156534\n",
            "Iteration 74, loss = 1.10146371\n",
            "Iteration 75, loss = 1.10222805\n",
            "Iteration 76, loss = 1.10617312\n",
            "Iteration 77, loss = 1.09237847\n",
            "Iteration 78, loss = 1.09776618\n",
            "Iteration 79, loss = 1.09531200\n",
            "Iteration 80, loss = 1.11128154\n",
            "Iteration 81, loss = 1.10578582\n",
            "Iteration 82, loss = 1.11449209\n",
            "Iteration 83, loss = 1.12452000\n",
            "Iteration 84, loss = 1.10114935\n",
            "Iteration 85, loss = 1.10002454\n",
            "Iteration 86, loss = 1.08881642\n",
            "Iteration 87, loss = 1.09024413\n",
            "Iteration 88, loss = 1.08768471\n",
            "Iteration 89, loss = 1.08457299\n",
            "Iteration 90, loss = 1.08970882\n",
            "Iteration 91, loss = 1.10400285\n",
            "Iteration 92, loss = 1.07516693\n",
            "Iteration 93, loss = 1.07545746\n",
            "Iteration 94, loss = 1.08370842\n",
            "Iteration 95, loss = 1.08499273\n",
            "Iteration 96, loss = 1.07229355\n",
            "Iteration 97, loss = 1.09378565\n",
            "Iteration 98, loss = 1.07025756\n",
            "Iteration 99, loss = 1.08010840\n",
            "Iteration 100, loss = 1.07533169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32811954\n",
            "Iteration 2, loss = 1.25324038\n",
            "Iteration 3, loss = 1.26016861\n",
            "Iteration 4, loss = 1.24034795\n",
            "Iteration 5, loss = 1.23660764\n",
            "Iteration 6, loss = 1.23510232\n",
            "Iteration 7, loss = 1.23090538\n",
            "Iteration 8, loss = 1.23059101\n",
            "Iteration 9, loss = 1.22603087\n",
            "Iteration 10, loss = 1.22373093\n",
            "Iteration 11, loss = 1.21731208\n",
            "Iteration 12, loss = 1.21862360\n",
            "Iteration 13, loss = 1.21158329\n",
            "Iteration 14, loss = 1.21320382\n",
            "Iteration 15, loss = 1.21209859\n",
            "Iteration 16, loss = 1.21856923\n",
            "Iteration 17, loss = 1.22937854\n",
            "Iteration 18, loss = 1.21090261\n",
            "Iteration 19, loss = 1.19615316\n",
            "Iteration 20, loss = 1.19578317\n",
            "Iteration 21, loss = 1.18988001\n",
            "Iteration 22, loss = 1.19230310\n",
            "Iteration 23, loss = 1.19624825\n",
            "Iteration 24, loss = 1.18852085\n",
            "Iteration 25, loss = 1.17925387\n",
            "Iteration 26, loss = 1.17658364\n",
            "Iteration 27, loss = 1.16964393\n",
            "Iteration 28, loss = 1.18432495\n",
            "Iteration 29, loss = 1.16990930\n",
            "Iteration 30, loss = 1.18800804\n",
            "Iteration 31, loss = 1.17264221\n",
            "Iteration 32, loss = 1.16288885\n",
            "Iteration 33, loss = 1.16006771\n",
            "Iteration 34, loss = 1.15950237\n",
            "Iteration 35, loss = 1.16543570\n",
            "Iteration 36, loss = 1.16208508\n",
            "Iteration 37, loss = 1.15991963\n",
            "Iteration 38, loss = 1.15941402\n",
            "Iteration 39, loss = 1.15740245\n",
            "Iteration 40, loss = 1.15062125\n",
            "Iteration 41, loss = 1.15628952\n",
            "Iteration 42, loss = 1.16004909\n",
            "Iteration 43, loss = 1.15135885\n",
            "Iteration 44, loss = 1.15948924\n",
            "Iteration 45, loss = 1.15747436\n",
            "Iteration 46, loss = 1.15868865\n",
            "Iteration 47, loss = 1.14719441\n",
            "Iteration 48, loss = 1.13869585\n",
            "Iteration 49, loss = 1.17749188\n",
            "Iteration 50, loss = 1.16121027\n",
            "Iteration 51, loss = 1.17538747\n",
            "Iteration 52, loss = 1.13626868\n",
            "Iteration 53, loss = 1.14111017\n",
            "Iteration 54, loss = 1.13620200\n",
            "Iteration 55, loss = 1.13579163\n",
            "Iteration 56, loss = 1.13139444\n",
            "Iteration 57, loss = 1.15111406\n",
            "Iteration 58, loss = 1.17170977\n",
            "Iteration 59, loss = 1.15449431\n",
            "Iteration 60, loss = 1.13570746\n",
            "Iteration 61, loss = 1.14436993\n",
            "Iteration 62, loss = 1.13134895\n",
            "Iteration 63, loss = 1.14099975\n",
            "Iteration 64, loss = 1.13182111\n",
            "Iteration 65, loss = 1.12789965\n",
            "Iteration 66, loss = 1.13071946\n",
            "Iteration 67, loss = 1.11905357\n",
            "Iteration 68, loss = 1.11762326\n",
            "Iteration 69, loss = 1.11431600\n",
            "Iteration 70, loss = 1.13800993\n",
            "Iteration 71, loss = 1.10448558\n",
            "Iteration 72, loss = 1.11251170\n",
            "Iteration 73, loss = 1.10221112\n",
            "Iteration 74, loss = 1.10090830\n",
            "Iteration 75, loss = 1.12026845\n",
            "Iteration 76, loss = 1.14562101\n",
            "Iteration 77, loss = 1.11627492\n",
            "Iteration 78, loss = 1.10030473\n",
            "Iteration 79, loss = 1.11098393\n",
            "Iteration 80, loss = 1.13766157\n",
            "Iteration 81, loss = 1.12130363\n",
            "Iteration 82, loss = 1.10089563\n",
            "Iteration 83, loss = 1.09319741\n",
            "Iteration 84, loss = 1.10881981\n",
            "Iteration 85, loss = 1.10683493\n",
            "Iteration 86, loss = 1.08331795\n",
            "Iteration 87, loss = 1.08010533\n",
            "Iteration 88, loss = 1.09880137\n",
            "Iteration 89, loss = 1.08601305\n",
            "Iteration 90, loss = 1.09096324\n",
            "Iteration 91, loss = 1.13935433\n",
            "Iteration 92, loss = 1.09583619\n",
            "Iteration 93, loss = 1.08165659\n",
            "Iteration 94, loss = 1.07353154\n",
            "Iteration 95, loss = 1.08854240\n",
            "Iteration 96, loss = 1.12678670\n",
            "Iteration 97, loss = 1.06339001\n",
            "Iteration 98, loss = 1.06491752\n",
            "Iteration 99, loss = 1.08021521\n",
            "Iteration 100, loss = 1.06243752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32640215\n",
            "Iteration 2, loss = 1.24489414\n",
            "Iteration 3, loss = 1.24921334\n",
            "Iteration 4, loss = 1.24005932\n",
            "Iteration 5, loss = 1.23785917\n",
            "Iteration 6, loss = 1.23484621\n",
            "Iteration 7, loss = 1.24065665\n",
            "Iteration 8, loss = 1.23309434\n",
            "Iteration 9, loss = 1.23120598\n",
            "Iteration 10, loss = 1.23521026\n",
            "Iteration 11, loss = 1.22166683\n",
            "Iteration 12, loss = 1.23141284\n",
            "Iteration 13, loss = 1.22860683\n",
            "Iteration 14, loss = 1.22661771\n",
            "Iteration 15, loss = 1.22079533\n",
            "Iteration 16, loss = 1.21937568\n",
            "Iteration 17, loss = 1.21761768\n",
            "Iteration 18, loss = 1.21423628\n",
            "Iteration 19, loss = 1.21268316\n",
            "Iteration 20, loss = 1.20766542\n",
            "Iteration 21, loss = 1.21136748\n",
            "Iteration 22, loss = 1.20766434\n",
            "Iteration 23, loss = 1.22228445\n",
            "Iteration 24, loss = 1.20872943\n",
            "Iteration 25, loss = 1.20016487\n",
            "Iteration 26, loss = 1.19937576\n",
            "Iteration 27, loss = 1.19257653\n",
            "Iteration 28, loss = 1.20662977\n",
            "Iteration 29, loss = 1.19190191\n",
            "Iteration 30, loss = 1.18967266\n",
            "Iteration 31, loss = 1.18318307\n",
            "Iteration 32, loss = 1.18967234\n",
            "Iteration 33, loss = 1.18011406\n",
            "Iteration 34, loss = 1.17741258\n",
            "Iteration 35, loss = 1.18063599\n",
            "Iteration 36, loss = 1.18375300\n",
            "Iteration 37, loss = 1.18393887\n",
            "Iteration 38, loss = 1.17730922\n",
            "Iteration 39, loss = 1.17977642\n",
            "Iteration 40, loss = 1.17243283\n",
            "Iteration 41, loss = 1.17796954\n",
            "Iteration 42, loss = 1.17605666\n",
            "Iteration 43, loss = 1.16455828\n",
            "Iteration 44, loss = 1.17066824\n",
            "Iteration 45, loss = 1.16830050\n",
            "Iteration 46, loss = 1.17765717\n",
            "Iteration 47, loss = 1.16352607\n",
            "Iteration 48, loss = 1.16457429\n",
            "Iteration 49, loss = 1.17777415\n",
            "Iteration 50, loss = 1.16043141\n",
            "Iteration 51, loss = 1.16262223\n",
            "Iteration 52, loss = 1.15289695\n",
            "Iteration 53, loss = 1.15984723\n",
            "Iteration 54, loss = 1.15609166\n",
            "Iteration 55, loss = 1.16467315\n",
            "Iteration 56, loss = 1.15462039\n",
            "Iteration 57, loss = 1.15772552\n",
            "Iteration 58, loss = 1.16960525\n",
            "Iteration 59, loss = 1.15393555\n",
            "Iteration 60, loss = 1.14048755\n",
            "Iteration 61, loss = 1.14848663\n",
            "Iteration 62, loss = 1.14896815\n",
            "Iteration 63, loss = 1.14433627\n",
            "Iteration 64, loss = 1.18234445\n",
            "Iteration 65, loss = 1.17966372\n",
            "Iteration 66, loss = 1.16330617\n",
            "Iteration 67, loss = 1.15666674\n",
            "Iteration 68, loss = 1.14516589\n",
            "Iteration 69, loss = 1.14823581\n",
            "Iteration 70, loss = 1.17539591\n",
            "Iteration 71, loss = 1.15744004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31144024\n",
            "Iteration 2, loss = 1.23291850\n",
            "Iteration 3, loss = 1.23483624\n",
            "Iteration 4, loss = 1.22747153\n",
            "Iteration 5, loss = 1.22133234\n",
            "Iteration 6, loss = 1.22071605\n",
            "Iteration 7, loss = 1.22373513\n",
            "Iteration 8, loss = 1.21684768\n",
            "Iteration 9, loss = 1.21575373\n",
            "Iteration 10, loss = 1.22145709\n",
            "Iteration 11, loss = 1.20764627\n",
            "Iteration 12, loss = 1.21158546\n",
            "Iteration 13, loss = 1.21249083\n",
            "Iteration 14, loss = 1.21218414\n",
            "Iteration 15, loss = 1.20191947\n",
            "Iteration 16, loss = 1.20699766\n",
            "Iteration 17, loss = 1.21137889\n",
            "Iteration 18, loss = 1.20381228\n",
            "Iteration 19, loss = 1.19207280\n",
            "Iteration 20, loss = 1.19593341\n",
            "Iteration 21, loss = 1.19242939\n",
            "Iteration 22, loss = 1.19307656\n",
            "Iteration 23, loss = 1.19499534\n",
            "Iteration 24, loss = 1.18603316\n",
            "Iteration 25, loss = 1.18625607\n",
            "Iteration 26, loss = 1.18257311\n",
            "Iteration 27, loss = 1.17251631\n",
            "Iteration 28, loss = 1.18592141\n",
            "Iteration 29, loss = 1.17921870\n",
            "Iteration 30, loss = 1.16878322\n",
            "Iteration 31, loss = 1.16420318\n",
            "Iteration 32, loss = 1.15939712\n",
            "Iteration 33, loss = 1.15910336\n",
            "Iteration 34, loss = 1.15589261\n",
            "Iteration 35, loss = 1.15347663\n",
            "Iteration 36, loss = 1.17343784\n",
            "Iteration 37, loss = 1.15649911\n",
            "Iteration 38, loss = 1.14928034\n",
            "Iteration 39, loss = 1.15006885\n",
            "Iteration 40, loss = 1.14550535\n",
            "Iteration 41, loss = 1.15949154\n",
            "Iteration 42, loss = 1.14970967\n",
            "Iteration 43, loss = 1.14068006\n",
            "Iteration 44, loss = 1.17924737\n",
            "Iteration 45, loss = 1.15223219\n",
            "Iteration 46, loss = 1.15041628\n",
            "Iteration 47, loss = 1.14781033\n",
            "Iteration 48, loss = 1.14955695\n",
            "Iteration 49, loss = 1.16343700\n",
            "Iteration 50, loss = 1.13409665\n",
            "Iteration 51, loss = 1.13571698\n",
            "Iteration 52, loss = 1.13231287\n",
            "Iteration 53, loss = 1.12660985\n",
            "Iteration 54, loss = 1.12595450\n",
            "Iteration 55, loss = 1.13260330\n",
            "Iteration 56, loss = 1.13496622\n",
            "Iteration 57, loss = 1.12819933\n",
            "Iteration 58, loss = 1.14415010\n",
            "Iteration 59, loss = 1.14074980\n",
            "Iteration 60, loss = 1.12101459\n",
            "Iteration 61, loss = 1.12326571\n",
            "Iteration 62, loss = 1.12005585\n",
            "Iteration 63, loss = 1.11924520\n",
            "Iteration 64, loss = 1.14459926\n",
            "Iteration 65, loss = 1.13614037\n",
            "Iteration 66, loss = 1.13653123\n",
            "Iteration 67, loss = 1.12645943\n",
            "Iteration 68, loss = 1.11508241\n",
            "Iteration 69, loss = 1.11117316\n",
            "Iteration 70, loss = 1.12801828\n",
            "Iteration 71, loss = 1.11127075\n",
            "Iteration 72, loss = 1.10939950\n",
            "Iteration 73, loss = 1.10501123\n",
            "Iteration 74, loss = 1.11697166\n",
            "Iteration 75, loss = 1.10795851\n",
            "Iteration 76, loss = 1.11522974\n",
            "Iteration 77, loss = 1.10310159\n",
            "Iteration 78, loss = 1.10279117\n",
            "Iteration 79, loss = 1.10630536\n",
            "Iteration 80, loss = 1.11074842\n",
            "Iteration 81, loss = 1.10639032\n",
            "Iteration 82, loss = 1.10091152\n",
            "Iteration 83, loss = 1.10114393\n",
            "Iteration 84, loss = 1.10782723\n",
            "Iteration 85, loss = 1.11480667\n",
            "Iteration 86, loss = 1.09652735\n",
            "Iteration 87, loss = 1.09292800\n",
            "Iteration 88, loss = 1.10050695\n",
            "Iteration 89, loss = 1.10257389\n",
            "Iteration 90, loss = 1.08633622\n",
            "Iteration 91, loss = 1.10379356\n",
            "Iteration 92, loss = 1.09505367\n",
            "Iteration 93, loss = 1.09757601\n",
            "Iteration 94, loss = 1.07754914\n",
            "Iteration 95, loss = 1.08973185\n",
            "Iteration 96, loss = 1.08881117\n",
            "Iteration 97, loss = 1.06911896\n",
            "Iteration 98, loss = 1.09271643\n",
            "Iteration 99, loss = 1.09376448\n",
            "Iteration 100, loss = 1.07247434\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30885264\n",
            "Iteration 2, loss = 1.24683234\n",
            "Iteration 3, loss = 1.22776710\n",
            "Iteration 4, loss = 1.22390464\n",
            "Iteration 5, loss = 1.22374359\n",
            "Iteration 6, loss = 1.19620511\n",
            "Iteration 7, loss = 1.18550513\n",
            "Iteration 8, loss = 1.16223973\n",
            "Iteration 9, loss = 1.11768977\n",
            "Iteration 10, loss = 1.09668595\n",
            "Iteration 11, loss = 1.09683681\n",
            "Iteration 12, loss = 1.05204779\n",
            "Iteration 13, loss = 1.02923018\n",
            "Iteration 14, loss = 1.00847089\n",
            "Iteration 15, loss = 0.97705926\n",
            "Iteration 16, loss = 0.97683899\n",
            "Iteration 17, loss = 0.96428484\n",
            "Iteration 18, loss = 0.93132731\n",
            "Iteration 19, loss = 0.92006387\n",
            "Iteration 20, loss = 0.90758047\n",
            "Iteration 21, loss = 0.93185984\n",
            "Iteration 22, loss = 0.95748554\n",
            "Iteration 23, loss = 0.92669016\n",
            "Iteration 24, loss = 0.88226708\n",
            "Iteration 25, loss = 0.85752384\n",
            "Iteration 26, loss = 0.85114847\n",
            "Iteration 27, loss = 0.84382548\n",
            "Iteration 28, loss = 0.83090514\n",
            "Iteration 29, loss = 0.86695363\n",
            "Iteration 30, loss = 0.82558599\n",
            "Iteration 31, loss = 0.81588380\n",
            "Iteration 32, loss = 0.79225385\n",
            "Iteration 33, loss = 0.80625070\n",
            "Iteration 34, loss = 0.78790156\n",
            "Iteration 35, loss = 0.83044474\n",
            "Iteration 36, loss = 0.77718239\n",
            "Iteration 37, loss = 0.76794494\n",
            "Iteration 38, loss = 0.74309938\n",
            "Iteration 39, loss = 0.73774488\n",
            "Iteration 40, loss = 0.74461211\n",
            "Iteration 41, loss = 0.76680445\n",
            "Iteration 42, loss = 0.74297334\n",
            "Iteration 43, loss = 0.74729329\n",
            "Iteration 44, loss = 0.70269481\n",
            "Iteration 45, loss = 0.69760908\n",
            "Iteration 46, loss = 0.69445551\n",
            "Iteration 47, loss = 0.69155523\n",
            "Iteration 48, loss = 0.68276156\n",
            "Iteration 49, loss = 0.68115327\n",
            "Iteration 50, loss = 0.68399816\n",
            "Iteration 51, loss = 0.68589142\n",
            "Iteration 52, loss = 0.67675300\n",
            "Iteration 53, loss = 0.66890130\n",
            "Iteration 54, loss = 0.68380492\n",
            "Iteration 55, loss = 0.69010525\n",
            "Iteration 56, loss = 0.72079930\n",
            "Iteration 57, loss = 0.71743031\n",
            "Iteration 58, loss = 0.70177610\n",
            "Iteration 59, loss = 0.67697783\n",
            "Iteration 60, loss = 0.65862018\n",
            "Iteration 61, loss = 0.65632793\n",
            "Iteration 62, loss = 0.66038246\n",
            "Iteration 63, loss = 0.65458727\n",
            "Iteration 64, loss = 0.64227791\n",
            "Iteration 65, loss = 0.62693660\n",
            "Iteration 66, loss = 0.61494095\n",
            "Iteration 67, loss = 0.61913644\n",
            "Iteration 68, loss = 0.64333594\n",
            "Iteration 69, loss = 0.63910226\n",
            "Iteration 70, loss = 0.62815264\n",
            "Iteration 71, loss = 0.61224715\n",
            "Iteration 72, loss = 0.60335280\n",
            "Iteration 73, loss = 0.60355264\n",
            "Iteration 74, loss = 0.60402157\n",
            "Iteration 75, loss = 0.60908821\n",
            "Iteration 76, loss = 0.58308937\n",
            "Iteration 77, loss = 0.63527489\n",
            "Iteration 78, loss = 0.62189538\n",
            "Iteration 79, loss = 0.58108023\n",
            "Iteration 80, loss = 0.56846043\n",
            "Iteration 81, loss = 0.55716723\n",
            "Iteration 82, loss = 0.56625554\n",
            "Iteration 83, loss = 0.57197014\n",
            "Iteration 84, loss = 0.54956018\n",
            "Iteration 85, loss = 0.54732649\n",
            "Iteration 86, loss = 0.57898957\n",
            "Iteration 87, loss = 0.59969690\n",
            "Iteration 88, loss = 0.60640782\n",
            "Iteration 89, loss = 0.58964079\n",
            "Iteration 90, loss = 0.58581717\n",
            "Iteration 91, loss = 0.60693307\n",
            "Iteration 92, loss = 0.61775394\n",
            "Iteration 93, loss = 0.58082079\n",
            "Iteration 94, loss = 0.58385223\n",
            "Iteration 95, loss = 0.54396840\n",
            "Iteration 96, loss = 0.52258626\n",
            "Iteration 97, loss = 0.51442287\n",
            "Iteration 98, loss = 0.50304021\n",
            "Iteration 99, loss = 0.50970983\n",
            "Iteration 100, loss = 0.51817530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30591540\n",
            "Iteration 2, loss = 1.24300090\n",
            "Iteration 3, loss = 1.22373348\n",
            "Iteration 4, loss = 1.21730018\n",
            "Iteration 5, loss = 1.21967258\n",
            "Iteration 6, loss = 1.19423943\n",
            "Iteration 7, loss = 1.18085195\n",
            "Iteration 8, loss = 1.15207315\n",
            "Iteration 9, loss = 1.13079942\n",
            "Iteration 10, loss = 1.10839545\n",
            "Iteration 11, loss = 1.09677497\n",
            "Iteration 12, loss = 1.06799426\n",
            "Iteration 13, loss = 1.03585406\n",
            "Iteration 14, loss = 1.03769363\n",
            "Iteration 15, loss = 1.00985607\n",
            "Iteration 16, loss = 0.98246406\n",
            "Iteration 17, loss = 0.97136273\n",
            "Iteration 18, loss = 0.94151793\n",
            "Iteration 19, loss = 0.93164500\n",
            "Iteration 20, loss = 0.92239344\n",
            "Iteration 21, loss = 0.93860083\n",
            "Iteration 22, loss = 0.90882623\n",
            "Iteration 23, loss = 0.87408009\n",
            "Iteration 24, loss = 0.86196525\n",
            "Iteration 25, loss = 0.88862146\n",
            "Iteration 26, loss = 0.89139180\n",
            "Iteration 27, loss = 0.87286849\n",
            "Iteration 28, loss = 0.83428771\n",
            "Iteration 29, loss = 0.87417810\n",
            "Iteration 30, loss = 0.83520720\n",
            "Iteration 31, loss = 0.81339451\n",
            "Iteration 32, loss = 0.80699881\n",
            "Iteration 33, loss = 0.77669201\n",
            "Iteration 34, loss = 0.78100118\n",
            "Iteration 35, loss = 0.77304306\n",
            "Iteration 36, loss = 0.77738078\n",
            "Iteration 37, loss = 0.75791246\n",
            "Iteration 38, loss = 0.73839582\n",
            "Iteration 39, loss = 0.74139110\n",
            "Iteration 40, loss = 0.73504420\n",
            "Iteration 41, loss = 0.76998597\n",
            "Iteration 42, loss = 0.75040611\n",
            "Iteration 43, loss = 0.76346221\n",
            "Iteration 44, loss = 0.73291823\n",
            "Iteration 45, loss = 0.70922688\n",
            "Iteration 46, loss = 0.71776567\n",
            "Iteration 47, loss = 0.71546172\n",
            "Iteration 48, loss = 0.72389413\n",
            "Iteration 49, loss = 0.69652615\n",
            "Iteration 50, loss = 0.69671526\n",
            "Iteration 51, loss = 0.69915563\n",
            "Iteration 52, loss = 0.69011504\n",
            "Iteration 53, loss = 0.69232073\n",
            "Iteration 54, loss = 0.67863203\n",
            "Iteration 55, loss = 0.68998782\n",
            "Iteration 56, loss = 0.72575481\n",
            "Iteration 57, loss = 0.72695944\n",
            "Iteration 58, loss = 0.69492409\n",
            "Iteration 59, loss = 0.72262033\n",
            "Iteration 60, loss = 0.70669636\n",
            "Iteration 61, loss = 0.67511672\n",
            "Iteration 62, loss = 0.65394422\n",
            "Iteration 63, loss = 0.65363068\n",
            "Iteration 64, loss = 0.65245677\n",
            "Iteration 65, loss = 0.65203795\n",
            "Iteration 66, loss = 0.63423281\n",
            "Iteration 67, loss = 0.63060499\n",
            "Iteration 68, loss = 0.64573976\n",
            "Iteration 69, loss = 0.64849611\n",
            "Iteration 70, loss = 0.65591105\n",
            "Iteration 71, loss = 0.64131243\n",
            "Iteration 72, loss = 0.62535590\n",
            "Iteration 73, loss = 0.62114062\n",
            "Iteration 74, loss = 0.61568511\n",
            "Iteration 75, loss = 0.60526952\n",
            "Iteration 76, loss = 0.60525870\n",
            "Iteration 77, loss = 0.64876783\n",
            "Iteration 78, loss = 0.62096289\n",
            "Iteration 79, loss = 0.61088752\n",
            "Iteration 80, loss = 0.59405411\n",
            "Iteration 81, loss = 0.58282679\n",
            "Iteration 82, loss = 0.56746338\n",
            "Iteration 83, loss = 0.57222134\n",
            "Iteration 84, loss = 0.57014714\n",
            "Iteration 85, loss = 0.59963529\n",
            "Iteration 86, loss = 0.61160582\n",
            "Iteration 87, loss = 0.64235552\n",
            "Iteration 88, loss = 0.64221575\n",
            "Iteration 89, loss = 0.58016002\n",
            "Iteration 90, loss = 0.59014715\n",
            "Iteration 91, loss = 0.58864527\n",
            "Iteration 92, loss = 0.63003421\n",
            "Iteration 93, loss = 0.60469261\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31540962\n",
            "Iteration 2, loss = 1.25016168\n",
            "Iteration 3, loss = 1.23044485\n",
            "Iteration 4, loss = 1.20664167\n",
            "Iteration 5, loss = 1.20113106\n",
            "Iteration 6, loss = 1.18735594\n",
            "Iteration 7, loss = 1.15112639\n",
            "Iteration 8, loss = 1.13619501\n",
            "Iteration 9, loss = 1.12061860\n",
            "Iteration 10, loss = 1.08031592\n",
            "Iteration 11, loss = 1.08734964\n",
            "Iteration 12, loss = 1.05025400\n",
            "Iteration 13, loss = 1.02917041\n",
            "Iteration 14, loss = 1.01757231\n",
            "Iteration 15, loss = 1.00694597\n",
            "Iteration 16, loss = 0.97164663\n",
            "Iteration 17, loss = 0.96214178\n",
            "Iteration 18, loss = 0.93494962\n",
            "Iteration 19, loss = 0.92388819\n",
            "Iteration 20, loss = 0.93476956\n",
            "Iteration 21, loss = 0.97060772\n",
            "Iteration 22, loss = 0.93310394\n",
            "Iteration 23, loss = 0.89829405\n",
            "Iteration 24, loss = 0.87307153\n",
            "Iteration 25, loss = 0.86873028\n",
            "Iteration 26, loss = 0.88948638\n",
            "Iteration 27, loss = 0.85400774\n",
            "Iteration 28, loss = 0.85235562\n",
            "Iteration 29, loss = 0.88721258\n",
            "Iteration 30, loss = 0.84723072\n",
            "Iteration 31, loss = 0.81077772\n",
            "Iteration 32, loss = 0.82287397\n",
            "Iteration 33, loss = 0.78794756\n",
            "Iteration 34, loss = 0.78624384\n",
            "Iteration 35, loss = 0.77937343\n",
            "Iteration 36, loss = 0.77559688\n",
            "Iteration 37, loss = 0.76106007\n",
            "Iteration 38, loss = 0.74996033\n",
            "Iteration 39, loss = 0.74437603\n",
            "Iteration 40, loss = 0.73249190\n",
            "Iteration 41, loss = 0.75251156\n",
            "Iteration 42, loss = 0.74665231\n",
            "Iteration 43, loss = 0.73900054\n",
            "Iteration 44, loss = 0.72797935\n",
            "Iteration 45, loss = 0.70493094\n",
            "Iteration 46, loss = 0.70622035\n",
            "Iteration 47, loss = 0.69992133\n",
            "Iteration 48, loss = 0.69464850\n",
            "Iteration 49, loss = 0.67736431\n",
            "Iteration 50, loss = 0.72737747\n",
            "Iteration 51, loss = 0.70979127\n",
            "Iteration 52, loss = 0.69869853\n",
            "Iteration 53, loss = 0.72386445\n",
            "Iteration 54, loss = 0.69835189\n",
            "Iteration 55, loss = 0.67460470\n",
            "Iteration 56, loss = 0.67482714\n",
            "Iteration 57, loss = 0.68117565\n",
            "Iteration 58, loss = 0.65394681\n",
            "Iteration 59, loss = 0.66037473\n",
            "Iteration 60, loss = 0.64672795\n",
            "Iteration 61, loss = 0.64360598\n",
            "Iteration 62, loss = 0.65847906\n",
            "Iteration 63, loss = 0.63116212\n",
            "Iteration 64, loss = 0.62672242\n",
            "Iteration 65, loss = 0.66125069\n",
            "Iteration 66, loss = 0.63559991\n",
            "Iteration 67, loss = 0.61894452\n",
            "Iteration 68, loss = 0.64967571\n",
            "Iteration 69, loss = 0.67047799\n",
            "Iteration 70, loss = 0.65104254\n",
            "Iteration 71, loss = 0.62447477\n",
            "Iteration 72, loss = 0.61286789\n",
            "Iteration 73, loss = 0.60052107\n",
            "Iteration 74, loss = 0.62668252\n",
            "Iteration 75, loss = 0.61563546\n",
            "Iteration 76, loss = 0.60212802\n",
            "Iteration 77, loss = 0.63747318\n",
            "Iteration 78, loss = 0.61787563\n",
            "Iteration 79, loss = 0.61670857\n",
            "Iteration 80, loss = 0.63869362\n",
            "Iteration 81, loss = 0.61018268\n",
            "Iteration 82, loss = 0.59644294\n",
            "Iteration 83, loss = 0.57770949\n",
            "Iteration 84, loss = 0.57035295\n",
            "Iteration 85, loss = 0.58205916\n",
            "Iteration 86, loss = 0.58962288\n",
            "Iteration 87, loss = 0.62942165\n",
            "Iteration 88, loss = 0.59800015\n",
            "Iteration 89, loss = 0.60395128\n",
            "Iteration 90, loss = 0.58520176\n",
            "Iteration 91, loss = 0.69558093\n",
            "Iteration 92, loss = 0.68300914\n",
            "Iteration 93, loss = 0.62390374\n",
            "Iteration 94, loss = 0.63030361\n",
            "Iteration 95, loss = 0.59303286\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31498457\n",
            "Iteration 2, loss = 1.24487920\n",
            "Iteration 3, loss = 1.21925900\n",
            "Iteration 4, loss = 1.19319643\n",
            "Iteration 5, loss = 1.19110102\n",
            "Iteration 6, loss = 1.18317117\n",
            "Iteration 7, loss = 1.13936342\n",
            "Iteration 8, loss = 1.11935547\n",
            "Iteration 9, loss = 1.10744611\n",
            "Iteration 10, loss = 1.07328540\n",
            "Iteration 11, loss = 1.08425488\n",
            "Iteration 12, loss = 1.03719866\n",
            "Iteration 13, loss = 1.01489832\n",
            "Iteration 14, loss = 1.00031461\n",
            "Iteration 15, loss = 0.97311380\n",
            "Iteration 16, loss = 0.94081533\n",
            "Iteration 17, loss = 0.94055493\n",
            "Iteration 18, loss = 0.92978256\n",
            "Iteration 19, loss = 0.90149341\n",
            "Iteration 20, loss = 0.92057390\n",
            "Iteration 21, loss = 0.92676684\n",
            "Iteration 22, loss = 0.89788073\n",
            "Iteration 23, loss = 0.84955000\n",
            "Iteration 24, loss = 0.84745026\n",
            "Iteration 25, loss = 0.86233929\n",
            "Iteration 26, loss = 0.85435456\n",
            "Iteration 27, loss = 0.83153192\n",
            "Iteration 28, loss = 0.85849645\n",
            "Iteration 29, loss = 0.90021992\n",
            "Iteration 30, loss = 0.83570589\n",
            "Iteration 31, loss = 0.80917624\n",
            "Iteration 32, loss = 0.82746815\n",
            "Iteration 33, loss = 0.79863827\n",
            "Iteration 34, loss = 0.77423607\n",
            "Iteration 35, loss = 0.77791660\n",
            "Iteration 36, loss = 0.79223980\n",
            "Iteration 37, loss = 0.77696283\n",
            "Iteration 38, loss = 0.75763383\n",
            "Iteration 39, loss = 0.74356177\n",
            "Iteration 40, loss = 0.73609168\n",
            "Iteration 41, loss = 0.75064931\n",
            "Iteration 42, loss = 0.73728152\n",
            "Iteration 43, loss = 0.76606211\n",
            "Iteration 44, loss = 0.73357781\n",
            "Iteration 45, loss = 0.71605292\n",
            "Iteration 46, loss = 0.71319297\n",
            "Iteration 47, loss = 0.70922497\n",
            "Iteration 48, loss = 0.69936569\n",
            "Iteration 49, loss = 0.70592122\n",
            "Iteration 50, loss = 0.74004727\n",
            "Iteration 51, loss = 0.70117676\n",
            "Iteration 52, loss = 0.69316946\n",
            "Iteration 53, loss = 0.71059743\n",
            "Iteration 54, loss = 0.70383866\n",
            "Iteration 55, loss = 0.70614221\n",
            "Iteration 56, loss = 0.73546532\n",
            "Iteration 57, loss = 0.73079685\n",
            "Iteration 58, loss = 0.70794068\n",
            "Iteration 59, loss = 0.70141577\n",
            "Iteration 60, loss = 0.67611871\n",
            "Iteration 61, loss = 0.67472597\n",
            "Iteration 62, loss = 0.66584163\n",
            "Iteration 63, loss = 0.65656081\n",
            "Iteration 64, loss = 0.66308986\n",
            "Iteration 65, loss = 0.64630596\n",
            "Iteration 66, loss = 0.64614993\n",
            "Iteration 67, loss = 0.63916229\n",
            "Iteration 68, loss = 0.66018909\n",
            "Iteration 69, loss = 0.70373491\n",
            "Iteration 70, loss = 0.68579269\n",
            "Iteration 71, loss = 0.64042829\n",
            "Iteration 72, loss = 0.63735632\n",
            "Iteration 73, loss = 0.63638731\n",
            "Iteration 74, loss = 0.63760814\n",
            "Iteration 75, loss = 0.62779759\n",
            "Iteration 76, loss = 0.61478023\n",
            "Iteration 77, loss = 0.63630308\n",
            "Iteration 78, loss = 0.65709110\n",
            "Iteration 79, loss = 0.63820696\n",
            "Iteration 80, loss = 0.66906411\n",
            "Iteration 81, loss = 0.62207828\n",
            "Iteration 82, loss = 0.63007347\n",
            "Iteration 83, loss = 0.62223515\n",
            "Iteration 84, loss = 0.60236527\n",
            "Iteration 85, loss = 0.62800306\n",
            "Iteration 86, loss = 0.60710739\n",
            "Iteration 87, loss = 0.61741123\n",
            "Iteration 88, loss = 0.62973564\n",
            "Iteration 89, loss = 0.61093628\n",
            "Iteration 90, loss = 0.64407039\n",
            "Iteration 91, loss = 0.74087406\n",
            "Iteration 92, loss = 0.75459727\n",
            "Iteration 93, loss = 0.75220949\n",
            "Iteration 94, loss = 0.71451348\n",
            "Iteration 95, loss = 0.66039397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31572376\n",
            "Iteration 2, loss = 1.24315533\n",
            "Iteration 3, loss = 1.21995841\n",
            "Iteration 4, loss = 1.19701940\n",
            "Iteration 5, loss = 1.20037617\n",
            "Iteration 6, loss = 1.17106358\n",
            "Iteration 7, loss = 1.13168315\n",
            "Iteration 8, loss = 1.10467247\n",
            "Iteration 9, loss = 1.09386563\n",
            "Iteration 10, loss = 1.09281032\n",
            "Iteration 11, loss = 1.11594157\n",
            "Iteration 12, loss = 1.04737771\n",
            "Iteration 13, loss = 1.02334782\n",
            "Iteration 14, loss = 1.00695471\n",
            "Iteration 15, loss = 0.96823589\n",
            "Iteration 16, loss = 0.95335511\n",
            "Iteration 17, loss = 0.94159316\n",
            "Iteration 18, loss = 0.93449396\n",
            "Iteration 19, loss = 0.91077999\n",
            "Iteration 20, loss = 0.92752670\n",
            "Iteration 21, loss = 0.93943867\n",
            "Iteration 22, loss = 0.93178200\n",
            "Iteration 23, loss = 0.86818862\n",
            "Iteration 24, loss = 0.86973917\n",
            "Iteration 25, loss = 0.88538377\n",
            "Iteration 26, loss = 0.84797541\n",
            "Iteration 27, loss = 0.82226697\n",
            "Iteration 28, loss = 0.81505855\n",
            "Iteration 29, loss = 0.83495386\n",
            "Iteration 30, loss = 0.80774320\n",
            "Iteration 31, loss = 0.81423313\n",
            "Iteration 32, loss = 0.81693879\n",
            "Iteration 33, loss = 0.79813248\n",
            "Iteration 34, loss = 0.76649364\n",
            "Iteration 35, loss = 0.77341468\n",
            "Iteration 36, loss = 0.82597004\n",
            "Iteration 37, loss = 0.78671038\n",
            "Iteration 38, loss = 0.74291935\n",
            "Iteration 39, loss = 0.74519931\n",
            "Iteration 40, loss = 0.73353236\n",
            "Iteration 41, loss = 0.75373630\n",
            "Iteration 42, loss = 0.73417422\n",
            "Iteration 43, loss = 0.72983165\n",
            "Iteration 44, loss = 0.71472273\n",
            "Iteration 45, loss = 0.70445819\n",
            "Iteration 46, loss = 0.72804022\n",
            "Iteration 47, loss = 0.70975114\n",
            "Iteration 48, loss = 0.69111382\n",
            "Iteration 49, loss = 0.70851087\n",
            "Iteration 50, loss = 0.74151625\n",
            "Iteration 51, loss = 0.70089914\n",
            "Iteration 52, loss = 0.68289847\n",
            "Iteration 53, loss = 0.67636641\n",
            "Iteration 54, loss = 0.66116798\n",
            "Iteration 55, loss = 0.67406256\n",
            "Iteration 56, loss = 0.70400850\n",
            "Iteration 57, loss = 0.66852184\n",
            "Iteration 58, loss = 0.66190432\n",
            "Iteration 59, loss = 0.66868486\n",
            "Iteration 60, loss = 0.65277248\n",
            "Iteration 61, loss = 0.64431804\n",
            "Iteration 62, loss = 0.64935831\n",
            "Iteration 63, loss = 0.64517667\n",
            "Iteration 64, loss = 0.65347575\n",
            "Iteration 65, loss = 0.62668290\n",
            "Iteration 66, loss = 0.63340805\n",
            "Iteration 67, loss = 0.62620481\n",
            "Iteration 68, loss = 0.65241637\n",
            "Iteration 69, loss = 0.67194393\n",
            "Iteration 70, loss = 0.61393853\n",
            "Iteration 71, loss = 0.62356580\n",
            "Iteration 72, loss = 0.62950586\n",
            "Iteration 73, loss = 0.61510868\n",
            "Iteration 74, loss = 0.59032303\n",
            "Iteration 75, loss = 0.61271177\n",
            "Iteration 76, loss = 0.58087068\n",
            "Iteration 77, loss = 0.59325981\n",
            "Iteration 78, loss = 0.59409639\n",
            "Iteration 79, loss = 0.60217826\n",
            "Iteration 80, loss = 0.57250509\n",
            "Iteration 81, loss = 0.54882833\n",
            "Iteration 82, loss = 0.55588554\n",
            "Iteration 83, loss = 0.60336136\n",
            "Iteration 84, loss = 0.58496661\n",
            "Iteration 85, loss = 0.55040967\n",
            "Iteration 86, loss = 0.54440976\n",
            "Iteration 87, loss = 0.56627165\n",
            "Iteration 88, loss = 0.58381107\n",
            "Iteration 89, loss = 0.58606296\n",
            "Iteration 90, loss = 0.60542980\n",
            "Iteration 91, loss = 0.69432100\n",
            "Iteration 92, loss = 0.71159014\n",
            "Iteration 93, loss = 0.66790363\n",
            "Iteration 94, loss = 0.64606371\n",
            "Iteration 95, loss = 0.59658804\n",
            "Iteration 96, loss = 0.57269820\n",
            "Iteration 97, loss = 0.54941889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30739064\n",
            "Iteration 2, loss = 1.22570448\n",
            "Iteration 3, loss = 1.20158876\n",
            "Iteration 4, loss = 1.16928455\n",
            "Iteration 5, loss = 1.17706794\n",
            "Iteration 6, loss = 1.14220118\n",
            "Iteration 7, loss = 1.10232717\n",
            "Iteration 8, loss = 1.08244699\n",
            "Iteration 9, loss = 1.06222172\n",
            "Iteration 10, loss = 1.03963954\n",
            "Iteration 11, loss = 1.03881483\n",
            "Iteration 12, loss = 0.99652341\n",
            "Iteration 13, loss = 0.98782653\n",
            "Iteration 14, loss = 0.96859968\n",
            "Iteration 15, loss = 0.95486014\n",
            "Iteration 16, loss = 0.95217053\n",
            "Iteration 17, loss = 0.93876290\n",
            "Iteration 18, loss = 0.92321098\n",
            "Iteration 19, loss = 0.89849403\n",
            "Iteration 20, loss = 0.88391679\n",
            "Iteration 21, loss = 0.84633349\n",
            "Iteration 22, loss = 0.84315071\n",
            "Iteration 23, loss = 0.82063267\n",
            "Iteration 24, loss = 0.81662583\n",
            "Iteration 25, loss = 0.81010382\n",
            "Iteration 26, loss = 0.82084248\n",
            "Iteration 27, loss = 0.79861545\n",
            "Iteration 28, loss = 0.85396620\n",
            "Iteration 29, loss = 0.86458138\n",
            "Iteration 30, loss = 0.78427294\n",
            "Iteration 31, loss = 0.77704347\n",
            "Iteration 32, loss = 0.76931748\n",
            "Iteration 33, loss = 0.75086582\n",
            "Iteration 34, loss = 0.74106210\n",
            "Iteration 35, loss = 0.74103174\n",
            "Iteration 36, loss = 0.76655094\n",
            "Iteration 37, loss = 0.76009542\n",
            "Iteration 38, loss = 0.71947258\n",
            "Iteration 39, loss = 0.71494641\n",
            "Iteration 40, loss = 0.71118318\n",
            "Iteration 41, loss = 0.73886376\n",
            "Iteration 42, loss = 0.72230561\n",
            "Iteration 43, loss = 0.71842127\n",
            "Iteration 44, loss = 0.69261690\n",
            "Iteration 45, loss = 0.69375179\n",
            "Iteration 46, loss = 0.72131607\n",
            "Iteration 47, loss = 0.70378508\n",
            "Iteration 48, loss = 0.67068349\n",
            "Iteration 49, loss = 0.67377421\n",
            "Iteration 50, loss = 0.72364179\n",
            "Iteration 51, loss = 0.69342732\n",
            "Iteration 52, loss = 0.68382169\n",
            "Iteration 53, loss = 0.68919060\n",
            "Iteration 54, loss = 0.66331158\n",
            "Iteration 55, loss = 0.66051638\n",
            "Iteration 56, loss = 0.65978196\n",
            "Iteration 57, loss = 0.64427347\n",
            "Iteration 58, loss = 0.63419811\n",
            "Iteration 59, loss = 0.68140067\n",
            "Iteration 60, loss = 0.63073928\n",
            "Iteration 61, loss = 0.63232663\n",
            "Iteration 62, loss = 0.64766543\n",
            "Iteration 63, loss = 0.64208846\n",
            "Iteration 64, loss = 0.65405253\n",
            "Iteration 65, loss = 0.62169836\n",
            "Iteration 66, loss = 0.64164264\n",
            "Iteration 67, loss = 0.64186411\n",
            "Iteration 68, loss = 0.63527796\n",
            "Iteration 69, loss = 0.64412676\n",
            "Iteration 70, loss = 0.67165924\n",
            "Iteration 71, loss = 0.61910712\n",
            "Iteration 72, loss = 0.62001392\n",
            "Iteration 73, loss = 0.60700445\n",
            "Iteration 74, loss = 0.58567028\n",
            "Iteration 75, loss = 0.59241921\n",
            "Iteration 76, loss = 0.57623364\n",
            "Iteration 77, loss = 0.57224063\n",
            "Iteration 78, loss = 0.59157667\n",
            "Iteration 79, loss = 0.61338652\n",
            "Iteration 80, loss = 0.63388283\n",
            "Iteration 81, loss = 0.63110711\n",
            "Iteration 82, loss = 0.67135141\n",
            "Iteration 83, loss = 0.70883508\n",
            "Iteration 84, loss = 0.63148617\n",
            "Iteration 85, loss = 0.61013208\n",
            "Iteration 86, loss = 0.60220742\n",
            "Iteration 87, loss = 0.61213539\n",
            "Iteration 88, loss = 0.59610409\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31799455\n",
            "Iteration 2, loss = 1.23357217\n",
            "Iteration 3, loss = 1.20826894\n",
            "Iteration 4, loss = 1.18705366\n",
            "Iteration 5, loss = 1.17878475\n",
            "Iteration 6, loss = 1.15018903\n",
            "Iteration 7, loss = 1.11932748\n",
            "Iteration 8, loss = 1.08309593\n",
            "Iteration 9, loss = 1.05464030\n",
            "Iteration 10, loss = 1.05969501\n",
            "Iteration 11, loss = 1.10092418\n",
            "Iteration 12, loss = 1.01714613\n",
            "Iteration 13, loss = 1.01273317\n",
            "Iteration 14, loss = 0.97496605\n",
            "Iteration 15, loss = 0.95414724\n",
            "Iteration 16, loss = 0.95781245\n",
            "Iteration 17, loss = 0.93384248\n",
            "Iteration 18, loss = 0.94393545\n",
            "Iteration 19, loss = 0.92169252\n",
            "Iteration 20, loss = 0.90384011\n",
            "Iteration 21, loss = 0.87111097\n",
            "Iteration 22, loss = 0.87353983\n",
            "Iteration 23, loss = 0.85390464\n",
            "Iteration 24, loss = 0.84800548\n",
            "Iteration 25, loss = 0.82981295\n",
            "Iteration 26, loss = 0.81823649\n",
            "Iteration 27, loss = 0.83684374\n",
            "Iteration 28, loss = 0.84648004\n",
            "Iteration 29, loss = 0.90009306\n",
            "Iteration 30, loss = 0.83025369\n",
            "Iteration 31, loss = 0.80704614\n",
            "Iteration 32, loss = 0.80005492\n",
            "Iteration 33, loss = 0.77442264\n",
            "Iteration 34, loss = 0.78405001\n",
            "Iteration 35, loss = 0.77664696\n",
            "Iteration 36, loss = 0.82623826\n",
            "Iteration 37, loss = 0.78736959\n",
            "Iteration 38, loss = 0.76432929\n",
            "Iteration 39, loss = 0.76339711\n",
            "Iteration 40, loss = 0.74402221\n",
            "Iteration 41, loss = 0.77623216\n",
            "Iteration 42, loss = 0.77328694\n",
            "Iteration 43, loss = 0.76069390\n",
            "Iteration 44, loss = 0.72890331\n",
            "Iteration 45, loss = 0.73040761\n",
            "Iteration 46, loss = 0.72561561\n",
            "Iteration 47, loss = 0.72121968\n",
            "Iteration 48, loss = 0.70351059\n",
            "Iteration 49, loss = 0.71574485\n",
            "Iteration 50, loss = 0.75941505\n",
            "Iteration 51, loss = 0.73779678\n",
            "Iteration 52, loss = 0.71025029\n",
            "Iteration 53, loss = 0.72482520\n",
            "Iteration 54, loss = 0.70199771\n",
            "Iteration 55, loss = 0.69126446\n",
            "Iteration 56, loss = 0.68512759\n",
            "Iteration 57, loss = 0.66771523\n",
            "Iteration 58, loss = 0.68185541\n",
            "Iteration 59, loss = 0.71214664\n",
            "Iteration 60, loss = 0.67482660\n",
            "Iteration 61, loss = 0.66085181\n",
            "Iteration 62, loss = 0.68028207\n",
            "Iteration 63, loss = 0.66487035\n",
            "Iteration 64, loss = 0.66310865\n",
            "Iteration 65, loss = 0.66311969\n",
            "Iteration 66, loss = 0.70679455\n",
            "Iteration 67, loss = 0.67145392\n",
            "Iteration 68, loss = 0.66559247\n",
            "Iteration 69, loss = 0.64748580\n",
            "Iteration 70, loss = 0.64963654\n",
            "Iteration 71, loss = 0.63089693\n",
            "Iteration 72, loss = 0.63032515\n",
            "Iteration 73, loss = 0.63449246\n",
            "Iteration 74, loss = 0.60947665\n",
            "Iteration 75, loss = 0.62727405\n",
            "Iteration 76, loss = 0.63019626\n",
            "Iteration 77, loss = 0.60952445\n",
            "Iteration 78, loss = 0.62401764\n",
            "Iteration 79, loss = 0.62151370\n",
            "Iteration 80, loss = 0.62915485\n",
            "Iteration 81, loss = 0.62892036\n",
            "Iteration 82, loss = 0.65240410\n",
            "Iteration 83, loss = 0.65735067\n",
            "Iteration 84, loss = 0.59731989\n",
            "Iteration 85, loss = 0.57907691\n",
            "Iteration 86, loss = 0.57492600\n",
            "Iteration 87, loss = 0.57273649\n",
            "Iteration 88, loss = 0.59454930\n",
            "Iteration 89, loss = 0.59396573\n",
            "Iteration 90, loss = 0.63133774\n",
            "Iteration 91, loss = 0.67988900\n",
            "Iteration 92, loss = 0.64107665\n",
            "Iteration 93, loss = 0.58700738\n",
            "Iteration 94, loss = 0.59717358\n",
            "Iteration 95, loss = 0.60063483\n",
            "Iteration 96, loss = 0.58660389\n",
            "Iteration 97, loss = 0.61237620\n",
            "Iteration 98, loss = 0.58758489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31966506\n",
            "Iteration 2, loss = 1.23422627\n",
            "Iteration 3, loss = 1.22096903\n",
            "Iteration 4, loss = 1.19203782\n",
            "Iteration 5, loss = 1.17315492\n",
            "Iteration 6, loss = 1.14599481\n",
            "Iteration 7, loss = 1.14375704\n",
            "Iteration 8, loss = 1.10929045\n",
            "Iteration 9, loss = 1.09736691\n",
            "Iteration 10, loss = 1.05287229\n",
            "Iteration 11, loss = 1.03881091\n",
            "Iteration 12, loss = 1.04851391\n",
            "Iteration 13, loss = 1.01212804\n",
            "Iteration 14, loss = 1.00030907\n",
            "Iteration 15, loss = 0.97157382\n",
            "Iteration 16, loss = 0.95871797\n",
            "Iteration 17, loss = 0.93289114\n",
            "Iteration 18, loss = 0.93153755\n",
            "Iteration 19, loss = 0.91375130\n",
            "Iteration 20, loss = 0.89286528\n",
            "Iteration 21, loss = 0.89835218\n",
            "Iteration 22, loss = 0.86120184\n",
            "Iteration 23, loss = 0.86670344\n",
            "Iteration 24, loss = 0.83890275\n",
            "Iteration 25, loss = 0.83485277\n",
            "Iteration 26, loss = 0.82715932\n",
            "Iteration 27, loss = 0.81754430\n",
            "Iteration 28, loss = 0.82469350\n",
            "Iteration 29, loss = 0.78307184\n",
            "Iteration 30, loss = 0.80170607\n",
            "Iteration 31, loss = 0.78681830\n",
            "Iteration 32, loss = 0.75726271\n",
            "Iteration 33, loss = 0.76067749\n",
            "Iteration 34, loss = 0.74065516\n",
            "Iteration 35, loss = 0.75415689\n",
            "Iteration 36, loss = 0.73976299\n",
            "Iteration 37, loss = 0.73217641\n",
            "Iteration 38, loss = 0.74548800\n",
            "Iteration 39, loss = 0.74171309\n",
            "Iteration 40, loss = 0.73366206\n",
            "Iteration 41, loss = 0.71876893\n",
            "Iteration 42, loss = 0.76752488\n",
            "Iteration 43, loss = 0.76059785\n",
            "Iteration 44, loss = 0.79373092\n",
            "Iteration 45, loss = 0.75967099\n",
            "Iteration 46, loss = 0.76345827\n",
            "Iteration 47, loss = 0.70982558\n",
            "Iteration 48, loss = 0.70212088\n",
            "Iteration 49, loss = 0.71953457\n",
            "Iteration 50, loss = 0.68809523\n",
            "Iteration 51, loss = 0.68427874\n",
            "Iteration 52, loss = 0.69524224\n",
            "Iteration 53, loss = 0.72039209\n",
            "Iteration 54, loss = 0.71415022\n",
            "Iteration 55, loss = 0.70091885\n",
            "Iteration 56, loss = 0.69521871\n",
            "Iteration 57, loss = 0.70790653\n",
            "Iteration 58, loss = 0.70353258\n",
            "Iteration 59, loss = 0.69106729\n",
            "Iteration 60, loss = 0.67109395\n",
            "Iteration 61, loss = 0.64702175\n",
            "Iteration 62, loss = 0.63560975\n",
            "Iteration 63, loss = 0.63483417\n",
            "Iteration 64, loss = 0.66451135\n",
            "Iteration 65, loss = 0.63988161\n",
            "Iteration 66, loss = 0.63576994\n",
            "Iteration 67, loss = 0.62825627\n",
            "Iteration 68, loss = 0.63499158\n",
            "Iteration 69, loss = 0.61229539\n",
            "Iteration 70, loss = 0.64688760\n",
            "Iteration 71, loss = 0.66384893\n",
            "Iteration 72, loss = 0.61651156\n",
            "Iteration 73, loss = 0.63022628\n",
            "Iteration 74, loss = 0.62395051\n",
            "Iteration 75, loss = 0.62057631\n",
            "Iteration 76, loss = 0.60831527\n",
            "Iteration 77, loss = 0.60151887\n",
            "Iteration 78, loss = 0.59816351\n",
            "Iteration 79, loss = 0.63503883\n",
            "Iteration 80, loss = 0.67143452\n",
            "Iteration 81, loss = 0.67047580\n",
            "Iteration 82, loss = 0.61681819\n",
            "Iteration 83, loss = 0.63812879\n",
            "Iteration 84, loss = 0.66461700\n",
            "Iteration 85, loss = 0.64078510\n",
            "Iteration 86, loss = 0.62451812\n",
            "Iteration 87, loss = 0.60722635\n",
            "Iteration 88, loss = 0.56709325\n",
            "Iteration 89, loss = 0.60106648\n",
            "Iteration 90, loss = 0.60302496\n",
            "Iteration 91, loss = 0.61702317\n",
            "Iteration 92, loss = 0.58165633\n",
            "Iteration 93, loss = 0.55605996\n",
            "Iteration 94, loss = 0.53963291\n",
            "Iteration 95, loss = 0.54719469\n",
            "Iteration 96, loss = 0.61584979\n",
            "Iteration 97, loss = 0.57558737\n",
            "Iteration 98, loss = 0.58840254\n",
            "Iteration 99, loss = 0.55647512\n",
            "Iteration 100, loss = 0.57048381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31621565\n",
            "Iteration 2, loss = 1.22285590\n",
            "Iteration 3, loss = 1.21433717\n",
            "Iteration 4, loss = 1.18522253\n",
            "Iteration 5, loss = 1.16031683\n",
            "Iteration 6, loss = 1.14581154\n",
            "Iteration 7, loss = 1.15861405\n",
            "Iteration 8, loss = 1.11648016\n",
            "Iteration 9, loss = 1.08800173\n",
            "Iteration 10, loss = 1.05927702\n",
            "Iteration 11, loss = 1.05664976\n",
            "Iteration 12, loss = 1.06718282\n",
            "Iteration 13, loss = 1.01254070\n",
            "Iteration 14, loss = 1.00935380\n",
            "Iteration 15, loss = 0.98646192\n",
            "Iteration 16, loss = 0.96348934\n",
            "Iteration 17, loss = 0.94773513\n",
            "Iteration 18, loss = 0.93349634\n",
            "Iteration 19, loss = 0.91112048\n",
            "Iteration 20, loss = 0.89180001\n",
            "Iteration 21, loss = 0.90153415\n",
            "Iteration 22, loss = 0.86715034\n",
            "Iteration 23, loss = 0.85325471\n",
            "Iteration 24, loss = 0.83573321\n",
            "Iteration 25, loss = 0.83749759\n",
            "Iteration 26, loss = 0.83093155\n",
            "Iteration 27, loss = 0.81365895\n",
            "Iteration 28, loss = 0.84464739\n",
            "Iteration 29, loss = 0.79098036\n",
            "Iteration 30, loss = 0.80468956\n",
            "Iteration 31, loss = 0.80847729\n",
            "Iteration 32, loss = 0.78098112\n",
            "Iteration 33, loss = 0.76268774\n",
            "Iteration 34, loss = 0.74570318\n",
            "Iteration 35, loss = 0.75447338\n",
            "Iteration 36, loss = 0.76864240\n",
            "Iteration 37, loss = 0.75041682\n",
            "Iteration 38, loss = 0.74825862\n",
            "Iteration 39, loss = 0.75535173\n",
            "Iteration 40, loss = 0.71257438\n",
            "Iteration 41, loss = 0.72622207\n",
            "Iteration 42, loss = 0.74961417\n",
            "Iteration 43, loss = 0.72775347\n",
            "Iteration 44, loss = 0.77013458\n",
            "Iteration 45, loss = 0.73826046\n",
            "Iteration 46, loss = 0.75239101\n",
            "Iteration 47, loss = 0.70039581\n",
            "Iteration 48, loss = 0.69262721\n",
            "Iteration 49, loss = 0.69126813\n",
            "Iteration 50, loss = 0.68302707\n",
            "Iteration 51, loss = 0.69798910\n",
            "Iteration 52, loss = 0.69582174\n",
            "Iteration 53, loss = 0.76464440\n",
            "Iteration 54, loss = 0.72543898\n",
            "Iteration 55, loss = 0.69734833\n",
            "Iteration 56, loss = 0.67833342\n",
            "Iteration 57, loss = 0.66983725\n",
            "Iteration 58, loss = 0.67547030\n",
            "Iteration 59, loss = 0.66642654\n",
            "Iteration 60, loss = 0.66030123\n",
            "Iteration 61, loss = 0.62540912\n",
            "Iteration 62, loss = 0.63605706\n",
            "Iteration 63, loss = 0.63950445\n",
            "Iteration 64, loss = 0.70561482\n",
            "Iteration 65, loss = 0.66071039\n",
            "Iteration 66, loss = 0.63994930\n",
            "Iteration 67, loss = 0.64328397\n",
            "Iteration 68, loss = 0.62385694\n",
            "Iteration 69, loss = 0.60100955\n",
            "Iteration 70, loss = 0.63732222\n",
            "Iteration 71, loss = 0.62530630\n",
            "Iteration 72, loss = 0.60803929\n",
            "Iteration 73, loss = 0.63373786\n",
            "Iteration 74, loss = 0.64684800\n",
            "Iteration 75, loss = 0.61887448\n",
            "Iteration 76, loss = 0.59568500\n",
            "Iteration 77, loss = 0.59167892\n",
            "Iteration 78, loss = 0.57892900\n",
            "Iteration 79, loss = 0.61387243\n",
            "Iteration 80, loss = 0.66807484\n",
            "Iteration 81, loss = 0.66020920\n",
            "Iteration 82, loss = 0.63277920\n",
            "Iteration 83, loss = 0.58479314\n",
            "Iteration 84, loss = 0.60021984\n",
            "Iteration 85, loss = 0.57403156\n",
            "Iteration 86, loss = 0.57932870\n",
            "Iteration 87, loss = 0.56186840\n",
            "Iteration 88, loss = 0.54157347\n",
            "Iteration 89, loss = 0.54519759\n",
            "Iteration 90, loss = 0.56066541\n",
            "Iteration 91, loss = 0.56724064\n",
            "Iteration 92, loss = 0.54301958\n",
            "Iteration 93, loss = 0.55865835\n",
            "Iteration 94, loss = 0.53457296\n",
            "Iteration 95, loss = 0.54474656\n",
            "Iteration 96, loss = 0.59359708\n",
            "Iteration 97, loss = 0.54520033\n",
            "Iteration 98, loss = 0.53625162\n",
            "Iteration 99, loss = 0.51689865\n",
            "Iteration 100, loss = 0.53662159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30204117\n",
            "Iteration 2, loss = 1.21928362\n",
            "Iteration 3, loss = 1.20051618\n",
            "Iteration 4, loss = 1.17725521\n",
            "Iteration 5, loss = 1.15566009\n",
            "Iteration 6, loss = 1.12796284\n",
            "Iteration 7, loss = 1.12690319\n",
            "Iteration 8, loss = 1.09856523\n",
            "Iteration 9, loss = 1.08959302\n",
            "Iteration 10, loss = 1.04323726\n",
            "Iteration 11, loss = 1.03714545\n",
            "Iteration 12, loss = 1.01963632\n",
            "Iteration 13, loss = 1.00610479\n",
            "Iteration 14, loss = 0.98322133\n",
            "Iteration 15, loss = 0.95780145\n",
            "Iteration 16, loss = 0.93720633\n",
            "Iteration 17, loss = 0.92329648\n",
            "Iteration 18, loss = 0.90558254\n",
            "Iteration 19, loss = 0.88521306\n",
            "Iteration 20, loss = 0.87061155\n",
            "Iteration 21, loss = 0.88055032\n",
            "Iteration 22, loss = 0.86638538\n",
            "Iteration 23, loss = 0.84789259\n",
            "Iteration 24, loss = 0.83189122\n",
            "Iteration 25, loss = 0.84039254\n",
            "Iteration 26, loss = 0.81496004\n",
            "Iteration 27, loss = 0.81436336\n",
            "Iteration 28, loss = 0.84638076\n",
            "Iteration 29, loss = 0.79900014\n",
            "Iteration 30, loss = 0.77790588\n",
            "Iteration 31, loss = 0.78429572\n",
            "Iteration 32, loss = 0.76379958\n",
            "Iteration 33, loss = 0.74381090\n",
            "Iteration 34, loss = 0.74570504\n",
            "Iteration 35, loss = 0.76121884\n",
            "Iteration 36, loss = 0.79180830\n",
            "Iteration 37, loss = 0.76369079\n",
            "Iteration 38, loss = 0.76076490\n",
            "Iteration 39, loss = 0.76070624\n",
            "Iteration 40, loss = 0.74435588\n",
            "Iteration 41, loss = 0.74602684\n",
            "Iteration 42, loss = 0.75146589\n",
            "Iteration 43, loss = 0.73032571\n",
            "Iteration 44, loss = 0.77741902\n",
            "Iteration 45, loss = 0.73307717\n",
            "Iteration 46, loss = 0.71631759\n",
            "Iteration 47, loss = 0.69210703\n",
            "Iteration 48, loss = 0.70377954\n",
            "Iteration 49, loss = 0.69946551\n",
            "Iteration 50, loss = 0.69782503\n",
            "Iteration 51, loss = 0.68796008\n",
            "Iteration 52, loss = 0.70214455\n",
            "Iteration 53, loss = 0.70627255\n",
            "Iteration 54, loss = 0.72793197\n",
            "Iteration 55, loss = 0.68443353\n",
            "Iteration 56, loss = 0.72095941\n",
            "Iteration 57, loss = 0.67550589\n",
            "Iteration 58, loss = 0.67364487\n",
            "Iteration 59, loss = 0.65195648\n",
            "Iteration 60, loss = 0.64065311\n",
            "Iteration 61, loss = 0.63517539\n",
            "Iteration 62, loss = 0.65652134\n",
            "Iteration 63, loss = 0.70496544\n",
            "Iteration 64, loss = 0.78410964\n",
            "Iteration 65, loss = 0.67770828\n",
            "Iteration 66, loss = 0.63422464\n",
            "Iteration 67, loss = 0.62724270\n",
            "Iteration 68, loss = 0.62028346\n",
            "Iteration 69, loss = 0.61653083\n",
            "Iteration 70, loss = 0.62786942\n",
            "Iteration 71, loss = 0.63561367\n",
            "Iteration 72, loss = 0.60550865\n",
            "Iteration 73, loss = 0.61632138\n",
            "Iteration 74, loss = 0.67539671\n",
            "Iteration 75, loss = 0.67085032\n",
            "Iteration 76, loss = 0.61461411\n",
            "Iteration 77, loss = 0.60910872\n",
            "Iteration 78, loss = 0.60512598\n",
            "Iteration 79, loss = 0.61170110\n",
            "Iteration 80, loss = 0.64851849\n",
            "Iteration 81, loss = 0.69343418\n",
            "Iteration 82, loss = 0.66676906\n",
            "Iteration 83, loss = 0.62248090\n",
            "Iteration 84, loss = 0.57284895\n",
            "Iteration 85, loss = 0.57593822\n",
            "Iteration 86, loss = 0.56746345\n",
            "Iteration 87, loss = 0.56229683\n",
            "Iteration 88, loss = 0.55318700\n",
            "Iteration 89, loss = 0.55300508\n",
            "Iteration 90, loss = 0.55541426\n",
            "Iteration 91, loss = 0.54980631\n",
            "Iteration 92, loss = 0.54022173\n",
            "Iteration 93, loss = 0.56171075\n",
            "Iteration 94, loss = 0.54660149\n",
            "Iteration 95, loss = 0.56053073\n",
            "Iteration 96, loss = 0.58433128\n",
            "Iteration 97, loss = 0.56573916\n",
            "Iteration 98, loss = 0.55646022\n",
            "Iteration 99, loss = 0.53992757\n",
            "Iteration 100, loss = 0.55332991\n",
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32869291\n",
            "Iteration 2, loss = 1.27106705\n",
            "Iteration 3, loss = 1.25442029\n",
            "Iteration 4, loss = 1.25323480\n",
            "Iteration 5, loss = 1.24458715\n",
            "Iteration 6, loss = 1.23595260\n",
            "Iteration 7, loss = 1.22427102\n",
            "Iteration 8, loss = 1.23259192\n",
            "Iteration 9, loss = 1.23997231\n",
            "Iteration 10, loss = 1.21428586\n",
            "Iteration 11, loss = 1.21806035\n",
            "Iteration 12, loss = 1.21145775\n",
            "Iteration 13, loss = 1.20749639\n",
            "Iteration 14, loss = 1.20516228\n",
            "Iteration 15, loss = 1.19968968\n",
            "Iteration 16, loss = 1.19830481\n",
            "Iteration 17, loss = 1.20188434\n",
            "Iteration 18, loss = 1.18434898\n",
            "Iteration 19, loss = 1.18287903\n",
            "Iteration 20, loss = 1.20658399\n",
            "Iteration 21, loss = 1.19323649\n",
            "Iteration 22, loss = 1.18137914\n",
            "Iteration 23, loss = 1.17208990\n",
            "Iteration 24, loss = 1.16772393\n",
            "Iteration 25, loss = 1.17828344\n",
            "Iteration 26, loss = 1.18349371\n",
            "Iteration 27, loss = 1.17689741\n",
            "Iteration 28, loss = 1.15967263\n",
            "Iteration 29, loss = 1.15341823\n",
            "Iteration 30, loss = 1.15300018\n",
            "Iteration 31, loss = 1.15985254\n",
            "Iteration 32, loss = 1.15712820\n",
            "Iteration 33, loss = 1.18683483\n",
            "Iteration 34, loss = 1.14873292\n",
            "Iteration 35, loss = 1.13471376\n",
            "Iteration 36, loss = 1.14751825\n",
            "Iteration 37, loss = 1.12968744\n",
            "Iteration 38, loss = 1.13650149\n",
            "Iteration 39, loss = 1.15197592\n",
            "Iteration 40, loss = 1.14678817\n",
            "Iteration 41, loss = 1.12053805\n",
            "Iteration 42, loss = 1.14265414\n",
            "Iteration 43, loss = 1.10954118\n",
            "Iteration 44, loss = 1.10047280\n",
            "Iteration 45, loss = 1.11045559\n",
            "Iteration 46, loss = 1.11983217\n",
            "Iteration 47, loss = 1.12221012\n",
            "Iteration 48, loss = 1.10835661\n",
            "Iteration 49, loss = 1.10452760\n",
            "Iteration 50, loss = 1.12860304\n",
            "Iteration 51, loss = 1.10693815\n",
            "Iteration 52, loss = 1.12084678\n",
            "Iteration 53, loss = 1.12117187\n",
            "Iteration 54, loss = 1.10474684\n",
            "Iteration 55, loss = 1.11083433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31639738\n",
            "Iteration 2, loss = 1.26881218\n",
            "Iteration 3, loss = 1.25822652\n",
            "Iteration 4, loss = 1.24077994\n",
            "Iteration 5, loss = 1.23368181\n",
            "Iteration 6, loss = 1.22545777\n",
            "Iteration 7, loss = 1.22338906\n",
            "Iteration 8, loss = 1.22951873\n",
            "Iteration 9, loss = 1.21549538\n",
            "Iteration 10, loss = 1.21900222\n",
            "Iteration 11, loss = 1.20224426\n",
            "Iteration 12, loss = 1.20843680\n",
            "Iteration 13, loss = 1.20016212\n",
            "Iteration 14, loss = 1.19038675\n",
            "Iteration 15, loss = 1.19212232\n",
            "Iteration 16, loss = 1.18523719\n",
            "Iteration 17, loss = 1.18753565\n",
            "Iteration 18, loss = 1.19218178\n",
            "Iteration 19, loss = 1.17765837\n",
            "Iteration 20, loss = 1.20912572\n",
            "Iteration 21, loss = 1.20346209\n",
            "Iteration 22, loss = 1.18321788\n",
            "Iteration 23, loss = 1.18082799\n",
            "Iteration 24, loss = 1.16895801\n",
            "Iteration 25, loss = 1.16847122\n",
            "Iteration 26, loss = 1.18611506\n",
            "Iteration 27, loss = 1.17274376\n",
            "Iteration 28, loss = 1.15796783\n",
            "Iteration 29, loss = 1.15634752\n",
            "Iteration 30, loss = 1.15312727\n",
            "Iteration 31, loss = 1.14178183\n",
            "Iteration 32, loss = 1.14118898\n",
            "Iteration 33, loss = 1.15996594\n",
            "Iteration 34, loss = 1.13380661\n",
            "Iteration 35, loss = 1.12347994\n",
            "Iteration 36, loss = 1.12582727\n",
            "Iteration 37, loss = 1.11366333\n",
            "Iteration 38, loss = 1.12437857\n",
            "Iteration 39, loss = 1.18752428\n",
            "Iteration 40, loss = 1.19920780\n",
            "Iteration 41, loss = 1.14155392\n",
            "Iteration 42, loss = 1.14901075\n",
            "Iteration 43, loss = 1.10998480\n",
            "Iteration 44, loss = 1.11436759\n",
            "Iteration 45, loss = 1.13933969\n",
            "Iteration 46, loss = 1.12236571\n",
            "Iteration 47, loss = 1.09383428\n",
            "Iteration 48, loss = 1.08957846\n",
            "Iteration 49, loss = 1.08140576\n",
            "Iteration 50, loss = 1.15989193\n",
            "Iteration 51, loss = 1.13122230\n",
            "Iteration 52, loss = 1.14013132\n",
            "Iteration 53, loss = 1.14043648\n",
            "Iteration 54, loss = 1.08935641\n",
            "Iteration 55, loss = 1.06858397\n",
            "Iteration 56, loss = 1.07823021\n",
            "Iteration 57, loss = 1.08857020\n",
            "Iteration 58, loss = 1.08453498\n",
            "Iteration 59, loss = 1.06361870\n",
            "Iteration 60, loss = 1.10364491\n",
            "Iteration 61, loss = 1.06129283\n",
            "Iteration 62, loss = 1.06452904\n",
            "Iteration 63, loss = 1.08053759\n",
            "Iteration 64, loss = 1.05887955\n",
            "Iteration 65, loss = 1.05947364\n",
            "Iteration 66, loss = 1.03743322\n",
            "Iteration 67, loss = 1.02728396\n",
            "Iteration 68, loss = 1.06606132\n",
            "Iteration 69, loss = 1.04334965\n",
            "Iteration 70, loss = 1.02861278\n",
            "Iteration 71, loss = 1.01666213\n",
            "Iteration 72, loss = 1.03768717\n",
            "Iteration 73, loss = 1.05678646\n",
            "Iteration 74, loss = 1.05779637\n",
            "Iteration 75, loss = 1.05216948\n",
            "Iteration 76, loss = 1.09398068\n",
            "Iteration 77, loss = 1.10330192\n",
            "Iteration 78, loss = 1.08409992\n",
            "Iteration 79, loss = 1.04415605\n",
            "Iteration 80, loss = 1.03498462\n",
            "Iteration 81, loss = 1.00420173\n",
            "Iteration 82, loss = 0.99622966\n",
            "Iteration 83, loss = 0.97893169\n",
            "Iteration 84, loss = 0.97790204\n",
            "Iteration 85, loss = 0.97143300\n",
            "Iteration 86, loss = 1.10707987\n",
            "Iteration 87, loss = 1.02174656\n",
            "Iteration 88, loss = 1.00154341\n",
            "Iteration 89, loss = 1.03897440\n",
            "Iteration 90, loss = 0.99435476\n",
            "Iteration 91, loss = 0.98050467\n",
            "Iteration 92, loss = 0.97007274\n",
            "Iteration 93, loss = 0.97728566\n",
            "Iteration 94, loss = 0.99631857\n",
            "Iteration 95, loss = 0.94603995\n",
            "Iteration 96, loss = 0.93191294\n",
            "Iteration 97, loss = 0.96843235\n",
            "Iteration 98, loss = 1.06771015\n",
            "Iteration 99, loss = 1.13948205\n",
            "Iteration 100, loss = 1.09096290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32572172\n",
            "Iteration 2, loss = 1.27333860\n",
            "Iteration 3, loss = 1.26817820\n",
            "Iteration 4, loss = 1.26258960\n",
            "Iteration 5, loss = 1.24400908\n",
            "Iteration 6, loss = 1.24029983\n",
            "Iteration 7, loss = 1.23440753\n",
            "Iteration 8, loss = 1.25101367\n",
            "Iteration 9, loss = 1.23527912\n",
            "Iteration 10, loss = 1.22777764\n",
            "Iteration 11, loss = 1.22959954\n",
            "Iteration 12, loss = 1.23705740\n",
            "Iteration 13, loss = 1.22219147\n",
            "Iteration 14, loss = 1.21942391\n",
            "Iteration 15, loss = 1.22853611\n",
            "Iteration 16, loss = 1.21787857\n",
            "Iteration 17, loss = 1.21092240\n",
            "Iteration 18, loss = 1.20416545\n",
            "Iteration 19, loss = 1.19623989\n",
            "Iteration 20, loss = 1.20818347\n",
            "Iteration 21, loss = 1.19609983\n",
            "Iteration 22, loss = 1.21229095\n",
            "Iteration 23, loss = 1.18804729\n",
            "Iteration 24, loss = 1.19023102\n",
            "Iteration 25, loss = 1.19746859\n",
            "Iteration 26, loss = 1.20559814\n",
            "Iteration 27, loss = 1.18332083\n",
            "Iteration 28, loss = 1.16966128\n",
            "Iteration 29, loss = 1.17069595\n",
            "Iteration 30, loss = 1.15791195\n",
            "Iteration 31, loss = 1.17541300\n",
            "Iteration 32, loss = 1.17339804\n",
            "Iteration 33, loss = 1.17258104\n",
            "Iteration 34, loss = 1.15420591\n",
            "Iteration 35, loss = 1.15062951\n",
            "Iteration 36, loss = 1.15392087\n",
            "Iteration 37, loss = 1.13828893\n",
            "Iteration 38, loss = 1.14663776\n",
            "Iteration 39, loss = 1.17198762\n",
            "Iteration 40, loss = 1.14804722\n",
            "Iteration 41, loss = 1.13560244\n",
            "Iteration 42, loss = 1.14923616\n",
            "Iteration 43, loss = 1.11754427\n",
            "Iteration 44, loss = 1.11865010\n",
            "Iteration 45, loss = 1.13471272\n",
            "Iteration 46, loss = 1.11907812\n",
            "Iteration 47, loss = 1.11539143\n",
            "Iteration 48, loss = 1.11345932\n",
            "Iteration 49, loss = 1.10835443\n",
            "Iteration 50, loss = 1.14505226\n",
            "Iteration 51, loss = 1.10531894\n",
            "Iteration 52, loss = 1.13984200\n",
            "Iteration 53, loss = 1.13418464\n",
            "Iteration 54, loss = 1.10426653\n",
            "Iteration 55, loss = 1.13225309\n",
            "Iteration 56, loss = 1.14301662\n",
            "Iteration 57, loss = 1.11094923\n",
            "Iteration 58, loss = 1.08302385\n",
            "Iteration 59, loss = 1.07382377\n",
            "Iteration 60, loss = 1.11284492\n",
            "Iteration 61, loss = 1.06823845\n",
            "Iteration 62, loss = 1.06969159\n",
            "Iteration 63, loss = 1.06809066\n",
            "Iteration 64, loss = 1.05817588\n",
            "Iteration 65, loss = 1.07943326\n",
            "Iteration 66, loss = 1.04582821\n",
            "Iteration 67, loss = 1.04729974\n",
            "Iteration 68, loss = 1.09663126\n",
            "Iteration 69, loss = 1.05924238\n",
            "Iteration 70, loss = 1.04597039\n",
            "Iteration 71, loss = 1.03810466\n",
            "Iteration 72, loss = 1.04701111\n",
            "Iteration 73, loss = 1.02913014\n",
            "Iteration 74, loss = 1.04732255\n",
            "Iteration 75, loss = 1.04132610\n",
            "Iteration 76, loss = 1.03785328\n",
            "Iteration 77, loss = 1.03978965\n",
            "Iteration 78, loss = 1.01998215\n",
            "Iteration 79, loss = 1.02104836\n",
            "Iteration 80, loss = 1.07747877\n",
            "Iteration 81, loss = 1.02875864\n",
            "Iteration 82, loss = 1.01336578\n",
            "Iteration 83, loss = 0.99475325\n",
            "Iteration 84, loss = 1.01360766\n",
            "Iteration 85, loss = 0.99359024\n",
            "Iteration 86, loss = 1.12794332\n",
            "Iteration 87, loss = 1.04999488\n",
            "Iteration 88, loss = 1.02356625\n",
            "Iteration 89, loss = 1.02131356\n",
            "Iteration 90, loss = 1.03076735\n",
            "Iteration 91, loss = 0.98858238\n",
            "Iteration 92, loss = 0.98512448\n",
            "Iteration 93, loss = 1.02231031\n",
            "Iteration 94, loss = 0.98032156\n",
            "Iteration 95, loss = 1.03750762\n",
            "Iteration 96, loss = 1.02509375\n",
            "Iteration 97, loss = 0.99537255\n",
            "Iteration 98, loss = 0.99018068\n",
            "Iteration 99, loss = 1.00097909\n",
            "Iteration 100, loss = 0.99670114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34106419\n",
            "Iteration 2, loss = 1.28201430\n",
            "Iteration 3, loss = 1.26139954\n",
            "Iteration 4, loss = 1.25056603\n",
            "Iteration 5, loss = 1.24869246\n",
            "Iteration 6, loss = 1.23409331\n",
            "Iteration 7, loss = 1.23613004\n",
            "Iteration 8, loss = 1.25750729\n",
            "Iteration 9, loss = 1.23474755\n",
            "Iteration 10, loss = 1.22055069\n",
            "Iteration 11, loss = 1.22733148\n",
            "Iteration 12, loss = 1.21693088\n",
            "Iteration 13, loss = 1.22345505\n",
            "Iteration 14, loss = 1.20888898\n",
            "Iteration 15, loss = 1.21760855\n",
            "Iteration 16, loss = 1.20946647\n",
            "Iteration 17, loss = 1.20005231\n",
            "Iteration 18, loss = 1.19323229\n",
            "Iteration 19, loss = 1.18904640\n",
            "Iteration 20, loss = 1.21497139\n",
            "Iteration 21, loss = 1.19065276\n",
            "Iteration 22, loss = 1.19821799\n",
            "Iteration 23, loss = 1.18735943\n",
            "Iteration 24, loss = 1.17150863\n",
            "Iteration 25, loss = 1.17786033\n",
            "Iteration 26, loss = 1.18480398\n",
            "Iteration 27, loss = 1.17703575\n",
            "Iteration 28, loss = 1.15426195\n",
            "Iteration 29, loss = 1.16420573\n",
            "Iteration 30, loss = 1.15120581\n",
            "Iteration 31, loss = 1.16520672\n",
            "Iteration 32, loss = 1.16758497\n",
            "Iteration 33, loss = 1.15987530\n",
            "Iteration 34, loss = 1.16144057\n",
            "Iteration 35, loss = 1.13626089\n",
            "Iteration 36, loss = 1.14346768\n",
            "Iteration 37, loss = 1.12580237\n",
            "Iteration 38, loss = 1.13940077\n",
            "Iteration 39, loss = 1.14355061\n",
            "Iteration 40, loss = 1.17462969\n",
            "Iteration 41, loss = 1.12232963\n",
            "Iteration 42, loss = 1.13140559\n",
            "Iteration 43, loss = 1.11380833\n",
            "Iteration 44, loss = 1.10157624\n",
            "Iteration 45, loss = 1.11593019\n",
            "Iteration 46, loss = 1.12233387\n",
            "Iteration 47, loss = 1.15387121\n",
            "Iteration 48, loss = 1.13323709\n",
            "Iteration 49, loss = 1.12442530\n",
            "Iteration 50, loss = 1.16300010\n",
            "Iteration 51, loss = 1.11710548\n",
            "Iteration 52, loss = 1.12150067\n",
            "Iteration 53, loss = 1.09428544\n",
            "Iteration 54, loss = 1.10479578\n",
            "Iteration 55, loss = 1.13869495\n",
            "Iteration 56, loss = 1.13965517\n",
            "Iteration 57, loss = 1.11525802\n",
            "Iteration 58, loss = 1.08815609\n",
            "Iteration 59, loss = 1.06411827\n",
            "Iteration 60, loss = 1.08787578\n",
            "Iteration 61, loss = 1.06326895\n",
            "Iteration 62, loss = 1.05927349\n",
            "Iteration 63, loss = 1.06209527\n",
            "Iteration 64, loss = 1.04885783\n",
            "Iteration 65, loss = 1.05329524\n",
            "Iteration 66, loss = 1.04174428\n",
            "Iteration 67, loss = 1.05236890\n",
            "Iteration 68, loss = 1.03859389\n",
            "Iteration 69, loss = 1.05481866\n",
            "Iteration 70, loss = 1.04231403\n",
            "Iteration 71, loss = 1.04833932\n",
            "Iteration 72, loss = 1.04209400\n",
            "Iteration 73, loss = 1.03277147\n",
            "Iteration 74, loss = 1.03175417\n",
            "Iteration 75, loss = 1.02160912\n",
            "Iteration 76, loss = 1.03385644\n",
            "Iteration 77, loss = 1.03491516\n",
            "Iteration 78, loss = 1.01445743\n",
            "Iteration 79, loss = 1.01132305\n",
            "Iteration 80, loss = 1.05724708\n",
            "Iteration 81, loss = 1.01020791\n",
            "Iteration 82, loss = 1.02918160\n",
            "Iteration 83, loss = 1.04716210\n",
            "Iteration 84, loss = 1.03979618\n",
            "Iteration 85, loss = 1.02001745\n",
            "Iteration 86, loss = 1.00617935\n",
            "Iteration 87, loss = 0.97960383\n",
            "Iteration 88, loss = 0.97837823\n",
            "Iteration 89, loss = 1.00523800\n",
            "Iteration 90, loss = 1.00914370\n",
            "Iteration 91, loss = 0.97829912\n",
            "Iteration 92, loss = 1.00713299\n",
            "Iteration 93, loss = 1.04665091\n",
            "Iteration 94, loss = 1.01226244\n",
            "Iteration 95, loss = 1.08448422\n",
            "Iteration 96, loss = 1.00082630\n",
            "Iteration 97, loss = 0.97896797\n",
            "Iteration 98, loss = 0.97911754\n",
            "Iteration 99, loss = 0.95841766\n",
            "Iteration 100, loss = 0.99016010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34574893\n",
            "Iteration 2, loss = 1.28619082\n",
            "Iteration 3, loss = 1.25671596\n",
            "Iteration 4, loss = 1.24143781\n",
            "Iteration 5, loss = 1.24459264\n",
            "Iteration 6, loss = 1.22955077\n",
            "Iteration 7, loss = 1.22895619\n",
            "Iteration 8, loss = 1.23946519\n",
            "Iteration 9, loss = 1.22913214\n",
            "Iteration 10, loss = 1.21736937\n",
            "Iteration 11, loss = 1.24276844\n",
            "Iteration 12, loss = 1.21561265\n",
            "Iteration 13, loss = 1.21647183\n",
            "Iteration 14, loss = 1.21192742\n",
            "Iteration 15, loss = 1.21119325\n",
            "Iteration 16, loss = 1.20758925\n",
            "Iteration 17, loss = 1.20123461\n",
            "Iteration 18, loss = 1.20120041\n",
            "Iteration 19, loss = 1.19411595\n",
            "Iteration 20, loss = 1.19313999\n",
            "Iteration 21, loss = 1.18270717\n",
            "Iteration 22, loss = 1.19705592\n",
            "Iteration 23, loss = 1.19637297\n",
            "Iteration 24, loss = 1.18645205\n",
            "Iteration 25, loss = 1.18425496\n",
            "Iteration 26, loss = 1.17742004\n",
            "Iteration 27, loss = 1.17533750\n",
            "Iteration 28, loss = 1.16619596\n",
            "Iteration 29, loss = 1.16992320\n",
            "Iteration 30, loss = 1.16714918\n",
            "Iteration 31, loss = 1.18552394\n",
            "Iteration 32, loss = 1.18085604\n",
            "Iteration 33, loss = 1.16581158\n",
            "Iteration 34, loss = 1.17413491\n",
            "Iteration 35, loss = 1.14291621\n",
            "Iteration 36, loss = 1.15727142\n",
            "Iteration 37, loss = 1.14150783\n",
            "Iteration 38, loss = 1.15290370\n",
            "Iteration 39, loss = 1.13469222\n",
            "Iteration 40, loss = 1.15690265\n",
            "Iteration 41, loss = 1.13218559\n",
            "Iteration 42, loss = 1.14461129\n",
            "Iteration 43, loss = 1.14045647\n",
            "Iteration 44, loss = 1.12750217\n",
            "Iteration 45, loss = 1.12782030\n",
            "Iteration 46, loss = 1.12423330\n",
            "Iteration 47, loss = 1.13896581\n",
            "Iteration 48, loss = 1.11372123\n",
            "Iteration 49, loss = 1.10821577\n",
            "Iteration 50, loss = 1.18659959\n",
            "Iteration 51, loss = 1.14727033\n",
            "Iteration 52, loss = 1.09074542\n",
            "Iteration 53, loss = 1.09955126\n",
            "Iteration 54, loss = 1.10910475\n",
            "Iteration 55, loss = 1.15252478\n",
            "Iteration 56, loss = 1.11745273\n",
            "Iteration 57, loss = 1.10341274\n",
            "Iteration 58, loss = 1.08436325\n",
            "Iteration 59, loss = 1.08049322\n",
            "Iteration 60, loss = 1.09080535\n",
            "Iteration 61, loss = 1.07440482\n",
            "Iteration 62, loss = 1.07654638\n",
            "Iteration 63, loss = 1.07474286\n",
            "Iteration 64, loss = 1.06778830\n",
            "Iteration 65, loss = 1.06267217\n",
            "Iteration 66, loss = 1.04435089\n",
            "Iteration 67, loss = 1.05511769\n",
            "Iteration 68, loss = 1.08788591\n",
            "Iteration 69, loss = 1.05642639\n",
            "Iteration 70, loss = 1.07362770\n",
            "Iteration 71, loss = 1.08237380\n",
            "Iteration 72, loss = 1.07516093\n",
            "Iteration 73, loss = 1.06461652\n",
            "Iteration 74, loss = 1.07782721\n",
            "Iteration 75, loss = 1.04023343\n",
            "Iteration 76, loss = 1.04429700\n",
            "Iteration 77, loss = 1.06724527\n",
            "Iteration 78, loss = 1.02933620\n",
            "Iteration 79, loss = 1.03834545\n",
            "Iteration 80, loss = 1.05414910\n",
            "Iteration 81, loss = 1.04361180\n",
            "Iteration 82, loss = 1.04697983\n",
            "Iteration 83, loss = 1.02659140\n",
            "Iteration 84, loss = 1.02088491\n",
            "Iteration 85, loss = 1.00631764\n",
            "Iteration 86, loss = 0.98178860\n",
            "Iteration 87, loss = 0.96720780\n",
            "Iteration 88, loss = 0.99661004\n",
            "Iteration 89, loss = 1.02371089\n",
            "Iteration 90, loss = 1.04569009\n",
            "Iteration 91, loss = 0.99891891\n",
            "Iteration 92, loss = 0.99804636\n",
            "Iteration 93, loss = 1.03177865\n",
            "Iteration 94, loss = 0.98440601\n",
            "Iteration 95, loss = 1.08701468\n",
            "Iteration 96, loss = 1.00776164\n",
            "Iteration 97, loss = 1.01644079\n",
            "Iteration 98, loss = 0.99158835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33422846\n",
            "Iteration 2, loss = 1.24675940\n",
            "Iteration 3, loss = 1.25301126\n",
            "Iteration 4, loss = 1.25826488\n",
            "Iteration 5, loss = 1.25736994\n",
            "Iteration 6, loss = 1.23368840\n",
            "Iteration 7, loss = 1.22268033\n",
            "Iteration 8, loss = 1.21794351\n",
            "Iteration 9, loss = 1.20653655\n",
            "Iteration 10, loss = 1.21171165\n",
            "Iteration 11, loss = 1.20396823\n",
            "Iteration 12, loss = 1.19695114\n",
            "Iteration 13, loss = 1.19892846\n",
            "Iteration 14, loss = 1.19532149\n",
            "Iteration 15, loss = 1.19468629\n",
            "Iteration 16, loss = 1.19308170\n",
            "Iteration 17, loss = 1.19023749\n",
            "Iteration 18, loss = 1.18895230\n",
            "Iteration 19, loss = 1.18354060\n",
            "Iteration 20, loss = 1.19510942\n",
            "Iteration 21, loss = 1.18276817\n",
            "Iteration 22, loss = 1.18231318\n",
            "Iteration 23, loss = 1.20353020\n",
            "Iteration 24, loss = 1.19534658\n",
            "Iteration 25, loss = 1.17771572\n",
            "Iteration 26, loss = 1.16401057\n",
            "Iteration 27, loss = 1.17063124\n",
            "Iteration 28, loss = 1.15424972\n",
            "Iteration 29, loss = 1.15749010\n",
            "Iteration 30, loss = 1.14400313\n",
            "Iteration 31, loss = 1.14796523\n",
            "Iteration 32, loss = 1.14106526\n",
            "Iteration 33, loss = 1.14091674\n",
            "Iteration 34, loss = 1.13808040\n",
            "Iteration 35, loss = 1.11421338\n",
            "Iteration 36, loss = 1.13337333\n",
            "Iteration 37, loss = 1.10004224\n",
            "Iteration 38, loss = 1.11945687\n",
            "Iteration 39, loss = 1.14072573\n",
            "Iteration 40, loss = 1.19762462\n",
            "Iteration 41, loss = 1.15565119\n",
            "Iteration 42, loss = 1.14267286\n",
            "Iteration 43, loss = 1.11031542\n",
            "Iteration 44, loss = 1.10617557\n",
            "Iteration 45, loss = 1.10443191\n",
            "Iteration 46, loss = 1.10648193\n",
            "Iteration 47, loss = 1.10968186\n",
            "Iteration 48, loss = 1.09037519\n",
            "Iteration 49, loss = 1.09441311\n",
            "Iteration 50, loss = 1.12098290\n",
            "Iteration 51, loss = 1.09542777\n",
            "Iteration 52, loss = 1.09837188\n",
            "Iteration 53, loss = 1.12062698\n",
            "Iteration 54, loss = 1.11573697\n",
            "Iteration 55, loss = 1.12015392\n",
            "Iteration 56, loss = 1.11424154\n",
            "Iteration 57, loss = 1.09791702\n",
            "Iteration 58, loss = 1.07223813\n",
            "Iteration 59, loss = 1.08828933\n",
            "Iteration 60, loss = 1.06584253\n",
            "Iteration 61, loss = 1.05484034\n",
            "Iteration 62, loss = 1.05823607\n",
            "Iteration 63, loss = 1.05584850\n",
            "Iteration 64, loss = 1.05708921\n",
            "Iteration 65, loss = 1.03711568\n",
            "Iteration 66, loss = 1.04007556\n",
            "Iteration 67, loss = 1.03707339\n",
            "Iteration 68, loss = 1.06744011\n",
            "Iteration 69, loss = 1.04404058\n",
            "Iteration 70, loss = 1.02806357\n",
            "Iteration 71, loss = 1.01897635\n",
            "Iteration 72, loss = 1.03781888\n",
            "Iteration 73, loss = 1.07720640\n",
            "Iteration 74, loss = 1.07892947\n",
            "Iteration 75, loss = 1.02803173\n",
            "Iteration 76, loss = 1.03116271\n",
            "Iteration 77, loss = 1.01714670\n",
            "Iteration 78, loss = 1.00183820\n",
            "Iteration 79, loss = 1.01739185\n",
            "Iteration 80, loss = 1.02266025\n",
            "Iteration 81, loss = 1.01416222\n",
            "Iteration 82, loss = 1.00942062\n",
            "Iteration 83, loss = 1.00647935\n",
            "Iteration 84, loss = 1.02634267\n",
            "Iteration 85, loss = 1.01178866\n",
            "Iteration 86, loss = 0.97635561\n",
            "Iteration 87, loss = 0.96304065\n",
            "Iteration 88, loss = 0.98392189\n",
            "Iteration 89, loss = 1.00381732\n",
            "Iteration 90, loss = 1.07124118\n",
            "Iteration 91, loss = 1.02314628\n",
            "Iteration 92, loss = 1.02627597\n",
            "Iteration 93, loss = 1.04124491\n",
            "Iteration 94, loss = 1.00518101\n",
            "Iteration 95, loss = 1.04917813\n",
            "Iteration 96, loss = 1.01946748\n",
            "Iteration 97, loss = 0.98850564\n",
            "Iteration 98, loss = 0.98524497\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33605026\n",
            "Iteration 2, loss = 1.26139565\n",
            "Iteration 3, loss = 1.24448884\n",
            "Iteration 4, loss = 1.27399021\n",
            "Iteration 5, loss = 1.26209246\n",
            "Iteration 6, loss = 1.22904521\n",
            "Iteration 7, loss = 1.21901465\n",
            "Iteration 8, loss = 1.21525531\n",
            "Iteration 9, loss = 1.21526396\n",
            "Iteration 10, loss = 1.20645031\n",
            "Iteration 11, loss = 1.20688641\n",
            "Iteration 12, loss = 1.19913093\n",
            "Iteration 13, loss = 1.19560061\n",
            "Iteration 14, loss = 1.19383253\n",
            "Iteration 15, loss = 1.18356154\n",
            "Iteration 16, loss = 1.19194227\n",
            "Iteration 17, loss = 1.18113588\n",
            "Iteration 18, loss = 1.20194453\n",
            "Iteration 19, loss = 1.17094335\n",
            "Iteration 20, loss = 1.16212154\n",
            "Iteration 21, loss = 1.15676118\n",
            "Iteration 22, loss = 1.17642457\n",
            "Iteration 23, loss = 1.15845232\n",
            "Iteration 24, loss = 1.18230217\n",
            "Iteration 25, loss = 1.16387325\n",
            "Iteration 26, loss = 1.13545128\n",
            "Iteration 27, loss = 1.14888252\n",
            "Iteration 28, loss = 1.13904451\n",
            "Iteration 29, loss = 1.13195122\n",
            "Iteration 30, loss = 1.13725324\n",
            "Iteration 31, loss = 1.13428178\n",
            "Iteration 32, loss = 1.12415309\n",
            "Iteration 33, loss = 1.11148309\n",
            "Iteration 34, loss = 1.14934144\n",
            "Iteration 35, loss = 1.12642111\n",
            "Iteration 36, loss = 1.12719288\n",
            "Iteration 37, loss = 1.09390046\n",
            "Iteration 38, loss = 1.09827338\n",
            "Iteration 39, loss = 1.09559101\n",
            "Iteration 40, loss = 1.12278994\n",
            "Iteration 41, loss = 1.12089187\n",
            "Iteration 42, loss = 1.09671801\n",
            "Iteration 43, loss = 1.10777756\n",
            "Iteration 44, loss = 1.11524691\n",
            "Iteration 45, loss = 1.09947948\n",
            "Iteration 46, loss = 1.10904989\n",
            "Iteration 47, loss = 1.11354788\n",
            "Iteration 48, loss = 1.08443215\n",
            "Iteration 49, loss = 1.09759701\n",
            "Iteration 50, loss = 1.13961849\n",
            "Iteration 51, loss = 1.10189428\n",
            "Iteration 52, loss = 1.10717398\n",
            "Iteration 53, loss = 1.08058711\n",
            "Iteration 54, loss = 1.05495991\n",
            "Iteration 55, loss = 1.06299440\n",
            "Iteration 56, loss = 1.08530213\n",
            "Iteration 57, loss = 1.13123334\n",
            "Iteration 58, loss = 1.14030272\n",
            "Iteration 59, loss = 1.10392788\n",
            "Iteration 60, loss = 1.16609176\n",
            "Iteration 61, loss = 1.09751401\n",
            "Iteration 62, loss = 1.12400507\n",
            "Iteration 63, loss = 1.07409723\n",
            "Iteration 64, loss = 1.05945564\n",
            "Iteration 65, loss = 1.04278049\n",
            "Iteration 66, loss = 1.05432289\n",
            "Iteration 67, loss = 1.07533189\n",
            "Iteration 68, loss = 1.06009150\n",
            "Iteration 69, loss = 1.03646273\n",
            "Iteration 70, loss = 1.04708793\n",
            "Iteration 71, loss = 1.03696107\n",
            "Iteration 72, loss = 1.07995261\n",
            "Iteration 73, loss = 1.09054465\n",
            "Iteration 74, loss = 1.09754041\n",
            "Iteration 75, loss = 1.06707010\n",
            "Iteration 76, loss = 1.08322804\n",
            "Iteration 77, loss = 1.09354746\n",
            "Iteration 78, loss = 1.07731716\n",
            "Iteration 79, loss = 1.04086469\n",
            "Iteration 80, loss = 1.02498185\n",
            "Iteration 81, loss = 1.01933872\n",
            "Iteration 82, loss = 1.02564894\n",
            "Iteration 83, loss = 1.00246632\n",
            "Iteration 84, loss = 1.00914339\n",
            "Iteration 85, loss = 1.01236418\n",
            "Iteration 86, loss = 1.01254888\n",
            "Iteration 87, loss = 0.98616341\n",
            "Iteration 88, loss = 0.98865825\n",
            "Iteration 89, loss = 1.00601465\n",
            "Iteration 90, loss = 1.07676450\n",
            "Iteration 91, loss = 1.01755401\n",
            "Iteration 92, loss = 1.01212672\n",
            "Iteration 93, loss = 1.02550732\n",
            "Iteration 94, loss = 0.99649960\n",
            "Iteration 95, loss = 1.07144306\n",
            "Iteration 96, loss = 1.01292613\n",
            "Iteration 97, loss = 0.98580612\n",
            "Iteration 98, loss = 1.01572121\n",
            "Iteration 99, loss = 1.03555916\n",
            "Iteration 100, loss = 0.99244851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34035393\n",
            "Iteration 2, loss = 1.26987350\n",
            "Iteration 3, loss = 1.23967440\n",
            "Iteration 4, loss = 1.23147558\n",
            "Iteration 5, loss = 1.23744178\n",
            "Iteration 6, loss = 1.22223058\n",
            "Iteration 7, loss = 1.21619509\n",
            "Iteration 8, loss = 1.21295245\n",
            "Iteration 9, loss = 1.21674040\n",
            "Iteration 10, loss = 1.20921070\n",
            "Iteration 11, loss = 1.20929944\n",
            "Iteration 12, loss = 1.23643564\n",
            "Iteration 13, loss = 1.24418142\n",
            "Iteration 14, loss = 1.20999661\n",
            "Iteration 15, loss = 1.20603131\n",
            "Iteration 16, loss = 1.19832768\n",
            "Iteration 17, loss = 1.19317656\n",
            "Iteration 18, loss = 1.22915865\n",
            "Iteration 19, loss = 1.21282473\n",
            "Iteration 20, loss = 1.21628914\n",
            "Iteration 21, loss = 1.23740116\n",
            "Iteration 22, loss = 1.20628900\n",
            "Iteration 23, loss = 1.19294661\n",
            "Iteration 24, loss = 1.17977112\n",
            "Iteration 25, loss = 1.18241265\n",
            "Iteration 26, loss = 1.18716899\n",
            "Iteration 27, loss = 1.18497678\n",
            "Iteration 28, loss = 1.16848033\n",
            "Iteration 29, loss = 1.16337528\n",
            "Iteration 30, loss = 1.18025167\n",
            "Iteration 31, loss = 1.20212318\n",
            "Iteration 32, loss = 1.17490167\n",
            "Iteration 33, loss = 1.14576538\n",
            "Iteration 34, loss = 1.14657394\n",
            "Iteration 35, loss = 1.14705420\n",
            "Iteration 36, loss = 1.13829757\n",
            "Iteration 37, loss = 1.14017045\n",
            "Iteration 38, loss = 1.13173013\n",
            "Iteration 39, loss = 1.12113502\n",
            "Iteration 40, loss = 1.12820854\n",
            "Iteration 41, loss = 1.12135081\n",
            "Iteration 42, loss = 1.12618429\n",
            "Iteration 43, loss = 1.12292562\n",
            "Iteration 44, loss = 1.19762883\n",
            "Iteration 45, loss = 1.21456335\n",
            "Iteration 46, loss = 1.19085952\n",
            "Iteration 47, loss = 1.18543891\n",
            "Iteration 48, loss = 1.16071381\n",
            "Iteration 49, loss = 1.15436588\n",
            "Iteration 50, loss = 1.14394788\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33527111\n",
            "Iteration 2, loss = 1.23742040\n",
            "Iteration 3, loss = 1.23001798\n",
            "Iteration 4, loss = 1.22930778\n",
            "Iteration 5, loss = 1.22031325\n",
            "Iteration 6, loss = 1.20595464\n",
            "Iteration 7, loss = 1.20007085\n",
            "Iteration 8, loss = 1.19633839\n",
            "Iteration 9, loss = 1.22278090\n",
            "Iteration 10, loss = 1.20083535\n",
            "Iteration 11, loss = 1.20471643\n",
            "Iteration 12, loss = 1.24299963\n",
            "Iteration 13, loss = 1.25471307\n",
            "Iteration 14, loss = 1.19900333\n",
            "Iteration 15, loss = 1.19662926\n",
            "Iteration 16, loss = 1.19448533\n",
            "Iteration 17, loss = 1.19035574\n",
            "Iteration 18, loss = 1.21426430\n",
            "Iteration 19, loss = 1.19150729\n",
            "Iteration 20, loss = 1.19260001\n",
            "Iteration 21, loss = 1.18769274\n",
            "Iteration 22, loss = 1.18339332\n",
            "Iteration 23, loss = 1.18084675\n",
            "Iteration 24, loss = 1.17667438\n",
            "Iteration 25, loss = 1.17635046\n",
            "Iteration 26, loss = 1.16189607\n",
            "Iteration 27, loss = 1.16405034\n",
            "Iteration 28, loss = 1.17340781\n",
            "Iteration 29, loss = 1.15180915\n",
            "Iteration 30, loss = 1.16511172\n",
            "Iteration 31, loss = 1.16761117\n",
            "Iteration 32, loss = 1.15539221\n",
            "Iteration 33, loss = 1.14494909\n",
            "Iteration 34, loss = 1.13113090\n",
            "Iteration 35, loss = 1.12799901\n",
            "Iteration 36, loss = 1.12516140\n",
            "Iteration 37, loss = 1.13678163\n",
            "Iteration 38, loss = 1.13393642\n",
            "Iteration 39, loss = 1.10997591\n",
            "Iteration 40, loss = 1.11987330\n",
            "Iteration 41, loss = 1.11252591\n",
            "Iteration 42, loss = 1.11057446\n",
            "Iteration 43, loss = 1.10442205\n",
            "Iteration 44, loss = 1.11345001\n",
            "Iteration 45, loss = 1.16670171\n",
            "Iteration 46, loss = 1.14751971\n",
            "Iteration 47, loss = 1.10747755\n",
            "Iteration 48, loss = 1.08661544\n",
            "Iteration 49, loss = 1.11369021\n",
            "Iteration 50, loss = 1.10630659\n",
            "Iteration 51, loss = 1.15278463\n",
            "Iteration 52, loss = 1.11882893\n",
            "Iteration 53, loss = 1.12980142\n",
            "Iteration 54, loss = 1.10655700\n",
            "Iteration 55, loss = 1.08105658\n",
            "Iteration 56, loss = 1.08350117\n",
            "Iteration 57, loss = 1.07539908\n",
            "Iteration 58, loss = 1.06181701\n",
            "Iteration 59, loss = 1.05601262\n",
            "Iteration 60, loss = 1.04352163\n",
            "Iteration 61, loss = 1.04762583\n",
            "Iteration 62, loss = 1.04340995\n",
            "Iteration 63, loss = 1.03041660\n",
            "Iteration 64, loss = 1.02538171\n",
            "Iteration 65, loss = 1.02279507\n",
            "Iteration 66, loss = 1.01179004\n",
            "Iteration 67, loss = 1.13069976\n",
            "Iteration 68, loss = 1.06584497\n",
            "Iteration 69, loss = 1.03774635\n",
            "Iteration 70, loss = 1.03886875\n",
            "Iteration 71, loss = 1.04474038\n",
            "Iteration 72, loss = 1.05336111\n",
            "Iteration 73, loss = 1.04396524\n",
            "Iteration 74, loss = 1.02666989\n",
            "Iteration 75, loss = 1.02898700\n",
            "Iteration 76, loss = 1.03983055\n",
            "Iteration 77, loss = 1.02720231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32342841\n",
            "Iteration 2, loss = 1.23687925\n",
            "Iteration 3, loss = 1.22652069\n",
            "Iteration 4, loss = 1.20816766\n",
            "Iteration 5, loss = 1.20284858\n",
            "Iteration 6, loss = 1.19581729\n",
            "Iteration 7, loss = 1.19353056\n",
            "Iteration 8, loss = 1.18734227\n",
            "Iteration 9, loss = 1.19626513\n",
            "Iteration 10, loss = 1.17752838\n",
            "Iteration 11, loss = 1.17363394\n",
            "Iteration 12, loss = 1.18860933\n",
            "Iteration 13, loss = 1.23683903\n",
            "Iteration 14, loss = 1.18185868\n",
            "Iteration 15, loss = 1.16390452\n",
            "Iteration 16, loss = 1.16673322\n",
            "Iteration 17, loss = 1.16011304\n",
            "Iteration 18, loss = 1.19457397\n",
            "Iteration 19, loss = 1.19938520\n",
            "Iteration 20, loss = 1.18980048\n",
            "Iteration 21, loss = 1.21356299\n",
            "Iteration 22, loss = 1.19657348\n",
            "Iteration 23, loss = 1.15969378\n",
            "Iteration 24, loss = 1.15024889\n",
            "Iteration 25, loss = 1.16168220\n",
            "Iteration 26, loss = 1.15546223\n",
            "Iteration 27, loss = 1.14821477\n",
            "Iteration 28, loss = 1.13371409\n",
            "Iteration 29, loss = 1.13882846\n",
            "Iteration 30, loss = 1.14048374\n",
            "Iteration 31, loss = 1.13581415\n",
            "Iteration 32, loss = 1.12517462\n",
            "Iteration 33, loss = 1.12212772\n",
            "Iteration 34, loss = 1.13045094\n",
            "Iteration 35, loss = 1.11630654\n",
            "Iteration 36, loss = 1.10374052\n",
            "Iteration 37, loss = 1.10927037\n",
            "Iteration 38, loss = 1.10322759\n",
            "Iteration 39, loss = 1.10025803\n",
            "Iteration 40, loss = 1.08664740\n",
            "Iteration 41, loss = 1.07920376\n",
            "Iteration 42, loss = 1.07028951\n",
            "Iteration 43, loss = 1.08171949\n",
            "Iteration 44, loss = 1.08385527\n",
            "Iteration 45, loss = 1.11410428\n",
            "Iteration 46, loss = 1.10896194\n",
            "Iteration 47, loss = 1.08905300\n",
            "Iteration 48, loss = 1.07656871\n",
            "Iteration 49, loss = 1.06991348\n",
            "Iteration 50, loss = 1.12032274\n",
            "Iteration 51, loss = 1.17138012\n",
            "Iteration 52, loss = 1.11346228\n",
            "Iteration 53, loss = 1.08627614\n",
            "Iteration 54, loss = 1.06528203\n",
            "Iteration 55, loss = 1.05896634\n",
            "Iteration 56, loss = 1.05248640\n",
            "Iteration 57, loss = 1.03823071\n",
            "Iteration 58, loss = 1.03803872\n",
            "Iteration 59, loss = 1.04148731\n",
            "Iteration 60, loss = 1.03561706\n",
            "Iteration 61, loss = 1.03156358\n",
            "Iteration 62, loss = 1.03914563\n",
            "Iteration 63, loss = 1.02206759\n",
            "Iteration 64, loss = 1.00732104\n",
            "Iteration 65, loss = 1.01143360\n",
            "Iteration 66, loss = 1.02232513\n",
            "Iteration 67, loss = 1.15247524\n",
            "Iteration 68, loss = 1.11126376\n",
            "Iteration 69, loss = 1.07224653\n",
            "Iteration 70, loss = 1.08393968\n",
            "Iteration 71, loss = 1.06929028\n",
            "Iteration 72, loss = 1.01170978\n",
            "Iteration 73, loss = 1.01519029\n",
            "Iteration 74, loss = 1.00822114\n",
            "Iteration 75, loss = 1.00122002\n",
            "Iteration 76, loss = 0.99435719\n",
            "Iteration 77, loss = 1.00746566\n",
            "Iteration 78, loss = 1.05404998\n",
            "Iteration 79, loss = 0.99009924\n",
            "Iteration 80, loss = 1.01208149\n",
            "Iteration 81, loss = 0.99863860\n",
            "Iteration 82, loss = 1.02859641\n",
            "Iteration 83, loss = 0.99655346\n",
            "Iteration 84, loss = 0.99425865\n",
            "Iteration 85, loss = 0.97154369\n",
            "Iteration 86, loss = 0.97256151\n",
            "Iteration 87, loss = 0.96175634\n",
            "Iteration 88, loss = 0.97391512\n",
            "Iteration 89, loss = 1.00110498\n",
            "Iteration 90, loss = 0.95000196\n",
            "Iteration 91, loss = 0.94976774\n",
            "Iteration 92, loss = 0.93865812\n",
            "Iteration 93, loss = 0.95883798\n",
            "Iteration 94, loss = 0.93346829\n",
            "Iteration 95, loss = 1.01270664\n",
            "Iteration 96, loss = 0.98717363\n",
            "Iteration 97, loss = 0.92757645\n",
            "Iteration 98, loss = 0.94492011\n",
            "Iteration 99, loss = 0.94361208\n",
            "Iteration 100, loss = 0.92023248\n",
            "8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39817958\n",
            "Iteration 2, loss = 1.30563883\n",
            "Iteration 3, loss = 1.23587909\n",
            "Iteration 4, loss = 1.16395434\n",
            "Iteration 5, loss = 1.07679269\n",
            "Iteration 6, loss = 0.96704646\n",
            "Iteration 7, loss = 0.84487172\n",
            "Iteration 8, loss = 0.71855207\n",
            "Iteration 9, loss = 0.59945513\n",
            "Iteration 10, loss = 0.49591634\n",
            "Iteration 11, loss = 0.41281753\n",
            "Iteration 12, loss = 0.34497087\n",
            "Iteration 13, loss = 0.29235839\n",
            "Iteration 14, loss = 0.25376162\n",
            "Iteration 15, loss = 0.21026280\n",
            "Iteration 16, loss = 0.18164266\n",
            "Iteration 17, loss = 0.16150010\n",
            "Iteration 18, loss = 0.13470669\n",
            "Iteration 19, loss = 0.11636399\n",
            "Iteration 20, loss = 0.09773863\n",
            "Iteration 21, loss = 0.09498341\n",
            "Iteration 22, loss = 0.07632404\n",
            "Iteration 23, loss = 0.06956398\n",
            "Iteration 24, loss = 0.05786444\n",
            "Iteration 25, loss = 0.04966042\n",
            "Iteration 26, loss = 0.04364944\n",
            "Iteration 27, loss = 0.03926662\n",
            "Iteration 28, loss = 0.03365590\n",
            "Iteration 29, loss = 0.03056945\n",
            "Iteration 30, loss = 0.02776477\n",
            "Iteration 31, loss = 0.02503142\n",
            "Iteration 32, loss = 0.02192037\n",
            "Iteration 33, loss = 0.01994054\n",
            "Iteration 34, loss = 0.01830046\n",
            "Iteration 35, loss = 0.01666564\n",
            "Iteration 36, loss = 0.01517437\n",
            "Iteration 37, loss = 0.01409599\n",
            "Iteration 38, loss = 0.01320720\n",
            "Iteration 39, loss = 0.01220502\n",
            "Iteration 40, loss = 0.01190877\n",
            "Iteration 41, loss = 0.01065697\n",
            "Iteration 42, loss = 0.01006393\n",
            "Iteration 43, loss = 0.00970356\n",
            "Iteration 44, loss = 0.00867519\n",
            "Iteration 45, loss = 0.00918738\n",
            "Iteration 46, loss = 0.00836764\n",
            "Iteration 47, loss = 0.00744783\n",
            "Iteration 48, loss = 0.00821178\n",
            "Iteration 49, loss = 0.00714097\n",
            "Iteration 50, loss = 0.00632576\n",
            "Iteration 51, loss = 0.00724842\n",
            "Iteration 52, loss = 0.00632320\n",
            "Iteration 53, loss = 0.00564250\n",
            "Iteration 54, loss = 0.00585744\n",
            "Iteration 55, loss = 0.00520004\n",
            "Iteration 56, loss = 0.00523347\n",
            "Iteration 57, loss = 0.00472571\n",
            "Iteration 58, loss = 0.00580228\n",
            "Iteration 59, loss = 0.00485522\n",
            "Iteration 60, loss = 0.00460199\n",
            "Iteration 61, loss = 0.00439957\n",
            "Iteration 62, loss = 0.00403731\n",
            "Iteration 63, loss = 0.00389290\n",
            "Iteration 64, loss = 0.00371767\n",
            "Iteration 65, loss = 0.00375347\n",
            "Iteration 66, loss = 0.00338624\n",
            "Iteration 67, loss = 0.00354521\n",
            "Iteration 68, loss = 0.00344188\n",
            "Iteration 69, loss = 0.00331587\n",
            "Iteration 70, loss = 0.00318867\n",
            "Iteration 71, loss = 0.00309594\n",
            "Iteration 72, loss = 0.00304386\n",
            "Iteration 73, loss = 0.00320321\n",
            "Iteration 74, loss = 0.00357601\n",
            "Iteration 75, loss = 0.00312776\n",
            "Iteration 76, loss = 0.00289731\n",
            "Iteration 77, loss = 0.00287015\n",
            "Iteration 78, loss = 0.00280436\n",
            "Iteration 79, loss = 0.00259374\n",
            "Iteration 80, loss = 0.00343301\n",
            "Iteration 81, loss = 0.00322553\n",
            "Iteration 82, loss = 0.00301963\n",
            "Iteration 83, loss = 0.00283729\n",
            "Iteration 84, loss = 0.00273563\n",
            "Iteration 85, loss = 0.00243542\n",
            "Iteration 86, loss = 0.00241168\n",
            "Iteration 87, loss = 0.00231246\n",
            "Iteration 88, loss = 0.00233616\n",
            "Iteration 89, loss = 0.00225254\n",
            "Iteration 90, loss = 0.00223820\n",
            "Iteration 91, loss = 0.00221830\n",
            "Iteration 92, loss = 0.00213090\n",
            "Iteration 93, loss = 0.00213935\n",
            "Iteration 94, loss = 0.00207262\n",
            "Iteration 95, loss = 0.00215941\n",
            "Iteration 96, loss = 0.00203407\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39961264\n",
            "Iteration 2, loss = 1.30421391\n",
            "Iteration 3, loss = 1.23462738\n",
            "Iteration 4, loss = 1.16376352\n",
            "Iteration 5, loss = 1.07952163\n",
            "Iteration 6, loss = 0.97436227\n",
            "Iteration 7, loss = 0.85752162\n",
            "Iteration 8, loss = 0.73578977\n",
            "Iteration 9, loss = 0.62018248\n",
            "Iteration 10, loss = 0.51643455\n",
            "Iteration 11, loss = 0.43435201\n",
            "Iteration 12, loss = 0.36275157\n",
            "Iteration 13, loss = 0.31051116\n",
            "Iteration 14, loss = 0.26919663\n",
            "Iteration 15, loss = 0.22907423\n",
            "Iteration 16, loss = 0.19675579\n",
            "Iteration 17, loss = 0.16995053\n",
            "Iteration 18, loss = 0.14832669\n",
            "Iteration 19, loss = 0.12627084\n",
            "Iteration 20, loss = 0.11078482\n",
            "Iteration 21, loss = 0.09538427\n",
            "Iteration 22, loss = 0.08299067\n",
            "Iteration 23, loss = 0.07090590\n",
            "Iteration 24, loss = 0.06253157\n",
            "Iteration 25, loss = 0.05591373\n",
            "Iteration 26, loss = 0.04692105\n",
            "Iteration 27, loss = 0.04277949\n",
            "Iteration 28, loss = 0.03689345\n",
            "Iteration 29, loss = 0.03227754\n",
            "Iteration 30, loss = 0.03198610\n",
            "Iteration 31, loss = 0.02655374\n",
            "Iteration 32, loss = 0.02371484\n",
            "Iteration 33, loss = 0.02126408\n",
            "Iteration 34, loss = 0.01942275\n",
            "Iteration 35, loss = 0.01774593\n",
            "Iteration 36, loss = 0.01607395\n",
            "Iteration 37, loss = 0.01486927\n",
            "Iteration 38, loss = 0.01354181\n",
            "Iteration 39, loss = 0.01254766\n",
            "Iteration 40, loss = 0.01328283\n",
            "Iteration 41, loss = 0.01127217\n",
            "Iteration 42, loss = 0.01060183\n",
            "Iteration 43, loss = 0.00996910\n",
            "Iteration 44, loss = 0.00891104\n",
            "Iteration 45, loss = 0.00924284\n",
            "Iteration 46, loss = 0.00853301\n",
            "Iteration 47, loss = 0.00757837\n",
            "Iteration 48, loss = 0.00794587\n",
            "Iteration 49, loss = 0.00714922\n",
            "Iteration 50, loss = 0.00647950\n",
            "Iteration 51, loss = 0.00719891\n",
            "Iteration 52, loss = 0.00643853\n",
            "Iteration 53, loss = 0.00566353\n",
            "Iteration 54, loss = 0.00571581\n",
            "Iteration 55, loss = 0.00511235\n",
            "Iteration 56, loss = 0.00528297\n",
            "Iteration 57, loss = 0.00481308\n",
            "Iteration 58, loss = 0.00562823\n",
            "Iteration 59, loss = 0.00474348\n",
            "Iteration 60, loss = 0.00471842\n",
            "Iteration 61, loss = 0.00437235\n",
            "Iteration 62, loss = 0.00409399\n",
            "Iteration 63, loss = 0.00392279\n",
            "Iteration 64, loss = 0.00374094\n",
            "Iteration 65, loss = 0.00385798\n",
            "Iteration 66, loss = 0.00331215\n",
            "Iteration 67, loss = 0.00374196\n",
            "Iteration 68, loss = 0.00354366\n",
            "Iteration 69, loss = 0.00335536\n",
            "Iteration 70, loss = 0.00323169\n",
            "Iteration 71, loss = 0.00308296\n",
            "Iteration 72, loss = 0.00299835\n",
            "Iteration 73, loss = 0.00314853\n",
            "Iteration 74, loss = 0.00347569\n",
            "Iteration 75, loss = 0.00305012\n",
            "Iteration 76, loss = 0.00285190\n",
            "Iteration 77, loss = 0.00281233\n",
            "Iteration 78, loss = 0.00273586\n",
            "Iteration 79, loss = 0.00256439\n",
            "Iteration 80, loss = 0.00327654\n",
            "Iteration 81, loss = 0.00307435\n",
            "Iteration 82, loss = 0.00291841\n",
            "Iteration 83, loss = 0.00281084\n",
            "Iteration 84, loss = 0.00267798\n",
            "Iteration 85, loss = 0.00237731\n",
            "Iteration 86, loss = 0.00236076\n",
            "Iteration 87, loss = 0.00225682\n",
            "Iteration 88, loss = 0.00227326\n",
            "Iteration 89, loss = 0.00218105\n",
            "Iteration 90, loss = 0.00216108\n",
            "Iteration 91, loss = 0.00214364\n",
            "Iteration 92, loss = 0.00205925\n",
            "Iteration 93, loss = 0.00206283\n",
            "Iteration 94, loss = 0.00199605\n",
            "Iteration 95, loss = 0.00206315\n",
            "Iteration 96, loss = 0.00195757\n",
            "Iteration 97, loss = 0.00196388\n",
            "Iteration 98, loss = 0.00189639\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39959437\n",
            "Iteration 2, loss = 1.30548580\n",
            "Iteration 3, loss = 1.23696211\n",
            "Iteration 4, loss = 1.16627684\n",
            "Iteration 5, loss = 1.08188282\n",
            "Iteration 6, loss = 0.97485695\n",
            "Iteration 7, loss = 0.85431766\n",
            "Iteration 8, loss = 0.72645150\n",
            "Iteration 9, loss = 0.60743964\n",
            "Iteration 10, loss = 0.50267564\n",
            "Iteration 11, loss = 0.42368541\n",
            "Iteration 12, loss = 0.35175607\n",
            "Iteration 13, loss = 0.30202983\n",
            "Iteration 14, loss = 0.26116869\n",
            "Iteration 15, loss = 0.22198958\n",
            "Iteration 16, loss = 0.19112478\n",
            "Iteration 17, loss = 0.16488115\n",
            "Iteration 18, loss = 0.14754329\n",
            "Iteration 19, loss = 0.12547941\n",
            "Iteration 20, loss = 0.11091406\n",
            "Iteration 21, loss = 0.09487566\n",
            "Iteration 22, loss = 0.08361466\n",
            "Iteration 23, loss = 0.06927561\n",
            "Iteration 24, loss = 0.06248238\n",
            "Iteration 25, loss = 0.05508866\n",
            "Iteration 26, loss = 0.04699104\n",
            "Iteration 27, loss = 0.04348982\n",
            "Iteration 28, loss = 0.03657019\n",
            "Iteration 29, loss = 0.03239541\n",
            "Iteration 30, loss = 0.03128429\n",
            "Iteration 31, loss = 0.02678127\n",
            "Iteration 32, loss = 0.02311152\n",
            "Iteration 33, loss = 0.02061990\n",
            "Iteration 34, loss = 0.01859951\n",
            "Iteration 35, loss = 0.01686260\n",
            "Iteration 36, loss = 0.01538833\n",
            "Iteration 37, loss = 0.01415153\n",
            "Iteration 38, loss = 0.01320684\n",
            "Iteration 39, loss = 0.01221581\n",
            "Iteration 40, loss = 0.01238388\n",
            "Iteration 41, loss = 0.01104168\n",
            "Iteration 42, loss = 0.01001916\n",
            "Iteration 43, loss = 0.00961001\n",
            "Iteration 44, loss = 0.00857840\n",
            "Iteration 45, loss = 0.00818582\n",
            "Iteration 46, loss = 0.00778175\n",
            "Iteration 47, loss = 0.00729553\n",
            "Iteration 48, loss = 0.00747764\n",
            "Iteration 49, loss = 0.00735266\n",
            "Iteration 50, loss = 0.00627293\n",
            "Iteration 51, loss = 0.00690123\n",
            "Iteration 52, loss = 0.00660531\n",
            "Iteration 53, loss = 0.00572760\n",
            "Iteration 54, loss = 0.00590694\n",
            "Iteration 55, loss = 0.00511947\n",
            "Iteration 56, loss = 0.00535970\n",
            "Iteration 57, loss = 0.00476608\n",
            "Iteration 58, loss = 0.00530504\n",
            "Iteration 59, loss = 0.00473078\n",
            "Iteration 60, loss = 0.00452479\n",
            "Iteration 61, loss = 0.00433149\n",
            "Iteration 62, loss = 0.00404474\n",
            "Iteration 63, loss = 0.00389923\n",
            "Iteration 64, loss = 0.00381754\n",
            "Iteration 65, loss = 0.00365070\n",
            "Iteration 66, loss = 0.00366974\n",
            "Iteration 67, loss = 0.00346193\n",
            "Iteration 68, loss = 0.00340367\n",
            "Iteration 69, loss = 0.00334987\n",
            "Iteration 70, loss = 0.00324519\n",
            "Iteration 71, loss = 0.00318843\n",
            "Iteration 72, loss = 0.00317727\n",
            "Iteration 73, loss = 0.00314290\n",
            "Iteration 74, loss = 0.00352529\n",
            "Iteration 75, loss = 0.00326716\n",
            "Iteration 76, loss = 0.00302105\n",
            "Iteration 77, loss = 0.00295366\n",
            "Iteration 78, loss = 0.00293364\n",
            "Iteration 79, loss = 0.00269224\n",
            "Iteration 80, loss = 0.00341832\n",
            "Iteration 81, loss = 0.00330919\n",
            "Iteration 82, loss = 0.00313965\n",
            "Iteration 83, loss = 0.00288179\n",
            "Iteration 84, loss = 0.00277940\n",
            "Iteration 85, loss = 0.00255727\n",
            "Iteration 86, loss = 0.00251713\n",
            "Iteration 87, loss = 0.00242761\n",
            "Iteration 88, loss = 0.00242483\n",
            "Iteration 89, loss = 0.00235976\n",
            "Iteration 90, loss = 0.00233783\n",
            "Iteration 91, loss = 0.00230983\n",
            "Iteration 92, loss = 0.00222434\n",
            "Iteration 93, loss = 0.00223783\n",
            "Iteration 94, loss = 0.00216601\n",
            "Iteration 95, loss = 0.00222481\n",
            "Iteration 96, loss = 0.00212224\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39947078\n",
            "Iteration 2, loss = 1.30651312\n",
            "Iteration 3, loss = 1.23852097\n",
            "Iteration 4, loss = 1.17003023\n",
            "Iteration 5, loss = 1.08685362\n",
            "Iteration 6, loss = 0.97941457\n",
            "Iteration 7, loss = 0.85991448\n",
            "Iteration 8, loss = 0.73078381\n",
            "Iteration 9, loss = 0.61374257\n",
            "Iteration 10, loss = 0.51082434\n",
            "Iteration 11, loss = 0.43578132\n",
            "Iteration 12, loss = 0.36602483\n",
            "Iteration 13, loss = 0.31620906\n",
            "Iteration 14, loss = 0.27728198\n",
            "Iteration 15, loss = 0.23432882\n",
            "Iteration 16, loss = 0.20335449\n",
            "Iteration 17, loss = 0.17889029\n",
            "Iteration 18, loss = 0.15534973\n",
            "Iteration 19, loss = 0.13393611\n",
            "Iteration 20, loss = 0.11936635\n",
            "Iteration 21, loss = 0.09761820\n",
            "Iteration 22, loss = 0.08706258\n",
            "Iteration 23, loss = 0.07356221\n",
            "Iteration 24, loss = 0.06483134\n",
            "Iteration 25, loss = 0.05719118\n",
            "Iteration 26, loss = 0.05237797\n",
            "Iteration 27, loss = 0.04456922\n",
            "Iteration 28, loss = 0.04125015\n",
            "Iteration 29, loss = 0.03444722\n",
            "Iteration 30, loss = 0.03297286\n",
            "Iteration 31, loss = 0.02795960\n",
            "Iteration 32, loss = 0.02534009\n",
            "Iteration 33, loss = 0.02289223\n",
            "Iteration 34, loss = 0.02113531\n",
            "Iteration 35, loss = 0.01914615\n",
            "Iteration 36, loss = 0.01729468\n",
            "Iteration 37, loss = 0.01604254\n",
            "Iteration 38, loss = 0.01486115\n",
            "Iteration 39, loss = 0.01367949\n",
            "Iteration 40, loss = 0.01434882\n",
            "Iteration 41, loss = 0.01254479\n",
            "Iteration 42, loss = 0.01120084\n",
            "Iteration 43, loss = 0.01054103\n",
            "Iteration 44, loss = 0.00976389\n",
            "Iteration 45, loss = 0.00975126\n",
            "Iteration 46, loss = 0.00892995\n",
            "Iteration 47, loss = 0.00843207\n",
            "Iteration 48, loss = 0.00808823\n",
            "Iteration 49, loss = 0.00767518\n",
            "Iteration 50, loss = 0.00709158\n",
            "Iteration 51, loss = 0.00744035\n",
            "Iteration 52, loss = 0.00738710\n",
            "Iteration 53, loss = 0.00633747\n",
            "Iteration 54, loss = 0.00646211\n",
            "Iteration 55, loss = 0.00567824\n",
            "Iteration 56, loss = 0.00585124\n",
            "Iteration 57, loss = 0.00545352\n",
            "Iteration 58, loss = 0.00586913\n",
            "Iteration 59, loss = 0.00499512\n",
            "Iteration 60, loss = 0.00505569\n",
            "Iteration 61, loss = 0.00471196\n",
            "Iteration 62, loss = 0.00444441\n",
            "Iteration 63, loss = 0.00428291\n",
            "Iteration 64, loss = 0.00414431\n",
            "Iteration 65, loss = 0.00396579\n",
            "Iteration 66, loss = 0.00382401\n",
            "Iteration 67, loss = 0.00385547\n",
            "Iteration 68, loss = 0.00370940\n",
            "Iteration 69, loss = 0.00360952\n",
            "Iteration 70, loss = 0.00344855\n",
            "Iteration 71, loss = 0.00334025\n",
            "Iteration 72, loss = 0.00325287\n",
            "Iteration 73, loss = 0.00331245\n",
            "Iteration 74, loss = 0.00354738\n",
            "Iteration 75, loss = 0.00317052\n",
            "Iteration 76, loss = 0.00297940\n",
            "Iteration 77, loss = 0.00292203\n",
            "Iteration 78, loss = 0.00286001\n",
            "Iteration 79, loss = 0.00271770\n",
            "Iteration 80, loss = 0.00331100\n",
            "Iteration 81, loss = 0.00301207\n",
            "Iteration 82, loss = 0.00284982\n",
            "Iteration 83, loss = 0.00278110\n",
            "Iteration 84, loss = 0.00266077\n",
            "Iteration 85, loss = 0.00242339\n",
            "Iteration 86, loss = 0.00247001\n",
            "Iteration 87, loss = 0.00230826\n",
            "Iteration 88, loss = 0.00227980\n",
            "Iteration 89, loss = 0.00224140\n",
            "Iteration 90, loss = 0.00222232\n",
            "Iteration 91, loss = 0.00216017\n",
            "Iteration 92, loss = 0.00209244\n",
            "Iteration 93, loss = 0.00210452\n",
            "Iteration 94, loss = 0.00202227\n",
            "Iteration 95, loss = 0.00205868\n",
            "Iteration 96, loss = 0.00197353\n",
            "Iteration 97, loss = 0.00196431\n",
            "Iteration 98, loss = 0.00191005\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39713087\n",
            "Iteration 2, loss = 1.30096610\n",
            "Iteration 3, loss = 1.22961905\n",
            "Iteration 4, loss = 1.16063091\n",
            "Iteration 5, loss = 1.07786901\n",
            "Iteration 6, loss = 0.97256074\n",
            "Iteration 7, loss = 0.85642764\n",
            "Iteration 8, loss = 0.72923043\n",
            "Iteration 9, loss = 0.61011005\n",
            "Iteration 10, loss = 0.50574555\n",
            "Iteration 11, loss = 0.42130489\n",
            "Iteration 12, loss = 0.35373322\n",
            "Iteration 13, loss = 0.30260040\n",
            "Iteration 14, loss = 0.25880472\n",
            "Iteration 15, loss = 0.22107250\n",
            "Iteration 16, loss = 0.18798773\n",
            "Iteration 17, loss = 0.16763517\n",
            "Iteration 18, loss = 0.13861603\n",
            "Iteration 19, loss = 0.12461116\n",
            "Iteration 20, loss = 0.10908330\n",
            "Iteration 21, loss = 0.08868270\n",
            "Iteration 22, loss = 0.07898002\n",
            "Iteration 23, loss = 0.06609945\n",
            "Iteration 24, loss = 0.05794554\n",
            "Iteration 25, loss = 0.04988811\n",
            "Iteration 26, loss = 0.04584579\n",
            "Iteration 27, loss = 0.03896469\n",
            "Iteration 28, loss = 0.03584445\n",
            "Iteration 29, loss = 0.03034814\n",
            "Iteration 30, loss = 0.02941913\n",
            "Iteration 31, loss = 0.02514112\n",
            "Iteration 32, loss = 0.02226796\n",
            "Iteration 33, loss = 0.02084335\n",
            "Iteration 34, loss = 0.01877684\n",
            "Iteration 35, loss = 0.01691675\n",
            "Iteration 36, loss = 0.01489579\n",
            "Iteration 37, loss = 0.01388841\n",
            "Iteration 38, loss = 0.01280389\n",
            "Iteration 39, loss = 0.01174908\n",
            "Iteration 40, loss = 0.01259433\n",
            "Iteration 41, loss = 0.01102765\n",
            "Iteration 42, loss = 0.00973230\n",
            "Iteration 43, loss = 0.00913415\n",
            "Iteration 44, loss = 0.00839829\n",
            "Iteration 45, loss = 0.00864926\n",
            "Iteration 46, loss = 0.00777647\n",
            "Iteration 47, loss = 0.00735678\n",
            "Iteration 48, loss = 0.00713695\n",
            "Iteration 49, loss = 0.00683941\n",
            "Iteration 50, loss = 0.00613206\n",
            "Iteration 51, loss = 0.00697822\n",
            "Iteration 52, loss = 0.00662776\n",
            "Iteration 53, loss = 0.00556612\n",
            "Iteration 54, loss = 0.00579284\n",
            "Iteration 55, loss = 0.00505470\n",
            "Iteration 56, loss = 0.00515869\n",
            "Iteration 57, loss = 0.00481114\n",
            "Iteration 58, loss = 0.00525148\n",
            "Iteration 59, loss = 0.00448989\n",
            "Iteration 60, loss = 0.00457970\n",
            "Iteration 61, loss = 0.00428742\n",
            "Iteration 62, loss = 0.00396756\n",
            "Iteration 63, loss = 0.00384398\n",
            "Iteration 64, loss = 0.00373099\n",
            "Iteration 65, loss = 0.00355860\n",
            "Iteration 66, loss = 0.00342066\n",
            "Iteration 67, loss = 0.00343153\n",
            "Iteration 68, loss = 0.00330974\n",
            "Iteration 69, loss = 0.00321584\n",
            "Iteration 70, loss = 0.00307091\n",
            "Iteration 71, loss = 0.00297894\n",
            "Iteration 72, loss = 0.00289554\n",
            "Iteration 73, loss = 0.00297215\n",
            "Iteration 74, loss = 0.00316702\n",
            "Iteration 75, loss = 0.00282792\n",
            "Iteration 76, loss = 0.00267424\n",
            "Iteration 77, loss = 0.00263009\n",
            "Iteration 78, loss = 0.00259243\n",
            "Iteration 79, loss = 0.00243148\n",
            "Iteration 80, loss = 0.00300196\n",
            "Iteration 81, loss = 0.00275970\n",
            "Iteration 82, loss = 0.00262503\n",
            "Iteration 83, loss = 0.00253472\n",
            "Iteration 84, loss = 0.00242360\n",
            "Iteration 85, loss = 0.00220854\n",
            "Iteration 86, loss = 0.00225153\n",
            "Iteration 87, loss = 0.00211049\n",
            "Iteration 88, loss = 0.00208892\n",
            "Iteration 89, loss = 0.00205413\n",
            "Iteration 90, loss = 0.00203480\n",
            "Iteration 91, loss = 0.00198361\n",
            "Iteration 92, loss = 0.00192198\n",
            "Iteration 93, loss = 0.00193343\n",
            "Iteration 94, loss = 0.00186177\n",
            "Iteration 95, loss = 0.00190346\n",
            "Iteration 96, loss = 0.00182193\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39878494\n",
            "Iteration 2, loss = 1.29916769\n",
            "Iteration 3, loss = 1.22406693\n",
            "Iteration 4, loss = 1.15378342\n",
            "Iteration 5, loss = 1.07155071\n",
            "Iteration 6, loss = 0.96810476\n",
            "Iteration 7, loss = 0.85483895\n",
            "Iteration 8, loss = 0.73011687\n",
            "Iteration 9, loss = 0.61528471\n",
            "Iteration 10, loss = 0.51684252\n",
            "Iteration 11, loss = 0.43393928\n",
            "Iteration 12, loss = 0.36627416\n",
            "Iteration 13, loss = 0.31804156\n",
            "Iteration 14, loss = 0.27445080\n",
            "Iteration 15, loss = 0.23693563\n",
            "Iteration 16, loss = 0.20305089\n",
            "Iteration 17, loss = 0.18141815\n",
            "Iteration 18, loss = 0.15327514\n",
            "Iteration 19, loss = 0.13741785\n",
            "Iteration 20, loss = 0.12062352\n",
            "Iteration 21, loss = 0.10062251\n",
            "Iteration 22, loss = 0.08796181\n",
            "Iteration 23, loss = 0.07588548\n",
            "Iteration 24, loss = 0.06586631\n",
            "Iteration 25, loss = 0.05660525\n",
            "Iteration 26, loss = 0.05121702\n",
            "Iteration 27, loss = 0.04339020\n",
            "Iteration 28, loss = 0.03841951\n",
            "Iteration 29, loss = 0.03324880\n",
            "Iteration 30, loss = 0.03028479\n",
            "Iteration 31, loss = 0.02600203\n",
            "Iteration 32, loss = 0.02299893\n",
            "Iteration 33, loss = 0.02061781\n",
            "Iteration 34, loss = 0.01859770\n",
            "Iteration 35, loss = 0.01678418\n",
            "Iteration 36, loss = 0.01488835\n",
            "Iteration 37, loss = 0.01371429\n",
            "Iteration 38, loss = 0.01258711\n",
            "Iteration 39, loss = 0.01157998\n",
            "Iteration 40, loss = 0.01220645\n",
            "Iteration 41, loss = 0.01060002\n",
            "Iteration 42, loss = 0.00944328\n",
            "Iteration 43, loss = 0.00879627\n",
            "Iteration 44, loss = 0.00802607\n",
            "Iteration 45, loss = 0.00815292\n",
            "Iteration 46, loss = 0.00738912\n",
            "Iteration 47, loss = 0.00690272\n",
            "Iteration 48, loss = 0.00679167\n",
            "Iteration 49, loss = 0.00638098\n",
            "Iteration 50, loss = 0.00580032\n",
            "Iteration 51, loss = 0.00651752\n",
            "Iteration 52, loss = 0.00610448\n",
            "Iteration 53, loss = 0.00525187\n",
            "Iteration 54, loss = 0.00546746\n",
            "Iteration 55, loss = 0.00476631\n",
            "Iteration 56, loss = 0.00496900\n",
            "Iteration 57, loss = 0.00456133\n",
            "Iteration 58, loss = 0.00533984\n",
            "Iteration 59, loss = 0.00431764\n",
            "Iteration 60, loss = 0.00441578\n",
            "Iteration 61, loss = 0.00406149\n",
            "Iteration 62, loss = 0.00376157\n",
            "Iteration 63, loss = 0.00362632\n",
            "Iteration 64, loss = 0.00350649\n",
            "Iteration 65, loss = 0.00335907\n",
            "Iteration 66, loss = 0.00325425\n",
            "Iteration 67, loss = 0.00326340\n",
            "Iteration 68, loss = 0.00318699\n",
            "Iteration 69, loss = 0.00311619\n",
            "Iteration 70, loss = 0.00298750\n",
            "Iteration 71, loss = 0.00290166\n",
            "Iteration 72, loss = 0.00284337\n",
            "Iteration 73, loss = 0.00296601\n",
            "Iteration 74, loss = 0.00321799\n",
            "Iteration 75, loss = 0.00282355\n",
            "Iteration 76, loss = 0.00270574\n",
            "Iteration 77, loss = 0.00267670\n",
            "Iteration 78, loss = 0.00259726\n",
            "Iteration 79, loss = 0.00248844\n",
            "Iteration 80, loss = 0.00311201\n",
            "Iteration 81, loss = 0.00274155\n",
            "Iteration 82, loss = 0.00267726\n",
            "Iteration 83, loss = 0.00261926\n",
            "Iteration 84, loss = 0.00247266\n",
            "Iteration 85, loss = 0.00226266\n",
            "Iteration 86, loss = 0.00231364\n",
            "Iteration 87, loss = 0.00216972\n",
            "Iteration 88, loss = 0.00215117\n",
            "Iteration 89, loss = 0.00211405\n",
            "Iteration 90, loss = 0.00209445\n",
            "Iteration 91, loss = 0.00207026\n",
            "Iteration 92, loss = 0.00199234\n",
            "Iteration 93, loss = 0.00201292\n",
            "Iteration 94, loss = 0.00193751\n",
            "Iteration 95, loss = 0.00198669\n",
            "Iteration 96, loss = 0.00189882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.40336373\n",
            "Iteration 2, loss = 1.30587984\n",
            "Iteration 3, loss = 1.23378565\n",
            "Iteration 4, loss = 1.16826028\n",
            "Iteration 5, loss = 1.09038674\n",
            "Iteration 6, loss = 0.99202443\n",
            "Iteration 7, loss = 0.87726171\n",
            "Iteration 8, loss = 0.74750133\n",
            "Iteration 9, loss = 0.62934409\n",
            "Iteration 10, loss = 0.52398677\n",
            "Iteration 11, loss = 0.43901095\n",
            "Iteration 12, loss = 0.37094111\n",
            "Iteration 13, loss = 0.31995548\n",
            "Iteration 14, loss = 0.27483553\n",
            "Iteration 15, loss = 0.23716914\n",
            "Iteration 16, loss = 0.20062704\n",
            "Iteration 17, loss = 0.18310561\n",
            "Iteration 18, loss = 0.15187441\n",
            "Iteration 19, loss = 0.13394390\n",
            "Iteration 20, loss = 0.11667032\n",
            "Iteration 21, loss = 0.09803531\n",
            "Iteration 22, loss = 0.08347658\n",
            "Iteration 23, loss = 0.07204768\n",
            "Iteration 24, loss = 0.06210106\n",
            "Iteration 25, loss = 0.05387673\n",
            "Iteration 26, loss = 0.04701897\n",
            "Iteration 27, loss = 0.04118006\n",
            "Iteration 28, loss = 0.03644451\n",
            "Iteration 29, loss = 0.03132348\n",
            "Iteration 30, loss = 0.02952796\n",
            "Iteration 31, loss = 0.02552448\n",
            "Iteration 32, loss = 0.02229198\n",
            "Iteration 33, loss = 0.02083653\n",
            "Iteration 34, loss = 0.01830860\n",
            "Iteration 35, loss = 0.01671764\n",
            "Iteration 36, loss = 0.01467677\n",
            "Iteration 37, loss = 0.01389387\n",
            "Iteration 38, loss = 0.01251099\n",
            "Iteration 39, loss = 0.01140103\n",
            "Iteration 40, loss = 0.01286118\n",
            "Iteration 41, loss = 0.01088300\n",
            "Iteration 42, loss = 0.00954521\n",
            "Iteration 43, loss = 0.00883817\n",
            "Iteration 44, loss = 0.00808083\n",
            "Iteration 45, loss = 0.00835503\n",
            "Iteration 46, loss = 0.00740587\n",
            "Iteration 47, loss = 0.00692781\n",
            "Iteration 48, loss = 0.00691481\n",
            "Iteration 49, loss = 0.00640770\n",
            "Iteration 50, loss = 0.00579956\n",
            "Iteration 51, loss = 0.00681961\n",
            "Iteration 52, loss = 0.00626772\n",
            "Iteration 53, loss = 0.00525675\n",
            "Iteration 54, loss = 0.00576477\n",
            "Iteration 55, loss = 0.00485984\n",
            "Iteration 56, loss = 0.00511839\n",
            "Iteration 57, loss = 0.00460952\n",
            "Iteration 58, loss = 0.00556627\n",
            "Iteration 59, loss = 0.00441182\n",
            "Iteration 60, loss = 0.00455830\n",
            "Iteration 61, loss = 0.00411036\n",
            "Iteration 62, loss = 0.00382764\n",
            "Iteration 63, loss = 0.00369573\n",
            "Iteration 64, loss = 0.00351390\n",
            "Iteration 65, loss = 0.00336590\n",
            "Iteration 66, loss = 0.00321461\n",
            "Iteration 67, loss = 0.00328338\n",
            "Iteration 68, loss = 0.00319580\n",
            "Iteration 69, loss = 0.00309558\n",
            "Iteration 70, loss = 0.00296518\n",
            "Iteration 71, loss = 0.00288117\n",
            "Iteration 72, loss = 0.00282358\n",
            "Iteration 73, loss = 0.00294273\n",
            "Iteration 74, loss = 0.00327795\n",
            "Iteration 75, loss = 0.00284664\n",
            "Iteration 76, loss = 0.00270344\n",
            "Iteration 77, loss = 0.00267151\n",
            "Iteration 78, loss = 0.00259255\n",
            "Iteration 79, loss = 0.00248385\n",
            "Iteration 80, loss = 0.00314713\n",
            "Iteration 81, loss = 0.00280289\n",
            "Iteration 82, loss = 0.00269874\n",
            "Iteration 83, loss = 0.00264118\n",
            "Iteration 84, loss = 0.00250872\n",
            "Iteration 85, loss = 0.00226598\n",
            "Iteration 86, loss = 0.00230472\n",
            "Iteration 87, loss = 0.00217619\n",
            "Iteration 88, loss = 0.00215860\n",
            "Iteration 89, loss = 0.00211015\n",
            "Iteration 90, loss = 0.00209224\n",
            "Iteration 91, loss = 0.00208026\n",
            "Iteration 92, loss = 0.00199917\n",
            "Iteration 93, loss = 0.00201485\n",
            "Iteration 94, loss = 0.00194142\n",
            "Iteration 95, loss = 0.00199770\n",
            "Iteration 96, loss = 0.00190802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39734400\n",
            "Iteration 2, loss = 1.30116799\n",
            "Iteration 3, loss = 1.23129331\n",
            "Iteration 4, loss = 1.16125583\n",
            "Iteration 5, loss = 1.07815509\n",
            "Iteration 6, loss = 0.97811026\n",
            "Iteration 7, loss = 0.86321441\n",
            "Iteration 8, loss = 0.74763678\n",
            "Iteration 9, loss = 0.62897770\n",
            "Iteration 10, loss = 0.52421752\n",
            "Iteration 11, loss = 0.43804623\n",
            "Iteration 12, loss = 0.36902233\n",
            "Iteration 13, loss = 0.31699444\n",
            "Iteration 14, loss = 0.26739155\n",
            "Iteration 15, loss = 0.23071825\n",
            "Iteration 16, loss = 0.19854580\n",
            "Iteration 17, loss = 0.17033799\n",
            "Iteration 18, loss = 0.14629462\n",
            "Iteration 19, loss = 0.12805114\n",
            "Iteration 20, loss = 0.10601888\n",
            "Iteration 21, loss = 0.09034058\n",
            "Iteration 22, loss = 0.08046653\n",
            "Iteration 23, loss = 0.06630154\n",
            "Iteration 24, loss = 0.05752544\n",
            "Iteration 25, loss = 0.04831846\n",
            "Iteration 26, loss = 0.04065588\n",
            "Iteration 27, loss = 0.03493513\n",
            "Iteration 28, loss = 0.03024934\n",
            "Iteration 29, loss = 0.02664066\n",
            "Iteration 30, loss = 0.02298867\n",
            "Iteration 31, loss = 0.02067456\n",
            "Iteration 32, loss = 0.01773479\n",
            "Iteration 33, loss = 0.01589848\n",
            "Iteration 34, loss = 0.01422628\n",
            "Iteration 35, loss = 0.01276073\n",
            "Iteration 36, loss = 0.01151055\n",
            "Iteration 37, loss = 0.01043290\n",
            "Iteration 38, loss = 0.00955679\n",
            "Iteration 39, loss = 0.00874743\n",
            "Iteration 40, loss = 0.00800551\n",
            "Iteration 41, loss = 0.00730375\n",
            "Iteration 42, loss = 0.00693111\n",
            "Iteration 43, loss = 0.00636388\n",
            "Iteration 44, loss = 0.00588694\n",
            "Iteration 45, loss = 0.00551286\n",
            "Iteration 46, loss = 0.00517009\n",
            "Iteration 47, loss = 0.00495245\n",
            "Iteration 48, loss = 0.00462780\n",
            "Iteration 49, loss = 0.00440082\n",
            "Iteration 50, loss = 0.00409497\n",
            "Iteration 51, loss = 0.00401001\n",
            "Iteration 52, loss = 0.00380919\n",
            "Iteration 53, loss = 0.00353522\n",
            "Iteration 54, loss = 0.00334133\n",
            "Iteration 55, loss = 0.00317356\n",
            "Iteration 56, loss = 0.00304375\n",
            "Iteration 57, loss = 0.00295598\n",
            "Iteration 58, loss = 0.00278588\n",
            "Iteration 59, loss = 0.00266943\n",
            "Iteration 60, loss = 0.00252622\n",
            "Iteration 61, loss = 0.00242180\n",
            "Iteration 62, loss = 0.00233779\n",
            "Iteration 63, loss = 0.00222607\n",
            "Iteration 64, loss = 0.00214499\n",
            "Iteration 65, loss = 0.00206921\n",
            "Iteration 66, loss = 0.00199541\n",
            "Iteration 67, loss = 0.00192720\n",
            "Iteration 68, loss = 0.00185611\n",
            "Iteration 69, loss = 0.00180549\n",
            "Iteration 70, loss = 0.00175251\n",
            "Iteration 71, loss = 0.00168148\n",
            "Iteration 72, loss = 0.00163207\n",
            "Iteration 73, loss = 0.00156724\n",
            "Iteration 74, loss = 0.00151646\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39529338\n",
            "Iteration 2, loss = 1.29640599\n",
            "Iteration 3, loss = 1.22424808\n",
            "Iteration 4, loss = 1.15342754\n",
            "Iteration 5, loss = 1.06759240\n",
            "Iteration 6, loss = 0.96444637\n",
            "Iteration 7, loss = 0.84793535\n",
            "Iteration 8, loss = 0.72864100\n",
            "Iteration 9, loss = 0.61135206\n",
            "Iteration 10, loss = 0.50594257\n",
            "Iteration 11, loss = 0.42106203\n",
            "Iteration 12, loss = 0.35359395\n",
            "Iteration 13, loss = 0.30403802\n",
            "Iteration 14, loss = 0.25475895\n",
            "Iteration 15, loss = 0.21992249\n",
            "Iteration 16, loss = 0.18895903\n",
            "Iteration 17, loss = 0.16061477\n",
            "Iteration 18, loss = 0.14103133\n",
            "Iteration 19, loss = 0.12346948\n",
            "Iteration 20, loss = 0.10040830\n",
            "Iteration 21, loss = 0.08470733\n",
            "Iteration 22, loss = 0.07554144\n",
            "Iteration 23, loss = 0.06326896\n",
            "Iteration 24, loss = 0.05392902\n",
            "Iteration 25, loss = 0.04524445\n",
            "Iteration 26, loss = 0.03820413\n",
            "Iteration 27, loss = 0.03377076\n",
            "Iteration 28, loss = 0.02895012\n",
            "Iteration 29, loss = 0.02566430\n",
            "Iteration 30, loss = 0.02185786\n",
            "Iteration 31, loss = 0.01923149\n",
            "Iteration 32, loss = 0.01786907\n",
            "Iteration 33, loss = 0.01549942\n",
            "Iteration 34, loss = 0.01500052\n",
            "Iteration 35, loss = 0.01312671\n",
            "Iteration 36, loss = 0.01225218\n",
            "Iteration 37, loss = 0.01085382\n",
            "Iteration 38, loss = 0.01005073\n",
            "Iteration 39, loss = 0.00944918\n",
            "Iteration 40, loss = 0.00912613\n",
            "Iteration 41, loss = 0.00825845\n",
            "Iteration 42, loss = 0.00907748\n",
            "Iteration 43, loss = 0.00794800\n",
            "Iteration 44, loss = 0.00685829\n",
            "Iteration 45, loss = 0.00700383\n",
            "Iteration 46, loss = 0.00613641\n",
            "Iteration 47, loss = 0.00589242\n",
            "Iteration 48, loss = 0.00558854\n",
            "Iteration 49, loss = 0.00535701\n",
            "Iteration 50, loss = 0.00511504\n",
            "Iteration 51, loss = 0.00532041\n",
            "Iteration 52, loss = 0.00500087\n",
            "Iteration 53, loss = 0.00459103\n",
            "Iteration 54, loss = 0.00461227\n",
            "Iteration 55, loss = 0.00440792\n",
            "Iteration 56, loss = 0.00423179\n",
            "Iteration 57, loss = 0.00404838\n",
            "Iteration 58, loss = 0.00382816\n",
            "Iteration 59, loss = 0.00376640\n",
            "Iteration 60, loss = 0.00365153\n",
            "Iteration 61, loss = 0.00355424\n",
            "Iteration 62, loss = 0.00343593\n",
            "Iteration 63, loss = 0.00333739\n",
            "Iteration 64, loss = 0.00327434\n",
            "Iteration 65, loss = 0.00320815\n",
            "Iteration 66, loss = 0.00314637\n",
            "Iteration 67, loss = 0.00315344\n",
            "Iteration 68, loss = 0.00309794\n",
            "Iteration 69, loss = 0.00384856\n",
            "Iteration 70, loss = 0.00321627\n",
            "Iteration 71, loss = 0.00287095\n",
            "Iteration 72, loss = 0.00324968\n",
            "Iteration 73, loss = 0.00298450\n",
            "Iteration 74, loss = 0.00271381\n",
            "Iteration 75, loss = 0.00269560\n",
            "Iteration 76, loss = 0.00261727\n",
            "Iteration 77, loss = 0.00266058\n",
            "Iteration 78, loss = 0.00306947\n",
            "Iteration 79, loss = 0.00344815\n",
            "Iteration 80, loss = 0.00274736\n",
            "Iteration 81, loss = 0.00253926\n",
            "Iteration 82, loss = 0.00256159\n",
            "Iteration 83, loss = 0.00245027\n",
            "Iteration 84, loss = 0.00231535\n",
            "Iteration 85, loss = 0.00232951\n",
            "Iteration 86, loss = 0.00230979\n",
            "Iteration 87, loss = 0.00219444\n",
            "Iteration 88, loss = 0.00223905\n",
            "Iteration 89, loss = 0.00225421\n",
            "Iteration 90, loss = 0.00218328\n",
            "Iteration 91, loss = 0.00229049\n",
            "Iteration 92, loss = 0.00227690\n",
            "Iteration 93, loss = 0.00241174\n",
            "Iteration 94, loss = 0.00322712\n",
            "Iteration 95, loss = 0.00276130\n",
            "Iteration 96, loss = 0.00204065\n",
            "Iteration 97, loss = 0.00230915\n",
            "Iteration 98, loss = 0.00210970\n",
            "Iteration 99, loss = 0.00193912\n",
            "Iteration 100, loss = 0.00200507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39136539\n",
            "Iteration 2, loss = 1.29561239\n",
            "Iteration 3, loss = 1.22542588\n",
            "Iteration 4, loss = 1.15634519\n",
            "Iteration 5, loss = 1.07713221\n",
            "Iteration 6, loss = 0.97946243\n",
            "Iteration 7, loss = 0.86401905\n",
            "Iteration 8, loss = 0.74094423\n",
            "Iteration 9, loss = 0.62449906\n",
            "Iteration 10, loss = 0.51887242\n",
            "Iteration 11, loss = 0.43549116\n",
            "Iteration 12, loss = 0.36695971\n",
            "Iteration 13, loss = 0.31865801\n",
            "Iteration 14, loss = 0.26820276\n",
            "Iteration 15, loss = 0.23234368\n",
            "Iteration 16, loss = 0.19850856\n",
            "Iteration 17, loss = 0.17131446\n",
            "Iteration 18, loss = 0.14919930\n",
            "Iteration 19, loss = 0.13107051\n",
            "Iteration 20, loss = 0.10932067\n",
            "Iteration 21, loss = 0.09378500\n",
            "Iteration 22, loss = 0.08432217\n",
            "Iteration 23, loss = 0.07132418\n",
            "Iteration 24, loss = 0.06111409\n",
            "Iteration 25, loss = 0.05217521\n",
            "Iteration 26, loss = 0.04451846\n",
            "Iteration 27, loss = 0.03915553\n",
            "Iteration 28, loss = 0.03421278\n",
            "Iteration 29, loss = 0.02986228\n",
            "Iteration 30, loss = 0.02620847\n",
            "Iteration 31, loss = 0.02344257\n",
            "Iteration 32, loss = 0.02222924\n",
            "Iteration 33, loss = 0.01977855\n",
            "Iteration 34, loss = 0.01861121\n",
            "Iteration 35, loss = 0.01691082\n",
            "Iteration 36, loss = 0.01510813\n",
            "Iteration 37, loss = 0.01370855\n",
            "Iteration 38, loss = 0.01233252\n",
            "Iteration 39, loss = 0.01154995\n",
            "Iteration 40, loss = 0.01128417\n",
            "Iteration 41, loss = 0.01009884\n",
            "Iteration 42, loss = 0.01141782\n",
            "Iteration 43, loss = 0.00936423\n",
            "Iteration 44, loss = 0.00831518\n",
            "Iteration 45, loss = 0.00785480\n",
            "Iteration 46, loss = 0.00666459\n",
            "Iteration 47, loss = 0.00628070\n",
            "Iteration 48, loss = 0.00599540\n",
            "Iteration 49, loss = 0.00562066\n",
            "Iteration 50, loss = 0.00528536\n",
            "Iteration 51, loss = 0.00500400\n",
            "Iteration 52, loss = 0.00489083\n",
            "Iteration 53, loss = 0.00464080\n",
            "Iteration 54, loss = 0.00432528\n",
            "Iteration 55, loss = 0.00411749\n",
            "Iteration 56, loss = 0.00392277\n",
            "Iteration 57, loss = 0.00370472\n",
            "Iteration 58, loss = 0.00353135\n",
            "Iteration 59, loss = 0.00336738\n",
            "Iteration 60, loss = 0.00324498\n",
            "Iteration 61, loss = 0.00309421\n",
            "Iteration 62, loss = 0.00297060\n",
            "Iteration 63, loss = 0.00286289\n",
            "Iteration 64, loss = 0.00273124\n",
            "Iteration 65, loss = 0.00262368\n",
            "Iteration 66, loss = 0.00252744\n",
            "Iteration 67, loss = 0.00245173\n",
            "Iteration 68, loss = 0.00234754\n",
            "Iteration 69, loss = 0.00228167\n",
            "Iteration 70, loss = 0.00218926\n",
            "Iteration 71, loss = 0.00209475\n",
            "Iteration 72, loss = 0.00206603\n",
            "Iteration 73, loss = 0.00199582\n",
            "Iteration 74, loss = 0.00190513\n",
            "Iteration 75, loss = 0.00184379\n",
            "Iteration 76, loss = 0.00180530\n",
            "Iteration 77, loss = 0.00173519\n",
            "Iteration 78, loss = 0.00166846\n",
            "Iteration 79, loss = 0.00162019\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "9\n",
            "Iteration 1, loss = 1.27475591\n",
            "Iteration 2, loss = 1.23835823\n",
            "Iteration 3, loss = 1.21563816\n",
            "Iteration 4, loss = 1.20088956\n",
            "Iteration 5, loss = 1.19972614\n",
            "Iteration 6, loss = 1.17528607\n",
            "Iteration 7, loss = 1.15764783\n",
            "Iteration 8, loss = 1.16059160\n",
            "Iteration 9, loss = 1.18194867\n",
            "Iteration 10, loss = 1.16014931\n",
            "Iteration 11, loss = 1.18374172\n",
            "Iteration 12, loss = 1.14905865\n",
            "Iteration 13, loss = 1.13967176\n",
            "Iteration 14, loss = 1.14704507\n",
            "Iteration 15, loss = 1.13774785\n",
            "Iteration 16, loss = 1.13309032\n",
            "Iteration 17, loss = 1.12846563\n",
            "Iteration 18, loss = 1.11736952\n",
            "Iteration 19, loss = 1.12219359\n",
            "Iteration 20, loss = 1.09773745\n",
            "Iteration 21, loss = 1.11110816\n",
            "Iteration 22, loss = 1.10090572\n",
            "Iteration 23, loss = 1.10341958\n",
            "Iteration 24, loss = 1.09396621\n",
            "Iteration 25, loss = 1.08972568\n",
            "Iteration 26, loss = 1.09417990\n",
            "Iteration 27, loss = 1.09333662\n",
            "Iteration 28, loss = 1.08428101\n",
            "Iteration 29, loss = 1.07612332\n",
            "Iteration 30, loss = 1.07267759\n",
            "Iteration 31, loss = 1.09967595\n",
            "Iteration 32, loss = 1.09838246\n",
            "Iteration 33, loss = 1.12902068\n",
            "Iteration 34, loss = 1.09554410\n",
            "Iteration 35, loss = 1.08480352\n",
            "Iteration 36, loss = 1.07169865\n",
            "Iteration 37, loss = 1.06824853\n",
            "Iteration 38, loss = 1.07054290\n",
            "Iteration 39, loss = 1.05475963\n",
            "Iteration 40, loss = 1.05862955\n",
            "Iteration 41, loss = 1.08443210\n",
            "Iteration 42, loss = 1.09042956\n",
            "Iteration 43, loss = 1.05480463\n",
            "Iteration 44, loss = 1.05720408\n",
            "Iteration 45, loss = 1.05877882\n",
            "Iteration 46, loss = 1.09027856\n",
            "Iteration 47, loss = 1.05792513\n",
            "Iteration 48, loss = 1.05648507\n",
            "Iteration 49, loss = 1.05991021\n",
            "Iteration 50, loss = 1.06170216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27608787\n",
            "Iteration 2, loss = 1.23271973\n",
            "Iteration 3, loss = 1.20831138\n",
            "Iteration 4, loss = 1.18702036\n",
            "Iteration 5, loss = 1.17469408\n",
            "Iteration 6, loss = 1.16171903\n",
            "Iteration 7, loss = 1.15316149\n",
            "Iteration 8, loss = 1.15000376\n",
            "Iteration 9, loss = 1.14553048\n",
            "Iteration 10, loss = 1.13845793\n",
            "Iteration 11, loss = 1.14098884\n",
            "Iteration 12, loss = 1.13000768\n",
            "Iteration 13, loss = 1.11833555\n",
            "Iteration 14, loss = 1.13630551\n",
            "Iteration 15, loss = 1.12219344\n",
            "Iteration 16, loss = 1.14127971\n",
            "Iteration 17, loss = 1.11688949\n",
            "Iteration 18, loss = 1.10556952\n",
            "Iteration 19, loss = 1.11144370\n",
            "Iteration 20, loss = 1.10113615\n",
            "Iteration 21, loss = 1.09814735\n",
            "Iteration 22, loss = 1.09976497\n",
            "Iteration 23, loss = 1.09467325\n",
            "Iteration 24, loss = 1.09298210\n",
            "Iteration 25, loss = 1.07931236\n",
            "Iteration 26, loss = 1.07765185\n",
            "Iteration 27, loss = 1.09778603\n",
            "Iteration 28, loss = 1.09211345\n",
            "Iteration 29, loss = 1.07988905\n",
            "Iteration 30, loss = 1.07108730\n",
            "Iteration 31, loss = 1.07525290\n",
            "Iteration 32, loss = 1.07509341\n",
            "Iteration 33, loss = 1.06217793\n",
            "Iteration 34, loss = 1.05969813\n",
            "Iteration 35, loss = 1.07033898\n",
            "Iteration 36, loss = 1.05993583\n",
            "Iteration 37, loss = 1.05806962\n",
            "Iteration 38, loss = 1.07074237\n",
            "Iteration 39, loss = 1.06063549\n",
            "Iteration 40, loss = 1.05023253\n",
            "Iteration 41, loss = 1.06231805\n",
            "Iteration 42, loss = 1.05237071\n",
            "Iteration 43, loss = 1.03595129\n",
            "Iteration 44, loss = 1.03963011\n",
            "Iteration 45, loss = 1.05465954\n",
            "Iteration 46, loss = 1.09664898\n",
            "Iteration 47, loss = 1.05351783\n",
            "Iteration 48, loss = 1.04882001\n",
            "Iteration 49, loss = 1.05231347\n",
            "Iteration 50, loss = 1.04797270\n",
            "Iteration 51, loss = 1.08849112\n",
            "Iteration 52, loss = 1.03844616\n",
            "Iteration 53, loss = 1.03442939\n",
            "Iteration 54, loss = 1.05172781\n",
            "Iteration 55, loss = 1.02927663\n",
            "Iteration 56, loss = 1.04306624\n",
            "Iteration 57, loss = 1.02930854\n",
            "Iteration 58, loss = 1.04762621\n",
            "Iteration 59, loss = 1.02393291\n",
            "Iteration 60, loss = 1.01984763\n",
            "Iteration 61, loss = 1.00799545\n",
            "Iteration 62, loss = 1.00405963\n",
            "Iteration 63, loss = 1.01546814\n",
            "Iteration 64, loss = 1.00579677\n",
            "Iteration 65, loss = 1.03070064\n",
            "Iteration 66, loss = 1.02437067\n",
            "Iteration 67, loss = 1.02908810\n",
            "Iteration 68, loss = 1.00838281\n",
            "Iteration 69, loss = 1.00429159\n",
            "Iteration 70, loss = 1.01687239\n",
            "Iteration 71, loss = 1.02311414\n",
            "Iteration 72, loss = 1.02110832\n",
            "Iteration 73, loss = 1.04646056\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27378109\n",
            "Iteration 2, loss = 1.22867720\n",
            "Iteration 3, loss = 1.21037497\n",
            "Iteration 4, loss = 1.19416824\n",
            "Iteration 5, loss = 1.18882355\n",
            "Iteration 6, loss = 1.17116842\n",
            "Iteration 7, loss = 1.15257129\n",
            "Iteration 8, loss = 1.15880332\n",
            "Iteration 9, loss = 1.14935622\n",
            "Iteration 10, loss = 1.14303148\n",
            "Iteration 11, loss = 1.16324974\n",
            "Iteration 12, loss = 1.13070965\n",
            "Iteration 13, loss = 1.12580465\n",
            "Iteration 14, loss = 1.12966778\n",
            "Iteration 15, loss = 1.12545239\n",
            "Iteration 16, loss = 1.15697765\n",
            "Iteration 17, loss = 1.12736890\n",
            "Iteration 18, loss = 1.12902744\n",
            "Iteration 19, loss = 1.12785716\n",
            "Iteration 20, loss = 1.12885743\n",
            "Iteration 21, loss = 1.10615829\n",
            "Iteration 22, loss = 1.09710805\n",
            "Iteration 23, loss = 1.10154110\n",
            "Iteration 24, loss = 1.10963962\n",
            "Iteration 25, loss = 1.09319667\n",
            "Iteration 26, loss = 1.09020660\n",
            "Iteration 27, loss = 1.10142038\n",
            "Iteration 28, loss = 1.09203931\n",
            "Iteration 29, loss = 1.09439830\n",
            "Iteration 30, loss = 1.07499141\n",
            "Iteration 31, loss = 1.07935272\n",
            "Iteration 32, loss = 1.09244834\n",
            "Iteration 33, loss = 1.06962167\n",
            "Iteration 34, loss = 1.06623071\n",
            "Iteration 35, loss = 1.08105055\n",
            "Iteration 36, loss = 1.07580841\n",
            "Iteration 37, loss = 1.07342895\n",
            "Iteration 38, loss = 1.08478888\n",
            "Iteration 39, loss = 1.07754079\n",
            "Iteration 40, loss = 1.05041641\n",
            "Iteration 41, loss = 1.04980356\n",
            "Iteration 42, loss = 1.06339973\n",
            "Iteration 43, loss = 1.03987583\n",
            "Iteration 44, loss = 1.05380987\n",
            "Iteration 45, loss = 1.05151864\n",
            "Iteration 46, loss = 1.08431416\n",
            "Iteration 47, loss = 1.06329951\n",
            "Iteration 48, loss = 1.05800154\n",
            "Iteration 49, loss = 1.05348160\n",
            "Iteration 50, loss = 1.05248412\n",
            "Iteration 51, loss = 1.06875935\n",
            "Iteration 52, loss = 1.04083669\n",
            "Iteration 53, loss = 1.04093829\n",
            "Iteration 54, loss = 1.05253558\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27901479\n",
            "Iteration 2, loss = 1.24711661\n",
            "Iteration 3, loss = 1.23261454\n",
            "Iteration 4, loss = 1.21643121\n",
            "Iteration 5, loss = 1.21424271\n",
            "Iteration 6, loss = 1.19353861\n",
            "Iteration 7, loss = 1.19113115\n",
            "Iteration 8, loss = 1.17375481\n",
            "Iteration 9, loss = 1.15811305\n",
            "Iteration 10, loss = 1.14837431\n",
            "Iteration 11, loss = 1.15334052\n",
            "Iteration 12, loss = 1.14002323\n",
            "Iteration 13, loss = 1.15065572\n",
            "Iteration 14, loss = 1.13525898\n",
            "Iteration 15, loss = 1.12460979\n",
            "Iteration 16, loss = 1.13006352\n",
            "Iteration 17, loss = 1.11769062\n",
            "Iteration 18, loss = 1.11398471\n",
            "Iteration 19, loss = 1.12426108\n",
            "Iteration 20, loss = 1.11541440\n",
            "Iteration 21, loss = 1.10399751\n",
            "Iteration 22, loss = 1.11075996\n",
            "Iteration 23, loss = 1.10507022\n",
            "Iteration 24, loss = 1.10909017\n",
            "Iteration 25, loss = 1.08731061\n",
            "Iteration 26, loss = 1.08871170\n",
            "Iteration 27, loss = 1.11690550\n",
            "Iteration 28, loss = 1.13209057\n",
            "Iteration 29, loss = 1.10169430\n",
            "Iteration 30, loss = 1.07453342\n",
            "Iteration 31, loss = 1.08874314\n",
            "Iteration 32, loss = 1.08303303\n",
            "Iteration 33, loss = 1.06684031\n",
            "Iteration 34, loss = 1.06482581\n",
            "Iteration 35, loss = 1.07096957\n",
            "Iteration 36, loss = 1.06616661\n",
            "Iteration 37, loss = 1.05938634\n",
            "Iteration 38, loss = 1.06789964\n",
            "Iteration 39, loss = 1.06574264\n",
            "Iteration 40, loss = 1.05350946\n",
            "Iteration 41, loss = 1.04848410\n",
            "Iteration 42, loss = 1.07481847\n",
            "Iteration 43, loss = 1.03832447\n",
            "Iteration 44, loss = 1.04775757\n",
            "Iteration 45, loss = 1.04242249\n",
            "Iteration 46, loss = 1.06270754\n",
            "Iteration 47, loss = 1.06533105\n",
            "Iteration 48, loss = 1.05209538\n",
            "Iteration 49, loss = 1.05483313\n",
            "Iteration 50, loss = 1.05339089\n",
            "Iteration 51, loss = 1.07439068\n",
            "Iteration 52, loss = 1.04075587\n",
            "Iteration 53, loss = 1.04602571\n",
            "Iteration 54, loss = 1.06257861\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27723097\n",
            "Iteration 2, loss = 1.23021482\n",
            "Iteration 3, loss = 1.21565718\n",
            "Iteration 4, loss = 1.20298443\n",
            "Iteration 5, loss = 1.18651476\n",
            "Iteration 6, loss = 1.18286160\n",
            "Iteration 7, loss = 1.16664760\n",
            "Iteration 8, loss = 1.16854764\n",
            "Iteration 9, loss = 1.14839097\n",
            "Iteration 10, loss = 1.15213875\n",
            "Iteration 11, loss = 1.13731251\n",
            "Iteration 12, loss = 1.13395254\n",
            "Iteration 13, loss = 1.13585235\n",
            "Iteration 14, loss = 1.12346547\n",
            "Iteration 15, loss = 1.12412911\n",
            "Iteration 16, loss = 1.14091280\n",
            "Iteration 17, loss = 1.12005901\n",
            "Iteration 18, loss = 1.11019875\n",
            "Iteration 19, loss = 1.11482732\n",
            "Iteration 20, loss = 1.12641794\n",
            "Iteration 21, loss = 1.11161435\n",
            "Iteration 22, loss = 1.09549077\n",
            "Iteration 23, loss = 1.08941044\n",
            "Iteration 24, loss = 1.10202372\n",
            "Iteration 25, loss = 1.09208315\n",
            "Iteration 26, loss = 1.08686217\n",
            "Iteration 27, loss = 1.10773838\n",
            "Iteration 28, loss = 1.08532209\n",
            "Iteration 29, loss = 1.08036353\n",
            "Iteration 30, loss = 1.08161944\n",
            "Iteration 31, loss = 1.08351307\n",
            "Iteration 32, loss = 1.07496203\n",
            "Iteration 33, loss = 1.06377753\n",
            "Iteration 34, loss = 1.05861947\n",
            "Iteration 35, loss = 1.06888660\n",
            "Iteration 36, loss = 1.06904534\n",
            "Iteration 37, loss = 1.05938750\n",
            "Iteration 38, loss = 1.08235986\n",
            "Iteration 39, loss = 1.09272590\n",
            "Iteration 40, loss = 1.05404098\n",
            "Iteration 41, loss = 1.04601390\n",
            "Iteration 42, loss = 1.04369671\n",
            "Iteration 43, loss = 1.04961473\n",
            "Iteration 44, loss = 1.04001268\n",
            "Iteration 45, loss = 1.04979891\n",
            "Iteration 46, loss = 1.07726831\n",
            "Iteration 47, loss = 1.05010167\n",
            "Iteration 48, loss = 1.04257086\n",
            "Iteration 49, loss = 1.05714569\n",
            "Iteration 50, loss = 1.05504169\n",
            "Iteration 51, loss = 1.05703364\n",
            "Iteration 52, loss = 1.05721775\n",
            "Iteration 53, loss = 1.05053887\n",
            "Iteration 54, loss = 1.07377109\n",
            "Iteration 55, loss = 1.07820320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27177929\n",
            "Iteration 2, loss = 1.22279692\n",
            "Iteration 3, loss = 1.20368924\n",
            "Iteration 4, loss = 1.18508412\n",
            "Iteration 5, loss = 1.17691112\n",
            "Iteration 6, loss = 1.17334908\n",
            "Iteration 7, loss = 1.17313174\n",
            "Iteration 8, loss = 1.17549592\n",
            "Iteration 9, loss = 1.15180918\n",
            "Iteration 10, loss = 1.14643858\n",
            "Iteration 11, loss = 1.14596959\n",
            "Iteration 12, loss = 1.13297508\n",
            "Iteration 13, loss = 1.13631255\n",
            "Iteration 14, loss = 1.12838753\n",
            "Iteration 15, loss = 1.13645133\n",
            "Iteration 16, loss = 1.14405085\n",
            "Iteration 17, loss = 1.12310195\n",
            "Iteration 18, loss = 1.11134668\n",
            "Iteration 19, loss = 1.13334688\n",
            "Iteration 20, loss = 1.14088151\n",
            "Iteration 21, loss = 1.14196928\n",
            "Iteration 22, loss = 1.11827494\n",
            "Iteration 23, loss = 1.12266111\n",
            "Iteration 24, loss = 1.11540385\n",
            "Iteration 25, loss = 1.12022654\n",
            "Iteration 26, loss = 1.09209519\n",
            "Iteration 27, loss = 1.10285387\n",
            "Iteration 28, loss = 1.08650211\n",
            "Iteration 29, loss = 1.08284523\n",
            "Iteration 30, loss = 1.07862758\n",
            "Iteration 31, loss = 1.09101552\n",
            "Iteration 32, loss = 1.08506179\n",
            "Iteration 33, loss = 1.08088784\n",
            "Iteration 34, loss = 1.06972815\n",
            "Iteration 35, loss = 1.07507970\n",
            "Iteration 36, loss = 1.06061413\n",
            "Iteration 37, loss = 1.05821532\n",
            "Iteration 38, loss = 1.06278087\n",
            "Iteration 39, loss = 1.06922921\n",
            "Iteration 40, loss = 1.06699557\n",
            "Iteration 41, loss = 1.07329567\n",
            "Iteration 42, loss = 1.04469901\n",
            "Iteration 43, loss = 1.04790931\n",
            "Iteration 44, loss = 1.04136809\n",
            "Iteration 45, loss = 1.03591859\n",
            "Iteration 46, loss = 1.03664432\n",
            "Iteration 47, loss = 1.04023057\n",
            "Iteration 48, loss = 1.03709400\n",
            "Iteration 49, loss = 1.03562675\n",
            "Iteration 50, loss = 1.04780330\n",
            "Iteration 51, loss = 1.05513517\n",
            "Iteration 52, loss = 1.04355481\n",
            "Iteration 53, loss = 1.05952797\n",
            "Iteration 54, loss = 1.03413555\n",
            "Iteration 55, loss = 1.03371258\n",
            "Iteration 56, loss = 1.02553483\n",
            "Iteration 57, loss = 1.05989345\n",
            "Iteration 58, loss = 1.03366030\n",
            "Iteration 59, loss = 1.01978278\n",
            "Iteration 60, loss = 1.00873907\n",
            "Iteration 61, loss = 1.01907576\n",
            "Iteration 62, loss = 1.02222720\n",
            "Iteration 63, loss = 1.00716201\n",
            "Iteration 64, loss = 1.00207261\n",
            "Iteration 65, loss = 1.01663110\n",
            "Iteration 66, loss = 1.02110071\n",
            "Iteration 67, loss = 1.01901824\n",
            "Iteration 68, loss = 0.98867958\n",
            "Iteration 69, loss = 0.98856624\n",
            "Iteration 70, loss = 1.00730175\n",
            "Iteration 71, loss = 1.01748717\n",
            "Iteration 72, loss = 1.00788671\n",
            "Iteration 73, loss = 1.02183579\n",
            "Iteration 74, loss = 1.01461584\n",
            "Iteration 75, loss = 1.01768416\n",
            "Iteration 76, loss = 0.98067111\n",
            "Iteration 77, loss = 0.97701141\n",
            "Iteration 78, loss = 0.97669762\n",
            "Iteration 79, loss = 0.97331493\n",
            "Iteration 80, loss = 0.96850439\n",
            "Iteration 81, loss = 0.98087794\n",
            "Iteration 82, loss = 0.98371666\n",
            "Iteration 83, loss = 0.98023900\n",
            "Iteration 84, loss = 0.98796896\n",
            "Iteration 85, loss = 0.98210299\n",
            "Iteration 86, loss = 0.97829721\n",
            "Iteration 87, loss = 0.98809276\n",
            "Iteration 88, loss = 0.99233697\n",
            "Iteration 89, loss = 0.98335611\n",
            "Iteration 90, loss = 0.96915235\n",
            "Iteration 91, loss = 0.99120959\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27541221\n",
            "Iteration 2, loss = 1.22978752\n",
            "Iteration 3, loss = 1.22084826\n",
            "Iteration 4, loss = 1.19692785\n",
            "Iteration 5, loss = 1.19288038\n",
            "Iteration 6, loss = 1.18572154\n",
            "Iteration 7, loss = 1.20198913\n",
            "Iteration 8, loss = 1.18597096\n",
            "Iteration 9, loss = 1.15807373\n",
            "Iteration 10, loss = 1.15091028\n",
            "Iteration 11, loss = 1.15365384\n",
            "Iteration 12, loss = 1.13789730\n",
            "Iteration 13, loss = 1.14079585\n",
            "Iteration 14, loss = 1.13469485\n",
            "Iteration 15, loss = 1.13247235\n",
            "Iteration 16, loss = 1.14536205\n",
            "Iteration 17, loss = 1.12212744\n",
            "Iteration 18, loss = 1.11622311\n",
            "Iteration 19, loss = 1.12442221\n",
            "Iteration 20, loss = 1.12773454\n",
            "Iteration 21, loss = 1.12922921\n",
            "Iteration 22, loss = 1.12485109\n",
            "Iteration 23, loss = 1.10378610\n",
            "Iteration 24, loss = 1.11028153\n",
            "Iteration 25, loss = 1.11122760\n",
            "Iteration 26, loss = 1.10333805\n",
            "Iteration 27, loss = 1.09930888\n",
            "Iteration 28, loss = 1.09203419\n",
            "Iteration 29, loss = 1.10791251\n",
            "Iteration 30, loss = 1.08155315\n",
            "Iteration 31, loss = 1.08912733\n",
            "Iteration 32, loss = 1.09083254\n",
            "Iteration 33, loss = 1.08215900\n",
            "Iteration 34, loss = 1.07837577\n",
            "Iteration 35, loss = 1.09777357\n",
            "Iteration 36, loss = 1.07228059\n",
            "Iteration 37, loss = 1.07670868\n",
            "Iteration 38, loss = 1.07737924\n",
            "Iteration 39, loss = 1.06810101\n",
            "Iteration 40, loss = 1.07113963\n",
            "Iteration 41, loss = 1.07206682\n",
            "Iteration 42, loss = 1.04925826\n",
            "Iteration 43, loss = 1.05114806\n",
            "Iteration 44, loss = 1.04655473\n",
            "Iteration 45, loss = 1.04346609\n",
            "Iteration 46, loss = 1.03683423\n",
            "Iteration 47, loss = 1.04506320\n",
            "Iteration 48, loss = 1.04330144\n",
            "Iteration 49, loss = 1.04795568\n",
            "Iteration 50, loss = 1.03525160\n",
            "Iteration 51, loss = 1.06884843\n",
            "Iteration 52, loss = 1.04551202\n",
            "Iteration 53, loss = 1.05769338\n",
            "Iteration 54, loss = 1.06170319\n",
            "Iteration 55, loss = 1.05884868\n",
            "Iteration 56, loss = 1.04602772\n",
            "Iteration 57, loss = 1.04058865\n",
            "Iteration 58, loss = 1.03641253\n",
            "Iteration 59, loss = 1.01456084\n",
            "Iteration 60, loss = 1.01800301\n",
            "Iteration 61, loss = 1.02232161\n",
            "Iteration 62, loss = 1.02039347\n",
            "Iteration 63, loss = 1.02305821\n",
            "Iteration 64, loss = 1.01240149\n",
            "Iteration 65, loss = 1.01414372\n",
            "Iteration 66, loss = 1.04732201\n",
            "Iteration 67, loss = 1.00562976\n",
            "Iteration 68, loss = 0.99664220\n",
            "Iteration 69, loss = 0.99272422\n",
            "Iteration 70, loss = 1.00573384\n",
            "Iteration 71, loss = 1.01278700\n",
            "Iteration 72, loss = 1.01583974\n",
            "Iteration 73, loss = 1.02296706\n",
            "Iteration 74, loss = 1.01830282\n",
            "Iteration 75, loss = 1.01833196\n",
            "Iteration 76, loss = 0.99407943\n",
            "Iteration 77, loss = 0.98691343\n",
            "Iteration 78, loss = 0.98462811\n",
            "Iteration 79, loss = 0.97779745\n",
            "Iteration 80, loss = 0.97342569\n",
            "Iteration 81, loss = 0.97533152\n",
            "Iteration 82, loss = 0.97102987\n",
            "Iteration 83, loss = 0.96516544\n",
            "Iteration 84, loss = 0.96923961\n",
            "Iteration 85, loss = 0.97648283\n",
            "Iteration 86, loss = 0.97556503\n",
            "Iteration 87, loss = 0.98840677\n",
            "Iteration 88, loss = 0.97360847\n",
            "Iteration 89, loss = 0.97673821\n",
            "Iteration 90, loss = 0.97048787\n",
            "Iteration 91, loss = 0.96726896\n",
            "Iteration 92, loss = 0.96560706\n",
            "Iteration 93, loss = 0.94868278\n",
            "Iteration 94, loss = 0.96031315\n",
            "Iteration 95, loss = 0.95041619\n",
            "Iteration 96, loss = 0.95075527\n",
            "Iteration 97, loss = 0.95089980\n",
            "Iteration 98, loss = 0.98585549\n",
            "Iteration 99, loss = 0.96094395\n",
            "Iteration 100, loss = 0.96066594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28037904\n",
            "Iteration 2, loss = 1.22782908\n",
            "Iteration 3, loss = 1.21533451\n",
            "Iteration 4, loss = 1.21224997\n",
            "Iteration 5, loss = 1.18624683\n",
            "Iteration 6, loss = 1.17049369\n",
            "Iteration 7, loss = 1.15804786\n",
            "Iteration 8, loss = 1.14870097\n",
            "Iteration 9, loss = 1.15197707\n",
            "Iteration 10, loss = 1.14688232\n",
            "Iteration 11, loss = 1.16275668\n",
            "Iteration 12, loss = 1.16937350\n",
            "Iteration 13, loss = 1.18735198\n",
            "Iteration 14, loss = 1.14966642\n",
            "Iteration 15, loss = 1.13810735\n",
            "Iteration 16, loss = 1.13816725\n",
            "Iteration 17, loss = 1.16147236\n",
            "Iteration 18, loss = 1.12657778\n",
            "Iteration 19, loss = 1.12241159\n",
            "Iteration 20, loss = 1.11639091\n",
            "Iteration 21, loss = 1.10834844\n",
            "Iteration 22, loss = 1.10998992\n",
            "Iteration 23, loss = 1.09946051\n",
            "Iteration 24, loss = 1.10133860\n",
            "Iteration 25, loss = 1.10954814\n",
            "Iteration 26, loss = 1.09460360\n",
            "Iteration 27, loss = 1.08597091\n",
            "Iteration 28, loss = 1.09294903\n",
            "Iteration 29, loss = 1.08967662\n",
            "Iteration 30, loss = 1.08346359\n",
            "Iteration 31, loss = 1.08241906\n",
            "Iteration 32, loss = 1.08309154\n",
            "Iteration 33, loss = 1.09318598\n",
            "Iteration 34, loss = 1.08528474\n",
            "Iteration 35, loss = 1.07973850\n",
            "Iteration 36, loss = 1.08787283\n",
            "Iteration 37, loss = 1.06533489\n",
            "Iteration 38, loss = 1.05547731\n",
            "Iteration 39, loss = 1.06679640\n",
            "Iteration 40, loss = 1.06219718\n",
            "Iteration 41, loss = 1.06838531\n",
            "Iteration 42, loss = 1.07467192\n",
            "Iteration 43, loss = 1.07546767\n",
            "Iteration 44, loss = 1.05642985\n",
            "Iteration 45, loss = 1.04773639\n",
            "Iteration 46, loss = 1.04507407\n",
            "Iteration 47, loss = 1.03767494\n",
            "Iteration 48, loss = 1.03266618\n",
            "Iteration 49, loss = 1.03736633\n",
            "Iteration 50, loss = 1.05194039\n",
            "Iteration 51, loss = 1.02777861\n",
            "Iteration 52, loss = 1.02273250\n",
            "Iteration 53, loss = 1.02198191\n",
            "Iteration 54, loss = 1.02151679\n",
            "Iteration 55, loss = 1.03845758\n",
            "Iteration 56, loss = 1.03993210\n",
            "Iteration 57, loss = 1.00814429\n",
            "Iteration 58, loss = 1.01680904\n",
            "Iteration 59, loss = 1.04476878\n",
            "Iteration 60, loss = 1.01674401\n",
            "Iteration 61, loss = 1.01804247\n",
            "Iteration 62, loss = 1.02053482\n",
            "Iteration 63, loss = 1.02820824\n",
            "Iteration 64, loss = 1.00122096\n",
            "Iteration 65, loss = 0.99986889\n",
            "Iteration 66, loss = 0.99931290\n",
            "Iteration 67, loss = 1.00692313\n",
            "Iteration 68, loss = 1.02996226\n",
            "Iteration 69, loss = 1.02010791\n",
            "Iteration 70, loss = 1.01645245\n",
            "Iteration 71, loss = 1.01740786\n",
            "Iteration 72, loss = 1.02713549\n",
            "Iteration 73, loss = 1.01038849\n",
            "Iteration 74, loss = 1.00936689\n",
            "Iteration 75, loss = 0.99373408\n",
            "Iteration 76, loss = 0.99947842\n",
            "Iteration 77, loss = 0.98533190\n",
            "Iteration 78, loss = 0.97172435\n",
            "Iteration 79, loss = 0.98086494\n",
            "Iteration 80, loss = 1.00162258\n",
            "Iteration 81, loss = 1.00153087\n",
            "Iteration 82, loss = 0.98129146\n",
            "Iteration 83, loss = 0.96425527\n",
            "Iteration 84, loss = 0.96775315\n",
            "Iteration 85, loss = 0.96808293\n",
            "Iteration 86, loss = 0.97531120\n",
            "Iteration 87, loss = 1.00441180\n",
            "Iteration 88, loss = 0.97118537\n",
            "Iteration 89, loss = 0.97236803\n",
            "Iteration 90, loss = 0.96796758\n",
            "Iteration 91, loss = 0.96197056\n",
            "Iteration 92, loss = 0.95052896\n",
            "Iteration 93, loss = 0.96420733\n",
            "Iteration 94, loss = 0.97838434\n",
            "Iteration 95, loss = 0.95464759\n",
            "Iteration 96, loss = 0.95274439\n",
            "Iteration 97, loss = 0.95826650\n",
            "Iteration 98, loss = 0.94946043\n",
            "Iteration 99, loss = 0.93639000\n",
            "Iteration 100, loss = 0.93376947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27471205\n",
            "Iteration 2, loss = 1.23479593\n",
            "Iteration 3, loss = 1.21420033\n",
            "Iteration 4, loss = 1.20394991\n",
            "Iteration 5, loss = 1.17672696\n",
            "Iteration 6, loss = 1.15718646\n",
            "Iteration 7, loss = 1.14985722\n",
            "Iteration 8, loss = 1.14468772\n",
            "Iteration 9, loss = 1.14395838\n",
            "Iteration 10, loss = 1.13838031\n",
            "Iteration 11, loss = 1.13115014\n",
            "Iteration 12, loss = 1.14346140\n",
            "Iteration 13, loss = 1.16589213\n",
            "Iteration 14, loss = 1.15055503\n",
            "Iteration 15, loss = 1.12224467\n",
            "Iteration 16, loss = 1.11564335\n",
            "Iteration 17, loss = 1.13024552\n",
            "Iteration 18, loss = 1.09816355\n",
            "Iteration 19, loss = 1.10732675\n",
            "Iteration 20, loss = 1.09963619\n",
            "Iteration 21, loss = 1.10033248\n",
            "Iteration 22, loss = 1.10097492\n",
            "Iteration 23, loss = 1.08733712\n",
            "Iteration 24, loss = 1.08388613\n",
            "Iteration 25, loss = 1.08036333\n",
            "Iteration 26, loss = 1.08438066\n",
            "Iteration 27, loss = 1.08899190\n",
            "Iteration 28, loss = 1.08107335\n",
            "Iteration 29, loss = 1.07428107\n",
            "Iteration 30, loss = 1.07167976\n",
            "Iteration 31, loss = 1.07861287\n",
            "Iteration 32, loss = 1.06748548\n",
            "Iteration 33, loss = 1.06751472\n",
            "Iteration 34, loss = 1.07148717\n",
            "Iteration 35, loss = 1.05755539\n",
            "Iteration 36, loss = 1.05474943\n",
            "Iteration 37, loss = 1.04684328\n",
            "Iteration 38, loss = 1.04697884\n",
            "Iteration 39, loss = 1.07287279\n",
            "Iteration 40, loss = 1.04947290\n",
            "Iteration 41, loss = 1.05336626\n",
            "Iteration 42, loss = 1.06672408\n",
            "Iteration 43, loss = 1.07676784\n",
            "Iteration 44, loss = 1.06075045\n",
            "Iteration 45, loss = 1.05072146\n",
            "Iteration 46, loss = 1.05315518\n",
            "Iteration 47, loss = 1.04960034\n",
            "Iteration 48, loss = 1.03697141\n",
            "Iteration 49, loss = 1.04517900\n",
            "Iteration 50, loss = 1.04090304\n",
            "Iteration 51, loss = 1.02847124\n",
            "Iteration 52, loss = 1.02814693\n",
            "Iteration 53, loss = 1.01737263\n",
            "Iteration 54, loss = 1.04456673\n",
            "Iteration 55, loss = 1.04177579\n",
            "Iteration 56, loss = 1.02942678\n",
            "Iteration 57, loss = 1.01913385\n",
            "Iteration 58, loss = 1.03202804\n",
            "Iteration 59, loss = 1.03093766\n",
            "Iteration 60, loss = 1.02110624\n",
            "Iteration 61, loss = 1.00920221\n",
            "Iteration 62, loss = 1.01363155\n",
            "Iteration 63, loss = 1.02295778\n",
            "Iteration 64, loss = 1.00266902\n",
            "Iteration 65, loss = 1.00972651\n",
            "Iteration 66, loss = 1.01018348\n",
            "Iteration 67, loss = 1.02646376\n",
            "Iteration 68, loss = 1.05942735\n",
            "Iteration 69, loss = 1.03571677\n",
            "Iteration 70, loss = 1.01254126\n",
            "Iteration 71, loss = 1.02201613\n",
            "Iteration 72, loss = 1.01634081\n",
            "Iteration 73, loss = 1.01144973\n",
            "Iteration 74, loss = 1.01122724\n",
            "Iteration 75, loss = 0.99871058\n",
            "Iteration 76, loss = 0.99384529\n",
            "Iteration 77, loss = 0.98732140\n",
            "Iteration 78, loss = 0.98604039\n",
            "Iteration 79, loss = 0.99587479\n",
            "Iteration 80, loss = 1.00804317\n",
            "Iteration 81, loss = 1.00563055\n",
            "Iteration 82, loss = 0.98662956\n",
            "Iteration 83, loss = 1.00300827\n",
            "Iteration 84, loss = 0.98240826\n",
            "Iteration 85, loss = 0.99182240\n",
            "Iteration 86, loss = 0.99232570\n",
            "Iteration 87, loss = 1.01084920\n",
            "Iteration 88, loss = 0.98864341\n",
            "Iteration 89, loss = 0.98697225\n",
            "Iteration 90, loss = 0.97835032\n",
            "Iteration 91, loss = 0.97785510\n",
            "Iteration 92, loss = 0.96841192\n",
            "Iteration 93, loss = 0.98093806\n",
            "Iteration 94, loss = 0.98875245\n",
            "Iteration 95, loss = 0.97839093\n",
            "Iteration 96, loss = 0.97487148\n",
            "Iteration 97, loss = 0.96857340\n",
            "Iteration 98, loss = 0.97010088\n",
            "Iteration 99, loss = 0.96155388\n",
            "Iteration 100, loss = 0.94686657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27418895\n",
            "Iteration 2, loss = 1.22694715\n",
            "Iteration 3, loss = 1.20951827\n",
            "Iteration 4, loss = 1.20489176\n",
            "Iteration 5, loss = 1.17913978\n",
            "Iteration 6, loss = 1.17580851\n",
            "Iteration 7, loss = 1.16286553\n",
            "Iteration 8, loss = 1.15127850\n",
            "Iteration 9, loss = 1.14270470\n",
            "Iteration 10, loss = 1.14191078\n",
            "Iteration 11, loss = 1.15295212\n",
            "Iteration 12, loss = 1.14342843\n",
            "Iteration 13, loss = 1.15313519\n",
            "Iteration 14, loss = 1.15589986\n",
            "Iteration 15, loss = 1.12764500\n",
            "Iteration 16, loss = 1.11372186\n",
            "Iteration 17, loss = 1.12589683\n",
            "Iteration 18, loss = 1.11732968\n",
            "Iteration 19, loss = 1.11666546\n",
            "Iteration 20, loss = 1.11208511\n",
            "Iteration 21, loss = 1.11332223\n",
            "Iteration 22, loss = 1.10513866\n",
            "Iteration 23, loss = 1.09597732\n",
            "Iteration 24, loss = 1.09388001\n",
            "Iteration 25, loss = 1.08782450\n",
            "Iteration 26, loss = 1.08949085\n",
            "Iteration 27, loss = 1.08909945\n",
            "Iteration 28, loss = 1.09103174\n",
            "Iteration 29, loss = 1.08249268\n",
            "Iteration 30, loss = 1.07417010\n",
            "Iteration 31, loss = 1.07972322\n",
            "Iteration 32, loss = 1.06997647\n",
            "Iteration 33, loss = 1.07100585\n",
            "Iteration 34, loss = 1.06845620\n",
            "Iteration 35, loss = 1.06087785\n",
            "Iteration 36, loss = 1.06240155\n",
            "Iteration 37, loss = 1.06085019\n",
            "Iteration 38, loss = 1.05659218\n",
            "Iteration 39, loss = 1.06282684\n",
            "Iteration 40, loss = 1.05076809\n",
            "Iteration 41, loss = 1.06042376\n",
            "Iteration 42, loss = 1.05132694\n",
            "Iteration 43, loss = 1.10916348\n",
            "Iteration 44, loss = 1.06321328\n",
            "Iteration 45, loss = 1.05388251\n",
            "Iteration 46, loss = 1.05323141\n",
            "Iteration 47, loss = 1.05695872\n",
            "Iteration 48, loss = 1.04179286\n",
            "Iteration 49, loss = 1.05215787\n",
            "Iteration 50, loss = 1.06250558\n",
            "Iteration 51, loss = 1.05444082\n",
            "Iteration 52, loss = 1.05367348\n",
            "Iteration 53, loss = 1.03718303\n",
            "Iteration 54, loss = 1.05244047\n",
            "Iteration 55, loss = 1.03807623\n",
            "Iteration 56, loss = 1.03469976\n",
            "Iteration 57, loss = 1.02799536\n",
            "Iteration 58, loss = 1.03068424\n",
            "Iteration 59, loss = 1.02140060\n",
            "Iteration 60, loss = 1.02039306\n",
            "Iteration 61, loss = 1.01199648\n",
            "Iteration 62, loss = 1.01433732\n",
            "Iteration 63, loss = 1.01468673\n",
            "Iteration 64, loss = 1.00318696\n",
            "Iteration 65, loss = 1.02065585\n",
            "Iteration 66, loss = 1.03190711\n",
            "Iteration 67, loss = 1.04013673\n",
            "Iteration 68, loss = 1.04981796\n",
            "Iteration 69, loss = 1.02545192\n",
            "Iteration 70, loss = 1.01615425\n",
            "Iteration 71, loss = 1.03344906\n",
            "Iteration 72, loss = 1.01519636\n",
            "Iteration 73, loss = 0.99956978\n",
            "Iteration 74, loss = 0.99964518\n",
            "Iteration 75, loss = 0.99378052\n",
            "Iteration 76, loss = 0.99706634\n",
            "Iteration 77, loss = 0.99600903\n",
            "Iteration 78, loss = 0.99070564\n",
            "Iteration 79, loss = 0.98881094\n",
            "Iteration 80, loss = 1.00048430\n",
            "Iteration 81, loss = 1.01482223\n",
            "Iteration 82, loss = 1.00039185\n",
            "Iteration 83, loss = 0.99492453\n",
            "Iteration 84, loss = 0.99288330\n",
            "Iteration 85, loss = 0.98480861\n",
            "Iteration 86, loss = 1.00632343\n",
            "Iteration 87, loss = 0.98740214\n",
            "Iteration 88, loss = 0.97116284\n",
            "Iteration 89, loss = 0.97545058\n",
            "Iteration 90, loss = 0.97745330\n",
            "Iteration 91, loss = 0.97138632\n",
            "Iteration 92, loss = 0.96107889\n",
            "Iteration 93, loss = 0.97903382\n",
            "Iteration 94, loss = 0.95928940\n",
            "Iteration 95, loss = 0.95824725\n",
            "Iteration 96, loss = 0.96542626\n",
            "Iteration 97, loss = 0.95488534\n",
            "Iteration 98, loss = 0.96835514\n",
            "Iteration 99, loss = 0.95511490\n",
            "Iteration 100, loss = 0.94033849\n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28134971\n",
            "Iteration 2, loss = 1.24968153\n",
            "Iteration 3, loss = 1.24928072\n",
            "Iteration 4, loss = 1.24399784\n",
            "Iteration 5, loss = 1.23847652\n",
            "Iteration 6, loss = 1.23576918\n",
            "Iteration 7, loss = 1.22989697\n",
            "Iteration 8, loss = 1.22582862\n",
            "Iteration 9, loss = 1.23156816\n",
            "Iteration 10, loss = 1.22197379\n",
            "Iteration 11, loss = 1.22750390\n",
            "Iteration 12, loss = 1.22186125\n",
            "Iteration 13, loss = 1.21455793\n",
            "Iteration 14, loss = 1.21986530\n",
            "Iteration 15, loss = 1.21521856\n",
            "Iteration 16, loss = 1.21483486\n",
            "Iteration 17, loss = 1.21454040\n",
            "Iteration 18, loss = 1.20525350\n",
            "Iteration 19, loss = 1.20545791\n",
            "Iteration 20, loss = 1.21351604\n",
            "Iteration 21, loss = 1.19748958\n",
            "Iteration 22, loss = 1.20275965\n",
            "Iteration 23, loss = 1.20564237\n",
            "Iteration 24, loss = 1.20000263\n",
            "Iteration 25, loss = 1.20572690\n",
            "Iteration 26, loss = 1.21998821\n",
            "Iteration 27, loss = 1.23046882\n",
            "Iteration 28, loss = 1.19864185\n",
            "Iteration 29, loss = 1.22548593\n",
            "Iteration 30, loss = 1.20298579\n",
            "Iteration 31, loss = 1.19681979\n",
            "Iteration 32, loss = 1.19566357\n",
            "Iteration 33, loss = 1.19222726\n",
            "Iteration 34, loss = 1.19573317\n",
            "Iteration 35, loss = 1.19428245\n",
            "Iteration 36, loss = 1.19876598\n",
            "Iteration 37, loss = 1.19055906\n",
            "Iteration 38, loss = 1.19381496\n",
            "Iteration 39, loss = 1.18161377\n",
            "Iteration 40, loss = 1.18618974\n",
            "Iteration 41, loss = 1.20577098\n",
            "Iteration 42, loss = 1.19352489\n",
            "Iteration 43, loss = 1.18512616\n",
            "Iteration 44, loss = 1.17836728\n",
            "Iteration 45, loss = 1.17102138\n",
            "Iteration 46, loss = 1.18076212\n",
            "Iteration 47, loss = 1.17688729\n",
            "Iteration 48, loss = 1.16953616\n",
            "Iteration 49, loss = 1.17142710\n",
            "Iteration 50, loss = 1.16792237\n",
            "Iteration 51, loss = 1.17477400\n",
            "Iteration 52, loss = 1.15758654\n",
            "Iteration 53, loss = 1.16356487\n",
            "Iteration 54, loss = 1.15164499\n",
            "Iteration 55, loss = 1.16219022\n",
            "Iteration 56, loss = 1.16164179\n",
            "Iteration 57, loss = 1.15250468\n",
            "Iteration 58, loss = 1.16435850\n",
            "Iteration 59, loss = 1.17640792\n",
            "Iteration 60, loss = 1.15703515\n",
            "Iteration 61, loss = 1.15331497\n",
            "Iteration 62, loss = 1.15144811\n",
            "Iteration 63, loss = 1.14583759\n",
            "Iteration 64, loss = 1.15565147\n",
            "Iteration 65, loss = 1.16422022\n",
            "Iteration 66, loss = 1.15140261\n",
            "Iteration 67, loss = 1.14695963\n",
            "Iteration 68, loss = 1.14733662\n",
            "Iteration 69, loss = 1.15713381\n",
            "Iteration 70, loss = 1.21749139\n",
            "Iteration 71, loss = 1.15404719\n",
            "Iteration 72, loss = 1.15411413\n",
            "Iteration 73, loss = 1.15642807\n",
            "Iteration 74, loss = 1.13934419\n",
            "Iteration 75, loss = 1.15520882\n",
            "Iteration 76, loss = 1.13745826\n",
            "Iteration 77, loss = 1.13950088\n",
            "Iteration 78, loss = 1.13773547\n",
            "Iteration 79, loss = 1.14272337\n",
            "Iteration 80, loss = 1.15959236\n",
            "Iteration 81, loss = 1.16163081\n",
            "Iteration 82, loss = 1.15318053\n",
            "Iteration 83, loss = 1.12958496\n",
            "Iteration 84, loss = 1.12842490\n",
            "Iteration 85, loss = 1.12730172\n",
            "Iteration 86, loss = 1.11588169\n",
            "Iteration 87, loss = 1.11660190\n",
            "Iteration 88, loss = 1.11815046\n",
            "Iteration 89, loss = 1.11651430\n",
            "Iteration 90, loss = 1.11972376\n",
            "Iteration 91, loss = 1.11722325\n",
            "Iteration 92, loss = 1.12147999\n",
            "Iteration 93, loss = 1.11085434\n",
            "Iteration 94, loss = 1.12107198\n",
            "Iteration 95, loss = 1.11523169\n",
            "Iteration 96, loss = 1.11574192\n",
            "Iteration 97, loss = 1.11962201\n",
            "Iteration 98, loss = 1.11591681\n",
            "Iteration 99, loss = 1.11957032\n",
            "Iteration 100, loss = 1.09586078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27400838\n",
            "Iteration 2, loss = 1.24948875\n",
            "Iteration 3, loss = 1.24169251\n",
            "Iteration 4, loss = 1.23676923\n",
            "Iteration 5, loss = 1.23254647\n",
            "Iteration 6, loss = 1.23857305\n",
            "Iteration 7, loss = 1.23118926\n",
            "Iteration 8, loss = 1.22572892\n",
            "Iteration 9, loss = 1.22594134\n",
            "Iteration 10, loss = 1.22013355\n",
            "Iteration 11, loss = 1.22446176\n",
            "Iteration 12, loss = 1.21668476\n",
            "Iteration 13, loss = 1.21150866\n",
            "Iteration 14, loss = 1.21224235\n",
            "Iteration 15, loss = 1.21541407\n",
            "Iteration 16, loss = 1.20474090\n",
            "Iteration 17, loss = 1.20391073\n",
            "Iteration 18, loss = 1.20123202\n",
            "Iteration 19, loss = 1.20327908\n",
            "Iteration 20, loss = 1.21074845\n",
            "Iteration 21, loss = 1.19526205\n",
            "Iteration 22, loss = 1.19977718\n",
            "Iteration 23, loss = 1.20649417\n",
            "Iteration 24, loss = 1.20728451\n",
            "Iteration 25, loss = 1.22209159\n",
            "Iteration 26, loss = 1.21196397\n",
            "Iteration 27, loss = 1.22330304\n",
            "Iteration 28, loss = 1.20543035\n",
            "Iteration 29, loss = 1.22960383\n",
            "Iteration 30, loss = 1.20197787\n",
            "Iteration 31, loss = 1.19528678\n",
            "Iteration 32, loss = 1.19693153\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.28490835\n",
            "Iteration 2, loss = 1.25191931\n",
            "Iteration 3, loss = 1.24392005\n",
            "Iteration 4, loss = 1.24117533\n",
            "Iteration 5, loss = 1.23734632\n",
            "Iteration 6, loss = 1.25027954\n",
            "Iteration 7, loss = 1.23431897\n",
            "Iteration 8, loss = 1.22971932\n",
            "Iteration 9, loss = 1.23847355\n",
            "Iteration 10, loss = 1.22342402\n",
            "Iteration 11, loss = 1.22293896\n",
            "Iteration 12, loss = 1.22055555\n",
            "Iteration 13, loss = 1.22208348\n",
            "Iteration 14, loss = 1.21669277\n",
            "Iteration 15, loss = 1.22068182\n",
            "Iteration 16, loss = 1.20711907\n",
            "Iteration 17, loss = 1.21029027\n",
            "Iteration 18, loss = 1.20595399\n",
            "Iteration 19, loss = 1.20199234\n",
            "Iteration 20, loss = 1.20625890\n",
            "Iteration 21, loss = 1.19916791\n",
            "Iteration 22, loss = 1.20504840\n",
            "Iteration 23, loss = 1.20061847\n",
            "Iteration 24, loss = 1.20343908\n",
            "Iteration 25, loss = 1.21937414\n",
            "Iteration 26, loss = 1.22222546\n",
            "Iteration 27, loss = 1.22673009\n",
            "Iteration 28, loss = 1.21735905\n",
            "Iteration 29, loss = 1.21638764\n",
            "Iteration 30, loss = 1.20459403\n",
            "Iteration 31, loss = 1.19343824\n",
            "Iteration 32, loss = 1.19564739\n",
            "Iteration 33, loss = 1.18538654\n",
            "Iteration 34, loss = 1.19142012\n",
            "Iteration 35, loss = 1.18022353\n",
            "Iteration 36, loss = 1.18387065\n",
            "Iteration 37, loss = 1.18216666\n",
            "Iteration 38, loss = 1.19129597\n",
            "Iteration 39, loss = 1.17453973\n",
            "Iteration 40, loss = 1.17665486\n",
            "Iteration 41, loss = 1.17104310\n",
            "Iteration 42, loss = 1.17126230\n",
            "Iteration 43, loss = 1.18386229\n",
            "Iteration 44, loss = 1.19138934\n",
            "Iteration 45, loss = 1.17307898\n",
            "Iteration 46, loss = 1.16038900\n",
            "Iteration 47, loss = 1.17216368\n",
            "Iteration 48, loss = 1.15567109\n",
            "Iteration 49, loss = 1.16538304\n",
            "Iteration 50, loss = 1.16666185\n",
            "Iteration 51, loss = 1.16327289\n",
            "Iteration 52, loss = 1.15148191\n",
            "Iteration 53, loss = 1.14973966\n",
            "Iteration 54, loss = 1.14377279\n",
            "Iteration 55, loss = 1.13998328\n",
            "Iteration 56, loss = 1.14062591\n",
            "Iteration 57, loss = 1.13407051\n",
            "Iteration 58, loss = 1.14586674\n",
            "Iteration 59, loss = 1.13996479\n",
            "Iteration 60, loss = 1.15406595\n",
            "Iteration 61, loss = 1.14898831\n",
            "Iteration 62, loss = 1.14930399\n",
            "Iteration 63, loss = 1.13652402\n",
            "Iteration 64, loss = 1.13752787\n",
            "Iteration 65, loss = 1.15196872\n",
            "Iteration 66, loss = 1.12270677\n",
            "Iteration 67, loss = 1.13620724\n",
            "Iteration 68, loss = 1.13197220\n",
            "Iteration 69, loss = 1.14358002\n",
            "Iteration 70, loss = 1.16131448\n",
            "Iteration 71, loss = 1.14062585\n",
            "Iteration 72, loss = 1.14812066\n",
            "Iteration 73, loss = 1.12848382\n",
            "Iteration 74, loss = 1.13017895\n",
            "Iteration 75, loss = 1.13443895\n",
            "Iteration 76, loss = 1.10976877\n",
            "Iteration 77, loss = 1.10973287\n",
            "Iteration 78, loss = 1.10736481\n",
            "Iteration 79, loss = 1.11991677\n",
            "Iteration 80, loss = 1.14439412\n",
            "Iteration 81, loss = 1.11043820\n",
            "Iteration 82, loss = 1.11818797\n",
            "Iteration 83, loss = 1.15411462\n",
            "Iteration 84, loss = 1.16076527\n",
            "Iteration 85, loss = 1.14794115\n",
            "Iteration 86, loss = 1.13039630\n",
            "Iteration 87, loss = 1.12410419\n",
            "Iteration 88, loss = 1.12993013\n",
            "Iteration 89, loss = 1.11484866\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29374039\n",
            "Iteration 2, loss = 1.25503970\n",
            "Iteration 3, loss = 1.24831839\n",
            "Iteration 4, loss = 1.24608288\n",
            "Iteration 5, loss = 1.24428536\n",
            "Iteration 6, loss = 1.26282880\n",
            "Iteration 7, loss = 1.24130604\n",
            "Iteration 8, loss = 1.24193543\n",
            "Iteration 9, loss = 1.24351410\n",
            "Iteration 10, loss = 1.23789271\n",
            "Iteration 11, loss = 1.24257893\n",
            "Iteration 12, loss = 1.22929575\n",
            "Iteration 13, loss = 1.23071251\n",
            "Iteration 14, loss = 1.22750385\n",
            "Iteration 15, loss = 1.22762122\n",
            "Iteration 16, loss = 1.21723549\n",
            "Iteration 17, loss = 1.21987534\n",
            "Iteration 18, loss = 1.21560584\n",
            "Iteration 19, loss = 1.21590889\n",
            "Iteration 20, loss = 1.21329199\n",
            "Iteration 21, loss = 1.21054119\n",
            "Iteration 22, loss = 1.20913025\n",
            "Iteration 23, loss = 1.21167693\n",
            "Iteration 24, loss = 1.21263796\n",
            "Iteration 25, loss = 1.21571411\n",
            "Iteration 26, loss = 1.22830636\n",
            "Iteration 27, loss = 1.24607004\n",
            "Iteration 28, loss = 1.23439226\n",
            "Iteration 29, loss = 1.23274524\n",
            "Iteration 30, loss = 1.22246035\n",
            "Iteration 31, loss = 1.21092003\n",
            "Iteration 32, loss = 1.21109209\n",
            "Iteration 33, loss = 1.20090879\n",
            "Iteration 34, loss = 1.20211186\n",
            "Iteration 35, loss = 1.19644246\n",
            "Iteration 36, loss = 1.19481182\n",
            "Iteration 37, loss = 1.19573179\n",
            "Iteration 38, loss = 1.19403635\n",
            "Iteration 39, loss = 1.19062999\n",
            "Iteration 40, loss = 1.18522843\n",
            "Iteration 41, loss = 1.18373835\n",
            "Iteration 42, loss = 1.18446254\n",
            "Iteration 43, loss = 1.19625146\n",
            "Iteration 44, loss = 1.22115218\n",
            "Iteration 45, loss = 1.20327158\n",
            "Iteration 46, loss = 1.17502376\n",
            "Iteration 47, loss = 1.18327154\n",
            "Iteration 48, loss = 1.17783923\n",
            "Iteration 49, loss = 1.20058947\n",
            "Iteration 50, loss = 1.18718690\n",
            "Iteration 51, loss = 1.18640377\n",
            "Iteration 52, loss = 1.16911725\n",
            "Iteration 53, loss = 1.16672292\n",
            "Iteration 54, loss = 1.15549318\n",
            "Iteration 55, loss = 1.15672318\n",
            "Iteration 56, loss = 1.16536554\n",
            "Iteration 57, loss = 1.15610344\n",
            "Iteration 58, loss = 1.16819346\n",
            "Iteration 59, loss = 1.15545398\n",
            "Iteration 60, loss = 1.16105336\n",
            "Iteration 61, loss = 1.16898498\n",
            "Iteration 62, loss = 1.15285957\n",
            "Iteration 63, loss = 1.15189803\n",
            "Iteration 64, loss = 1.15378251\n",
            "Iteration 65, loss = 1.15608513\n",
            "Iteration 66, loss = 1.14289886\n",
            "Iteration 67, loss = 1.15353864\n",
            "Iteration 68, loss = 1.15880948\n",
            "Iteration 69, loss = 1.15839563\n",
            "Iteration 70, loss = 1.18351017\n",
            "Iteration 71, loss = 1.15753476\n",
            "Iteration 72, loss = 1.14470955\n",
            "Iteration 73, loss = 1.15472398\n",
            "Iteration 74, loss = 1.14004279\n",
            "Iteration 75, loss = 1.16269647\n",
            "Iteration 76, loss = 1.13499838\n",
            "Iteration 77, loss = 1.13928729\n",
            "Iteration 78, loss = 1.12677487\n",
            "Iteration 79, loss = 1.13126883\n",
            "Iteration 80, loss = 1.14947971\n",
            "Iteration 81, loss = 1.14314457\n",
            "Iteration 82, loss = 1.14099959\n",
            "Iteration 83, loss = 1.17055839\n",
            "Iteration 84, loss = 1.18139923\n",
            "Iteration 85, loss = 1.17738912\n",
            "Iteration 86, loss = 1.15630341\n",
            "Iteration 87, loss = 1.13570522\n",
            "Iteration 88, loss = 1.14476999\n",
            "Iteration 89, loss = 1.11999524\n",
            "Iteration 90, loss = 1.13051713\n",
            "Iteration 91, loss = 1.12428424\n",
            "Iteration 92, loss = 1.12671155\n",
            "Iteration 93, loss = 1.11927612\n",
            "Iteration 94, loss = 1.12627808\n",
            "Iteration 95, loss = 1.11756073\n",
            "Iteration 96, loss = 1.11633461\n",
            "Iteration 97, loss = 1.11006222\n",
            "Iteration 98, loss = 1.11607812\n",
            "Iteration 99, loss = 1.11774787\n",
            "Iteration 100, loss = 1.09685726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27742014\n",
            "Iteration 2, loss = 1.24473580\n",
            "Iteration 3, loss = 1.23735457\n",
            "Iteration 4, loss = 1.24219171\n",
            "Iteration 5, loss = 1.24268255\n",
            "Iteration 6, loss = 1.25004119\n",
            "Iteration 7, loss = 1.23513723\n",
            "Iteration 8, loss = 1.23909834\n",
            "Iteration 9, loss = 1.23163887\n",
            "Iteration 10, loss = 1.23216959\n",
            "Iteration 11, loss = 1.23825771\n",
            "Iteration 12, loss = 1.23016884\n",
            "Iteration 13, loss = 1.23224087\n",
            "Iteration 14, loss = 1.22773978\n",
            "Iteration 15, loss = 1.22187705\n",
            "Iteration 16, loss = 1.21693751\n",
            "Iteration 17, loss = 1.22061683\n",
            "Iteration 18, loss = 1.21877655\n",
            "Iteration 19, loss = 1.21614400\n",
            "Iteration 20, loss = 1.21796000\n",
            "Iteration 21, loss = 1.20986480\n",
            "Iteration 22, loss = 1.21356944\n",
            "Iteration 23, loss = 1.21263282\n",
            "Iteration 24, loss = 1.21355749\n",
            "Iteration 25, loss = 1.21632261\n",
            "Iteration 26, loss = 1.21729848\n",
            "Iteration 27, loss = 1.21848600\n",
            "Iteration 28, loss = 1.20719499\n",
            "Iteration 29, loss = 1.21213372\n",
            "Iteration 30, loss = 1.20748820\n",
            "Iteration 31, loss = 1.20295037\n",
            "Iteration 32, loss = 1.20034328\n",
            "Iteration 33, loss = 1.20003456\n",
            "Iteration 34, loss = 1.19993379\n",
            "Iteration 35, loss = 1.19674306\n",
            "Iteration 36, loss = 1.19485838\n",
            "Iteration 37, loss = 1.19259671\n",
            "Iteration 38, loss = 1.19277665\n",
            "Iteration 39, loss = 1.18731768\n",
            "Iteration 40, loss = 1.18645661\n",
            "Iteration 41, loss = 1.18549414\n",
            "Iteration 42, loss = 1.18493005\n",
            "Iteration 43, loss = 1.18535978\n",
            "Iteration 44, loss = 1.18937230\n",
            "Iteration 45, loss = 1.18433855\n",
            "Iteration 46, loss = 1.17480941\n",
            "Iteration 47, loss = 1.17788282\n",
            "Iteration 48, loss = 1.17215287\n",
            "Iteration 49, loss = 1.18195373\n",
            "Iteration 50, loss = 1.18125046\n",
            "Iteration 51, loss = 1.17356513\n",
            "Iteration 52, loss = 1.17052941\n",
            "Iteration 53, loss = 1.16866408\n",
            "Iteration 54, loss = 1.16669847\n",
            "Iteration 55, loss = 1.15715145\n",
            "Iteration 56, loss = 1.15801671\n",
            "Iteration 57, loss = 1.15917031\n",
            "Iteration 58, loss = 1.15450628\n",
            "Iteration 59, loss = 1.16752090\n",
            "Iteration 60, loss = 1.16214925\n",
            "Iteration 61, loss = 1.15968008\n",
            "Iteration 62, loss = 1.14905243\n",
            "Iteration 63, loss = 1.16272895\n",
            "Iteration 64, loss = 1.15456407\n",
            "Iteration 65, loss = 1.14876574\n",
            "Iteration 66, loss = 1.13882387\n",
            "Iteration 67, loss = 1.13593608\n",
            "Iteration 68, loss = 1.15724012\n",
            "Iteration 69, loss = 1.15819086\n",
            "Iteration 70, loss = 1.16833368\n",
            "Iteration 71, loss = 1.15988224\n",
            "Iteration 72, loss = 1.15624529\n",
            "Iteration 73, loss = 1.13921573\n",
            "Iteration 74, loss = 1.14311090\n",
            "Iteration 75, loss = 1.13710129\n",
            "Iteration 76, loss = 1.12918920\n",
            "Iteration 77, loss = 1.12384926\n",
            "Iteration 78, loss = 1.11670659\n",
            "Iteration 79, loss = 1.11684136\n",
            "Iteration 80, loss = 1.12953228\n",
            "Iteration 81, loss = 1.10857936\n",
            "Iteration 82, loss = 1.11400882\n",
            "Iteration 83, loss = 1.13119541\n",
            "Iteration 84, loss = 1.11954922\n",
            "Iteration 85, loss = 1.11876966\n",
            "Iteration 86, loss = 1.11171881\n",
            "Iteration 87, loss = 1.11356475\n",
            "Iteration 88, loss = 1.09799410\n",
            "Iteration 89, loss = 1.11776362\n",
            "Iteration 90, loss = 1.10647243\n",
            "Iteration 91, loss = 1.11310574\n",
            "Iteration 92, loss = 1.11399574\n",
            "Iteration 93, loss = 1.11963771\n",
            "Iteration 94, loss = 1.11511493\n",
            "Iteration 95, loss = 1.10612208\n",
            "Iteration 96, loss = 1.08831527\n",
            "Iteration 97, loss = 1.11341813\n",
            "Iteration 98, loss = 1.10498954\n",
            "Iteration 99, loss = 1.10117113\n",
            "Iteration 100, loss = 1.10139586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.26308896\n",
            "Iteration 2, loss = 1.23456876\n",
            "Iteration 3, loss = 1.22497460\n",
            "Iteration 4, loss = 1.22768871\n",
            "Iteration 5, loss = 1.22209917\n",
            "Iteration 6, loss = 1.22502193\n",
            "Iteration 7, loss = 1.22270257\n",
            "Iteration 8, loss = 1.21247109\n",
            "Iteration 9, loss = 1.21748371\n",
            "Iteration 10, loss = 1.20873867\n",
            "Iteration 11, loss = 1.20897460\n",
            "Iteration 12, loss = 1.20317599\n",
            "Iteration 13, loss = 1.21903358\n",
            "Iteration 14, loss = 1.21412515\n",
            "Iteration 15, loss = 1.20566527\n",
            "Iteration 16, loss = 1.20193704\n",
            "Iteration 17, loss = 1.20451547\n",
            "Iteration 18, loss = 1.19796466\n",
            "Iteration 19, loss = 1.20473115\n",
            "Iteration 20, loss = 1.19138539\n",
            "Iteration 21, loss = 1.18923920\n",
            "Iteration 22, loss = 1.18685243\n",
            "Iteration 23, loss = 1.19295295\n",
            "Iteration 24, loss = 1.19069312\n",
            "Iteration 25, loss = 1.18757325\n",
            "Iteration 26, loss = 1.18948431\n",
            "Iteration 27, loss = 1.18770659\n",
            "Iteration 28, loss = 1.18697402\n",
            "Iteration 29, loss = 1.19065665\n",
            "Iteration 30, loss = 1.20599152\n",
            "Iteration 31, loss = 1.18795598\n",
            "Iteration 32, loss = 1.18264388\n",
            "Iteration 33, loss = 1.18002800\n",
            "Iteration 34, loss = 1.18029122\n",
            "Iteration 35, loss = 1.17106202\n",
            "Iteration 36, loss = 1.17812523\n",
            "Iteration 37, loss = 1.18361552\n",
            "Iteration 38, loss = 1.17360557\n",
            "Iteration 39, loss = 1.16705763\n",
            "Iteration 40, loss = 1.16503530\n",
            "Iteration 41, loss = 1.16356377\n",
            "Iteration 42, loss = 1.16176719\n",
            "Iteration 43, loss = 1.16085460\n",
            "Iteration 44, loss = 1.16452598\n",
            "Iteration 45, loss = 1.15506220\n",
            "Iteration 46, loss = 1.15303506\n",
            "Iteration 47, loss = 1.16398359\n",
            "Iteration 48, loss = 1.17346892\n",
            "Iteration 49, loss = 1.19288050\n",
            "Iteration 50, loss = 1.17824075\n",
            "Iteration 51, loss = 1.16775086\n",
            "Iteration 52, loss = 1.15668310\n",
            "Iteration 53, loss = 1.14764404\n",
            "Iteration 54, loss = 1.14064408\n",
            "Iteration 55, loss = 1.14223278\n",
            "Iteration 56, loss = 1.13843774\n",
            "Iteration 57, loss = 1.13799942\n",
            "Iteration 58, loss = 1.13810926\n",
            "Iteration 59, loss = 1.15748502\n",
            "Iteration 60, loss = 1.14237995\n",
            "Iteration 61, loss = 1.16117149\n",
            "Iteration 62, loss = 1.14909871\n",
            "Iteration 63, loss = 1.13387702\n",
            "Iteration 64, loss = 1.13795874\n",
            "Iteration 65, loss = 1.13514841\n",
            "Iteration 66, loss = 1.12397257\n",
            "Iteration 67, loss = 1.12164230\n",
            "Iteration 68, loss = 1.12450474\n",
            "Iteration 69, loss = 1.13924134\n",
            "Iteration 70, loss = 1.13884722\n",
            "Iteration 71, loss = 1.12655207\n",
            "Iteration 72, loss = 1.12156014\n",
            "Iteration 73, loss = 1.10789791\n",
            "Iteration 74, loss = 1.12358196\n",
            "Iteration 75, loss = 1.12381854\n",
            "Iteration 76, loss = 1.11182957\n",
            "Iteration 77, loss = 1.11820389\n",
            "Iteration 78, loss = 1.10648601\n",
            "Iteration 79, loss = 1.11026085\n",
            "Iteration 80, loss = 1.15586014\n",
            "Iteration 81, loss = 1.11925172\n",
            "Iteration 82, loss = 1.10973699\n",
            "Iteration 83, loss = 1.11149516\n",
            "Iteration 84, loss = 1.12157991\n",
            "Iteration 85, loss = 1.10620142\n",
            "Iteration 86, loss = 1.09288153\n",
            "Iteration 87, loss = 1.08430313\n",
            "Iteration 88, loss = 1.08756546\n",
            "Iteration 89, loss = 1.09813192\n",
            "Iteration 90, loss = 1.07706364\n",
            "Iteration 91, loss = 1.08006312\n",
            "Iteration 92, loss = 1.08241179\n",
            "Iteration 93, loss = 1.08999061\n",
            "Iteration 94, loss = 1.09331307\n",
            "Iteration 95, loss = 1.08833581\n",
            "Iteration 96, loss = 1.08988376\n",
            "Iteration 97, loss = 1.07292987\n",
            "Iteration 98, loss = 1.07486332\n",
            "Iteration 99, loss = 1.08048383\n",
            "Iteration 100, loss = 1.08028993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.26437982\n",
            "Iteration 2, loss = 1.24419985\n",
            "Iteration 3, loss = 1.23351415\n",
            "Iteration 4, loss = 1.22619250\n",
            "Iteration 5, loss = 1.22507358\n",
            "Iteration 6, loss = 1.23030521\n",
            "Iteration 7, loss = 1.22295226\n",
            "Iteration 8, loss = 1.22090614\n",
            "Iteration 9, loss = 1.21590376\n",
            "Iteration 10, loss = 1.21012705\n",
            "Iteration 11, loss = 1.21481222\n",
            "Iteration 12, loss = 1.20309164\n",
            "Iteration 13, loss = 1.22573445\n",
            "Iteration 14, loss = 1.21930118\n",
            "Iteration 15, loss = 1.21026606\n",
            "Iteration 16, loss = 1.20695725\n",
            "Iteration 17, loss = 1.21070311\n",
            "Iteration 18, loss = 1.20512610\n",
            "Iteration 19, loss = 1.20400506\n",
            "Iteration 20, loss = 1.20249848\n",
            "Iteration 21, loss = 1.19921929\n",
            "Iteration 22, loss = 1.19371756\n",
            "Iteration 23, loss = 1.19942684\n",
            "Iteration 24, loss = 1.19390279\n",
            "Iteration 25, loss = 1.19245930\n",
            "Iteration 26, loss = 1.19103052\n",
            "Iteration 27, loss = 1.19266040\n",
            "Iteration 28, loss = 1.19487017\n",
            "Iteration 29, loss = 1.18104812\n",
            "Iteration 30, loss = 1.20710537\n",
            "Iteration 31, loss = 1.18653629\n",
            "Iteration 32, loss = 1.18392818\n",
            "Iteration 33, loss = 1.18711049\n",
            "Iteration 34, loss = 1.18677344\n",
            "Iteration 35, loss = 1.18346830\n",
            "Iteration 36, loss = 1.18419555\n",
            "Iteration 37, loss = 1.18653109\n",
            "Iteration 38, loss = 1.17509346\n",
            "Iteration 39, loss = 1.18191423\n",
            "Iteration 40, loss = 1.18334460\n",
            "Iteration 41, loss = 1.17438411\n",
            "Iteration 42, loss = 1.16482918\n",
            "Iteration 43, loss = 1.16638290\n",
            "Iteration 44, loss = 1.16569133\n",
            "Iteration 45, loss = 1.16035397\n",
            "Iteration 46, loss = 1.15385276\n",
            "Iteration 47, loss = 1.15401309\n",
            "Iteration 48, loss = 1.16110496\n",
            "Iteration 49, loss = 1.17375499\n",
            "Iteration 50, loss = 1.16856505\n",
            "Iteration 51, loss = 1.15802164\n",
            "Iteration 52, loss = 1.14956927\n",
            "Iteration 53, loss = 1.14280728\n",
            "Iteration 54, loss = 1.14042938\n",
            "Iteration 55, loss = 1.14214377\n",
            "Iteration 56, loss = 1.13943208\n",
            "Iteration 57, loss = 1.13490857\n",
            "Iteration 58, loss = 1.13184427\n",
            "Iteration 59, loss = 1.13857090\n",
            "Iteration 60, loss = 1.13473596\n",
            "Iteration 61, loss = 1.16341035\n",
            "Iteration 62, loss = 1.17013934\n",
            "Iteration 63, loss = 1.17069099\n",
            "Iteration 64, loss = 1.13402173\n",
            "Iteration 65, loss = 1.14308399\n",
            "Iteration 66, loss = 1.14834973\n",
            "Iteration 67, loss = 1.12472165\n",
            "Iteration 68, loss = 1.12474721\n",
            "Iteration 69, loss = 1.12135000\n",
            "Iteration 70, loss = 1.14028585\n",
            "Iteration 71, loss = 1.12570475\n",
            "Iteration 72, loss = 1.13141080\n",
            "Iteration 73, loss = 1.10384708\n",
            "Iteration 74, loss = 1.11110797\n",
            "Iteration 75, loss = 1.10871611\n",
            "Iteration 76, loss = 1.10198284\n",
            "Iteration 77, loss = 1.10742819\n",
            "Iteration 78, loss = 1.10330431\n",
            "Iteration 79, loss = 1.13069942\n",
            "Iteration 80, loss = 1.14380557\n",
            "Iteration 81, loss = 1.11853084\n",
            "Iteration 82, loss = 1.14273253\n",
            "Iteration 83, loss = 1.14988406\n",
            "Iteration 84, loss = 1.12960249\n",
            "Iteration 85, loss = 1.10267709\n",
            "Iteration 86, loss = 1.10434359\n",
            "Iteration 87, loss = 1.09509510\n",
            "Iteration 88, loss = 1.08531141\n",
            "Iteration 89, loss = 1.09244972\n",
            "Iteration 90, loss = 1.09354904\n",
            "Iteration 91, loss = 1.08864564\n",
            "Iteration 92, loss = 1.07848000\n",
            "Iteration 93, loss = 1.09168479\n",
            "Iteration 94, loss = 1.09576028\n",
            "Iteration 95, loss = 1.09764752\n",
            "Iteration 96, loss = 1.08910598\n",
            "Iteration 97, loss = 1.07454456\n",
            "Iteration 98, loss = 1.08672509\n",
            "Iteration 99, loss = 1.07212586\n",
            "Iteration 100, loss = 1.06542954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.26055235\n",
            "Iteration 2, loss = 1.24189389\n",
            "Iteration 3, loss = 1.23106001\n",
            "Iteration 4, loss = 1.22834096\n",
            "Iteration 5, loss = 1.22140075\n",
            "Iteration 6, loss = 1.22198809\n",
            "Iteration 7, loss = 1.22016803\n",
            "Iteration 8, loss = 1.21788426\n",
            "Iteration 9, loss = 1.21702636\n",
            "Iteration 10, loss = 1.21034684\n",
            "Iteration 11, loss = 1.20650810\n",
            "Iteration 12, loss = 1.20775632\n",
            "Iteration 13, loss = 1.20367683\n",
            "Iteration 14, loss = 1.20281641\n",
            "Iteration 15, loss = 1.20012601\n",
            "Iteration 16, loss = 1.20318119\n",
            "Iteration 17, loss = 1.20858776\n",
            "Iteration 18, loss = 1.21088945\n",
            "Iteration 19, loss = 1.20727020\n",
            "Iteration 20, loss = 1.19986682\n",
            "Iteration 21, loss = 1.19790248\n",
            "Iteration 22, loss = 1.19451997\n",
            "Iteration 23, loss = 1.20342105\n",
            "Iteration 24, loss = 1.19008644\n",
            "Iteration 25, loss = 1.18649423\n",
            "Iteration 26, loss = 1.18646617\n",
            "Iteration 27, loss = 1.20126329\n",
            "Iteration 28, loss = 1.18631416\n",
            "Iteration 29, loss = 1.18908882\n",
            "Iteration 30, loss = 1.17873597\n",
            "Iteration 31, loss = 1.16998230\n",
            "Iteration 32, loss = 1.19105673\n",
            "Iteration 33, loss = 1.19368190\n",
            "Iteration 34, loss = 1.18610156\n",
            "Iteration 35, loss = 1.18593112\n",
            "Iteration 36, loss = 1.17732478\n",
            "Iteration 37, loss = 1.17973163\n",
            "Iteration 38, loss = 1.17273591\n",
            "Iteration 39, loss = 1.16454928\n",
            "Iteration 40, loss = 1.16146463\n",
            "Iteration 41, loss = 1.18924380\n",
            "Iteration 42, loss = 1.15705788\n",
            "Iteration 43, loss = 1.17600501\n",
            "Iteration 44, loss = 1.19030674\n",
            "Iteration 45, loss = 1.15752750\n",
            "Iteration 46, loss = 1.16028449\n",
            "Iteration 47, loss = 1.16165227\n",
            "Iteration 48, loss = 1.15696869\n",
            "Iteration 49, loss = 1.16626088\n",
            "Iteration 50, loss = 1.15886351\n",
            "Iteration 51, loss = 1.15712446\n",
            "Iteration 52, loss = 1.14740462\n",
            "Iteration 53, loss = 1.15251906\n",
            "Iteration 54, loss = 1.15393634\n",
            "Iteration 55, loss = 1.14014928\n",
            "Iteration 56, loss = 1.14493321\n",
            "Iteration 57, loss = 1.13839465\n",
            "Iteration 58, loss = 1.13125826\n",
            "Iteration 59, loss = 1.14477851\n",
            "Iteration 60, loss = 1.13358206\n",
            "Iteration 61, loss = 1.13628897\n",
            "Iteration 62, loss = 1.12990922\n",
            "Iteration 63, loss = 1.13075083\n",
            "Iteration 64, loss = 1.14463485\n",
            "Iteration 65, loss = 1.15389269\n",
            "Iteration 66, loss = 1.14881036\n",
            "Iteration 67, loss = 1.12305124\n",
            "Iteration 68, loss = 1.13204667\n",
            "Iteration 69, loss = 1.13015696\n",
            "Iteration 70, loss = 1.11448780\n",
            "Iteration 71, loss = 1.12777282\n",
            "Iteration 72, loss = 1.11942611\n",
            "Iteration 73, loss = 1.13351045\n",
            "Iteration 74, loss = 1.10486881\n",
            "Iteration 75, loss = 1.11581847\n",
            "Iteration 76, loss = 1.10313489\n",
            "Iteration 77, loss = 1.10399462\n",
            "Iteration 78, loss = 1.11325475\n",
            "Iteration 79, loss = 1.10900917\n",
            "Iteration 80, loss = 1.15189302\n",
            "Iteration 81, loss = 1.11107926\n",
            "Iteration 82, loss = 1.12928135\n",
            "Iteration 83, loss = 1.14767466\n",
            "Iteration 84, loss = 1.10758042\n",
            "Iteration 85, loss = 1.09851652\n",
            "Iteration 86, loss = 1.09588846\n",
            "Iteration 87, loss = 1.09690765\n",
            "Iteration 88, loss = 1.10893940\n",
            "Iteration 89, loss = 1.10403026\n",
            "Iteration 90, loss = 1.08625221\n",
            "Iteration 91, loss = 1.10532543\n",
            "Iteration 92, loss = 1.10992169\n",
            "Iteration 93, loss = 1.09446858\n",
            "Iteration 94, loss = 1.09691176\n",
            "Iteration 95, loss = 1.07944786\n",
            "Iteration 96, loss = 1.08500411\n",
            "Iteration 97, loss = 1.08741232\n",
            "Iteration 98, loss = 1.08950567\n",
            "Iteration 99, loss = 1.09214424\n",
            "Iteration 100, loss = 1.09602648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.25215569\n",
            "Iteration 2, loss = 1.24009761\n",
            "Iteration 3, loss = 1.22727746\n",
            "Iteration 4, loss = 1.22252874\n",
            "Iteration 5, loss = 1.22189545\n",
            "Iteration 6, loss = 1.22324662\n",
            "Iteration 7, loss = 1.22992788\n",
            "Iteration 8, loss = 1.21216088\n",
            "Iteration 9, loss = 1.21413189\n",
            "Iteration 10, loss = 1.20581871\n",
            "Iteration 11, loss = 1.21138151\n",
            "Iteration 12, loss = 1.20722803\n",
            "Iteration 13, loss = 1.20067829\n",
            "Iteration 14, loss = 1.19919442\n",
            "Iteration 15, loss = 1.20159988\n",
            "Iteration 16, loss = 1.20158602\n",
            "Iteration 17, loss = 1.20279458\n",
            "Iteration 18, loss = 1.19808641\n",
            "Iteration 19, loss = 1.20183534\n",
            "Iteration 20, loss = 1.19459273\n",
            "Iteration 21, loss = 1.20239392\n",
            "Iteration 22, loss = 1.19545715\n",
            "Iteration 23, loss = 1.20046352\n",
            "Iteration 24, loss = 1.19131978\n",
            "Iteration 25, loss = 1.19868721\n",
            "Iteration 26, loss = 1.19253618\n",
            "Iteration 27, loss = 1.20622580\n",
            "Iteration 28, loss = 1.18963964\n",
            "Iteration 29, loss = 1.19715812\n",
            "Iteration 30, loss = 1.18175912\n",
            "Iteration 31, loss = 1.18383022\n",
            "Iteration 32, loss = 1.19545127\n",
            "Iteration 33, loss = 1.18322909\n",
            "Iteration 34, loss = 1.18818593\n",
            "Iteration 35, loss = 1.18060390\n",
            "Iteration 36, loss = 1.18081890\n",
            "Iteration 37, loss = 1.17147931\n",
            "Iteration 38, loss = 1.17123276\n",
            "Iteration 39, loss = 1.16945660\n",
            "Iteration 40, loss = 1.16358187\n",
            "Iteration 41, loss = 1.16959024\n",
            "Iteration 42, loss = 1.16072782\n",
            "Iteration 43, loss = 1.16467837\n",
            "Iteration 44, loss = 1.17165899\n",
            "Iteration 45, loss = 1.16118370\n",
            "Iteration 46, loss = 1.15420159\n",
            "Iteration 47, loss = 1.17585654\n",
            "Iteration 48, loss = 1.15979361\n",
            "Iteration 49, loss = 1.20560121\n",
            "Iteration 50, loss = 1.17126276\n",
            "Iteration 51, loss = 1.16111125\n",
            "Iteration 52, loss = 1.15070991\n",
            "Iteration 53, loss = 1.16375358\n",
            "Iteration 54, loss = 1.15765172\n",
            "Iteration 55, loss = 1.14645754\n",
            "Iteration 56, loss = 1.14275434\n",
            "Iteration 57, loss = 1.14374420\n",
            "Iteration 58, loss = 1.13904984\n",
            "Iteration 59, loss = 1.14494843\n",
            "Iteration 60, loss = 1.13413956\n",
            "Iteration 61, loss = 1.13513120\n",
            "Iteration 62, loss = 1.13529616\n",
            "Iteration 63, loss = 1.13273883\n",
            "Iteration 64, loss = 1.12360977\n",
            "Iteration 65, loss = 1.11719823\n",
            "Iteration 66, loss = 1.12263534\n",
            "Iteration 67, loss = 1.11647587\n",
            "Iteration 68, loss = 1.12071909\n",
            "Iteration 69, loss = 1.11107380\n",
            "Iteration 70, loss = 1.11967374\n",
            "Iteration 71, loss = 1.14640897\n",
            "Iteration 72, loss = 1.12764118\n",
            "Iteration 73, loss = 1.15379639\n",
            "Iteration 74, loss = 1.13794921\n",
            "Iteration 75, loss = 1.12675854\n",
            "Iteration 76, loss = 1.11752159\n",
            "Iteration 77, loss = 1.11074164\n",
            "Iteration 78, loss = 1.11820124\n",
            "Iteration 79, loss = 1.11398436\n",
            "Iteration 80, loss = 1.17019704\n",
            "Iteration 81, loss = 1.13560522\n",
            "Iteration 82, loss = 1.15495517\n",
            "Iteration 83, loss = 1.13578898\n",
            "Iteration 84, loss = 1.11614036\n",
            "Iteration 85, loss = 1.11798708\n",
            "Iteration 86, loss = 1.10845633\n",
            "Iteration 87, loss = 1.10778272\n",
            "Iteration 88, loss = 1.12038172\n",
            "Iteration 89, loss = 1.09738773\n",
            "Iteration 90, loss = 1.09360747\n",
            "Iteration 91, loss = 1.11327846\n",
            "Iteration 92, loss = 1.09739688\n",
            "Iteration 93, loss = 1.09978856\n",
            "Iteration 94, loss = 1.10324332\n",
            "Iteration 95, loss = 1.10871189\n",
            "Iteration 96, loss = 1.09177936\n",
            "Iteration 97, loss = 1.09985987\n",
            "Iteration 98, loss = 1.09081100\n",
            "Iteration 99, loss = 1.08811983\n",
            "Iteration 100, loss = 1.07906434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.25105350\n",
            "Iteration 2, loss = 1.22978639\n",
            "Iteration 3, loss = 1.23658030\n",
            "Iteration 4, loss = 1.21946762\n",
            "Iteration 5, loss = 1.21262229\n",
            "Iteration 6, loss = 1.20602953\n",
            "Iteration 7, loss = 1.21530157\n",
            "Iteration 8, loss = 1.21395434\n",
            "Iteration 9, loss = 1.20538368\n",
            "Iteration 10, loss = 1.19783874\n",
            "Iteration 11, loss = 1.20685455\n",
            "Iteration 12, loss = 1.19993786\n",
            "Iteration 13, loss = 1.19374864\n",
            "Iteration 14, loss = 1.19424265\n",
            "Iteration 15, loss = 1.19555752\n",
            "Iteration 16, loss = 1.19477305\n",
            "Iteration 17, loss = 1.19337386\n",
            "Iteration 18, loss = 1.19040183\n",
            "Iteration 19, loss = 1.19289640\n",
            "Iteration 20, loss = 1.18649140\n",
            "Iteration 21, loss = 1.20461258\n",
            "Iteration 22, loss = 1.18612133\n",
            "Iteration 23, loss = 1.19075455\n",
            "Iteration 24, loss = 1.18554724\n",
            "Iteration 25, loss = 1.18218403\n",
            "Iteration 26, loss = 1.18495927\n",
            "Iteration 27, loss = 1.19563260\n",
            "Iteration 28, loss = 1.18311646\n",
            "Iteration 29, loss = 1.18086843\n",
            "Iteration 30, loss = 1.18033043\n",
            "Iteration 31, loss = 1.18582536\n",
            "Iteration 32, loss = 1.18150803\n",
            "Iteration 33, loss = 1.17539887\n",
            "Iteration 34, loss = 1.17908892\n",
            "Iteration 35, loss = 1.17827442\n",
            "Iteration 36, loss = 1.17617928\n",
            "Iteration 37, loss = 1.17176568\n",
            "Iteration 38, loss = 1.17316793\n",
            "Iteration 39, loss = 1.16840280\n",
            "Iteration 40, loss = 1.18216113\n",
            "Iteration 41, loss = 1.15865332\n",
            "Iteration 42, loss = 1.17478357\n",
            "Iteration 43, loss = 1.17839947\n",
            "Iteration 44, loss = 1.17553480\n",
            "Iteration 45, loss = 1.16745890\n",
            "Iteration 46, loss = 1.16107430\n",
            "Iteration 47, loss = 1.19301634\n",
            "Iteration 48, loss = 1.18152309\n",
            "Iteration 49, loss = 1.17005482\n",
            "Iteration 50, loss = 1.16377745\n",
            "Iteration 51, loss = 1.15514223\n",
            "Iteration 52, loss = 1.15793409\n",
            "Iteration 53, loss = 1.16168693\n",
            "Iteration 54, loss = 1.15585942\n",
            "Iteration 55, loss = 1.15228861\n",
            "Iteration 56, loss = 1.15248774\n",
            "Iteration 57, loss = 1.15040709\n",
            "Iteration 58, loss = 1.14995052\n",
            "Iteration 59, loss = 1.14526664\n",
            "Iteration 60, loss = 1.13931652\n",
            "Iteration 61, loss = 1.14711842\n",
            "Iteration 62, loss = 1.14377932\n",
            "Iteration 63, loss = 1.13463139\n",
            "Iteration 64, loss = 1.13422611\n",
            "Iteration 65, loss = 1.13122204\n",
            "Iteration 66, loss = 1.13326702\n",
            "Iteration 67, loss = 1.12957202\n",
            "Iteration 68, loss = 1.13234195\n",
            "Iteration 69, loss = 1.12277908\n",
            "Iteration 70, loss = 1.14833741\n",
            "Iteration 71, loss = 1.14193653\n",
            "Iteration 72, loss = 1.12287913\n",
            "Iteration 73, loss = 1.13078986\n",
            "Iteration 74, loss = 1.12347095\n",
            "Iteration 75, loss = 1.11868320\n",
            "Iteration 76, loss = 1.11570147\n",
            "Iteration 77, loss = 1.10967096\n",
            "Iteration 78, loss = 1.12295318\n",
            "Iteration 79, loss = 1.11678262\n",
            "Iteration 80, loss = 1.16059060\n",
            "Iteration 81, loss = 1.13519806\n",
            "Iteration 82, loss = 1.14615358\n",
            "Iteration 83, loss = 1.12053762\n",
            "Iteration 84, loss = 1.11527832\n",
            "Iteration 85, loss = 1.11209246\n",
            "Iteration 86, loss = 1.10064230\n",
            "Iteration 87, loss = 1.10602870\n",
            "Iteration 88, loss = 1.10934042\n",
            "Iteration 89, loss = 1.09223755\n",
            "Iteration 90, loss = 1.10179428\n",
            "Iteration 91, loss = 1.13748122\n",
            "Iteration 92, loss = 1.11054676\n",
            "Iteration 93, loss = 1.12680530\n",
            "Iteration 94, loss = 1.13991266\n",
            "Iteration 95, loss = 1.12355790\n",
            "Iteration 96, loss = 1.10095144\n",
            "Iteration 97, loss = 1.10438793\n",
            "Iteration 98, loss = 1.09993632\n",
            "Iteration 99, loss = 1.09222454\n",
            "Iteration 100, loss = 1.08986360\n",
            "11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31206679\n",
            "Iteration 2, loss = 1.22227176\n",
            "Iteration 3, loss = 1.19238334\n",
            "Iteration 4, loss = 1.15939252\n",
            "Iteration 5, loss = 1.12595823\n",
            "Iteration 6, loss = 1.08674707\n",
            "Iteration 7, loss = 1.05450000\n",
            "Iteration 8, loss = 1.03335736\n",
            "Iteration 9, loss = 0.99995804\n",
            "Iteration 10, loss = 0.96894399\n",
            "Iteration 11, loss = 0.99360719\n",
            "Iteration 12, loss = 0.99146151\n",
            "Iteration 13, loss = 0.93954467\n",
            "Iteration 14, loss = 0.93284849\n",
            "Iteration 15, loss = 0.90225323\n",
            "Iteration 16, loss = 0.87419107\n",
            "Iteration 17, loss = 0.86609248\n",
            "Iteration 18, loss = 0.90106991\n",
            "Iteration 19, loss = 0.84584743\n",
            "Iteration 20, loss = 0.83511846\n",
            "Iteration 21, loss = 0.85207433\n",
            "Iteration 22, loss = 0.81037208\n",
            "Iteration 23, loss = 0.84495985\n",
            "Iteration 24, loss = 0.81882040\n",
            "Iteration 25, loss = 0.78721987\n",
            "Iteration 26, loss = 0.79367162\n",
            "Iteration 27, loss = 0.81405110\n",
            "Iteration 28, loss = 0.77055420\n",
            "Iteration 29, loss = 0.76741401\n",
            "Iteration 30, loss = 0.74948000\n",
            "Iteration 31, loss = 0.75557410\n",
            "Iteration 32, loss = 0.74450326\n",
            "Iteration 33, loss = 0.71707938\n",
            "Iteration 34, loss = 0.71516549\n",
            "Iteration 35, loss = 0.70170086\n",
            "Iteration 36, loss = 0.69205440\n",
            "Iteration 37, loss = 0.68441384\n",
            "Iteration 38, loss = 0.68242864\n",
            "Iteration 39, loss = 0.67246879\n",
            "Iteration 40, loss = 0.67126485\n",
            "Iteration 41, loss = 0.66863332\n",
            "Iteration 42, loss = 0.65869002\n",
            "Iteration 43, loss = 0.67986954\n",
            "Iteration 44, loss = 0.66001661\n",
            "Iteration 45, loss = 0.64465281\n",
            "Iteration 46, loss = 0.62836423\n",
            "Iteration 47, loss = 0.64345572\n",
            "Iteration 48, loss = 0.67189819\n",
            "Iteration 49, loss = 0.67150539\n",
            "Iteration 50, loss = 0.62222616\n",
            "Iteration 51, loss = 0.60412504\n",
            "Iteration 52, loss = 0.61023913\n",
            "Iteration 53, loss = 0.59503019\n",
            "Iteration 54, loss = 0.64642836\n",
            "Iteration 55, loss = 0.65895753\n",
            "Iteration 56, loss = 0.61067035\n",
            "Iteration 57, loss = 0.63301351\n",
            "Iteration 58, loss = 0.59667720\n",
            "Iteration 59, loss = 0.57753277\n",
            "Iteration 60, loss = 0.56340867\n",
            "Iteration 61, loss = 0.57201392\n",
            "Iteration 62, loss = 0.56866511\n",
            "Iteration 63, loss = 0.56598655\n",
            "Iteration 64, loss = 0.56223961\n",
            "Iteration 65, loss = 0.56851225\n",
            "Iteration 66, loss = 0.59805657\n",
            "Iteration 67, loss = 0.56465219\n",
            "Iteration 68, loss = 0.59383263\n",
            "Iteration 69, loss = 0.56662641\n",
            "Iteration 70, loss = 0.53980471\n",
            "Iteration 71, loss = 0.54768730\n",
            "Iteration 72, loss = 0.57516693\n",
            "Iteration 73, loss = 0.55391418\n",
            "Iteration 74, loss = 0.54014101\n",
            "Iteration 75, loss = 0.54962195\n",
            "Iteration 76, loss = 0.56639654\n",
            "Iteration 77, loss = 0.58332381\n",
            "Iteration 78, loss = 0.58781068\n",
            "Iteration 79, loss = 0.52281071\n",
            "Iteration 80, loss = 0.55628591\n",
            "Iteration 81, loss = 0.54436518\n",
            "Iteration 82, loss = 0.52037007\n",
            "Iteration 83, loss = 0.51634868\n",
            "Iteration 84, loss = 0.49503868\n",
            "Iteration 85, loss = 0.50068523\n",
            "Iteration 86, loss = 0.49875100\n",
            "Iteration 87, loss = 0.48141517\n",
            "Iteration 88, loss = 0.46893534\n",
            "Iteration 89, loss = 0.48507519\n",
            "Iteration 90, loss = 0.47911816\n",
            "Iteration 91, loss = 0.48195956\n",
            "Iteration 92, loss = 0.46226136\n",
            "Iteration 93, loss = 0.48486790\n",
            "Iteration 94, loss = 0.45283790\n",
            "Iteration 95, loss = 0.45321171\n",
            "Iteration 96, loss = 0.48621455\n",
            "Iteration 97, loss = 0.47035390\n",
            "Iteration 98, loss = 0.44397546\n",
            "Iteration 99, loss = 0.46434973\n",
            "Iteration 100, loss = 0.49257924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31033709\n",
            "Iteration 2, loss = 1.21841711\n",
            "Iteration 3, loss = 1.18654310\n",
            "Iteration 4, loss = 1.16464005\n",
            "Iteration 5, loss = 1.14957080\n",
            "Iteration 6, loss = 1.10905943\n",
            "Iteration 7, loss = 1.07300983\n",
            "Iteration 8, loss = 1.06300510\n",
            "Iteration 9, loss = 1.02169270\n",
            "Iteration 10, loss = 1.00166898\n",
            "Iteration 11, loss = 1.02066253\n",
            "Iteration 12, loss = 1.04082308\n",
            "Iteration 13, loss = 0.98657417\n",
            "Iteration 14, loss = 0.93550360\n",
            "Iteration 15, loss = 0.91657453\n",
            "Iteration 16, loss = 0.89162368\n",
            "Iteration 17, loss = 0.88092746\n",
            "Iteration 18, loss = 0.89218346\n",
            "Iteration 19, loss = 0.86640793\n",
            "Iteration 20, loss = 0.85531174\n",
            "Iteration 21, loss = 0.85069969\n",
            "Iteration 22, loss = 0.80856805\n",
            "Iteration 23, loss = 0.82185659\n",
            "Iteration 24, loss = 0.80896669\n",
            "Iteration 25, loss = 0.78299644\n",
            "Iteration 26, loss = 0.79515583\n",
            "Iteration 27, loss = 0.83734973\n",
            "Iteration 28, loss = 0.78580665\n",
            "Iteration 29, loss = 0.77771539\n",
            "Iteration 30, loss = 0.76742657\n",
            "Iteration 31, loss = 0.77910158\n",
            "Iteration 32, loss = 0.75815851\n",
            "Iteration 33, loss = 0.72467748\n",
            "Iteration 34, loss = 0.70770227\n",
            "Iteration 35, loss = 0.70556226\n",
            "Iteration 36, loss = 0.70270892\n",
            "Iteration 37, loss = 0.68693080\n",
            "Iteration 38, loss = 0.69695498\n",
            "Iteration 39, loss = 0.68911856\n",
            "Iteration 40, loss = 0.68218905\n",
            "Iteration 41, loss = 0.68560691\n",
            "Iteration 42, loss = 0.67316999\n",
            "Iteration 43, loss = 0.70312783\n",
            "Iteration 44, loss = 0.68951461\n",
            "Iteration 45, loss = 0.66652557\n",
            "Iteration 46, loss = 0.64583379\n",
            "Iteration 47, loss = 0.66777166\n",
            "Iteration 48, loss = 0.66188390\n",
            "Iteration 49, loss = 0.67992430\n",
            "Iteration 50, loss = 0.65309714\n",
            "Iteration 51, loss = 0.63480622\n",
            "Iteration 52, loss = 0.62889505\n",
            "Iteration 53, loss = 0.60604387\n",
            "Iteration 54, loss = 0.61060138\n",
            "Iteration 55, loss = 0.61435169\n",
            "Iteration 56, loss = 0.59930059\n",
            "Iteration 57, loss = 0.62251530\n",
            "Iteration 58, loss = 0.58140129\n",
            "Iteration 59, loss = 0.58055130\n",
            "Iteration 60, loss = 0.56978876\n",
            "Iteration 61, loss = 0.56878216\n",
            "Iteration 62, loss = 0.56852430\n",
            "Iteration 63, loss = 0.57842204\n",
            "Iteration 64, loss = 0.57098879\n",
            "Iteration 65, loss = 0.58442194\n",
            "Iteration 66, loss = 0.59759127\n",
            "Iteration 67, loss = 0.60916026\n",
            "Iteration 68, loss = 0.61209584\n",
            "Iteration 69, loss = 0.59451493\n",
            "Iteration 70, loss = 0.55577120\n",
            "Iteration 71, loss = 0.55558971\n",
            "Iteration 72, loss = 0.54216354\n",
            "Iteration 73, loss = 0.52797568\n",
            "Iteration 74, loss = 0.52537776\n",
            "Iteration 75, loss = 0.54288082\n",
            "Iteration 76, loss = 0.57635489\n",
            "Iteration 77, loss = 0.57329680\n",
            "Iteration 78, loss = 0.56197416\n",
            "Iteration 79, loss = 0.52192842\n",
            "Iteration 80, loss = 0.56240491\n",
            "Iteration 81, loss = 0.49928736\n",
            "Iteration 82, loss = 0.52711440\n",
            "Iteration 83, loss = 0.50269039\n",
            "Iteration 84, loss = 0.47907421\n",
            "Iteration 85, loss = 0.47198212\n",
            "Iteration 86, loss = 0.48103554\n",
            "Iteration 87, loss = 0.46815594\n",
            "Iteration 88, loss = 0.44906362\n",
            "Iteration 89, loss = 0.46858365\n",
            "Iteration 90, loss = 0.46085697\n",
            "Iteration 91, loss = 0.45928102\n",
            "Iteration 92, loss = 0.43570713\n",
            "Iteration 93, loss = 0.47405817\n",
            "Iteration 94, loss = 0.42399911\n",
            "Iteration 95, loss = 0.42127376\n",
            "Iteration 96, loss = 0.47954887\n",
            "Iteration 97, loss = 0.47705920\n",
            "Iteration 98, loss = 0.44219239\n",
            "Iteration 99, loss = 0.44330277\n",
            "Iteration 100, loss = 0.41967475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31090174\n",
            "Iteration 2, loss = 1.22050074\n",
            "Iteration 3, loss = 1.18593445\n",
            "Iteration 4, loss = 1.15513058\n",
            "Iteration 5, loss = 1.14324930\n",
            "Iteration 6, loss = 1.09869759\n",
            "Iteration 7, loss = 1.07372463\n",
            "Iteration 8, loss = 1.05256377\n",
            "Iteration 9, loss = 1.02279844\n",
            "Iteration 10, loss = 1.04942352\n",
            "Iteration 11, loss = 1.02632920\n",
            "Iteration 12, loss = 1.03888204\n",
            "Iteration 13, loss = 1.01576141\n",
            "Iteration 14, loss = 0.95932895\n",
            "Iteration 15, loss = 0.96202119\n",
            "Iteration 16, loss = 0.93366046\n",
            "Iteration 17, loss = 0.92694273\n",
            "Iteration 18, loss = 0.92414407\n",
            "Iteration 19, loss = 0.88651887\n",
            "Iteration 20, loss = 0.85983311\n",
            "Iteration 21, loss = 0.86864879\n",
            "Iteration 22, loss = 0.83297504\n",
            "Iteration 23, loss = 0.83920781\n",
            "Iteration 24, loss = 0.82475615\n",
            "Iteration 25, loss = 0.81613324\n",
            "Iteration 26, loss = 0.82194111\n",
            "Iteration 27, loss = 0.82813957\n",
            "Iteration 28, loss = 0.80690260\n",
            "Iteration 29, loss = 0.79038298\n",
            "Iteration 30, loss = 0.77100866\n",
            "Iteration 31, loss = 0.79564003\n",
            "Iteration 32, loss = 0.75075873\n",
            "Iteration 33, loss = 0.75346899\n",
            "Iteration 34, loss = 0.72382835\n",
            "Iteration 35, loss = 0.71357188\n",
            "Iteration 36, loss = 0.71413885\n",
            "Iteration 37, loss = 0.70557251\n",
            "Iteration 38, loss = 0.73835235\n",
            "Iteration 39, loss = 0.73130541\n",
            "Iteration 40, loss = 0.68577452\n",
            "Iteration 41, loss = 0.68140506\n",
            "Iteration 42, loss = 0.69994821\n",
            "Iteration 43, loss = 0.70521851\n",
            "Iteration 44, loss = 0.72923215\n",
            "Iteration 45, loss = 0.67555903\n",
            "Iteration 46, loss = 0.66754809\n",
            "Iteration 47, loss = 0.69974485\n",
            "Iteration 48, loss = 0.68077470\n",
            "Iteration 49, loss = 0.70162471\n",
            "Iteration 50, loss = 0.64775037\n",
            "Iteration 51, loss = 0.66068185\n",
            "Iteration 52, loss = 0.66610107\n",
            "Iteration 53, loss = 0.65155238\n",
            "Iteration 54, loss = 0.65176910\n",
            "Iteration 55, loss = 0.69299770\n",
            "Iteration 56, loss = 0.63310755\n",
            "Iteration 57, loss = 0.61955438\n",
            "Iteration 58, loss = 0.61777933\n",
            "Iteration 59, loss = 0.63514701\n",
            "Iteration 60, loss = 0.62567287\n",
            "Iteration 61, loss = 0.62193738\n",
            "Iteration 62, loss = 0.62483678\n",
            "Iteration 63, loss = 0.62258006\n",
            "Iteration 64, loss = 0.64390666\n",
            "Iteration 65, loss = 0.62302239\n",
            "Iteration 66, loss = 0.60092103\n",
            "Iteration 67, loss = 0.57505915\n",
            "Iteration 68, loss = 0.58205226\n",
            "Iteration 69, loss = 0.58176294\n",
            "Iteration 70, loss = 0.60068923\n",
            "Iteration 71, loss = 0.60706069\n",
            "Iteration 72, loss = 0.59924950\n",
            "Iteration 73, loss = 0.59469665\n",
            "Iteration 74, loss = 0.56620276\n",
            "Iteration 75, loss = 0.57614422\n",
            "Iteration 76, loss = 0.60540056\n",
            "Iteration 77, loss = 0.61030539\n",
            "Iteration 78, loss = 0.59211186\n",
            "Iteration 79, loss = 0.55364192\n",
            "Iteration 80, loss = 0.59739920\n",
            "Iteration 81, loss = 0.54251056\n",
            "Iteration 82, loss = 0.52983721\n",
            "Iteration 83, loss = 0.57224601\n",
            "Iteration 84, loss = 0.52857552\n",
            "Iteration 85, loss = 0.52825794\n",
            "Iteration 86, loss = 0.54063270\n",
            "Iteration 87, loss = 0.56448817\n",
            "Iteration 88, loss = 0.53003081\n",
            "Iteration 89, loss = 0.57786271\n",
            "Iteration 90, loss = 0.53845858\n",
            "Iteration 91, loss = 0.52615196\n",
            "Iteration 92, loss = 0.49861455\n",
            "Iteration 93, loss = 0.55983443\n",
            "Iteration 94, loss = 0.54505539\n",
            "Iteration 95, loss = 0.53555576\n",
            "Iteration 96, loss = 0.55030494\n",
            "Iteration 97, loss = 0.55184207\n",
            "Iteration 98, loss = 0.51188818\n",
            "Iteration 99, loss = 0.49906768\n",
            "Iteration 100, loss = 0.49084700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31414955\n",
            "Iteration 2, loss = 1.23218903\n",
            "Iteration 3, loss = 1.19519630\n",
            "Iteration 4, loss = 1.16542177\n",
            "Iteration 5, loss = 1.14263327\n",
            "Iteration 6, loss = 1.10089302\n",
            "Iteration 7, loss = 1.07890166\n",
            "Iteration 8, loss = 1.06482431\n",
            "Iteration 9, loss = 1.02965189\n",
            "Iteration 10, loss = 1.01589297\n",
            "Iteration 11, loss = 0.99753007\n",
            "Iteration 12, loss = 1.01827514\n",
            "Iteration 13, loss = 1.01615303\n",
            "Iteration 14, loss = 0.94537166\n",
            "Iteration 15, loss = 0.94330972\n",
            "Iteration 16, loss = 0.92785941\n",
            "Iteration 17, loss = 0.92906798\n",
            "Iteration 18, loss = 0.92080269\n",
            "Iteration 19, loss = 0.92422171\n",
            "Iteration 20, loss = 0.88669351\n",
            "Iteration 21, loss = 0.91018697\n",
            "Iteration 22, loss = 0.85314966\n",
            "Iteration 23, loss = 0.84674870\n",
            "Iteration 24, loss = 0.83983064\n",
            "Iteration 25, loss = 0.82987578\n",
            "Iteration 26, loss = 0.81379594\n",
            "Iteration 27, loss = 0.83154809\n",
            "Iteration 28, loss = 0.80965893\n",
            "Iteration 29, loss = 0.81150641\n",
            "Iteration 30, loss = 0.81608338\n",
            "Iteration 31, loss = 0.85685356\n",
            "Iteration 32, loss = 0.78551601\n",
            "Iteration 33, loss = 0.75405891\n",
            "Iteration 34, loss = 0.74588208\n",
            "Iteration 35, loss = 0.73132268\n",
            "Iteration 36, loss = 0.74888221\n",
            "Iteration 37, loss = 0.73286401\n",
            "Iteration 38, loss = 0.75170907\n",
            "Iteration 39, loss = 0.72493216\n",
            "Iteration 40, loss = 0.70769904\n",
            "Iteration 41, loss = 0.71219589\n",
            "Iteration 42, loss = 0.71794357\n",
            "Iteration 43, loss = 0.72460779\n",
            "Iteration 44, loss = 0.72080763\n",
            "Iteration 45, loss = 0.69658203\n",
            "Iteration 46, loss = 0.68771474\n",
            "Iteration 47, loss = 0.67573840\n",
            "Iteration 48, loss = 0.67105251\n",
            "Iteration 49, loss = 0.66618135\n",
            "Iteration 50, loss = 0.65319829\n",
            "Iteration 51, loss = 0.66309949\n",
            "Iteration 52, loss = 0.64762017\n",
            "Iteration 53, loss = 0.64826997\n",
            "Iteration 54, loss = 0.64147947\n",
            "Iteration 55, loss = 0.68344316\n",
            "Iteration 56, loss = 0.63960804\n",
            "Iteration 57, loss = 0.63761591\n",
            "Iteration 58, loss = 0.61712745\n",
            "Iteration 59, loss = 0.62023682\n",
            "Iteration 60, loss = 0.60519574\n",
            "Iteration 61, loss = 0.60517252\n",
            "Iteration 62, loss = 0.59938599\n",
            "Iteration 63, loss = 0.60856955\n",
            "Iteration 64, loss = 0.69733008\n",
            "Iteration 65, loss = 0.67085795\n",
            "Iteration 66, loss = 0.64043073\n",
            "Iteration 67, loss = 0.59893026\n",
            "Iteration 68, loss = 0.59614265\n",
            "Iteration 69, loss = 0.59741885\n",
            "Iteration 70, loss = 0.57772274\n",
            "Iteration 71, loss = 0.59035906\n",
            "Iteration 72, loss = 0.58309201\n",
            "Iteration 73, loss = 0.58499868\n",
            "Iteration 74, loss = 0.57393312\n",
            "Iteration 75, loss = 0.58646772\n",
            "Iteration 76, loss = 0.60368831\n",
            "Iteration 77, loss = 0.60356744\n",
            "Iteration 78, loss = 0.58019640\n",
            "Iteration 79, loss = 0.56518533\n",
            "Iteration 80, loss = 0.60156355\n",
            "Iteration 81, loss = 0.57221350\n",
            "Iteration 82, loss = 0.53890377\n",
            "Iteration 83, loss = 0.56438721\n",
            "Iteration 84, loss = 0.53629972\n",
            "Iteration 85, loss = 0.54407835\n",
            "Iteration 86, loss = 0.54863535\n",
            "Iteration 87, loss = 0.52706031\n",
            "Iteration 88, loss = 0.50234358\n",
            "Iteration 89, loss = 0.52808084\n",
            "Iteration 90, loss = 0.52237951\n",
            "Iteration 91, loss = 0.51076508\n",
            "Iteration 92, loss = 0.51942198\n",
            "Iteration 93, loss = 0.57408245\n",
            "Iteration 94, loss = 0.53605463\n",
            "Iteration 95, loss = 0.53088626\n",
            "Iteration 96, loss = 0.56795680\n",
            "Iteration 97, loss = 0.55399139\n",
            "Iteration 98, loss = 0.56819613\n",
            "Iteration 99, loss = 0.53948638\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29965288\n",
            "Iteration 2, loss = 1.22257033\n",
            "Iteration 3, loss = 1.18357068\n",
            "Iteration 4, loss = 1.16562728\n",
            "Iteration 5, loss = 1.13687454\n",
            "Iteration 6, loss = 1.11328792\n",
            "Iteration 7, loss = 1.08279814\n",
            "Iteration 8, loss = 1.06485335\n",
            "Iteration 9, loss = 1.02455515\n",
            "Iteration 10, loss = 1.00474598\n",
            "Iteration 11, loss = 0.98370654\n",
            "Iteration 12, loss = 0.97580406\n",
            "Iteration 13, loss = 0.94845071\n",
            "Iteration 14, loss = 0.95255875\n",
            "Iteration 15, loss = 0.90500746\n",
            "Iteration 16, loss = 0.87950932\n",
            "Iteration 17, loss = 0.87151241\n",
            "Iteration 18, loss = 0.90740690\n",
            "Iteration 19, loss = 0.87222147\n",
            "Iteration 20, loss = 0.84104377\n",
            "Iteration 21, loss = 0.82072100\n",
            "Iteration 22, loss = 0.80302206\n",
            "Iteration 23, loss = 0.79486247\n",
            "Iteration 24, loss = 0.79450940\n",
            "Iteration 25, loss = 0.78264583\n",
            "Iteration 26, loss = 0.77802929\n",
            "Iteration 27, loss = 0.77384714\n",
            "Iteration 28, loss = 0.78535077\n",
            "Iteration 29, loss = 0.77394651\n",
            "Iteration 30, loss = 0.74229699\n",
            "Iteration 31, loss = 0.75038364\n",
            "Iteration 32, loss = 0.73080706\n",
            "Iteration 33, loss = 0.73181340\n",
            "Iteration 34, loss = 0.72894521\n",
            "Iteration 35, loss = 0.71475665\n",
            "Iteration 36, loss = 0.72197578\n",
            "Iteration 37, loss = 0.70139786\n",
            "Iteration 38, loss = 0.75183790\n",
            "Iteration 39, loss = 0.71287026\n",
            "Iteration 40, loss = 0.71866438\n",
            "Iteration 41, loss = 0.70095854\n",
            "Iteration 42, loss = 0.69694811\n",
            "Iteration 43, loss = 0.71307770\n",
            "Iteration 44, loss = 0.72053860\n",
            "Iteration 45, loss = 0.69485548\n",
            "Iteration 46, loss = 0.69662273\n",
            "Iteration 47, loss = 0.69955774\n",
            "Iteration 48, loss = 0.66298725\n",
            "Iteration 49, loss = 0.65155421\n",
            "Iteration 50, loss = 0.63756640\n",
            "Iteration 51, loss = 0.64800933\n",
            "Iteration 52, loss = 0.61721669\n",
            "Iteration 53, loss = 0.61100735\n",
            "Iteration 54, loss = 0.60897479\n",
            "Iteration 55, loss = 0.62229336\n",
            "Iteration 56, loss = 0.60768208\n",
            "Iteration 57, loss = 0.61479384\n",
            "Iteration 58, loss = 0.60998940\n",
            "Iteration 59, loss = 0.58743559\n",
            "Iteration 60, loss = 0.59086532\n",
            "Iteration 61, loss = 0.59380745\n",
            "Iteration 62, loss = 0.59105269\n",
            "Iteration 63, loss = 0.58958416\n",
            "Iteration 64, loss = 0.63632177\n",
            "Iteration 65, loss = 0.64481893\n",
            "Iteration 66, loss = 0.59169538\n",
            "Iteration 67, loss = 0.56427657\n",
            "Iteration 68, loss = 0.56762223\n",
            "Iteration 69, loss = 0.54334641\n",
            "Iteration 70, loss = 0.53730340\n",
            "Iteration 71, loss = 0.54788396\n",
            "Iteration 72, loss = 0.54937512\n",
            "Iteration 73, loss = 0.54619038\n",
            "Iteration 74, loss = 0.53345328\n",
            "Iteration 75, loss = 0.54956448\n",
            "Iteration 76, loss = 0.57479970\n",
            "Iteration 77, loss = 0.58387593\n",
            "Iteration 78, loss = 0.56754312\n",
            "Iteration 79, loss = 0.54181643\n",
            "Iteration 80, loss = 0.54479689\n",
            "Iteration 81, loss = 0.60440295\n",
            "Iteration 82, loss = 0.56603510\n",
            "Iteration 83, loss = 0.54221418\n",
            "Iteration 84, loss = 0.49578002\n",
            "Iteration 85, loss = 0.53855379\n",
            "Iteration 86, loss = 0.54158419\n",
            "Iteration 87, loss = 0.48473919\n",
            "Iteration 88, loss = 0.49192212\n",
            "Iteration 89, loss = 0.47036234\n",
            "Iteration 90, loss = 0.47110891\n",
            "Iteration 91, loss = 0.46900687\n",
            "Iteration 92, loss = 0.46527700\n",
            "Iteration 93, loss = 0.49751318\n",
            "Iteration 94, loss = 0.47784664\n",
            "Iteration 95, loss = 0.46614807\n",
            "Iteration 96, loss = 0.50260996\n",
            "Iteration 97, loss = 0.49759223\n",
            "Iteration 98, loss = 0.49111187\n",
            "Iteration 99, loss = 0.48763452\n",
            "Iteration 100, loss = 0.44634181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29383087\n",
            "Iteration 2, loss = 1.20765186\n",
            "Iteration 3, loss = 1.17155709\n",
            "Iteration 4, loss = 1.14907238\n",
            "Iteration 5, loss = 1.11615296\n",
            "Iteration 6, loss = 1.10244133\n",
            "Iteration 7, loss = 1.08046655\n",
            "Iteration 8, loss = 1.05583820\n",
            "Iteration 9, loss = 1.00346372\n",
            "Iteration 10, loss = 0.98106818\n",
            "Iteration 11, loss = 0.97565323\n",
            "Iteration 12, loss = 0.95788992\n",
            "Iteration 13, loss = 0.94523446\n",
            "Iteration 14, loss = 0.91293824\n",
            "Iteration 15, loss = 0.88054093\n",
            "Iteration 16, loss = 0.89788767\n",
            "Iteration 17, loss = 0.89129047\n",
            "Iteration 18, loss = 0.88068979\n",
            "Iteration 19, loss = 0.85682112\n",
            "Iteration 20, loss = 0.85353148\n",
            "Iteration 21, loss = 0.82835755\n",
            "Iteration 22, loss = 0.80466094\n",
            "Iteration 23, loss = 0.81267803\n",
            "Iteration 24, loss = 0.83286682\n",
            "Iteration 25, loss = 0.80896929\n",
            "Iteration 26, loss = 0.79824870\n",
            "Iteration 27, loss = 0.77596295\n",
            "Iteration 28, loss = 0.77649596\n",
            "Iteration 29, loss = 0.76322555\n",
            "Iteration 30, loss = 0.74498890\n",
            "Iteration 31, loss = 0.75592482\n",
            "Iteration 32, loss = 0.71970356\n",
            "Iteration 33, loss = 0.70687451\n",
            "Iteration 34, loss = 0.70149266\n",
            "Iteration 35, loss = 0.70597842\n",
            "Iteration 36, loss = 0.70053241\n",
            "Iteration 37, loss = 0.68437998\n",
            "Iteration 38, loss = 0.69260528\n",
            "Iteration 39, loss = 0.68261834\n",
            "Iteration 40, loss = 0.68064893\n",
            "Iteration 41, loss = 0.67630762\n",
            "Iteration 42, loss = 0.68594373\n",
            "Iteration 43, loss = 0.69862872\n",
            "Iteration 44, loss = 0.67093525\n",
            "Iteration 45, loss = 0.67007714\n",
            "Iteration 46, loss = 0.66353718\n",
            "Iteration 47, loss = 0.66640297\n",
            "Iteration 48, loss = 0.64110005\n",
            "Iteration 49, loss = 0.63477667\n",
            "Iteration 50, loss = 0.62769186\n",
            "Iteration 51, loss = 0.61958503\n",
            "Iteration 52, loss = 0.60779247\n",
            "Iteration 53, loss = 0.60390408\n",
            "Iteration 54, loss = 0.60657896\n",
            "Iteration 55, loss = 0.60678092\n",
            "Iteration 56, loss = 0.60632527\n",
            "Iteration 57, loss = 0.61018001\n",
            "Iteration 58, loss = 0.60244109\n",
            "Iteration 59, loss = 0.59111961\n",
            "Iteration 60, loss = 0.57684603\n",
            "Iteration 61, loss = 0.62184231\n",
            "Iteration 62, loss = 0.58428934\n",
            "Iteration 63, loss = 0.59373838\n",
            "Iteration 64, loss = 0.61496267\n",
            "Iteration 65, loss = 0.63698974\n",
            "Iteration 66, loss = 0.61939949\n",
            "Iteration 67, loss = 0.58133115\n",
            "Iteration 68, loss = 0.56383572\n",
            "Iteration 69, loss = 0.54265373\n",
            "Iteration 70, loss = 0.53845838\n",
            "Iteration 71, loss = 0.54883944\n",
            "Iteration 72, loss = 0.56149831\n",
            "Iteration 73, loss = 0.54829698\n",
            "Iteration 74, loss = 0.54912386\n",
            "Iteration 75, loss = 0.53385075\n",
            "Iteration 76, loss = 0.54803681\n",
            "Iteration 77, loss = 0.55298439\n",
            "Iteration 78, loss = 0.54746373\n",
            "Iteration 79, loss = 0.54902441\n",
            "Iteration 80, loss = 0.53263799\n",
            "Iteration 81, loss = 0.60058645\n",
            "Iteration 82, loss = 0.53290768\n",
            "Iteration 83, loss = 0.52321949\n",
            "Iteration 84, loss = 0.50389307\n",
            "Iteration 85, loss = 0.57955284\n",
            "Iteration 86, loss = 0.58468548\n",
            "Iteration 87, loss = 0.51569502\n",
            "Iteration 88, loss = 0.53236906\n",
            "Iteration 89, loss = 0.50211805\n",
            "Iteration 90, loss = 0.52025494\n",
            "Iteration 91, loss = 0.50259763\n",
            "Iteration 92, loss = 0.50531556\n",
            "Iteration 93, loss = 0.50386503\n",
            "Iteration 94, loss = 0.51084168\n",
            "Iteration 95, loss = 0.49590526\n",
            "Iteration 96, loss = 0.51398743\n",
            "Iteration 97, loss = 0.52621971\n",
            "Iteration 98, loss = 0.49157852\n",
            "Iteration 99, loss = 0.50790408\n",
            "Iteration 100, loss = 0.46014184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29978360\n",
            "Iteration 2, loss = 1.21389810\n",
            "Iteration 3, loss = 1.18263844\n",
            "Iteration 4, loss = 1.17296748\n",
            "Iteration 5, loss = 1.14403377\n",
            "Iteration 6, loss = 1.11230730\n",
            "Iteration 7, loss = 1.08476432\n",
            "Iteration 8, loss = 1.07270119\n",
            "Iteration 9, loss = 1.03260472\n",
            "Iteration 10, loss = 1.00533327\n",
            "Iteration 11, loss = 1.00447004\n",
            "Iteration 12, loss = 0.99485442\n",
            "Iteration 13, loss = 0.98136311\n",
            "Iteration 14, loss = 0.95466826\n",
            "Iteration 15, loss = 0.93028431\n",
            "Iteration 16, loss = 0.95646856\n",
            "Iteration 17, loss = 0.93370482\n",
            "Iteration 18, loss = 0.91816584\n",
            "Iteration 19, loss = 0.87130771\n",
            "Iteration 20, loss = 0.86182785\n",
            "Iteration 21, loss = 0.88225638\n",
            "Iteration 22, loss = 0.83400922\n",
            "Iteration 23, loss = 0.84309692\n",
            "Iteration 24, loss = 0.83888232\n",
            "Iteration 25, loss = 0.79728237\n",
            "Iteration 26, loss = 0.79228902\n",
            "Iteration 27, loss = 0.79964245\n",
            "Iteration 28, loss = 0.78489935\n",
            "Iteration 29, loss = 0.77532099\n",
            "Iteration 30, loss = 0.75846776\n",
            "Iteration 31, loss = 0.76694406\n",
            "Iteration 32, loss = 0.75036734\n",
            "Iteration 33, loss = 0.73316588\n",
            "Iteration 34, loss = 0.72978158\n",
            "Iteration 35, loss = 0.71928953\n",
            "Iteration 36, loss = 0.71634271\n",
            "Iteration 37, loss = 0.72110328\n",
            "Iteration 38, loss = 0.71963649\n",
            "Iteration 39, loss = 0.71736155\n",
            "Iteration 40, loss = 0.72220564\n",
            "Iteration 41, loss = 0.71985563\n",
            "Iteration 42, loss = 0.70878403\n",
            "Iteration 43, loss = 0.72857960\n",
            "Iteration 44, loss = 0.69041866\n",
            "Iteration 45, loss = 0.68134515\n",
            "Iteration 46, loss = 0.67957013\n",
            "Iteration 47, loss = 0.68953139\n",
            "Iteration 48, loss = 0.66137553\n",
            "Iteration 49, loss = 0.66184317\n",
            "Iteration 50, loss = 0.64571067\n",
            "Iteration 51, loss = 0.62699674\n",
            "Iteration 52, loss = 0.64628572\n",
            "Iteration 53, loss = 0.62453847\n",
            "Iteration 54, loss = 0.62147379\n",
            "Iteration 55, loss = 0.62261512\n",
            "Iteration 56, loss = 0.63617060\n",
            "Iteration 57, loss = 0.63717988\n",
            "Iteration 58, loss = 0.63247549\n",
            "Iteration 59, loss = 0.61217732\n",
            "Iteration 60, loss = 0.60778787\n",
            "Iteration 61, loss = 0.59902124\n",
            "Iteration 62, loss = 0.58871159\n",
            "Iteration 63, loss = 0.59218883\n",
            "Iteration 64, loss = 0.61401709\n",
            "Iteration 65, loss = 0.59833770\n",
            "Iteration 66, loss = 0.62611629\n",
            "Iteration 67, loss = 0.58519400\n",
            "Iteration 68, loss = 0.59310897\n",
            "Iteration 69, loss = 0.55551939\n",
            "Iteration 70, loss = 0.55026228\n",
            "Iteration 71, loss = 0.54244177\n",
            "Iteration 72, loss = 0.55420136\n",
            "Iteration 73, loss = 0.58166003\n",
            "Iteration 74, loss = 0.57828593\n",
            "Iteration 75, loss = 0.54578795\n",
            "Iteration 76, loss = 0.55661585\n",
            "Iteration 77, loss = 0.54002262\n",
            "Iteration 78, loss = 0.59692431\n",
            "Iteration 79, loss = 0.57687394\n",
            "Iteration 80, loss = 0.59066315\n",
            "Iteration 81, loss = 0.54349093\n",
            "Iteration 82, loss = 0.54673284\n",
            "Iteration 83, loss = 0.53658823\n",
            "Iteration 84, loss = 0.53419831\n",
            "Iteration 85, loss = 0.56320492\n",
            "Iteration 86, loss = 0.55336250\n",
            "Iteration 87, loss = 0.52066082\n",
            "Iteration 88, loss = 0.52572908\n",
            "Iteration 89, loss = 0.48646552\n",
            "Iteration 90, loss = 0.51761970\n",
            "Iteration 91, loss = 0.51911056\n",
            "Iteration 92, loss = 0.51703410\n",
            "Iteration 93, loss = 0.54269841\n",
            "Iteration 94, loss = 0.55079072\n",
            "Iteration 95, loss = 0.52833403\n",
            "Iteration 96, loss = 0.56004762\n",
            "Iteration 97, loss = 0.55074747\n",
            "Iteration 98, loss = 0.57041508\n",
            "Iteration 99, loss = 0.56458212\n",
            "Iteration 100, loss = 0.48953179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29817937\n",
            "Iteration 2, loss = 1.22384350\n",
            "Iteration 3, loss = 1.19873918\n",
            "Iteration 4, loss = 1.16369216\n",
            "Iteration 5, loss = 1.14756470\n",
            "Iteration 6, loss = 1.13217963\n",
            "Iteration 7, loss = 1.10424714\n",
            "Iteration 8, loss = 1.07865010\n",
            "Iteration 9, loss = 1.05304916\n",
            "Iteration 10, loss = 1.01925950\n",
            "Iteration 11, loss = 0.99301303\n",
            "Iteration 12, loss = 0.98193751\n",
            "Iteration 13, loss = 0.95083837\n",
            "Iteration 14, loss = 0.96398665\n",
            "Iteration 15, loss = 0.94406077\n",
            "Iteration 16, loss = 0.90829091\n",
            "Iteration 17, loss = 0.90083236\n",
            "Iteration 18, loss = 0.89524226\n",
            "Iteration 19, loss = 0.89163623\n",
            "Iteration 20, loss = 0.85869807\n",
            "Iteration 21, loss = 0.83816690\n",
            "Iteration 22, loss = 0.82635876\n",
            "Iteration 23, loss = 0.82718664\n",
            "Iteration 24, loss = 0.80570997\n",
            "Iteration 25, loss = 0.79211098\n",
            "Iteration 26, loss = 0.77493076\n",
            "Iteration 27, loss = 0.76530335\n",
            "Iteration 28, loss = 0.77083215\n",
            "Iteration 29, loss = 0.77018648\n",
            "Iteration 30, loss = 0.78097311\n",
            "Iteration 31, loss = 0.76923077\n",
            "Iteration 32, loss = 0.76643850\n",
            "Iteration 33, loss = 0.75224470\n",
            "Iteration 34, loss = 0.74137698\n",
            "Iteration 35, loss = 0.75223751\n",
            "Iteration 36, loss = 0.76835637\n",
            "Iteration 37, loss = 0.73756179\n",
            "Iteration 38, loss = 0.76572272\n",
            "Iteration 39, loss = 0.76411379\n",
            "Iteration 40, loss = 0.73581784\n",
            "Iteration 41, loss = 0.72906709\n",
            "Iteration 42, loss = 0.72312999\n",
            "Iteration 43, loss = 0.68575018\n",
            "Iteration 44, loss = 0.67975926\n",
            "Iteration 45, loss = 0.72053339\n",
            "Iteration 46, loss = 0.68775953\n",
            "Iteration 47, loss = 0.66439401\n",
            "Iteration 48, loss = 0.66542597\n",
            "Iteration 49, loss = 0.65618612\n",
            "Iteration 50, loss = 0.66720785\n",
            "Iteration 51, loss = 0.67162736\n",
            "Iteration 52, loss = 0.72021169\n",
            "Iteration 53, loss = 0.69868772\n",
            "Iteration 54, loss = 0.71323030\n",
            "Iteration 55, loss = 0.71397387\n",
            "Iteration 56, loss = 0.67665238\n",
            "Iteration 57, loss = 0.70043184\n",
            "Iteration 58, loss = 0.69604318\n",
            "Iteration 59, loss = 0.69107960\n",
            "Iteration 60, loss = 0.65104398\n",
            "Iteration 61, loss = 0.64606073\n",
            "Iteration 62, loss = 0.62251243\n",
            "Iteration 63, loss = 0.62305155\n",
            "Iteration 64, loss = 0.62032750\n",
            "Iteration 65, loss = 0.63033128\n",
            "Iteration 66, loss = 0.64736852\n",
            "Iteration 67, loss = 0.61402820\n",
            "Iteration 68, loss = 0.59361871\n",
            "Iteration 69, loss = 0.60123173\n",
            "Iteration 70, loss = 0.60683295\n",
            "Iteration 71, loss = 0.61320262\n",
            "Iteration 72, loss = 0.58993835\n",
            "Iteration 73, loss = 0.59284061\n",
            "Iteration 74, loss = 0.58506305\n",
            "Iteration 75, loss = 0.57046491\n",
            "Iteration 76, loss = 0.57157166\n",
            "Iteration 77, loss = 0.59735769\n",
            "Iteration 78, loss = 0.56738777\n",
            "Iteration 79, loss = 0.57488040\n",
            "Iteration 80, loss = 0.58958938\n",
            "Iteration 81, loss = 0.59623979\n",
            "Iteration 82, loss = 0.57367024\n",
            "Iteration 83, loss = 0.55209843\n",
            "Iteration 84, loss = 0.53713732\n",
            "Iteration 85, loss = 0.53119372\n",
            "Iteration 86, loss = 0.54780280\n",
            "Iteration 87, loss = 0.55528275\n",
            "Iteration 88, loss = 0.56456969\n",
            "Iteration 89, loss = 0.61081951\n",
            "Iteration 90, loss = 0.58733546\n",
            "Iteration 91, loss = 0.54533614\n",
            "Iteration 92, loss = 0.51951852\n",
            "Iteration 93, loss = 0.52270906\n",
            "Iteration 94, loss = 0.49822792\n",
            "Iteration 95, loss = 0.51334183\n",
            "Iteration 96, loss = 0.51875018\n",
            "Iteration 97, loss = 0.52236433\n",
            "Iteration 98, loss = 0.53200709\n",
            "Iteration 99, loss = 0.54898648\n",
            "Iteration 100, loss = 0.53117264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30025749\n",
            "Iteration 2, loss = 1.21279247\n",
            "Iteration 3, loss = 1.18990392\n",
            "Iteration 4, loss = 1.14516402\n",
            "Iteration 5, loss = 1.12202020\n",
            "Iteration 6, loss = 1.10447080\n",
            "Iteration 7, loss = 1.09673636\n",
            "Iteration 8, loss = 1.09904270\n",
            "Iteration 9, loss = 1.08617271\n",
            "Iteration 10, loss = 1.02797585\n",
            "Iteration 11, loss = 1.00599239\n",
            "Iteration 12, loss = 0.98378928\n",
            "Iteration 13, loss = 0.96390702\n",
            "Iteration 14, loss = 0.98188777\n",
            "Iteration 15, loss = 0.94961249\n",
            "Iteration 16, loss = 0.93085825\n",
            "Iteration 17, loss = 0.92118911\n",
            "Iteration 18, loss = 0.90590495\n",
            "Iteration 19, loss = 0.91444207\n",
            "Iteration 20, loss = 0.88844077\n",
            "Iteration 21, loss = 0.86721830\n",
            "Iteration 22, loss = 0.84939392\n",
            "Iteration 23, loss = 0.82621046\n",
            "Iteration 24, loss = 0.81141656\n",
            "Iteration 25, loss = 0.80403347\n",
            "Iteration 26, loss = 0.78709230\n",
            "Iteration 27, loss = 0.77584402\n",
            "Iteration 28, loss = 0.77033139\n",
            "Iteration 29, loss = 0.76849943\n",
            "Iteration 30, loss = 0.80164063\n",
            "Iteration 31, loss = 0.77346062\n",
            "Iteration 32, loss = 0.77588679\n",
            "Iteration 33, loss = 0.75773675\n",
            "Iteration 34, loss = 0.73594010\n",
            "Iteration 35, loss = 0.72670753\n",
            "Iteration 36, loss = 0.73864672\n",
            "Iteration 37, loss = 0.73398562\n",
            "Iteration 38, loss = 0.73280599\n",
            "Iteration 39, loss = 0.72492260\n",
            "Iteration 40, loss = 0.71854732\n",
            "Iteration 41, loss = 0.72390902\n",
            "Iteration 42, loss = 0.72533926\n",
            "Iteration 43, loss = 0.71775430\n",
            "Iteration 44, loss = 0.68019892\n",
            "Iteration 45, loss = 0.69618079\n",
            "Iteration 46, loss = 0.68374352\n",
            "Iteration 47, loss = 0.69257555\n",
            "Iteration 48, loss = 0.67812408\n",
            "Iteration 49, loss = 0.65563105\n",
            "Iteration 50, loss = 0.67660215\n",
            "Iteration 51, loss = 0.65897401\n",
            "Iteration 52, loss = 0.68216701\n",
            "Iteration 53, loss = 0.68572054\n",
            "Iteration 54, loss = 0.68528620\n",
            "Iteration 55, loss = 0.68080970\n",
            "Iteration 56, loss = 0.66430036\n",
            "Iteration 57, loss = 0.68612961\n",
            "Iteration 58, loss = 0.65226767\n",
            "Iteration 59, loss = 0.65788385\n",
            "Iteration 60, loss = 0.65704790\n",
            "Iteration 61, loss = 0.64269009\n",
            "Iteration 62, loss = 0.64024193\n",
            "Iteration 63, loss = 0.62871361\n",
            "Iteration 64, loss = 0.61668364\n",
            "Iteration 65, loss = 0.64548357\n",
            "Iteration 66, loss = 0.69096567\n",
            "Iteration 67, loss = 0.61829824\n",
            "Iteration 68, loss = 0.60712432\n",
            "Iteration 69, loss = 0.61452073\n",
            "Iteration 70, loss = 0.59781841\n",
            "Iteration 71, loss = 0.63031330\n",
            "Iteration 72, loss = 0.61565690\n",
            "Iteration 73, loss = 0.61331365\n",
            "Iteration 74, loss = 0.62108052\n",
            "Iteration 75, loss = 0.61635211\n",
            "Iteration 76, loss = 0.59181717\n",
            "Iteration 77, loss = 0.57011671\n",
            "Iteration 78, loss = 0.56720870\n",
            "Iteration 79, loss = 0.58105579\n",
            "Iteration 80, loss = 0.59156326\n",
            "Iteration 81, loss = 0.59706202\n",
            "Iteration 82, loss = 0.58025650\n",
            "Iteration 83, loss = 0.57649866\n",
            "Iteration 84, loss = 0.55067801\n",
            "Iteration 85, loss = 0.54485963\n",
            "Iteration 86, loss = 0.55501454\n",
            "Iteration 87, loss = 0.57144360\n",
            "Iteration 88, loss = 0.56942455\n",
            "Iteration 89, loss = 0.60518967\n",
            "Iteration 90, loss = 0.55138395\n",
            "Iteration 91, loss = 0.53994714\n",
            "Iteration 92, loss = 0.53752086\n",
            "Iteration 93, loss = 0.58995759\n",
            "Iteration 94, loss = 0.56345702\n",
            "Iteration 95, loss = 0.58009737\n",
            "Iteration 96, loss = 0.57587686\n",
            "Iteration 97, loss = 0.54030744\n",
            "Iteration 98, loss = 0.56167163\n",
            "Iteration 99, loss = 0.56794131\n",
            "Iteration 100, loss = 0.57215010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30271081\n",
            "Iteration 2, loss = 1.19866347\n",
            "Iteration 3, loss = 1.17403890\n",
            "Iteration 4, loss = 1.13043906\n",
            "Iteration 5, loss = 1.10549055\n",
            "Iteration 6, loss = 1.09433625\n",
            "Iteration 7, loss = 1.07148819\n",
            "Iteration 8, loss = 1.06662018\n",
            "Iteration 9, loss = 1.05322784\n",
            "Iteration 10, loss = 1.00897642\n",
            "Iteration 11, loss = 1.00703127\n",
            "Iteration 12, loss = 0.97283313\n",
            "Iteration 13, loss = 0.95787298\n",
            "Iteration 14, loss = 0.94479357\n",
            "Iteration 15, loss = 0.93881037\n",
            "Iteration 16, loss = 0.91899946\n",
            "Iteration 17, loss = 0.91223321\n",
            "Iteration 18, loss = 0.91263561\n",
            "Iteration 19, loss = 0.89600194\n",
            "Iteration 20, loss = 0.86081516\n",
            "Iteration 21, loss = 0.84144908\n",
            "Iteration 22, loss = 0.83312350\n",
            "Iteration 23, loss = 0.81618456\n",
            "Iteration 24, loss = 0.80835627\n",
            "Iteration 25, loss = 0.79206359\n",
            "Iteration 26, loss = 0.78718713\n",
            "Iteration 27, loss = 0.77787550\n",
            "Iteration 28, loss = 0.75627628\n",
            "Iteration 29, loss = 0.75457770\n",
            "Iteration 30, loss = 0.77625170\n",
            "Iteration 31, loss = 0.73825140\n",
            "Iteration 32, loss = 0.74539047\n",
            "Iteration 33, loss = 0.73890779\n",
            "Iteration 34, loss = 0.75137451\n",
            "Iteration 35, loss = 0.74909842\n",
            "Iteration 36, loss = 0.77506263\n",
            "Iteration 37, loss = 0.79754143\n",
            "Iteration 38, loss = 0.70499260\n",
            "Iteration 39, loss = 0.69986589\n",
            "Iteration 40, loss = 0.70206261\n",
            "Iteration 41, loss = 0.69316533\n",
            "Iteration 42, loss = 0.70216541\n",
            "Iteration 43, loss = 0.69702721\n",
            "Iteration 44, loss = 0.67808488\n",
            "Iteration 45, loss = 0.68140903\n",
            "Iteration 46, loss = 0.69761023\n",
            "Iteration 47, loss = 0.65612545\n",
            "Iteration 48, loss = 0.66812329\n",
            "Iteration 49, loss = 0.66719280\n",
            "Iteration 50, loss = 0.69687073\n",
            "Iteration 51, loss = 0.65149682\n",
            "Iteration 52, loss = 0.65527218\n",
            "Iteration 53, loss = 0.65697812\n",
            "Iteration 54, loss = 0.63436528\n",
            "Iteration 55, loss = 0.61914064\n",
            "Iteration 56, loss = 0.62533171\n",
            "Iteration 57, loss = 0.66601631\n",
            "Iteration 58, loss = 0.63733539\n",
            "Iteration 59, loss = 0.61103979\n",
            "Iteration 60, loss = 0.61833131\n",
            "Iteration 61, loss = 0.61162555\n",
            "Iteration 62, loss = 0.61140537\n",
            "Iteration 63, loss = 0.60243764\n",
            "Iteration 64, loss = 0.60366566\n",
            "Iteration 65, loss = 0.60867379\n",
            "Iteration 66, loss = 0.65768318\n",
            "Iteration 67, loss = 0.57464571\n",
            "Iteration 68, loss = 0.58008330\n",
            "Iteration 69, loss = 0.57218863\n",
            "Iteration 70, loss = 0.55666408\n",
            "Iteration 71, loss = 0.57407385\n",
            "Iteration 72, loss = 0.59069052\n",
            "Iteration 73, loss = 0.58112308\n",
            "Iteration 74, loss = 0.54185528\n",
            "Iteration 75, loss = 0.53527607\n",
            "Iteration 76, loss = 0.53016100\n",
            "Iteration 77, loss = 0.53676994\n",
            "Iteration 78, loss = 0.52639091\n",
            "Iteration 79, loss = 0.52145340\n",
            "Iteration 80, loss = 0.52710570\n",
            "Iteration 81, loss = 0.57221803\n",
            "Iteration 82, loss = 0.67115215\n",
            "Iteration 83, loss = 0.65664399\n",
            "Iteration 84, loss = 0.60062004\n",
            "Iteration 85, loss = 0.54854809\n",
            "Iteration 86, loss = 0.55716315\n",
            "Iteration 87, loss = 0.55728808\n",
            "Iteration 88, loss = 0.52921750\n",
            "Iteration 89, loss = 0.53781292\n",
            "Iteration 90, loss = 0.49929654\n",
            "Iteration 91, loss = 0.50154870\n",
            "Iteration 92, loss = 0.48598610\n",
            "Iteration 93, loss = 0.53240779\n",
            "Iteration 94, loss = 0.51417135\n",
            "Iteration 95, loss = 0.51098362\n",
            "Iteration 96, loss = 0.50585023\n",
            "Iteration 97, loss = 0.48926171\n",
            "Iteration 98, loss = 0.51039622\n",
            "Iteration 99, loss = 0.48883042\n",
            "Iteration 100, loss = 0.51136814\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33548023\n",
            "Iteration 2, loss = 1.24907446\n",
            "Iteration 3, loss = 1.24855616\n",
            "Iteration 4, loss = 1.23606697\n",
            "Iteration 5, loss = 1.23337609\n",
            "Iteration 6, loss = 1.22855884\n",
            "Iteration 7, loss = 1.22356490\n",
            "Iteration 8, loss = 1.21942791\n",
            "Iteration 9, loss = 1.21119856\n",
            "Iteration 10, loss = 1.26629928\n",
            "Iteration 11, loss = 1.23369692\n",
            "Iteration 12, loss = 1.22312290\n",
            "Iteration 13, loss = 1.21414717\n",
            "Iteration 14, loss = 1.20562852\n",
            "Iteration 15, loss = 1.20092338\n",
            "Iteration 16, loss = 1.21317774\n",
            "Iteration 17, loss = 1.20026008\n",
            "Iteration 18, loss = 1.20351843\n",
            "Iteration 19, loss = 1.20061655\n",
            "Iteration 20, loss = 1.19240224\n",
            "Iteration 21, loss = 1.18514872\n",
            "Iteration 22, loss = 1.18696417\n",
            "Iteration 23, loss = 1.20502644\n",
            "Iteration 24, loss = 1.18080370\n",
            "Iteration 25, loss = 1.18050770\n",
            "Iteration 26, loss = 1.17697083\n",
            "Iteration 27, loss = 1.17479820\n",
            "Iteration 28, loss = 1.16983791\n",
            "Iteration 29, loss = 1.18011983\n",
            "Iteration 30, loss = 1.18493057\n",
            "Iteration 31, loss = 1.17514293\n",
            "Iteration 32, loss = 1.17835468\n",
            "Iteration 33, loss = 1.16909947\n",
            "Iteration 34, loss = 1.16930501\n",
            "Iteration 35, loss = 1.19385851\n",
            "Iteration 36, loss = 1.16441197\n",
            "Iteration 37, loss = 1.16348251\n",
            "Iteration 38, loss = 1.15937615\n",
            "Iteration 39, loss = 1.16010663\n",
            "Iteration 40, loss = 1.15468746\n",
            "Iteration 41, loss = 1.16336546\n",
            "Iteration 42, loss = 1.16158079\n",
            "Iteration 43, loss = 1.16232676\n",
            "Iteration 44, loss = 1.16789532\n",
            "Iteration 45, loss = 1.16585367\n",
            "Iteration 46, loss = 1.15552177\n",
            "Iteration 47, loss = 1.14851847\n",
            "Iteration 48, loss = 1.16594000\n",
            "Iteration 49, loss = 1.16739461\n",
            "Iteration 50, loss = 1.17980141\n",
            "Iteration 51, loss = 1.16637595\n",
            "Iteration 52, loss = 1.14454679\n",
            "Iteration 53, loss = 1.14979094\n",
            "Iteration 54, loss = 1.14482562\n",
            "Iteration 55, loss = 1.14057413\n",
            "Iteration 56, loss = 1.13529082\n",
            "Iteration 57, loss = 1.12994883\n",
            "Iteration 58, loss = 1.13291949\n",
            "Iteration 59, loss = 1.12999594\n",
            "Iteration 60, loss = 1.12890199\n",
            "Iteration 61, loss = 1.13860975\n",
            "Iteration 62, loss = 1.14399817\n",
            "Iteration 63, loss = 1.12803204\n",
            "Iteration 64, loss = 1.12411861\n",
            "Iteration 65, loss = 1.12561898\n",
            "Iteration 66, loss = 1.11964055\n",
            "Iteration 67, loss = 1.12526923\n",
            "Iteration 68, loss = 1.12251025\n",
            "Iteration 69, loss = 1.12399679\n",
            "Iteration 70, loss = 1.11975944\n",
            "Iteration 71, loss = 1.13719592\n",
            "Iteration 72, loss = 1.11820297\n",
            "Iteration 73, loss = 1.12660802\n",
            "Iteration 74, loss = 1.11024925\n",
            "Iteration 75, loss = 1.12198053\n",
            "Iteration 76, loss = 1.11442564\n",
            "Iteration 77, loss = 1.11520088\n",
            "Iteration 78, loss = 1.09929446\n",
            "Iteration 79, loss = 1.11242579\n",
            "Iteration 80, loss = 1.08690015\n",
            "Iteration 81, loss = 1.09263593\n",
            "Iteration 82, loss = 1.08775109\n",
            "Iteration 83, loss = 1.09948837\n",
            "Iteration 84, loss = 1.09652237\n",
            "Iteration 85, loss = 1.09182190\n",
            "Iteration 86, loss = 1.08318470\n",
            "Iteration 87, loss = 1.08679381\n",
            "Iteration 88, loss = 1.07371604\n",
            "Iteration 89, loss = 1.09565136\n",
            "Iteration 90, loss = 1.10046413\n",
            "Iteration 91, loss = 1.09008465\n",
            "Iteration 92, loss = 1.06983583\n",
            "Iteration 93, loss = 1.08150001\n",
            "Iteration 94, loss = 1.06904617\n",
            "Iteration 95, loss = 1.07048173\n",
            "Iteration 96, loss = 1.06756475\n",
            "Iteration 97, loss = 1.05847326\n",
            "Iteration 98, loss = 1.05462012\n",
            "Iteration 99, loss = 1.04700383\n",
            "Iteration 100, loss = 1.04686351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33244526\n",
            "Iteration 2, loss = 1.24594662\n",
            "Iteration 3, loss = 1.23796688\n",
            "Iteration 4, loss = 1.23276302\n",
            "Iteration 5, loss = 1.23090842\n",
            "Iteration 6, loss = 1.22146301\n",
            "Iteration 7, loss = 1.21818699\n",
            "Iteration 8, loss = 1.21259046\n",
            "Iteration 9, loss = 1.21316821\n",
            "Iteration 10, loss = 1.23700258\n",
            "Iteration 11, loss = 1.22380012\n",
            "Iteration 12, loss = 1.21216299\n",
            "Iteration 13, loss = 1.19985411\n",
            "Iteration 14, loss = 1.20056188\n",
            "Iteration 15, loss = 1.19247857\n",
            "Iteration 16, loss = 1.20067286\n",
            "Iteration 17, loss = 1.19994295\n",
            "Iteration 18, loss = 1.18817150\n",
            "Iteration 19, loss = 1.19036854\n",
            "Iteration 20, loss = 1.17882669\n",
            "Iteration 21, loss = 1.18150040\n",
            "Iteration 22, loss = 1.18618658\n",
            "Iteration 23, loss = 1.20082889\n",
            "Iteration 24, loss = 1.17925666\n",
            "Iteration 25, loss = 1.17865235\n",
            "Iteration 26, loss = 1.17739995\n",
            "Iteration 27, loss = 1.17950536\n",
            "Iteration 28, loss = 1.17045006\n",
            "Iteration 29, loss = 1.17311727\n",
            "Iteration 30, loss = 1.17699447\n",
            "Iteration 31, loss = 1.17467313\n",
            "Iteration 32, loss = 1.17186831\n",
            "Iteration 33, loss = 1.17371326\n",
            "Iteration 34, loss = 1.17029721\n",
            "Iteration 35, loss = 1.19730883\n",
            "Iteration 36, loss = 1.16668859\n",
            "Iteration 37, loss = 1.17285138\n",
            "Iteration 38, loss = 1.16445175\n",
            "Iteration 39, loss = 1.16316159\n",
            "Iteration 40, loss = 1.15858456\n",
            "Iteration 41, loss = 1.16233337\n",
            "Iteration 42, loss = 1.15800229\n",
            "Iteration 43, loss = 1.16113540\n",
            "Iteration 44, loss = 1.16172639\n",
            "Iteration 45, loss = 1.16501559\n",
            "Iteration 46, loss = 1.15061947\n",
            "Iteration 47, loss = 1.15568416\n",
            "Iteration 48, loss = 1.16210742\n",
            "Iteration 49, loss = 1.15748339\n",
            "Iteration 50, loss = 1.15743085\n",
            "Iteration 51, loss = 1.15247959\n",
            "Iteration 52, loss = 1.13721831\n",
            "Iteration 53, loss = 1.14388975\n",
            "Iteration 54, loss = 1.13318501\n",
            "Iteration 55, loss = 1.13268875\n",
            "Iteration 56, loss = 1.13369437\n",
            "Iteration 57, loss = 1.12580745\n",
            "Iteration 58, loss = 1.12810555\n",
            "Iteration 59, loss = 1.12791171\n",
            "Iteration 60, loss = 1.12623238\n",
            "Iteration 61, loss = 1.12420394\n",
            "Iteration 62, loss = 1.13572384\n",
            "Iteration 63, loss = 1.12532993\n",
            "Iteration 64, loss = 1.12194794\n",
            "Iteration 65, loss = 1.12185532\n",
            "Iteration 66, loss = 1.13218110\n",
            "Iteration 67, loss = 1.12277579\n",
            "Iteration 68, loss = 1.12451768\n",
            "Iteration 69, loss = 1.12662372\n",
            "Iteration 70, loss = 1.12280263\n",
            "Iteration 71, loss = 1.12437305\n",
            "Iteration 72, loss = 1.10950241\n",
            "Iteration 73, loss = 1.12371834\n",
            "Iteration 74, loss = 1.11220767\n",
            "Iteration 75, loss = 1.11120241\n",
            "Iteration 76, loss = 1.10588051\n",
            "Iteration 77, loss = 1.10542267\n",
            "Iteration 78, loss = 1.10525433\n",
            "Iteration 79, loss = 1.12053573\n",
            "Iteration 80, loss = 1.09158788\n",
            "Iteration 81, loss = 1.09874395\n",
            "Iteration 82, loss = 1.09347384\n",
            "Iteration 83, loss = 1.09980967\n",
            "Iteration 84, loss = 1.09820704\n",
            "Iteration 85, loss = 1.09234693\n",
            "Iteration 86, loss = 1.09125153\n",
            "Iteration 87, loss = 1.09987159\n",
            "Iteration 88, loss = 1.08880031\n",
            "Iteration 89, loss = 1.09074321\n",
            "Iteration 90, loss = 1.09591035\n",
            "Iteration 91, loss = 1.08914094\n",
            "Iteration 92, loss = 1.06929965\n",
            "Iteration 93, loss = 1.09147813\n",
            "Iteration 94, loss = 1.07843980\n",
            "Iteration 95, loss = 1.08146610\n",
            "Iteration 96, loss = 1.08891721\n",
            "Iteration 97, loss = 1.07215810\n",
            "Iteration 98, loss = 1.07697725\n",
            "Iteration 99, loss = 1.05971959\n",
            "Iteration 100, loss = 1.06304022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33498574\n",
            "Iteration 2, loss = 1.25297194\n",
            "Iteration 3, loss = 1.24231996\n",
            "Iteration 4, loss = 1.23448586\n",
            "Iteration 5, loss = 1.23263589\n",
            "Iteration 6, loss = 1.22174854\n",
            "Iteration 7, loss = 1.22470917\n",
            "Iteration 8, loss = 1.21734529\n",
            "Iteration 9, loss = 1.20435993\n",
            "Iteration 10, loss = 1.21848972\n",
            "Iteration 11, loss = 1.22608261\n",
            "Iteration 12, loss = 1.20472712\n",
            "Iteration 13, loss = 1.19698716\n",
            "Iteration 14, loss = 1.19769846\n",
            "Iteration 15, loss = 1.19089561\n",
            "Iteration 16, loss = 1.20117502\n",
            "Iteration 17, loss = 1.20271051\n",
            "Iteration 18, loss = 1.18899268\n",
            "Iteration 19, loss = 1.21024226\n",
            "Iteration 20, loss = 1.17877624\n",
            "Iteration 21, loss = 1.18835211\n",
            "Iteration 22, loss = 1.19352771\n",
            "Iteration 23, loss = 1.19774444\n",
            "Iteration 24, loss = 1.17875616\n",
            "Iteration 25, loss = 1.18200251\n",
            "Iteration 26, loss = 1.17133299\n",
            "Iteration 27, loss = 1.16705786\n",
            "Iteration 28, loss = 1.16908447\n",
            "Iteration 29, loss = 1.17339152\n",
            "Iteration 30, loss = 1.17879667\n",
            "Iteration 31, loss = 1.18545273\n",
            "Iteration 32, loss = 1.16957486\n",
            "Iteration 33, loss = 1.16544283\n",
            "Iteration 34, loss = 1.16691304\n",
            "Iteration 35, loss = 1.16834903\n",
            "Iteration 36, loss = 1.16569071\n",
            "Iteration 37, loss = 1.16985218\n",
            "Iteration 38, loss = 1.16527303\n",
            "Iteration 39, loss = 1.16053796\n",
            "Iteration 40, loss = 1.15625442\n",
            "Iteration 41, loss = 1.15921522\n",
            "Iteration 42, loss = 1.15874966\n",
            "Iteration 43, loss = 1.16169922\n",
            "Iteration 44, loss = 1.14811430\n",
            "Iteration 45, loss = 1.15866017\n",
            "Iteration 46, loss = 1.15246020\n",
            "Iteration 47, loss = 1.15596421\n",
            "Iteration 48, loss = 1.15067338\n",
            "Iteration 49, loss = 1.16043560\n",
            "Iteration 50, loss = 1.14971194\n",
            "Iteration 51, loss = 1.14481285\n",
            "Iteration 52, loss = 1.13244125\n",
            "Iteration 53, loss = 1.13792397\n",
            "Iteration 54, loss = 1.12865905\n",
            "Iteration 55, loss = 1.13097177\n",
            "Iteration 56, loss = 1.12883318\n",
            "Iteration 57, loss = 1.12825027\n",
            "Iteration 58, loss = 1.12921101\n",
            "Iteration 59, loss = 1.12356806\n",
            "Iteration 60, loss = 1.12790296\n",
            "Iteration 61, loss = 1.11669461\n",
            "Iteration 62, loss = 1.13155989\n",
            "Iteration 63, loss = 1.12939000\n",
            "Iteration 64, loss = 1.11865458\n",
            "Iteration 65, loss = 1.14568263\n",
            "Iteration 66, loss = 1.14278189\n",
            "Iteration 67, loss = 1.13136280\n",
            "Iteration 68, loss = 1.11620683\n",
            "Iteration 69, loss = 1.12147966\n",
            "Iteration 70, loss = 1.11480826\n",
            "Iteration 71, loss = 1.12108729\n",
            "Iteration 72, loss = 1.11276371\n",
            "Iteration 73, loss = 1.11921526\n",
            "Iteration 74, loss = 1.10637350\n",
            "Iteration 75, loss = 1.11429808\n",
            "Iteration 76, loss = 1.10131687\n",
            "Iteration 77, loss = 1.10707985\n",
            "Iteration 78, loss = 1.09762650\n",
            "Iteration 79, loss = 1.11134712\n",
            "Iteration 80, loss = 1.09752315\n",
            "Iteration 81, loss = 1.09671077\n",
            "Iteration 82, loss = 1.09439680\n",
            "Iteration 83, loss = 1.09718948\n",
            "Iteration 84, loss = 1.09146769\n",
            "Iteration 85, loss = 1.08883733\n",
            "Iteration 86, loss = 1.09203775\n",
            "Iteration 87, loss = 1.10228313\n",
            "Iteration 88, loss = 1.10380302\n",
            "Iteration 89, loss = 1.08505057\n",
            "Iteration 90, loss = 1.08945823\n",
            "Iteration 91, loss = 1.07639313\n",
            "Iteration 92, loss = 1.08770254\n",
            "Iteration 93, loss = 1.08952825\n",
            "Iteration 94, loss = 1.07812728\n",
            "Iteration 95, loss = 1.08268175\n",
            "Iteration 96, loss = 1.08399648\n",
            "Iteration 97, loss = 1.08416571\n",
            "Iteration 98, loss = 1.07547533\n",
            "Iteration 99, loss = 1.06468247\n",
            "Iteration 100, loss = 1.08315124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34038584\n",
            "Iteration 2, loss = 1.25847910\n",
            "Iteration 3, loss = 1.24854322\n",
            "Iteration 4, loss = 1.24215511\n",
            "Iteration 5, loss = 1.23797126\n",
            "Iteration 6, loss = 1.23172050\n",
            "Iteration 7, loss = 1.23183231\n",
            "Iteration 8, loss = 1.22049759\n",
            "Iteration 9, loss = 1.21583575\n",
            "Iteration 10, loss = 1.23932343\n",
            "Iteration 11, loss = 1.24087909\n",
            "Iteration 12, loss = 1.22076531\n",
            "Iteration 13, loss = 1.21910968\n",
            "Iteration 14, loss = 1.20458629\n",
            "Iteration 15, loss = 1.20177188\n",
            "Iteration 16, loss = 1.22312811\n",
            "Iteration 17, loss = 1.21842821\n",
            "Iteration 18, loss = 1.19852716\n",
            "Iteration 19, loss = 1.21877023\n",
            "Iteration 20, loss = 1.19876501\n",
            "Iteration 21, loss = 1.20173542\n",
            "Iteration 22, loss = 1.20281388\n",
            "Iteration 23, loss = 1.21353718\n",
            "Iteration 24, loss = 1.19216151\n",
            "Iteration 25, loss = 1.19263249\n",
            "Iteration 26, loss = 1.18885902\n",
            "Iteration 27, loss = 1.18441856\n",
            "Iteration 28, loss = 1.17874128\n",
            "Iteration 29, loss = 1.18535786\n",
            "Iteration 30, loss = 1.18479678\n",
            "Iteration 31, loss = 1.19080246\n",
            "Iteration 32, loss = 1.18033048\n",
            "Iteration 33, loss = 1.17253000\n",
            "Iteration 34, loss = 1.17524369\n",
            "Iteration 35, loss = 1.17904485\n",
            "Iteration 36, loss = 1.17208992\n",
            "Iteration 37, loss = 1.18137323\n",
            "Iteration 38, loss = 1.17651870\n",
            "Iteration 39, loss = 1.17354135\n",
            "Iteration 40, loss = 1.16684574\n",
            "Iteration 41, loss = 1.16538845\n",
            "Iteration 42, loss = 1.16092931\n",
            "Iteration 43, loss = 1.17163058\n",
            "Iteration 44, loss = 1.17451886\n",
            "Iteration 45, loss = 1.16410754\n",
            "Iteration 46, loss = 1.17051657\n",
            "Iteration 47, loss = 1.17108777\n",
            "Iteration 48, loss = 1.16889186\n",
            "Iteration 49, loss = 1.16893614\n",
            "Iteration 50, loss = 1.15258555\n",
            "Iteration 51, loss = 1.15088013\n",
            "Iteration 52, loss = 1.14504703\n",
            "Iteration 53, loss = 1.14892289\n",
            "Iteration 54, loss = 1.14285244\n",
            "Iteration 55, loss = 1.14705384\n",
            "Iteration 56, loss = 1.15035887\n",
            "Iteration 57, loss = 1.15513703\n",
            "Iteration 58, loss = 1.14765485\n",
            "Iteration 59, loss = 1.14110509\n",
            "Iteration 60, loss = 1.13929236\n",
            "Iteration 61, loss = 1.12919355\n",
            "Iteration 62, loss = 1.13609284\n",
            "Iteration 63, loss = 1.13494681\n",
            "Iteration 64, loss = 1.12679509\n",
            "Iteration 65, loss = 1.15036630\n",
            "Iteration 66, loss = 1.14920088\n",
            "Iteration 67, loss = 1.13430063\n",
            "Iteration 68, loss = 1.12889553\n",
            "Iteration 69, loss = 1.12457200\n",
            "Iteration 70, loss = 1.12671271\n",
            "Iteration 71, loss = 1.13335739\n",
            "Iteration 72, loss = 1.11654611\n",
            "Iteration 73, loss = 1.12558654\n",
            "Iteration 74, loss = 1.10999889\n",
            "Iteration 75, loss = 1.12918400\n",
            "Iteration 76, loss = 1.10878855\n",
            "Iteration 77, loss = 1.12312968\n",
            "Iteration 78, loss = 1.11253508\n",
            "Iteration 79, loss = 1.12152574\n",
            "Iteration 80, loss = 1.10253244\n",
            "Iteration 81, loss = 1.09938096\n",
            "Iteration 82, loss = 1.08908826\n",
            "Iteration 83, loss = 1.09425975\n",
            "Iteration 84, loss = 1.09467575\n",
            "Iteration 85, loss = 1.09250922\n",
            "Iteration 86, loss = 1.09851160\n",
            "Iteration 87, loss = 1.09630731\n",
            "Iteration 88, loss = 1.10276891\n",
            "Iteration 89, loss = 1.09897616\n",
            "Iteration 90, loss = 1.08760059\n",
            "Iteration 91, loss = 1.08508424\n",
            "Iteration 92, loss = 1.07619912\n",
            "Iteration 93, loss = 1.07740373\n",
            "Iteration 94, loss = 1.08854654\n",
            "Iteration 95, loss = 1.08981193\n",
            "Iteration 96, loss = 1.10370196\n",
            "Iteration 97, loss = 1.10097880\n",
            "Iteration 98, loss = 1.07442434\n",
            "Iteration 99, loss = 1.06591333\n",
            "Iteration 100, loss = 1.08552697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33598195\n",
            "Iteration 2, loss = 1.25354698\n",
            "Iteration 3, loss = 1.24506165\n",
            "Iteration 4, loss = 1.24010171\n",
            "Iteration 5, loss = 1.23345154\n",
            "Iteration 6, loss = 1.22622934\n",
            "Iteration 7, loss = 1.22732780\n",
            "Iteration 8, loss = 1.21598447\n",
            "Iteration 9, loss = 1.20699844\n",
            "Iteration 10, loss = 1.21947825\n",
            "Iteration 11, loss = 1.21722717\n",
            "Iteration 12, loss = 1.20103428\n",
            "Iteration 13, loss = 1.20606282\n",
            "Iteration 14, loss = 1.19365486\n",
            "Iteration 15, loss = 1.19156779\n",
            "Iteration 16, loss = 1.19662051\n",
            "Iteration 17, loss = 1.18855163\n",
            "Iteration 18, loss = 1.18809535\n",
            "Iteration 19, loss = 1.19857626\n",
            "Iteration 20, loss = 1.18118839\n",
            "Iteration 21, loss = 1.17969574\n",
            "Iteration 22, loss = 1.18672763\n",
            "Iteration 23, loss = 1.18717214\n",
            "Iteration 24, loss = 1.18069676\n",
            "Iteration 25, loss = 1.17771759\n",
            "Iteration 26, loss = 1.16876183\n",
            "Iteration 27, loss = 1.16592043\n",
            "Iteration 28, loss = 1.16381389\n",
            "Iteration 29, loss = 1.17083383\n",
            "Iteration 30, loss = 1.18148709\n",
            "Iteration 31, loss = 1.20428369\n",
            "Iteration 32, loss = 1.17675657\n",
            "Iteration 33, loss = 1.17918619\n",
            "Iteration 34, loss = 1.16959248\n",
            "Iteration 35, loss = 1.15869828\n",
            "Iteration 36, loss = 1.16748802\n",
            "Iteration 37, loss = 1.17492037\n",
            "Iteration 38, loss = 1.16789304\n",
            "Iteration 39, loss = 1.16559838\n",
            "Iteration 40, loss = 1.15504910\n",
            "Iteration 41, loss = 1.15458405\n",
            "Iteration 42, loss = 1.15265541\n",
            "Iteration 43, loss = 1.15010876\n",
            "Iteration 44, loss = 1.15414473\n",
            "Iteration 45, loss = 1.15927761\n",
            "Iteration 46, loss = 1.15503287\n",
            "Iteration 47, loss = 1.15265029\n",
            "Iteration 48, loss = 1.16007125\n",
            "Iteration 49, loss = 1.15863160\n",
            "Iteration 50, loss = 1.13758955\n",
            "Iteration 51, loss = 1.14126003\n",
            "Iteration 52, loss = 1.12839917\n",
            "Iteration 53, loss = 1.13502409\n",
            "Iteration 54, loss = 1.12692183\n",
            "Iteration 55, loss = 1.12903293\n",
            "Iteration 56, loss = 1.13774101\n",
            "Iteration 57, loss = 1.13866122\n",
            "Iteration 58, loss = 1.12789164\n",
            "Iteration 59, loss = 1.13053832\n",
            "Iteration 60, loss = 1.12343493\n",
            "Iteration 61, loss = 1.11810085\n",
            "Iteration 62, loss = 1.12130693\n",
            "Iteration 63, loss = 1.12081636\n",
            "Iteration 64, loss = 1.11781941\n",
            "Iteration 65, loss = 1.14876764\n",
            "Iteration 66, loss = 1.15418686\n",
            "Iteration 67, loss = 1.13179869\n",
            "Iteration 68, loss = 1.12953201\n",
            "Iteration 69, loss = 1.13269529\n",
            "Iteration 70, loss = 1.11566857\n",
            "Iteration 71, loss = 1.11404555\n",
            "Iteration 72, loss = 1.11912699\n",
            "Iteration 73, loss = 1.14451615\n",
            "Iteration 74, loss = 1.11004367\n",
            "Iteration 75, loss = 1.11754695\n",
            "Iteration 76, loss = 1.10154195\n",
            "Iteration 77, loss = 1.10744691\n",
            "Iteration 78, loss = 1.09254936\n",
            "Iteration 79, loss = 1.09663744\n",
            "Iteration 80, loss = 1.09547786\n",
            "Iteration 81, loss = 1.09122052\n",
            "Iteration 82, loss = 1.09026458\n",
            "Iteration 83, loss = 1.09625915\n",
            "Iteration 84, loss = 1.08826280\n",
            "Iteration 85, loss = 1.09668599\n",
            "Iteration 86, loss = 1.09109528\n",
            "Iteration 87, loss = 1.09012817\n",
            "Iteration 88, loss = 1.08375516\n",
            "Iteration 89, loss = 1.07624048\n",
            "Iteration 90, loss = 1.07454477\n",
            "Iteration 91, loss = 1.07139183\n",
            "Iteration 92, loss = 1.08546050\n",
            "Iteration 93, loss = 1.06806410\n",
            "Iteration 94, loss = 1.07219917\n",
            "Iteration 95, loss = 1.06411788\n",
            "Iteration 96, loss = 1.08695968\n",
            "Iteration 97, loss = 1.06850178\n",
            "Iteration 98, loss = 1.06162330\n",
            "Iteration 99, loss = 1.06136025\n",
            "Iteration 100, loss = 1.06081440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33268812\n",
            "Iteration 2, loss = 1.25213668\n",
            "Iteration 3, loss = 1.24363089\n",
            "Iteration 4, loss = 1.23850368\n",
            "Iteration 5, loss = 1.22923990\n",
            "Iteration 6, loss = 1.23490579\n",
            "Iteration 7, loss = 1.22436726\n",
            "Iteration 8, loss = 1.22302778\n",
            "Iteration 9, loss = 1.21782098\n",
            "Iteration 10, loss = 1.21249265\n",
            "Iteration 11, loss = 1.20624962\n",
            "Iteration 12, loss = 1.20071965\n",
            "Iteration 13, loss = 1.20039962\n",
            "Iteration 14, loss = 1.19625664\n",
            "Iteration 15, loss = 1.19260229\n",
            "Iteration 16, loss = 1.19406619\n",
            "Iteration 17, loss = 1.18542187\n",
            "Iteration 18, loss = 1.19270240\n",
            "Iteration 19, loss = 1.20837801\n",
            "Iteration 20, loss = 1.18020987\n",
            "Iteration 21, loss = 1.18588160\n",
            "Iteration 22, loss = 1.20514802\n",
            "Iteration 23, loss = 1.18601412\n",
            "Iteration 24, loss = 1.19038136\n",
            "Iteration 25, loss = 1.17966519\n",
            "Iteration 26, loss = 1.17508806\n",
            "Iteration 27, loss = 1.16826088\n",
            "Iteration 28, loss = 1.17531748\n",
            "Iteration 29, loss = 1.16865229\n",
            "Iteration 30, loss = 1.16472025\n",
            "Iteration 31, loss = 1.17959949\n",
            "Iteration 32, loss = 1.16663113\n",
            "Iteration 33, loss = 1.16883514\n",
            "Iteration 34, loss = 1.16162694\n",
            "Iteration 35, loss = 1.15909800\n",
            "Iteration 36, loss = 1.16088807\n",
            "Iteration 37, loss = 1.17238742\n",
            "Iteration 38, loss = 1.16324993\n",
            "Iteration 39, loss = 1.16138216\n",
            "Iteration 40, loss = 1.15651201\n",
            "Iteration 41, loss = 1.15420917\n",
            "Iteration 42, loss = 1.14877145\n",
            "Iteration 43, loss = 1.15086398\n",
            "Iteration 44, loss = 1.16029255\n",
            "Iteration 45, loss = 1.15626747\n",
            "Iteration 46, loss = 1.15499267\n",
            "Iteration 47, loss = 1.14688913\n",
            "Iteration 48, loss = 1.16705513\n",
            "Iteration 49, loss = 1.16675381\n",
            "Iteration 50, loss = 1.14423716\n",
            "Iteration 51, loss = 1.13832377\n",
            "Iteration 52, loss = 1.13492939\n",
            "Iteration 53, loss = 1.14255734\n",
            "Iteration 54, loss = 1.13291101\n",
            "Iteration 55, loss = 1.13256888\n",
            "Iteration 56, loss = 1.14508923\n",
            "Iteration 57, loss = 1.13434782\n",
            "Iteration 58, loss = 1.12063847\n",
            "Iteration 59, loss = 1.12466807\n",
            "Iteration 60, loss = 1.11799321\n",
            "Iteration 61, loss = 1.12012673\n",
            "Iteration 62, loss = 1.12321925\n",
            "Iteration 63, loss = 1.11492458\n",
            "Iteration 64, loss = 1.12379957\n",
            "Iteration 65, loss = 1.13189655\n",
            "Iteration 66, loss = 1.12585994\n",
            "Iteration 67, loss = 1.12066185\n",
            "Iteration 68, loss = 1.12768138\n",
            "Iteration 69, loss = 1.12149745\n",
            "Iteration 70, loss = 1.11825534\n",
            "Iteration 71, loss = 1.11784595\n",
            "Iteration 72, loss = 1.10815399\n",
            "Iteration 73, loss = 1.11509348\n",
            "Iteration 74, loss = 1.09903953\n",
            "Iteration 75, loss = 1.12448410\n",
            "Iteration 76, loss = 1.09871795\n",
            "Iteration 77, loss = 1.10577981\n",
            "Iteration 78, loss = 1.09300098\n",
            "Iteration 79, loss = 1.09788182\n",
            "Iteration 80, loss = 1.08853853\n",
            "Iteration 81, loss = 1.08720947\n",
            "Iteration 82, loss = 1.10117105\n",
            "Iteration 83, loss = 1.08780649\n",
            "Iteration 84, loss = 1.09654876\n",
            "Iteration 85, loss = 1.09829457\n",
            "Iteration 86, loss = 1.08670503\n",
            "Iteration 87, loss = 1.08774425\n",
            "Iteration 88, loss = 1.07126005\n",
            "Iteration 89, loss = 1.07115219\n",
            "Iteration 90, loss = 1.06480928\n",
            "Iteration 91, loss = 1.05556107\n",
            "Iteration 92, loss = 1.06244847\n",
            "Iteration 93, loss = 1.07205637\n",
            "Iteration 94, loss = 1.06996393\n",
            "Iteration 95, loss = 1.06258266\n",
            "Iteration 96, loss = 1.09141980\n",
            "Iteration 97, loss = 1.06842252\n",
            "Iteration 98, loss = 1.06767244\n",
            "Iteration 99, loss = 1.06316865\n",
            "Iteration 100, loss = 1.06068961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33347795\n",
            "Iteration 2, loss = 1.26322145\n",
            "Iteration 3, loss = 1.24443127\n",
            "Iteration 4, loss = 1.24296726\n",
            "Iteration 5, loss = 1.23420686\n",
            "Iteration 6, loss = 1.23436467\n",
            "Iteration 7, loss = 1.22261598\n",
            "Iteration 8, loss = 1.22447595\n",
            "Iteration 9, loss = 1.21673793\n",
            "Iteration 10, loss = 1.21241322\n",
            "Iteration 11, loss = 1.20399263\n",
            "Iteration 12, loss = 1.19931447\n",
            "Iteration 13, loss = 1.19943603\n",
            "Iteration 14, loss = 1.20223651\n",
            "Iteration 15, loss = 1.19282872\n",
            "Iteration 16, loss = 1.19578583\n",
            "Iteration 17, loss = 1.18666475\n",
            "Iteration 18, loss = 1.19726036\n",
            "Iteration 19, loss = 1.21005854\n",
            "Iteration 20, loss = 1.17859429\n",
            "Iteration 21, loss = 1.20801576\n",
            "Iteration 22, loss = 1.21348775\n",
            "Iteration 23, loss = 1.18422010\n",
            "Iteration 24, loss = 1.20293622\n",
            "Iteration 25, loss = 1.17693980\n",
            "Iteration 26, loss = 1.18036633\n",
            "Iteration 27, loss = 1.17130753\n",
            "Iteration 28, loss = 1.17165998\n",
            "Iteration 29, loss = 1.16807200\n",
            "Iteration 30, loss = 1.16440473\n",
            "Iteration 31, loss = 1.17691911\n",
            "Iteration 32, loss = 1.16378401\n",
            "Iteration 33, loss = 1.16152004\n",
            "Iteration 34, loss = 1.15616921\n",
            "Iteration 35, loss = 1.15937890\n",
            "Iteration 36, loss = 1.16544342\n",
            "Iteration 37, loss = 1.17558252\n",
            "Iteration 38, loss = 1.16090700\n",
            "Iteration 39, loss = 1.15899962\n",
            "Iteration 40, loss = 1.15249899\n",
            "Iteration 41, loss = 1.16149699\n",
            "Iteration 42, loss = 1.15870723\n",
            "Iteration 43, loss = 1.15994169\n",
            "Iteration 44, loss = 1.16252133\n",
            "Iteration 45, loss = 1.15904294\n",
            "Iteration 46, loss = 1.15296031\n",
            "Iteration 47, loss = 1.15766902\n",
            "Iteration 48, loss = 1.17506057\n",
            "Iteration 49, loss = 1.16388031\n",
            "Iteration 50, loss = 1.15274209\n",
            "Iteration 51, loss = 1.14514274\n",
            "Iteration 52, loss = 1.14306900\n",
            "Iteration 53, loss = 1.14943361\n",
            "Iteration 54, loss = 1.13369335\n",
            "Iteration 55, loss = 1.13169043\n",
            "Iteration 56, loss = 1.16753692\n",
            "Iteration 57, loss = 1.14722945\n",
            "Iteration 58, loss = 1.13422434\n",
            "Iteration 59, loss = 1.12801955\n",
            "Iteration 60, loss = 1.12852819\n",
            "Iteration 61, loss = 1.12460736\n",
            "Iteration 62, loss = 1.12640501\n",
            "Iteration 63, loss = 1.11777468\n",
            "Iteration 64, loss = 1.12432161\n",
            "Iteration 65, loss = 1.14451215\n",
            "Iteration 66, loss = 1.14036657\n",
            "Iteration 67, loss = 1.12993817\n",
            "Iteration 68, loss = 1.12613544\n",
            "Iteration 69, loss = 1.13116450\n",
            "Iteration 70, loss = 1.11742085\n",
            "Iteration 71, loss = 1.11619786\n",
            "Iteration 72, loss = 1.11549478\n",
            "Iteration 73, loss = 1.11552405\n",
            "Iteration 74, loss = 1.10677030\n",
            "Iteration 75, loss = 1.13675716\n",
            "Iteration 76, loss = 1.11450536\n",
            "Iteration 77, loss = 1.11328821\n",
            "Iteration 78, loss = 1.11527434\n",
            "Iteration 79, loss = 1.10117781\n",
            "Iteration 80, loss = 1.09942572\n",
            "Iteration 81, loss = 1.09190688\n",
            "Iteration 82, loss = 1.10247145\n",
            "Iteration 83, loss = 1.09920097\n",
            "Iteration 84, loss = 1.11516367\n",
            "Iteration 85, loss = 1.08663394\n",
            "Iteration 86, loss = 1.08846540\n",
            "Iteration 87, loss = 1.10675983\n",
            "Iteration 88, loss = 1.09841556\n",
            "Iteration 89, loss = 1.09865329\n",
            "Iteration 90, loss = 1.10325166\n",
            "Iteration 91, loss = 1.07909593\n",
            "Iteration 92, loss = 1.07957128\n",
            "Iteration 93, loss = 1.08688002\n",
            "Iteration 94, loss = 1.07831898\n",
            "Iteration 95, loss = 1.08539771\n",
            "Iteration 96, loss = 1.12680305\n",
            "Iteration 97, loss = 1.08170895\n",
            "Iteration 98, loss = 1.08215840\n",
            "Iteration 99, loss = 1.07260251\n",
            "Iteration 100, loss = 1.06758472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33354588\n",
            "Iteration 2, loss = 1.25934912\n",
            "Iteration 3, loss = 1.23939915\n",
            "Iteration 4, loss = 1.23748344\n",
            "Iteration 5, loss = 1.22671204\n",
            "Iteration 6, loss = 1.22299061\n",
            "Iteration 7, loss = 1.21276879\n",
            "Iteration 8, loss = 1.20711534\n",
            "Iteration 9, loss = 1.20226888\n",
            "Iteration 10, loss = 1.19925104\n",
            "Iteration 11, loss = 1.19595460\n",
            "Iteration 12, loss = 1.19581835\n",
            "Iteration 13, loss = 1.19117724\n",
            "Iteration 14, loss = 1.20284488\n",
            "Iteration 15, loss = 1.18748101\n",
            "Iteration 16, loss = 1.19072832\n",
            "Iteration 17, loss = 1.19525346\n",
            "Iteration 18, loss = 1.20235806\n",
            "Iteration 19, loss = 1.19542052\n",
            "Iteration 20, loss = 1.17433935\n",
            "Iteration 21, loss = 1.19175194\n",
            "Iteration 22, loss = 1.17713681\n",
            "Iteration 23, loss = 1.17484257\n",
            "Iteration 24, loss = 1.17858698\n",
            "Iteration 25, loss = 1.18188370\n",
            "Iteration 26, loss = 1.17537360\n",
            "Iteration 27, loss = 1.16374398\n",
            "Iteration 28, loss = 1.17828239\n",
            "Iteration 29, loss = 1.16448943\n",
            "Iteration 30, loss = 1.17312084\n",
            "Iteration 31, loss = 1.15815847\n",
            "Iteration 32, loss = 1.15654684\n",
            "Iteration 33, loss = 1.15805272\n",
            "Iteration 34, loss = 1.15826167\n",
            "Iteration 35, loss = 1.15240864\n",
            "Iteration 36, loss = 1.16010486\n",
            "Iteration 37, loss = 1.16213301\n",
            "Iteration 38, loss = 1.15500392\n",
            "Iteration 39, loss = 1.15532521\n",
            "Iteration 40, loss = 1.15171269\n",
            "Iteration 41, loss = 1.14716130\n",
            "Iteration 42, loss = 1.15005224\n",
            "Iteration 43, loss = 1.14440062\n",
            "Iteration 44, loss = 1.14458398\n",
            "Iteration 45, loss = 1.15082919\n",
            "Iteration 46, loss = 1.14317728\n",
            "Iteration 47, loss = 1.13622272\n",
            "Iteration 48, loss = 1.14112409\n",
            "Iteration 49, loss = 1.15317516\n",
            "Iteration 50, loss = 1.13118983\n",
            "Iteration 51, loss = 1.13383818\n",
            "Iteration 52, loss = 1.13382735\n",
            "Iteration 53, loss = 1.13300177\n",
            "Iteration 54, loss = 1.13111199\n",
            "Iteration 55, loss = 1.12582718\n",
            "Iteration 56, loss = 1.13413738\n",
            "Iteration 57, loss = 1.13081883\n",
            "Iteration 58, loss = 1.13582110\n",
            "Iteration 59, loss = 1.16965100\n",
            "Iteration 60, loss = 1.13899882\n",
            "Iteration 61, loss = 1.13554044\n",
            "Iteration 62, loss = 1.12556212\n",
            "Iteration 63, loss = 1.15453882\n",
            "Iteration 64, loss = 1.14282298\n",
            "Iteration 65, loss = 1.11996333\n",
            "Iteration 66, loss = 1.11425886\n",
            "Iteration 67, loss = 1.12375589\n",
            "Iteration 68, loss = 1.12317846\n",
            "Iteration 69, loss = 1.12688709\n",
            "Iteration 70, loss = 1.10568069\n",
            "Iteration 71, loss = 1.12981269\n",
            "Iteration 72, loss = 1.10745735\n",
            "Iteration 73, loss = 1.12581910\n",
            "Iteration 74, loss = 1.12284835\n",
            "Iteration 75, loss = 1.12271864\n",
            "Iteration 76, loss = 1.10980629\n",
            "Iteration 77, loss = 1.09805691\n",
            "Iteration 78, loss = 1.09675699\n",
            "Iteration 79, loss = 1.10362493\n",
            "Iteration 80, loss = 1.09542894\n",
            "Iteration 81, loss = 1.10353392\n",
            "Iteration 82, loss = 1.09208279\n",
            "Iteration 83, loss = 1.09999676\n",
            "Iteration 84, loss = 1.08935544\n",
            "Iteration 85, loss = 1.09759332\n",
            "Iteration 86, loss = 1.08129772\n",
            "Iteration 87, loss = 1.08450906\n",
            "Iteration 88, loss = 1.07548597\n",
            "Iteration 89, loss = 1.10465394\n",
            "Iteration 90, loss = 1.06931211\n",
            "Iteration 91, loss = 1.07319811\n",
            "Iteration 92, loss = 1.07250952\n",
            "Iteration 93, loss = 1.07674277\n",
            "Iteration 94, loss = 1.08594386\n",
            "Iteration 95, loss = 1.07273620\n",
            "Iteration 96, loss = 1.07751736\n",
            "Iteration 97, loss = 1.07464186\n",
            "Iteration 98, loss = 1.08437202\n",
            "Iteration 99, loss = 1.07569451\n",
            "Iteration 100, loss = 1.07436528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33472192\n",
            "Iteration 2, loss = 1.25722551\n",
            "Iteration 3, loss = 1.23148899\n",
            "Iteration 4, loss = 1.23684329\n",
            "Iteration 5, loss = 1.22424031\n",
            "Iteration 6, loss = 1.22262833\n",
            "Iteration 7, loss = 1.21710763\n",
            "Iteration 8, loss = 1.21639791\n",
            "Iteration 9, loss = 1.20779402\n",
            "Iteration 10, loss = 1.20405880\n",
            "Iteration 11, loss = 1.20282599\n",
            "Iteration 12, loss = 1.20063730\n",
            "Iteration 13, loss = 1.19644161\n",
            "Iteration 14, loss = 1.20350944\n",
            "Iteration 15, loss = 1.19250992\n",
            "Iteration 16, loss = 1.19957152\n",
            "Iteration 17, loss = 1.18906567\n",
            "Iteration 18, loss = 1.18771783\n",
            "Iteration 19, loss = 1.19627071\n",
            "Iteration 20, loss = 1.18411678\n",
            "Iteration 21, loss = 1.19039035\n",
            "Iteration 22, loss = 1.18550993\n",
            "Iteration 23, loss = 1.18067919\n",
            "Iteration 24, loss = 1.18184032\n",
            "Iteration 25, loss = 1.17174862\n",
            "Iteration 26, loss = 1.18335706\n",
            "Iteration 27, loss = 1.18074206\n",
            "Iteration 28, loss = 1.17486124\n",
            "Iteration 29, loss = 1.17251191\n",
            "Iteration 30, loss = 1.16623400\n",
            "Iteration 31, loss = 1.16499148\n",
            "Iteration 32, loss = 1.15903845\n",
            "Iteration 33, loss = 1.16579439\n",
            "Iteration 34, loss = 1.15745472\n",
            "Iteration 35, loss = 1.15663777\n",
            "Iteration 36, loss = 1.17332046\n",
            "Iteration 37, loss = 1.18392904\n",
            "Iteration 38, loss = 1.16193443\n",
            "Iteration 39, loss = 1.15967846\n",
            "Iteration 40, loss = 1.15975850\n",
            "Iteration 41, loss = 1.15558677\n",
            "Iteration 42, loss = 1.15407805\n",
            "Iteration 43, loss = 1.14897568\n",
            "Iteration 44, loss = 1.16250708\n",
            "Iteration 45, loss = 1.16464327\n",
            "Iteration 46, loss = 1.14986232\n",
            "Iteration 47, loss = 1.14818851\n",
            "Iteration 48, loss = 1.15208775\n",
            "Iteration 49, loss = 1.15504780\n",
            "Iteration 50, loss = 1.13901734\n",
            "Iteration 51, loss = 1.13899670\n",
            "Iteration 52, loss = 1.13952202\n",
            "Iteration 53, loss = 1.13701592\n",
            "Iteration 54, loss = 1.13661884\n",
            "Iteration 55, loss = 1.13205303\n",
            "Iteration 56, loss = 1.13404178\n",
            "Iteration 57, loss = 1.15272514\n",
            "Iteration 58, loss = 1.16521710\n",
            "Iteration 59, loss = 1.18083620\n",
            "Iteration 60, loss = 1.15733156\n",
            "Iteration 61, loss = 1.15900523\n",
            "Iteration 62, loss = 1.14465462\n",
            "Iteration 63, loss = 1.18233940\n",
            "Iteration 64, loss = 1.15159287\n",
            "Iteration 65, loss = 1.13144850\n",
            "Iteration 66, loss = 1.13781961\n",
            "Iteration 67, loss = 1.13496661\n",
            "Iteration 68, loss = 1.12462787\n",
            "Iteration 69, loss = 1.12475493\n",
            "Iteration 70, loss = 1.12757561\n",
            "Iteration 71, loss = 1.15416859\n",
            "Iteration 72, loss = 1.12614055\n",
            "Iteration 73, loss = 1.12435843\n",
            "Iteration 74, loss = 1.11850249\n",
            "Iteration 75, loss = 1.12023586\n",
            "Iteration 76, loss = 1.11688346\n",
            "Iteration 77, loss = 1.10902964\n",
            "Iteration 78, loss = 1.10495854\n",
            "Iteration 79, loss = 1.12055125\n",
            "Iteration 80, loss = 1.11443340\n",
            "Iteration 81, loss = 1.12031012\n",
            "Iteration 82, loss = 1.11251186\n",
            "Iteration 83, loss = 1.13612796\n",
            "Iteration 84, loss = 1.12598014\n",
            "Iteration 85, loss = 1.12311094\n",
            "Iteration 86, loss = 1.11242447\n",
            "Iteration 87, loss = 1.11747652\n",
            "Iteration 88, loss = 1.10447205\n",
            "Iteration 89, loss = 1.10032244\n",
            "Iteration 90, loss = 1.09279879\n",
            "Iteration 91, loss = 1.09870030\n",
            "Iteration 92, loss = 1.09495939\n",
            "Iteration 93, loss = 1.08828984\n",
            "Iteration 94, loss = 1.09986968\n",
            "Iteration 95, loss = 1.08653292\n",
            "Iteration 96, loss = 1.09819651\n",
            "Iteration 97, loss = 1.08874091\n",
            "Iteration 98, loss = 1.08904085\n",
            "Iteration 99, loss = 1.09722874\n",
            "Iteration 100, loss = 1.08467962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32851116\n",
            "Iteration 2, loss = 1.24558887\n",
            "Iteration 3, loss = 1.22085098\n",
            "Iteration 4, loss = 1.22640728\n",
            "Iteration 5, loss = 1.21627556\n",
            "Iteration 6, loss = 1.21299215\n",
            "Iteration 7, loss = 1.20285385\n",
            "Iteration 8, loss = 1.20502597\n",
            "Iteration 9, loss = 1.19740615\n",
            "Iteration 10, loss = 1.19638676\n",
            "Iteration 11, loss = 1.18969895\n",
            "Iteration 12, loss = 1.19063959\n",
            "Iteration 13, loss = 1.18256040\n",
            "Iteration 14, loss = 1.19082091\n",
            "Iteration 15, loss = 1.17795638\n",
            "Iteration 16, loss = 1.18131526\n",
            "Iteration 17, loss = 1.17222994\n",
            "Iteration 18, loss = 1.16703122\n",
            "Iteration 19, loss = 1.17670321\n",
            "Iteration 20, loss = 1.17270499\n",
            "Iteration 21, loss = 1.17466159\n",
            "Iteration 22, loss = 1.16470532\n",
            "Iteration 23, loss = 1.16230341\n",
            "Iteration 24, loss = 1.16511206\n",
            "Iteration 25, loss = 1.15673016\n",
            "Iteration 26, loss = 1.15652004\n",
            "Iteration 27, loss = 1.16585477\n",
            "Iteration 28, loss = 1.15745064\n",
            "Iteration 29, loss = 1.15209909\n",
            "Iteration 30, loss = 1.14951831\n",
            "Iteration 31, loss = 1.15285965\n",
            "Iteration 32, loss = 1.14679210\n",
            "Iteration 33, loss = 1.14683936\n",
            "Iteration 34, loss = 1.14263089\n",
            "Iteration 35, loss = 1.14079680\n",
            "Iteration 36, loss = 1.15302570\n",
            "Iteration 37, loss = 1.15882316\n",
            "Iteration 38, loss = 1.13749969\n",
            "Iteration 39, loss = 1.14510394\n",
            "Iteration 40, loss = 1.15428845\n",
            "Iteration 41, loss = 1.15475895\n",
            "Iteration 42, loss = 1.14690636\n",
            "Iteration 43, loss = 1.13673285\n",
            "Iteration 44, loss = 1.14170329\n",
            "Iteration 45, loss = 1.14042394\n",
            "Iteration 46, loss = 1.13499068\n",
            "Iteration 47, loss = 1.13421073\n",
            "Iteration 48, loss = 1.13071750\n",
            "Iteration 49, loss = 1.12890914\n",
            "Iteration 50, loss = 1.12856364\n",
            "Iteration 51, loss = 1.12195211\n",
            "Iteration 52, loss = 1.11750966\n",
            "Iteration 53, loss = 1.13452837\n",
            "Iteration 54, loss = 1.12234604\n",
            "Iteration 55, loss = 1.11605901\n",
            "Iteration 56, loss = 1.11939283\n",
            "Iteration 57, loss = 1.12973012\n",
            "Iteration 58, loss = 1.15189202\n",
            "Iteration 59, loss = 1.16862664\n",
            "Iteration 60, loss = 1.13923263\n",
            "Iteration 61, loss = 1.13600598\n",
            "Iteration 62, loss = 1.11762338\n",
            "Iteration 63, loss = 1.15011282\n",
            "Iteration 64, loss = 1.13338207\n",
            "Iteration 65, loss = 1.11406015\n",
            "Iteration 66, loss = 1.13034830\n",
            "Iteration 67, loss = 1.12066444\n",
            "Iteration 68, loss = 1.12325126\n",
            "Iteration 69, loss = 1.10594256\n",
            "Iteration 70, loss = 1.11629504\n",
            "Iteration 71, loss = 1.15199916\n",
            "Iteration 72, loss = 1.11032860\n",
            "Iteration 73, loss = 1.11526758\n",
            "Iteration 74, loss = 1.11253284\n",
            "Iteration 75, loss = 1.11475884\n",
            "Iteration 76, loss = 1.10252937\n",
            "Iteration 77, loss = 1.09397321\n",
            "Iteration 78, loss = 1.08741765\n",
            "Iteration 79, loss = 1.09511657\n",
            "Iteration 80, loss = 1.09907214\n",
            "Iteration 81, loss = 1.10274309\n",
            "Iteration 82, loss = 1.10436994\n",
            "Iteration 83, loss = 1.11176083\n",
            "Iteration 84, loss = 1.09599340\n",
            "Iteration 85, loss = 1.12064926\n",
            "Iteration 86, loss = 1.07834745\n",
            "Iteration 87, loss = 1.08389181\n",
            "Iteration 88, loss = 1.08227097\n",
            "Iteration 89, loss = 1.10653281\n",
            "Iteration 90, loss = 1.07933247\n",
            "Iteration 91, loss = 1.09124521\n",
            "Iteration 92, loss = 1.06879158\n",
            "Iteration 93, loss = 1.08015042\n",
            "Iteration 94, loss = 1.07179936\n",
            "Iteration 95, loss = 1.06542287\n",
            "Iteration 96, loss = 1.07039944\n",
            "Iteration 97, loss = 1.06416644\n",
            "Iteration 98, loss = 1.05967304\n",
            "Iteration 99, loss = 1.07072684\n",
            "Iteration 100, loss = 1.04906120\n",
            "13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40159041\n",
            "Iteration 2, loss = 1.26826557\n",
            "Iteration 3, loss = 1.23129470\n",
            "Iteration 4, loss = 1.22270040\n",
            "Iteration 5, loss = 1.22006322\n",
            "Iteration 6, loss = 1.19035470\n",
            "Iteration 7, loss = 1.16912952\n",
            "Iteration 8, loss = 1.15013574\n",
            "Iteration 9, loss = 1.11329523\n",
            "Iteration 10, loss = 1.08818617\n",
            "Iteration 11, loss = 1.10355954\n",
            "Iteration 12, loss = 1.06861928\n",
            "Iteration 13, loss = 1.06608379\n",
            "Iteration 14, loss = 1.03491996\n",
            "Iteration 15, loss = 1.01379240\n",
            "Iteration 16, loss = 1.04439960\n",
            "Iteration 17, loss = 0.98930455\n",
            "Iteration 18, loss = 0.97650110\n",
            "Iteration 19, loss = 0.94692368\n",
            "Iteration 20, loss = 0.94034252\n",
            "Iteration 21, loss = 0.91662680\n",
            "Iteration 22, loss = 0.90004995\n",
            "Iteration 23, loss = 0.91956210\n",
            "Iteration 24, loss = 0.90236951\n",
            "Iteration 25, loss = 0.90399203\n",
            "Iteration 26, loss = 0.88658216\n",
            "Iteration 27, loss = 0.86025924\n",
            "Iteration 28, loss = 0.84771903\n",
            "Iteration 29, loss = 0.84160581\n",
            "Iteration 30, loss = 0.84043675\n",
            "Iteration 31, loss = 0.84139248\n",
            "Iteration 32, loss = 0.80078276\n",
            "Iteration 33, loss = 0.79410999\n",
            "Iteration 34, loss = 0.78348405\n",
            "Iteration 35, loss = 0.77306062\n",
            "Iteration 36, loss = 0.77437330\n",
            "Iteration 37, loss = 0.76080088\n",
            "Iteration 38, loss = 0.74056360\n",
            "Iteration 39, loss = 0.73785200\n",
            "Iteration 40, loss = 0.75567280\n",
            "Iteration 41, loss = 0.75484714\n",
            "Iteration 42, loss = 0.72609976\n",
            "Iteration 43, loss = 0.71529546\n",
            "Iteration 44, loss = 0.73684912\n",
            "Iteration 45, loss = 0.71999498\n",
            "Iteration 46, loss = 0.75716030\n",
            "Iteration 47, loss = 0.74811409\n",
            "Iteration 48, loss = 0.74317066\n",
            "Iteration 49, loss = 0.71426837\n",
            "Iteration 50, loss = 0.73578834\n",
            "Iteration 51, loss = 0.73541405\n",
            "Iteration 52, loss = 0.73255312\n",
            "Iteration 53, loss = 0.69531082\n",
            "Iteration 54, loss = 0.67390769\n",
            "Iteration 55, loss = 0.70242910\n",
            "Iteration 56, loss = 0.68589768\n",
            "Iteration 57, loss = 0.69295183\n",
            "Iteration 58, loss = 0.64633335\n",
            "Iteration 59, loss = 0.64878910\n",
            "Iteration 60, loss = 0.65093534\n",
            "Iteration 61, loss = 0.70044524\n",
            "Iteration 62, loss = 0.71189563\n",
            "Iteration 63, loss = 0.73766104\n",
            "Iteration 64, loss = 0.72019906\n",
            "Iteration 65, loss = 0.81874207\n",
            "Iteration 66, loss = 0.72323511\n",
            "Iteration 67, loss = 0.73466483\n",
            "Iteration 68, loss = 0.69838019\n",
            "Iteration 69, loss = 0.69262393\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39817284\n",
            "Iteration 2, loss = 1.26522348\n",
            "Iteration 3, loss = 1.23808980\n",
            "Iteration 4, loss = 1.22841311\n",
            "Iteration 5, loss = 1.21302131\n",
            "Iteration 6, loss = 1.19563289\n",
            "Iteration 7, loss = 1.17640556\n",
            "Iteration 8, loss = 1.15630148\n",
            "Iteration 9, loss = 1.14227980\n",
            "Iteration 10, loss = 1.14356717\n",
            "Iteration 11, loss = 1.16844070\n",
            "Iteration 12, loss = 1.11394390\n",
            "Iteration 13, loss = 1.08580623\n",
            "Iteration 14, loss = 1.07181766\n",
            "Iteration 15, loss = 1.04928350\n",
            "Iteration 16, loss = 1.03226927\n",
            "Iteration 17, loss = 1.01730323\n",
            "Iteration 18, loss = 0.99807010\n",
            "Iteration 19, loss = 0.98183068\n",
            "Iteration 20, loss = 0.96865984\n",
            "Iteration 21, loss = 0.97760384\n",
            "Iteration 22, loss = 0.94926632\n",
            "Iteration 23, loss = 0.94980474\n",
            "Iteration 24, loss = 0.94625121\n",
            "Iteration 25, loss = 0.91850203\n",
            "Iteration 26, loss = 0.90407201\n",
            "Iteration 27, loss = 0.88712001\n",
            "Iteration 28, loss = 0.86927279\n",
            "Iteration 29, loss = 0.87240565\n",
            "Iteration 30, loss = 0.87120806\n",
            "Iteration 31, loss = 0.85243574\n",
            "Iteration 32, loss = 0.83110683\n",
            "Iteration 33, loss = 0.83575518\n",
            "Iteration 34, loss = 0.84240937\n",
            "Iteration 35, loss = 0.82505697\n",
            "Iteration 36, loss = 0.80589208\n",
            "Iteration 37, loss = 0.79805047\n",
            "Iteration 38, loss = 0.77076640\n",
            "Iteration 39, loss = 0.77290907\n",
            "Iteration 40, loss = 0.78373837\n",
            "Iteration 41, loss = 0.75865941\n",
            "Iteration 42, loss = 0.75795834\n",
            "Iteration 43, loss = 0.73598376\n",
            "Iteration 44, loss = 0.73489086\n",
            "Iteration 45, loss = 0.72290875\n",
            "Iteration 46, loss = 0.72557971\n",
            "Iteration 47, loss = 0.72058939\n",
            "Iteration 48, loss = 0.73023713\n",
            "Iteration 49, loss = 0.72129865\n",
            "Iteration 50, loss = 0.76655540\n",
            "Iteration 51, loss = 0.71618927\n",
            "Iteration 52, loss = 0.75409717\n",
            "Iteration 53, loss = 0.70344527\n",
            "Iteration 54, loss = 0.68691102\n",
            "Iteration 55, loss = 0.73595633\n",
            "Iteration 56, loss = 0.74019176\n",
            "Iteration 57, loss = 0.73863747\n",
            "Iteration 58, loss = 0.70191746\n",
            "Iteration 59, loss = 0.67463943\n",
            "Iteration 60, loss = 0.67947120\n",
            "Iteration 61, loss = 0.73644045\n",
            "Iteration 62, loss = 0.70404106\n",
            "Iteration 63, loss = 0.69291510\n",
            "Iteration 64, loss = 0.71290832\n",
            "Iteration 65, loss = 0.70851413\n",
            "Iteration 66, loss = 0.70981923\n",
            "Iteration 67, loss = 0.68870071\n",
            "Iteration 68, loss = 0.67529220\n",
            "Iteration 69, loss = 0.68803286\n",
            "Iteration 70, loss = 0.65791370\n",
            "Iteration 71, loss = 0.65461316\n",
            "Iteration 72, loss = 0.64245734\n",
            "Iteration 73, loss = 0.64061823\n",
            "Iteration 74, loss = 0.62421951\n",
            "Iteration 75, loss = 0.62001382\n",
            "Iteration 76, loss = 0.61560099\n",
            "Iteration 77, loss = 0.61941523\n",
            "Iteration 78, loss = 0.64428640\n",
            "Iteration 79, loss = 0.62955589\n",
            "Iteration 80, loss = 0.61533931\n",
            "Iteration 81, loss = 0.63412373\n",
            "Iteration 82, loss = 0.60381390\n",
            "Iteration 83, loss = 0.60072805\n",
            "Iteration 84, loss = 0.59266699\n",
            "Iteration 85, loss = 0.59364260\n",
            "Iteration 86, loss = 0.61288223\n",
            "Iteration 87, loss = 0.57935378\n",
            "Iteration 88, loss = 0.60976749\n",
            "Iteration 89, loss = 0.66073606\n",
            "Iteration 90, loss = 0.57872175\n",
            "Iteration 91, loss = 0.59218057\n",
            "Iteration 92, loss = 0.59500705\n",
            "Iteration 93, loss = 0.60572937\n",
            "Iteration 94, loss = 0.64535176\n",
            "Iteration 95, loss = 0.61588548\n",
            "Iteration 96, loss = 0.57652530\n",
            "Iteration 97, loss = 0.58575522\n",
            "Iteration 98, loss = 0.56792667\n",
            "Iteration 99, loss = 0.57985237\n",
            "Iteration 100, loss = 0.58387491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40109758\n",
            "Iteration 2, loss = 1.26752614\n",
            "Iteration 3, loss = 1.24087531\n",
            "Iteration 4, loss = 1.23074037\n",
            "Iteration 5, loss = 1.21796931\n",
            "Iteration 6, loss = 1.19103494\n",
            "Iteration 7, loss = 1.16508924\n",
            "Iteration 8, loss = 1.14794081\n",
            "Iteration 9, loss = 1.12208923\n",
            "Iteration 10, loss = 1.10030337\n",
            "Iteration 11, loss = 1.09148016\n",
            "Iteration 12, loss = 1.06124707\n",
            "Iteration 13, loss = 1.05161120\n",
            "Iteration 14, loss = 1.02659810\n",
            "Iteration 15, loss = 1.02779129\n",
            "Iteration 16, loss = 1.00386592\n",
            "Iteration 17, loss = 0.98896010\n",
            "Iteration 18, loss = 0.98188909\n",
            "Iteration 19, loss = 0.96308291\n",
            "Iteration 20, loss = 0.95231610\n",
            "Iteration 21, loss = 0.95680223\n",
            "Iteration 22, loss = 0.93827517\n",
            "Iteration 23, loss = 0.95483740\n",
            "Iteration 24, loss = 0.96028230\n",
            "Iteration 25, loss = 0.91784547\n",
            "Iteration 26, loss = 0.88861328\n",
            "Iteration 27, loss = 0.85875358\n",
            "Iteration 28, loss = 0.84575252\n",
            "Iteration 29, loss = 0.84346298\n",
            "Iteration 30, loss = 0.85512552\n",
            "Iteration 31, loss = 0.82950764\n",
            "Iteration 32, loss = 0.80391139\n",
            "Iteration 33, loss = 0.80323894\n",
            "Iteration 34, loss = 0.81796311\n",
            "Iteration 35, loss = 0.81440707\n",
            "Iteration 36, loss = 0.81357358\n",
            "Iteration 37, loss = 0.79653805\n",
            "Iteration 38, loss = 0.76132235\n",
            "Iteration 39, loss = 0.77135035\n",
            "Iteration 40, loss = 0.78840694\n",
            "Iteration 41, loss = 0.77004073\n",
            "Iteration 42, loss = 0.75747143\n",
            "Iteration 43, loss = 0.73952366\n",
            "Iteration 44, loss = 0.72388507\n",
            "Iteration 45, loss = 0.71339985\n",
            "Iteration 46, loss = 0.74339643\n",
            "Iteration 47, loss = 0.72240779\n",
            "Iteration 48, loss = 0.76468069\n",
            "Iteration 49, loss = 0.75131848\n",
            "Iteration 50, loss = 0.79911119\n",
            "Iteration 51, loss = 0.75101737\n",
            "Iteration 52, loss = 0.75185543\n",
            "Iteration 53, loss = 0.70359979\n",
            "Iteration 54, loss = 0.70141056\n",
            "Iteration 55, loss = 0.70868487\n",
            "Iteration 56, loss = 0.72294960\n",
            "Iteration 57, loss = 0.67463026\n",
            "Iteration 58, loss = 0.66553140\n",
            "Iteration 59, loss = 0.66032183\n",
            "Iteration 60, loss = 0.70542821\n",
            "Iteration 61, loss = 0.72327010\n",
            "Iteration 62, loss = 0.71811047\n",
            "Iteration 63, loss = 0.70488892\n",
            "Iteration 64, loss = 0.70327766\n",
            "Iteration 65, loss = 0.72168818\n",
            "Iteration 66, loss = 0.68282440\n",
            "Iteration 67, loss = 0.68036476\n",
            "Iteration 68, loss = 0.67095184\n",
            "Iteration 69, loss = 0.68070536\n",
            "Iteration 70, loss = 0.66949979\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.40057542\n",
            "Iteration 2, loss = 1.27109948\n",
            "Iteration 3, loss = 1.24644398\n",
            "Iteration 4, loss = 1.23025974\n",
            "Iteration 5, loss = 1.20270447\n",
            "Iteration 6, loss = 1.18439250\n",
            "Iteration 7, loss = 1.15577707\n",
            "Iteration 8, loss = 1.13605314\n",
            "Iteration 9, loss = 1.11272386\n",
            "Iteration 10, loss = 1.09467588\n",
            "Iteration 11, loss = 1.06153220\n",
            "Iteration 12, loss = 1.04209098\n",
            "Iteration 13, loss = 1.04682586\n",
            "Iteration 14, loss = 1.04082614\n",
            "Iteration 15, loss = 1.00898729\n",
            "Iteration 16, loss = 1.00963357\n",
            "Iteration 17, loss = 0.99372526\n",
            "Iteration 18, loss = 0.98825928\n",
            "Iteration 19, loss = 1.00209537\n",
            "Iteration 20, loss = 0.97585274\n",
            "Iteration 21, loss = 0.95837861\n",
            "Iteration 22, loss = 0.94602097\n",
            "Iteration 23, loss = 0.97561268\n",
            "Iteration 24, loss = 0.97512380\n",
            "Iteration 25, loss = 0.97256595\n",
            "Iteration 26, loss = 0.99588626\n",
            "Iteration 27, loss = 0.93109586\n",
            "Iteration 28, loss = 0.90892640\n",
            "Iteration 29, loss = 0.88972394\n",
            "Iteration 30, loss = 0.89208923\n",
            "Iteration 31, loss = 0.87082890\n",
            "Iteration 32, loss = 0.84740495\n",
            "Iteration 33, loss = 0.85448066\n",
            "Iteration 34, loss = 0.85809611\n",
            "Iteration 35, loss = 0.85663522\n",
            "Iteration 36, loss = 0.84552623\n",
            "Iteration 37, loss = 0.83442715\n",
            "Iteration 38, loss = 0.80685041\n",
            "Iteration 39, loss = 0.80114216\n",
            "Iteration 40, loss = 0.79827969\n",
            "Iteration 41, loss = 0.78933556\n",
            "Iteration 42, loss = 0.78532288\n",
            "Iteration 43, loss = 0.77078916\n",
            "Iteration 44, loss = 0.76643405\n",
            "Iteration 45, loss = 0.74628295\n",
            "Iteration 46, loss = 0.76854077\n",
            "Iteration 47, loss = 0.74883851\n",
            "Iteration 48, loss = 0.76868734\n",
            "Iteration 49, loss = 0.77681091\n",
            "Iteration 50, loss = 0.79942129\n",
            "Iteration 51, loss = 0.74530441\n",
            "Iteration 52, loss = 0.76095326\n",
            "Iteration 53, loss = 0.74370822\n",
            "Iteration 54, loss = 0.72643073\n",
            "Iteration 55, loss = 0.77099931\n",
            "Iteration 56, loss = 0.80474926\n",
            "Iteration 57, loss = 0.73907147\n",
            "Iteration 58, loss = 0.73072781\n",
            "Iteration 59, loss = 0.70154907\n",
            "Iteration 60, loss = 0.72665030\n",
            "Iteration 61, loss = 0.75749805\n",
            "Iteration 62, loss = 0.73590368\n",
            "Iteration 63, loss = 0.68814668\n",
            "Iteration 64, loss = 0.69357682\n",
            "Iteration 65, loss = 0.69528875\n",
            "Iteration 66, loss = 0.70107460\n",
            "Iteration 67, loss = 0.68221870\n",
            "Iteration 68, loss = 0.66072414\n",
            "Iteration 69, loss = 0.67552795\n",
            "Iteration 70, loss = 0.66405371\n",
            "Iteration 71, loss = 0.65576173\n",
            "Iteration 72, loss = 0.66128415\n",
            "Iteration 73, loss = 0.66038609\n",
            "Iteration 74, loss = 0.65774199\n",
            "Iteration 75, loss = 0.65974736\n",
            "Iteration 76, loss = 0.63784271\n",
            "Iteration 77, loss = 0.65491176\n",
            "Iteration 78, loss = 0.65427682\n",
            "Iteration 79, loss = 0.66976700\n",
            "Iteration 80, loss = 0.63656166\n",
            "Iteration 81, loss = 0.64259721\n",
            "Iteration 82, loss = 0.63681815\n",
            "Iteration 83, loss = 0.66725975\n",
            "Iteration 84, loss = 0.64124602\n",
            "Iteration 85, loss = 0.65539300\n",
            "Iteration 86, loss = 0.66335718\n",
            "Iteration 87, loss = 0.63316028\n",
            "Iteration 88, loss = 0.67118143\n",
            "Iteration 89, loss = 0.72313730\n",
            "Iteration 90, loss = 0.65899789\n",
            "Iteration 91, loss = 0.62143305\n",
            "Iteration 92, loss = 0.63141355\n",
            "Iteration 93, loss = 0.62236415\n",
            "Iteration 94, loss = 0.62039521\n",
            "Iteration 95, loss = 0.64152766\n",
            "Iteration 96, loss = 0.63327823\n",
            "Iteration 97, loss = 0.63349940\n",
            "Iteration 98, loss = 0.63316499\n",
            "Iteration 99, loss = 0.64784168\n",
            "Iteration 100, loss = 0.64491512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39410846\n",
            "Iteration 2, loss = 1.26532904\n",
            "Iteration 3, loss = 1.24003309\n",
            "Iteration 4, loss = 1.22990654\n",
            "Iteration 5, loss = 1.21092843\n",
            "Iteration 6, loss = 1.19359548\n",
            "Iteration 7, loss = 1.16861919\n",
            "Iteration 8, loss = 1.15215502\n",
            "Iteration 9, loss = 1.13788221\n",
            "Iteration 10, loss = 1.11292999\n",
            "Iteration 11, loss = 1.08082526\n",
            "Iteration 12, loss = 1.07154172\n",
            "Iteration 13, loss = 1.06601241\n",
            "Iteration 14, loss = 1.02940379\n",
            "Iteration 15, loss = 1.02326780\n",
            "Iteration 16, loss = 1.00705998\n",
            "Iteration 17, loss = 1.00933653\n",
            "Iteration 18, loss = 1.01788670\n",
            "Iteration 19, loss = 1.00433767\n",
            "Iteration 20, loss = 0.98792767\n",
            "Iteration 21, loss = 0.95431084\n",
            "Iteration 22, loss = 0.96736056\n",
            "Iteration 23, loss = 0.98723556\n",
            "Iteration 24, loss = 0.97574889\n",
            "Iteration 25, loss = 0.93493921\n",
            "Iteration 26, loss = 0.91645326\n",
            "Iteration 27, loss = 0.87680579\n",
            "Iteration 28, loss = 0.86840156\n",
            "Iteration 29, loss = 0.86276517\n",
            "Iteration 30, loss = 0.86840761\n",
            "Iteration 31, loss = 0.83728715\n",
            "Iteration 32, loss = 0.82383423\n",
            "Iteration 33, loss = 0.82578889\n",
            "Iteration 34, loss = 0.80747113\n",
            "Iteration 35, loss = 0.82273546\n",
            "Iteration 36, loss = 0.81785302\n",
            "Iteration 37, loss = 0.80241124\n",
            "Iteration 38, loss = 0.78298892\n",
            "Iteration 39, loss = 0.77416312\n",
            "Iteration 40, loss = 0.75990916\n",
            "Iteration 41, loss = 0.75971293\n",
            "Iteration 42, loss = 0.75824656\n",
            "Iteration 43, loss = 0.73092316\n",
            "Iteration 44, loss = 0.72343322\n",
            "Iteration 45, loss = 0.71852429\n",
            "Iteration 46, loss = 0.74447116\n",
            "Iteration 47, loss = 0.74868891\n",
            "Iteration 48, loss = 0.82120601\n",
            "Iteration 49, loss = 0.80182661\n",
            "Iteration 50, loss = 0.79670079\n",
            "Iteration 51, loss = 0.74879769\n",
            "Iteration 52, loss = 0.70709519\n",
            "Iteration 53, loss = 0.70008511\n",
            "Iteration 54, loss = 0.70647978\n",
            "Iteration 55, loss = 0.73393040\n",
            "Iteration 56, loss = 0.72835722\n",
            "Iteration 57, loss = 0.70895890\n",
            "Iteration 58, loss = 0.71738703\n",
            "Iteration 59, loss = 0.69864692\n",
            "Iteration 60, loss = 0.68135995\n",
            "Iteration 61, loss = 0.74208664\n",
            "Iteration 62, loss = 0.72442529\n",
            "Iteration 63, loss = 0.68580943\n",
            "Iteration 64, loss = 0.69125940\n",
            "Iteration 65, loss = 0.69743749\n",
            "Iteration 66, loss = 0.67436706\n",
            "Iteration 67, loss = 0.70278897\n",
            "Iteration 68, loss = 0.69020143\n",
            "Iteration 69, loss = 0.64502580\n",
            "Iteration 70, loss = 0.63977437\n",
            "Iteration 71, loss = 0.64169992\n",
            "Iteration 72, loss = 0.65425251\n",
            "Iteration 73, loss = 0.65238789\n",
            "Iteration 74, loss = 0.64499661\n",
            "Iteration 75, loss = 0.62770217\n",
            "Iteration 76, loss = 0.63461711\n",
            "Iteration 77, loss = 0.65056847\n",
            "Iteration 78, loss = 0.67939430\n",
            "Iteration 79, loss = 0.65316755\n",
            "Iteration 80, loss = 0.63568701\n",
            "Iteration 81, loss = 0.64878286\n",
            "Iteration 82, loss = 0.63841237\n",
            "Iteration 83, loss = 0.65658301\n",
            "Iteration 84, loss = 0.62371892\n",
            "Iteration 85, loss = 0.60651887\n",
            "Iteration 86, loss = 0.61685265\n",
            "Iteration 87, loss = 0.60907284\n",
            "Iteration 88, loss = 0.61339748\n",
            "Iteration 89, loss = 0.63302293\n",
            "Iteration 90, loss = 0.61420276\n",
            "Iteration 91, loss = 0.59278826\n",
            "Iteration 92, loss = 0.58617033\n",
            "Iteration 93, loss = 0.58661652\n",
            "Iteration 94, loss = 0.59065158\n",
            "Iteration 95, loss = 0.61526590\n",
            "Iteration 96, loss = 0.61598877\n",
            "Iteration 97, loss = 0.66097952\n",
            "Iteration 98, loss = 0.62418914\n",
            "Iteration 99, loss = 0.61196968\n",
            "Iteration 100, loss = 0.63038794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38994669\n",
            "Iteration 2, loss = 1.25820480\n",
            "Iteration 3, loss = 1.22650249\n",
            "Iteration 4, loss = 1.22935859\n",
            "Iteration 5, loss = 1.20347565\n",
            "Iteration 6, loss = 1.18256777\n",
            "Iteration 7, loss = 1.16826366\n",
            "Iteration 8, loss = 1.14419037\n",
            "Iteration 9, loss = 1.13348429\n",
            "Iteration 10, loss = 1.10745680\n",
            "Iteration 11, loss = 1.09191161\n",
            "Iteration 12, loss = 1.08623270\n",
            "Iteration 13, loss = 1.04687077\n",
            "Iteration 14, loss = 1.04715277\n",
            "Iteration 15, loss = 1.01546131\n",
            "Iteration 16, loss = 0.99341311\n",
            "Iteration 17, loss = 0.99052114\n",
            "Iteration 18, loss = 0.98685686\n",
            "Iteration 19, loss = 0.97065781\n",
            "Iteration 20, loss = 0.95077982\n",
            "Iteration 21, loss = 0.93133242\n",
            "Iteration 22, loss = 0.93629830\n",
            "Iteration 23, loss = 0.97143037\n",
            "Iteration 24, loss = 0.94865834\n",
            "Iteration 25, loss = 0.91608411\n",
            "Iteration 26, loss = 0.93050808\n",
            "Iteration 27, loss = 0.88663458\n",
            "Iteration 28, loss = 0.87785293\n",
            "Iteration 29, loss = 0.85730861\n",
            "Iteration 30, loss = 0.86224539\n",
            "Iteration 31, loss = 0.84139802\n",
            "Iteration 32, loss = 0.82360271\n",
            "Iteration 33, loss = 0.84159655\n",
            "Iteration 34, loss = 0.81666200\n",
            "Iteration 35, loss = 0.86053758\n",
            "Iteration 36, loss = 0.82757014\n",
            "Iteration 37, loss = 0.80766379\n",
            "Iteration 38, loss = 0.78547416\n",
            "Iteration 39, loss = 0.76976605\n",
            "Iteration 40, loss = 0.77520688\n",
            "Iteration 41, loss = 0.75843845\n",
            "Iteration 42, loss = 0.75380407\n",
            "Iteration 43, loss = 0.74517943\n",
            "Iteration 44, loss = 0.74344813\n",
            "Iteration 45, loss = 0.72552709\n",
            "Iteration 46, loss = 0.74077989\n",
            "Iteration 47, loss = 0.74416490\n",
            "Iteration 48, loss = 0.77708573\n",
            "Iteration 49, loss = 0.74169514\n",
            "Iteration 50, loss = 0.76508687\n",
            "Iteration 51, loss = 0.74292468\n",
            "Iteration 52, loss = 0.72266704\n",
            "Iteration 53, loss = 0.69794251\n",
            "Iteration 54, loss = 0.73359612\n",
            "Iteration 55, loss = 0.77419718\n",
            "Iteration 56, loss = 0.75046288\n",
            "Iteration 57, loss = 0.71514840\n",
            "Iteration 58, loss = 0.68731349\n",
            "Iteration 59, loss = 0.67863341\n",
            "Iteration 60, loss = 0.72996787\n",
            "Iteration 61, loss = 0.70456974\n",
            "Iteration 62, loss = 0.66861179\n",
            "Iteration 63, loss = 0.65660822\n",
            "Iteration 64, loss = 0.65774765\n",
            "Iteration 65, loss = 0.67853737\n",
            "Iteration 66, loss = 0.68092055\n",
            "Iteration 67, loss = 0.69262273\n",
            "Iteration 68, loss = 0.65823194\n",
            "Iteration 69, loss = 0.64509163\n",
            "Iteration 70, loss = 0.65729869\n",
            "Iteration 71, loss = 0.68060103\n",
            "Iteration 72, loss = 0.72285164\n",
            "Iteration 73, loss = 0.74054969\n",
            "Iteration 74, loss = 0.69424847\n",
            "Iteration 75, loss = 0.66571106\n",
            "Iteration 76, loss = 0.63139402\n",
            "Iteration 77, loss = 0.62098476\n",
            "Iteration 78, loss = 0.63575438\n",
            "Iteration 79, loss = 0.62553436\n",
            "Iteration 80, loss = 0.66480011\n",
            "Iteration 81, loss = 0.63007066\n",
            "Iteration 82, loss = 0.62580870\n",
            "Iteration 83, loss = 0.62033607\n",
            "Iteration 84, loss = 0.60904600\n",
            "Iteration 85, loss = 0.60967805\n",
            "Iteration 86, loss = 0.60671354\n",
            "Iteration 87, loss = 0.61188684\n",
            "Iteration 88, loss = 0.58661150\n",
            "Iteration 89, loss = 0.59353403\n",
            "Iteration 90, loss = 0.59783797\n",
            "Iteration 91, loss = 0.58953248\n",
            "Iteration 92, loss = 0.59179580\n",
            "Iteration 93, loss = 0.57996300\n",
            "Iteration 94, loss = 0.60768802\n",
            "Iteration 95, loss = 0.63239454\n",
            "Iteration 96, loss = 0.61619010\n",
            "Iteration 97, loss = 0.66086043\n",
            "Iteration 98, loss = 0.60964059\n",
            "Iteration 99, loss = 0.57598452\n",
            "Iteration 100, loss = 0.58138053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39676334\n",
            "Iteration 2, loss = 1.25450012\n",
            "Iteration 3, loss = 1.23526842\n",
            "Iteration 4, loss = 1.23697707\n",
            "Iteration 5, loss = 1.21893555\n",
            "Iteration 6, loss = 1.20560740\n",
            "Iteration 7, loss = 1.19238765\n",
            "Iteration 8, loss = 1.17952617\n",
            "Iteration 9, loss = 1.17300673\n",
            "Iteration 10, loss = 1.15133640\n",
            "Iteration 11, loss = 1.13944789\n",
            "Iteration 12, loss = 1.11731348\n",
            "Iteration 13, loss = 1.08719441\n",
            "Iteration 14, loss = 1.09115557\n",
            "Iteration 15, loss = 1.05177365\n",
            "Iteration 16, loss = 1.03488449\n",
            "Iteration 17, loss = 1.00955329\n",
            "Iteration 18, loss = 1.00796598\n",
            "Iteration 19, loss = 1.00007671\n",
            "Iteration 20, loss = 0.96721740\n",
            "Iteration 21, loss = 0.95812366\n",
            "Iteration 22, loss = 0.98694536\n",
            "Iteration 23, loss = 1.00751920\n",
            "Iteration 24, loss = 0.97979108\n",
            "Iteration 25, loss = 0.93933355\n",
            "Iteration 26, loss = 0.93293899\n",
            "Iteration 27, loss = 0.89557745\n",
            "Iteration 28, loss = 0.89515183\n",
            "Iteration 29, loss = 0.87179075\n",
            "Iteration 30, loss = 0.87405960\n",
            "Iteration 31, loss = 0.84882026\n",
            "Iteration 32, loss = 0.83104290\n",
            "Iteration 33, loss = 0.86293105\n",
            "Iteration 34, loss = 0.84824644\n",
            "Iteration 35, loss = 0.92631121\n",
            "Iteration 36, loss = 0.85172445\n",
            "Iteration 37, loss = 0.84232099\n",
            "Iteration 38, loss = 0.79990357\n",
            "Iteration 39, loss = 0.79469895\n",
            "Iteration 40, loss = 0.79699629\n",
            "Iteration 41, loss = 0.77933372\n",
            "Iteration 42, loss = 0.76737771\n",
            "Iteration 43, loss = 0.76895608\n",
            "Iteration 44, loss = 0.75300466\n",
            "Iteration 45, loss = 0.75073197\n",
            "Iteration 46, loss = 0.75894970\n",
            "Iteration 47, loss = 0.75528738\n",
            "Iteration 48, loss = 0.75450869\n",
            "Iteration 49, loss = 0.73444199\n",
            "Iteration 50, loss = 0.72613925\n",
            "Iteration 51, loss = 0.72358158\n",
            "Iteration 52, loss = 0.71981706\n",
            "Iteration 53, loss = 0.72393810\n",
            "Iteration 54, loss = 0.70339229\n",
            "Iteration 55, loss = 0.74260849\n",
            "Iteration 56, loss = 0.74894036\n",
            "Iteration 57, loss = 0.73349756\n",
            "Iteration 58, loss = 0.72148622\n",
            "Iteration 59, loss = 0.69149602\n",
            "Iteration 60, loss = 0.74298222\n",
            "Iteration 61, loss = 0.73056496\n",
            "Iteration 62, loss = 0.69053294\n",
            "Iteration 63, loss = 0.68071683\n",
            "Iteration 64, loss = 0.68386068\n",
            "Iteration 65, loss = 0.71104167\n",
            "Iteration 66, loss = 0.71852799\n",
            "Iteration 67, loss = 0.76473622\n",
            "Iteration 68, loss = 0.68850782\n",
            "Iteration 69, loss = 0.65799097\n",
            "Iteration 70, loss = 0.67706638\n",
            "Iteration 71, loss = 0.71298209\n",
            "Iteration 72, loss = 0.72506552\n",
            "Iteration 73, loss = 0.74128750\n",
            "Iteration 74, loss = 0.69891044\n",
            "Iteration 75, loss = 0.67463582\n",
            "Iteration 76, loss = 0.66179312\n",
            "Iteration 77, loss = 0.64843855\n",
            "Iteration 78, loss = 0.64570349\n",
            "Iteration 79, loss = 0.63218792\n",
            "Iteration 80, loss = 0.69120244\n",
            "Iteration 81, loss = 0.63585299\n",
            "Iteration 82, loss = 0.63251063\n",
            "Iteration 83, loss = 0.61772745\n",
            "Iteration 84, loss = 0.62509309\n",
            "Iteration 85, loss = 0.62426565\n",
            "Iteration 86, loss = 0.62646179\n",
            "Iteration 87, loss = 0.63837721\n",
            "Iteration 88, loss = 0.61480050\n",
            "Iteration 89, loss = 0.59589104\n",
            "Iteration 90, loss = 0.59662239\n",
            "Iteration 91, loss = 0.59441914\n",
            "Iteration 92, loss = 0.58890897\n",
            "Iteration 93, loss = 0.59923678\n",
            "Iteration 94, loss = 0.59293894\n",
            "Iteration 95, loss = 0.58916566\n",
            "Iteration 96, loss = 0.59319266\n",
            "Iteration 97, loss = 0.64900925\n",
            "Iteration 98, loss = 0.63694611\n",
            "Iteration 99, loss = 0.59957173\n",
            "Iteration 100, loss = 0.56007662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39835941\n",
            "Iteration 2, loss = 1.26797557\n",
            "Iteration 3, loss = 1.23400236\n",
            "Iteration 4, loss = 1.21665702\n",
            "Iteration 5, loss = 1.20936301\n",
            "Iteration 6, loss = 1.18407715\n",
            "Iteration 7, loss = 1.16259974\n",
            "Iteration 8, loss = 1.13951200\n",
            "Iteration 9, loss = 1.14059181\n",
            "Iteration 10, loss = 1.11670810\n",
            "Iteration 11, loss = 1.11023919\n",
            "Iteration 12, loss = 1.07646826\n",
            "Iteration 13, loss = 1.06121148\n",
            "Iteration 14, loss = 1.05289406\n",
            "Iteration 15, loss = 1.02575864\n",
            "Iteration 16, loss = 1.06385959\n",
            "Iteration 17, loss = 1.02443638\n",
            "Iteration 18, loss = 1.00669760\n",
            "Iteration 19, loss = 0.98163771\n",
            "Iteration 20, loss = 0.97601153\n",
            "Iteration 21, loss = 0.95272742\n",
            "Iteration 22, loss = 0.94846515\n",
            "Iteration 23, loss = 0.98466543\n",
            "Iteration 24, loss = 0.96762358\n",
            "Iteration 25, loss = 0.92379687\n",
            "Iteration 26, loss = 0.92013016\n",
            "Iteration 27, loss = 0.88395996\n",
            "Iteration 28, loss = 0.88934277\n",
            "Iteration 29, loss = 0.87022725\n",
            "Iteration 30, loss = 0.88111737\n",
            "Iteration 31, loss = 0.87038735\n",
            "Iteration 32, loss = 0.85055172\n",
            "Iteration 33, loss = 0.84105506\n",
            "Iteration 34, loss = 0.84198285\n",
            "Iteration 35, loss = 0.80435073\n",
            "Iteration 36, loss = 0.80226362\n",
            "Iteration 37, loss = 0.77864513\n",
            "Iteration 38, loss = 0.77333828\n",
            "Iteration 39, loss = 0.78069426\n",
            "Iteration 40, loss = 0.78350212\n",
            "Iteration 41, loss = 0.75835338\n",
            "Iteration 42, loss = 0.74798508\n",
            "Iteration 43, loss = 0.73725530\n",
            "Iteration 44, loss = 0.74036306\n",
            "Iteration 45, loss = 0.74869221\n",
            "Iteration 46, loss = 0.75204918\n",
            "Iteration 47, loss = 0.72648103\n",
            "Iteration 48, loss = 0.73785898\n",
            "Iteration 49, loss = 0.74787648\n",
            "Iteration 50, loss = 0.71318215\n",
            "Iteration 51, loss = 0.73077995\n",
            "Iteration 52, loss = 0.70833525\n",
            "Iteration 53, loss = 0.72502134\n",
            "Iteration 54, loss = 0.70537276\n",
            "Iteration 55, loss = 0.70002518\n",
            "Iteration 56, loss = 0.71446505\n",
            "Iteration 57, loss = 0.68503405\n",
            "Iteration 58, loss = 0.69174713\n",
            "Iteration 59, loss = 0.68574646\n",
            "Iteration 60, loss = 0.67014550\n",
            "Iteration 61, loss = 0.66451475\n",
            "Iteration 62, loss = 0.65510809\n",
            "Iteration 63, loss = 0.69438230\n",
            "Iteration 64, loss = 0.70975100\n",
            "Iteration 65, loss = 0.67375345\n",
            "Iteration 66, loss = 0.68046365\n",
            "Iteration 67, loss = 0.66331911\n",
            "Iteration 68, loss = 0.63937561\n",
            "Iteration 69, loss = 0.66063792\n",
            "Iteration 70, loss = 0.64382432\n",
            "Iteration 71, loss = 0.65255753\n",
            "Iteration 72, loss = 0.63947239\n",
            "Iteration 73, loss = 0.66087365\n",
            "Iteration 74, loss = 0.65234834\n",
            "Iteration 75, loss = 0.64123673\n",
            "Iteration 76, loss = 0.63667060\n",
            "Iteration 77, loss = 0.63275056\n",
            "Iteration 78, loss = 0.63579704\n",
            "Iteration 79, loss = 0.63141802\n",
            "Iteration 80, loss = 0.60790250\n",
            "Iteration 81, loss = 0.60758319\n",
            "Iteration 82, loss = 0.60547496\n",
            "Iteration 83, loss = 0.60920811\n",
            "Iteration 84, loss = 0.60709167\n",
            "Iteration 85, loss = 0.59238451\n",
            "Iteration 86, loss = 0.60070735\n",
            "Iteration 87, loss = 0.64963659\n",
            "Iteration 88, loss = 0.66048153\n",
            "Iteration 89, loss = 0.70820537\n",
            "Iteration 90, loss = 0.65241485\n",
            "Iteration 91, loss = 0.59403087\n",
            "Iteration 92, loss = 0.59892461\n",
            "Iteration 93, loss = 0.63241500\n",
            "Iteration 94, loss = 0.63196945\n",
            "Iteration 95, loss = 0.62426088\n",
            "Iteration 96, loss = 0.61834560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39872808\n",
            "Iteration 2, loss = 1.26486695\n",
            "Iteration 3, loss = 1.23145030\n",
            "Iteration 4, loss = 1.22031842\n",
            "Iteration 5, loss = 1.20547765\n",
            "Iteration 6, loss = 1.18594327\n",
            "Iteration 7, loss = 1.16436737\n",
            "Iteration 8, loss = 1.14465798\n",
            "Iteration 9, loss = 1.15292755\n",
            "Iteration 10, loss = 1.12933878\n",
            "Iteration 11, loss = 1.11881388\n",
            "Iteration 12, loss = 1.07793073\n",
            "Iteration 13, loss = 1.06652843\n",
            "Iteration 14, loss = 1.08565116\n",
            "Iteration 15, loss = 1.04736480\n",
            "Iteration 16, loss = 1.04384843\n",
            "Iteration 17, loss = 1.01594103\n",
            "Iteration 18, loss = 0.98896722\n",
            "Iteration 19, loss = 0.97091340\n",
            "Iteration 20, loss = 0.95882386\n",
            "Iteration 21, loss = 0.94172864\n",
            "Iteration 22, loss = 0.93694382\n",
            "Iteration 23, loss = 0.93145245\n",
            "Iteration 24, loss = 0.92300376\n",
            "Iteration 25, loss = 0.88841271\n",
            "Iteration 26, loss = 0.87790661\n",
            "Iteration 27, loss = 0.87808547\n",
            "Iteration 28, loss = 0.86925081\n",
            "Iteration 29, loss = 0.84395179\n",
            "Iteration 30, loss = 0.85804210\n",
            "Iteration 31, loss = 0.87058866\n",
            "Iteration 32, loss = 0.81740238\n",
            "Iteration 33, loss = 0.80916337\n",
            "Iteration 34, loss = 0.81829278\n",
            "Iteration 35, loss = 0.80415251\n",
            "Iteration 36, loss = 0.80024247\n",
            "Iteration 37, loss = 0.78795322\n",
            "Iteration 38, loss = 0.76065745\n",
            "Iteration 39, loss = 0.76173253\n",
            "Iteration 40, loss = 0.76270470\n",
            "Iteration 41, loss = 0.75813186\n",
            "Iteration 42, loss = 0.75499734\n",
            "Iteration 43, loss = 0.74254857\n",
            "Iteration 44, loss = 0.72984514\n",
            "Iteration 45, loss = 0.75918870\n",
            "Iteration 46, loss = 0.77677525\n",
            "Iteration 47, loss = 0.71522711\n",
            "Iteration 48, loss = 0.71982540\n",
            "Iteration 49, loss = 0.73266155\n",
            "Iteration 50, loss = 0.70805705\n",
            "Iteration 51, loss = 0.68238747\n",
            "Iteration 52, loss = 0.68592394\n",
            "Iteration 53, loss = 0.69731113\n",
            "Iteration 54, loss = 0.68386719\n",
            "Iteration 55, loss = 0.67810973\n",
            "Iteration 56, loss = 0.69446813\n",
            "Iteration 57, loss = 0.68758606\n",
            "Iteration 58, loss = 0.69075166\n",
            "Iteration 59, loss = 0.71090999\n",
            "Iteration 60, loss = 0.68215373\n",
            "Iteration 61, loss = 0.66115809\n",
            "Iteration 62, loss = 0.66064162\n",
            "Iteration 63, loss = 0.70441543\n",
            "Iteration 64, loss = 0.67318097\n",
            "Iteration 65, loss = 0.64345406\n",
            "Iteration 66, loss = 0.65412989\n",
            "Iteration 67, loss = 0.63458580\n",
            "Iteration 68, loss = 0.61990933\n",
            "Iteration 69, loss = 0.63623403\n",
            "Iteration 70, loss = 0.61348729\n",
            "Iteration 71, loss = 0.64466436\n",
            "Iteration 72, loss = 0.61609040\n",
            "Iteration 73, loss = 0.64668631\n",
            "Iteration 74, loss = 0.63990373\n",
            "Iteration 75, loss = 0.63002214\n",
            "Iteration 76, loss = 0.64342436\n",
            "Iteration 77, loss = 0.64677468\n",
            "Iteration 78, loss = 0.64323082\n",
            "Iteration 79, loss = 0.63078357\n",
            "Iteration 80, loss = 0.60958148\n",
            "Iteration 81, loss = 0.59804727\n",
            "Iteration 82, loss = 0.62127431\n",
            "Iteration 83, loss = 0.59076369\n",
            "Iteration 84, loss = 0.57231822\n",
            "Iteration 85, loss = 0.58015809\n",
            "Iteration 86, loss = 0.57612365\n",
            "Iteration 87, loss = 0.61571194\n",
            "Iteration 88, loss = 0.60669870\n",
            "Iteration 89, loss = 0.64122752\n",
            "Iteration 90, loss = 0.60489984\n",
            "Iteration 91, loss = 0.59008381\n",
            "Iteration 92, loss = 0.62816061\n",
            "Iteration 93, loss = 0.65688364\n",
            "Iteration 94, loss = 0.64680508\n",
            "Iteration 95, loss = 0.64530688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.40007297\n",
            "Iteration 2, loss = 1.25283821\n",
            "Iteration 3, loss = 1.21698600\n",
            "Iteration 4, loss = 1.20088288\n",
            "Iteration 5, loss = 1.18852993\n",
            "Iteration 6, loss = 1.17122544\n",
            "Iteration 7, loss = 1.15455508\n",
            "Iteration 8, loss = 1.13508118\n",
            "Iteration 9, loss = 1.13531302\n",
            "Iteration 10, loss = 1.11421673\n",
            "Iteration 11, loss = 1.08968832\n",
            "Iteration 12, loss = 1.05474915\n",
            "Iteration 13, loss = 1.04797960\n",
            "Iteration 14, loss = 1.04948742\n",
            "Iteration 15, loss = 1.04332254\n",
            "Iteration 16, loss = 1.05292904\n",
            "Iteration 17, loss = 1.02444501\n",
            "Iteration 18, loss = 1.00630345\n",
            "Iteration 19, loss = 0.97406265\n",
            "Iteration 20, loss = 0.97047505\n",
            "Iteration 21, loss = 0.95373254\n",
            "Iteration 22, loss = 0.96259081\n",
            "Iteration 23, loss = 0.99084902\n",
            "Iteration 24, loss = 0.96658015\n",
            "Iteration 25, loss = 0.92541101\n",
            "Iteration 26, loss = 0.90014363\n",
            "Iteration 27, loss = 0.90339038\n",
            "Iteration 28, loss = 0.87313842\n",
            "Iteration 29, loss = 0.86946229\n",
            "Iteration 30, loss = 0.85870630\n",
            "Iteration 31, loss = 0.86360031\n",
            "Iteration 32, loss = 0.84265896\n",
            "Iteration 33, loss = 0.81983473\n",
            "Iteration 34, loss = 0.82740972\n",
            "Iteration 35, loss = 0.80654492\n",
            "Iteration 36, loss = 0.80076530\n",
            "Iteration 37, loss = 0.78285996\n",
            "Iteration 38, loss = 0.77716967\n",
            "Iteration 39, loss = 0.80099089\n",
            "Iteration 40, loss = 0.78310257\n",
            "Iteration 41, loss = 0.76926502\n",
            "Iteration 42, loss = 0.75942341\n",
            "Iteration 43, loss = 0.76430649\n",
            "Iteration 44, loss = 0.78529599\n",
            "Iteration 45, loss = 0.79372147\n",
            "Iteration 46, loss = 0.81076141\n",
            "Iteration 47, loss = 0.74524186\n",
            "Iteration 48, loss = 0.75567805\n",
            "Iteration 49, loss = 0.77613831\n",
            "Iteration 50, loss = 0.72679782\n",
            "Iteration 51, loss = 0.72019126\n",
            "Iteration 52, loss = 0.71316180\n",
            "Iteration 53, loss = 0.71984421\n",
            "Iteration 54, loss = 0.71095920\n",
            "Iteration 55, loss = 0.71883313\n",
            "Iteration 56, loss = 0.72256056\n",
            "Iteration 57, loss = 0.70787947\n",
            "Iteration 58, loss = 0.70938383\n",
            "Iteration 59, loss = 0.71557757\n",
            "Iteration 60, loss = 0.70401103\n",
            "Iteration 61, loss = 0.69829433\n",
            "Iteration 62, loss = 0.68535596\n",
            "Iteration 63, loss = 0.72613642\n",
            "Iteration 64, loss = 0.68570809\n",
            "Iteration 65, loss = 0.66758600\n",
            "Iteration 66, loss = 0.66576825\n",
            "Iteration 67, loss = 0.67375441\n",
            "Iteration 68, loss = 0.65521489\n",
            "Iteration 69, loss = 0.64958787\n",
            "Iteration 70, loss = 0.66622529\n",
            "Iteration 71, loss = 0.64741288\n",
            "Iteration 72, loss = 0.63442712\n",
            "Iteration 73, loss = 0.64541403\n",
            "Iteration 74, loss = 0.64240749\n",
            "Iteration 75, loss = 0.65520913\n",
            "Iteration 76, loss = 0.64157842\n",
            "Iteration 77, loss = 0.64042832\n",
            "Iteration 78, loss = 0.64980676\n",
            "Iteration 79, loss = 0.64895078\n",
            "Iteration 80, loss = 0.64141754\n",
            "Iteration 81, loss = 0.62525563\n",
            "Iteration 82, loss = 0.62220255\n",
            "Iteration 83, loss = 0.61861431\n",
            "Iteration 84, loss = 0.60589563\n",
            "Iteration 85, loss = 0.60316771\n",
            "Iteration 86, loss = 0.60594873\n",
            "Iteration 87, loss = 0.63059499\n",
            "Iteration 88, loss = 0.62456385\n",
            "Iteration 89, loss = 0.62729112\n",
            "Iteration 90, loss = 0.60108802\n",
            "Iteration 91, loss = 0.64332504\n",
            "Iteration 92, loss = 0.62797510\n",
            "Iteration 93, loss = 0.68454941\n",
            "Iteration 94, loss = 0.65566790\n",
            "Iteration 95, loss = 0.66541282\n",
            "Iteration 96, loss = 0.59995731\n",
            "Iteration 97, loss = 0.60594088\n",
            "Iteration 98, loss = 0.59327841\n",
            "Iteration 99, loss = 0.57505791\n",
            "Iteration 100, loss = 0.58814856\n",
            "14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28993069\n",
            "Iteration 2, loss = 1.24153351\n",
            "Iteration 3, loss = 1.23146732\n",
            "Iteration 4, loss = 1.23894141\n",
            "Iteration 5, loss = 1.22696927\n",
            "Iteration 6, loss = 1.21921798\n",
            "Iteration 7, loss = 1.22559934\n",
            "Iteration 8, loss = 1.21613402\n",
            "Iteration 9, loss = 1.20796606\n",
            "Iteration 10, loss = 1.21088523\n",
            "Iteration 11, loss = 1.21536015\n",
            "Iteration 12, loss = 1.20479726\n",
            "Iteration 13, loss = 1.21076940\n",
            "Iteration 14, loss = 1.20749084\n",
            "Iteration 15, loss = 1.19934472\n",
            "Iteration 16, loss = 1.20594615\n",
            "Iteration 17, loss = 1.19572030\n",
            "Iteration 18, loss = 1.19854786\n",
            "Iteration 19, loss = 1.20913752\n",
            "Iteration 20, loss = 1.18974594\n",
            "Iteration 21, loss = 1.19011281\n",
            "Iteration 22, loss = 1.18223297\n",
            "Iteration 23, loss = 1.18560628\n",
            "Iteration 24, loss = 1.17736432\n",
            "Iteration 25, loss = 1.16889644\n",
            "Iteration 26, loss = 1.16911099\n",
            "Iteration 27, loss = 1.15646386\n",
            "Iteration 28, loss = 1.15871264\n",
            "Iteration 29, loss = 1.15194071\n",
            "Iteration 30, loss = 1.16408924\n",
            "Iteration 31, loss = 1.17503769\n",
            "Iteration 32, loss = 1.16252021\n",
            "Iteration 33, loss = 1.14071315\n",
            "Iteration 34, loss = 1.15392097\n",
            "Iteration 35, loss = 1.14243957\n",
            "Iteration 36, loss = 1.12910241\n",
            "Iteration 37, loss = 1.12537072\n",
            "Iteration 38, loss = 1.12964475\n",
            "Iteration 39, loss = 1.13453654\n",
            "Iteration 40, loss = 1.13219306\n",
            "Iteration 41, loss = 1.14168505\n",
            "Iteration 42, loss = 1.14237639\n",
            "Iteration 43, loss = 1.15675314\n",
            "Iteration 44, loss = 1.12466049\n",
            "Iteration 45, loss = 1.11969288\n",
            "Iteration 46, loss = 1.13057271\n",
            "Iteration 47, loss = 1.10348686\n",
            "Iteration 48, loss = 1.10243036\n",
            "Iteration 49, loss = 1.09482352\n",
            "Iteration 50, loss = 1.10321114\n",
            "Iteration 51, loss = 1.08936587\n",
            "Iteration 52, loss = 1.09971570\n",
            "Iteration 53, loss = 1.15126837\n",
            "Iteration 54, loss = 1.13761457\n",
            "Iteration 55, loss = 1.08484543\n",
            "Iteration 56, loss = 1.09236171\n",
            "Iteration 57, loss = 1.07665054\n",
            "Iteration 58, loss = 1.08189152\n",
            "Iteration 59, loss = 1.08475364\n",
            "Iteration 60, loss = 1.05785701\n",
            "Iteration 61, loss = 1.04190701\n",
            "Iteration 62, loss = 1.07645066\n",
            "Iteration 63, loss = 1.04758799\n",
            "Iteration 64, loss = 1.03966748\n",
            "Iteration 65, loss = 1.08029099\n",
            "Iteration 66, loss = 1.05162723\n",
            "Iteration 67, loss = 1.02901254\n",
            "Iteration 68, loss = 1.03323178\n",
            "Iteration 69, loss = 1.01587796\n",
            "Iteration 70, loss = 1.02804832\n",
            "Iteration 71, loss = 1.02447891\n",
            "Iteration 72, loss = 1.01585806\n",
            "Iteration 73, loss = 1.00370699\n",
            "Iteration 74, loss = 1.01483311\n",
            "Iteration 75, loss = 1.00849040\n",
            "Iteration 76, loss = 0.98525103\n",
            "Iteration 77, loss = 0.98206944\n",
            "Iteration 78, loss = 0.99549640\n",
            "Iteration 79, loss = 0.99714008\n",
            "Iteration 80, loss = 0.99420398\n",
            "Iteration 81, loss = 1.00178382\n",
            "Iteration 82, loss = 0.98555146\n",
            "Iteration 83, loss = 0.97676897\n",
            "Iteration 84, loss = 0.96454063\n",
            "Iteration 85, loss = 0.98138864\n",
            "Iteration 86, loss = 0.95073817\n",
            "Iteration 87, loss = 0.98486495\n",
            "Iteration 88, loss = 0.97885122\n",
            "Iteration 89, loss = 1.01853380\n",
            "Iteration 90, loss = 0.96114743\n",
            "Iteration 91, loss = 0.94100019\n",
            "Iteration 92, loss = 0.98882060\n",
            "Iteration 93, loss = 1.01679871\n",
            "Iteration 94, loss = 0.97304480\n",
            "Iteration 95, loss = 0.95548303\n",
            "Iteration 96, loss = 0.96323592\n",
            "Iteration 97, loss = 0.97804263\n",
            "Iteration 98, loss = 0.94221201\n",
            "Iteration 99, loss = 0.94519686\n",
            "Iteration 100, loss = 0.99460993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28817560\n",
            "Iteration 2, loss = 1.24302096\n",
            "Iteration 3, loss = 1.22803023\n",
            "Iteration 4, loss = 1.22819753\n",
            "Iteration 5, loss = 1.22818519\n",
            "Iteration 6, loss = 1.21787977\n",
            "Iteration 7, loss = 1.21752419\n",
            "Iteration 8, loss = 1.21948396\n",
            "Iteration 9, loss = 1.21422930\n",
            "Iteration 10, loss = 1.20404011\n",
            "Iteration 11, loss = 1.21290982\n",
            "Iteration 12, loss = 1.21104726\n",
            "Iteration 13, loss = 1.21403251\n",
            "Iteration 14, loss = 1.21979158\n",
            "Iteration 15, loss = 1.20557090\n",
            "Iteration 16, loss = 1.19688991\n",
            "Iteration 17, loss = 1.19162198\n",
            "Iteration 18, loss = 1.18890806\n",
            "Iteration 19, loss = 1.19136077\n",
            "Iteration 20, loss = 1.17934675\n",
            "Iteration 21, loss = 1.17556270\n",
            "Iteration 22, loss = 1.17740944\n",
            "Iteration 23, loss = 1.16881736\n",
            "Iteration 24, loss = 1.16410554\n",
            "Iteration 25, loss = 1.15811338\n",
            "Iteration 26, loss = 1.15019487\n",
            "Iteration 27, loss = 1.13897208\n",
            "Iteration 28, loss = 1.14985566\n",
            "Iteration 29, loss = 1.14735633\n",
            "Iteration 30, loss = 1.14212940\n",
            "Iteration 31, loss = 1.15167293\n",
            "Iteration 32, loss = 1.14316433\n",
            "Iteration 33, loss = 1.13006929\n",
            "Iteration 34, loss = 1.12570696\n",
            "Iteration 35, loss = 1.11704548\n",
            "Iteration 36, loss = 1.12078896\n",
            "Iteration 37, loss = 1.10971103\n",
            "Iteration 38, loss = 1.17451987\n",
            "Iteration 39, loss = 1.15699152\n",
            "Iteration 40, loss = 1.12871774\n",
            "Iteration 41, loss = 1.12133060\n",
            "Iteration 42, loss = 1.12538124\n",
            "Iteration 43, loss = 1.12359163\n",
            "Iteration 44, loss = 1.09598891\n",
            "Iteration 45, loss = 1.09511899\n",
            "Iteration 46, loss = 1.10883030\n",
            "Iteration 47, loss = 1.08433671\n",
            "Iteration 48, loss = 1.09294774\n",
            "Iteration 49, loss = 1.10247836\n",
            "Iteration 50, loss = 1.08809782\n",
            "Iteration 51, loss = 1.07923902\n",
            "Iteration 52, loss = 1.09346000\n",
            "Iteration 53, loss = 1.13772739\n",
            "Iteration 54, loss = 1.12117662\n",
            "Iteration 55, loss = 1.09673102\n",
            "Iteration 56, loss = 1.09575918\n",
            "Iteration 57, loss = 1.08622897\n",
            "Iteration 58, loss = 1.07239210\n",
            "Iteration 59, loss = 1.06409605\n",
            "Iteration 60, loss = 1.05192235\n",
            "Iteration 61, loss = 1.04661138\n",
            "Iteration 62, loss = 1.05776484\n",
            "Iteration 63, loss = 1.04020716\n",
            "Iteration 64, loss = 1.03424421\n",
            "Iteration 65, loss = 1.06323841\n",
            "Iteration 66, loss = 1.03773187\n",
            "Iteration 67, loss = 1.04466692\n",
            "Iteration 68, loss = 1.02973279\n",
            "Iteration 69, loss = 1.01900828\n",
            "Iteration 70, loss = 1.01469398\n",
            "Iteration 71, loss = 1.01687876\n",
            "Iteration 72, loss = 1.02461747\n",
            "Iteration 73, loss = 1.00423450\n",
            "Iteration 74, loss = 0.99819031\n",
            "Iteration 75, loss = 0.99199327\n",
            "Iteration 76, loss = 0.99727834\n",
            "Iteration 77, loss = 0.98931007\n",
            "Iteration 78, loss = 1.00502415\n",
            "Iteration 79, loss = 0.99648924\n",
            "Iteration 80, loss = 0.98806834\n",
            "Iteration 81, loss = 1.04420114\n",
            "Iteration 82, loss = 0.99184312\n",
            "Iteration 83, loss = 0.98565666\n",
            "Iteration 84, loss = 0.99924228\n",
            "Iteration 85, loss = 0.99621349\n",
            "Iteration 86, loss = 0.99298822\n",
            "Iteration 87, loss = 0.98965534\n",
            "Iteration 88, loss = 1.00032762\n",
            "Iteration 89, loss = 1.03936443\n",
            "Iteration 90, loss = 0.99658316\n",
            "Iteration 91, loss = 0.99860500\n",
            "Iteration 92, loss = 0.98368507\n",
            "Iteration 93, loss = 1.01730871\n",
            "Iteration 94, loss = 1.03597825\n",
            "Iteration 95, loss = 0.98417544\n",
            "Iteration 96, loss = 0.95435510\n",
            "Iteration 97, loss = 0.99506609\n",
            "Iteration 98, loss = 0.96226127\n",
            "Iteration 99, loss = 0.95436402\n",
            "Iteration 100, loss = 0.96400797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29315171\n",
            "Iteration 2, loss = 1.24927128\n",
            "Iteration 3, loss = 1.24133304\n",
            "Iteration 4, loss = 1.23095372\n",
            "Iteration 5, loss = 1.23532777\n",
            "Iteration 6, loss = 1.22510525\n",
            "Iteration 7, loss = 1.23089493\n",
            "Iteration 8, loss = 1.22468764\n",
            "Iteration 9, loss = 1.22688456\n",
            "Iteration 10, loss = 1.21526775\n",
            "Iteration 11, loss = 1.21009206\n",
            "Iteration 12, loss = 1.20875542\n",
            "Iteration 13, loss = 1.20904793\n",
            "Iteration 14, loss = 1.20947491\n",
            "Iteration 15, loss = 1.19946443\n",
            "Iteration 16, loss = 1.19986523\n",
            "Iteration 17, loss = 1.20613043\n",
            "Iteration 18, loss = 1.20453996\n",
            "Iteration 19, loss = 1.19418399\n",
            "Iteration 20, loss = 1.19072037\n",
            "Iteration 21, loss = 1.18049618\n",
            "Iteration 22, loss = 1.18861655\n",
            "Iteration 23, loss = 1.17267688\n",
            "Iteration 24, loss = 1.17583617\n",
            "Iteration 25, loss = 1.16728525\n",
            "Iteration 26, loss = 1.16694243\n",
            "Iteration 27, loss = 1.15501295\n",
            "Iteration 28, loss = 1.16324501\n",
            "Iteration 29, loss = 1.15654453\n",
            "Iteration 30, loss = 1.15139321\n",
            "Iteration 31, loss = 1.16009321\n",
            "Iteration 32, loss = 1.15008008\n",
            "Iteration 33, loss = 1.15290700\n",
            "Iteration 34, loss = 1.13906715\n",
            "Iteration 35, loss = 1.12452213\n",
            "Iteration 36, loss = 1.13203505\n",
            "Iteration 37, loss = 1.13420861\n",
            "Iteration 38, loss = 1.21162221\n",
            "Iteration 39, loss = 1.17236125\n",
            "Iteration 40, loss = 1.13923459\n",
            "Iteration 41, loss = 1.13778803\n",
            "Iteration 42, loss = 1.13269450\n",
            "Iteration 43, loss = 1.12734521\n",
            "Iteration 44, loss = 1.12287184\n",
            "Iteration 45, loss = 1.10771387\n",
            "Iteration 46, loss = 1.13610904\n",
            "Iteration 47, loss = 1.09373293\n",
            "Iteration 48, loss = 1.11565612\n",
            "Iteration 49, loss = 1.10290251\n",
            "Iteration 50, loss = 1.10541226\n",
            "Iteration 51, loss = 1.08659221\n",
            "Iteration 52, loss = 1.07654572\n",
            "Iteration 53, loss = 1.10979148\n",
            "Iteration 54, loss = 1.08841944\n",
            "Iteration 55, loss = 1.13140768\n",
            "Iteration 56, loss = 1.09982372\n",
            "Iteration 57, loss = 1.09061219\n",
            "Iteration 58, loss = 1.08610898\n",
            "Iteration 59, loss = 1.09563032\n",
            "Iteration 60, loss = 1.08061867\n",
            "Iteration 61, loss = 1.05902686\n",
            "Iteration 62, loss = 1.09574871\n",
            "Iteration 63, loss = 1.07390103\n",
            "Iteration 64, loss = 1.06039419\n",
            "Iteration 65, loss = 1.06742336\n",
            "Iteration 66, loss = 1.06261244\n",
            "Iteration 67, loss = 1.06808039\n",
            "Iteration 68, loss = 1.03379528\n",
            "Iteration 69, loss = 1.03957747\n",
            "Iteration 70, loss = 1.02959315\n",
            "Iteration 71, loss = 1.04191219\n",
            "Iteration 72, loss = 1.04805588\n",
            "Iteration 73, loss = 1.02104867\n",
            "Iteration 74, loss = 1.04994827\n",
            "Iteration 75, loss = 1.02986644\n",
            "Iteration 76, loss = 1.03468102\n",
            "Iteration 77, loss = 1.01565029\n",
            "Iteration 78, loss = 1.03855412\n",
            "Iteration 79, loss = 1.05258247\n",
            "Iteration 80, loss = 1.02129662\n",
            "Iteration 81, loss = 1.07379134\n",
            "Iteration 82, loss = 1.02660149\n",
            "Iteration 83, loss = 1.00992751\n",
            "Iteration 84, loss = 1.02353265\n",
            "Iteration 85, loss = 1.01879339\n",
            "Iteration 86, loss = 1.00977914\n",
            "Iteration 87, loss = 1.02396011\n",
            "Iteration 88, loss = 1.05984587\n",
            "Iteration 89, loss = 1.10369853\n",
            "Iteration 90, loss = 1.05020692\n",
            "Iteration 91, loss = 1.03538795\n",
            "Iteration 92, loss = 1.01050849\n",
            "Iteration 93, loss = 1.03115299\n",
            "Iteration 94, loss = 0.98395185\n",
            "Iteration 95, loss = 0.98052465\n",
            "Iteration 96, loss = 0.97992729\n",
            "Iteration 97, loss = 1.01454213\n",
            "Iteration 98, loss = 1.00144856\n",
            "Iteration 99, loss = 1.01242506\n",
            "Iteration 100, loss = 0.98405984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29942726\n",
            "Iteration 2, loss = 1.26020436\n",
            "Iteration 3, loss = 1.25093384\n",
            "Iteration 4, loss = 1.23270449\n",
            "Iteration 5, loss = 1.23048951\n",
            "Iteration 6, loss = 1.22724924\n",
            "Iteration 7, loss = 1.22182607\n",
            "Iteration 8, loss = 1.22225877\n",
            "Iteration 9, loss = 1.22371179\n",
            "Iteration 10, loss = 1.20973244\n",
            "Iteration 11, loss = 1.20829727\n",
            "Iteration 12, loss = 1.20786491\n",
            "Iteration 13, loss = 1.20645671\n",
            "Iteration 14, loss = 1.20692756\n",
            "Iteration 15, loss = 1.19681971\n",
            "Iteration 16, loss = 1.19279189\n",
            "Iteration 17, loss = 1.19378456\n",
            "Iteration 18, loss = 1.18946509\n",
            "Iteration 19, loss = 1.18081856\n",
            "Iteration 20, loss = 1.17975031\n",
            "Iteration 21, loss = 1.17052972\n",
            "Iteration 22, loss = 1.17975266\n",
            "Iteration 23, loss = 1.17483302\n",
            "Iteration 24, loss = 1.16872645\n",
            "Iteration 25, loss = 1.18019167\n",
            "Iteration 26, loss = 1.18285710\n",
            "Iteration 27, loss = 1.14590388\n",
            "Iteration 28, loss = 1.16276343\n",
            "Iteration 29, loss = 1.14063335\n",
            "Iteration 30, loss = 1.15183067\n",
            "Iteration 31, loss = 1.15147944\n",
            "Iteration 32, loss = 1.15015065\n",
            "Iteration 33, loss = 1.13609075\n",
            "Iteration 34, loss = 1.13909837\n",
            "Iteration 35, loss = 1.12195403\n",
            "Iteration 36, loss = 1.11594436\n",
            "Iteration 37, loss = 1.11269622\n",
            "Iteration 38, loss = 1.18573503\n",
            "Iteration 39, loss = 1.15594440\n",
            "Iteration 40, loss = 1.11123042\n",
            "Iteration 41, loss = 1.11945840\n",
            "Iteration 42, loss = 1.12833267\n",
            "Iteration 43, loss = 1.14947464\n",
            "Iteration 44, loss = 1.11227803\n",
            "Iteration 45, loss = 1.10013097\n",
            "Iteration 46, loss = 1.10131759\n",
            "Iteration 47, loss = 1.09376494\n",
            "Iteration 48, loss = 1.09601331\n",
            "Iteration 49, loss = 1.09381545\n",
            "Iteration 50, loss = 1.09950595\n",
            "Iteration 51, loss = 1.08662390\n",
            "Iteration 52, loss = 1.08558173\n",
            "Iteration 53, loss = 1.09830919\n",
            "Iteration 54, loss = 1.07743110\n",
            "Iteration 55, loss = 1.10324903\n",
            "Iteration 56, loss = 1.09057612\n",
            "Iteration 57, loss = 1.12519995\n",
            "Iteration 58, loss = 1.10239136\n",
            "Iteration 59, loss = 1.07621134\n",
            "Iteration 60, loss = 1.05689770\n",
            "Iteration 61, loss = 1.06601145\n",
            "Iteration 62, loss = 1.07335553\n",
            "Iteration 63, loss = 1.06213367\n",
            "Iteration 64, loss = 1.05554370\n",
            "Iteration 65, loss = 1.06709035\n",
            "Iteration 66, loss = 1.04911830\n",
            "Iteration 67, loss = 1.03619936\n",
            "Iteration 68, loss = 1.04297201\n",
            "Iteration 69, loss = 1.04307825\n",
            "Iteration 70, loss = 1.04391178\n",
            "Iteration 71, loss = 1.05450125\n",
            "Iteration 72, loss = 1.05914294\n",
            "Iteration 73, loss = 1.03177016\n",
            "Iteration 74, loss = 1.03121769\n",
            "Iteration 75, loss = 1.04080950\n",
            "Iteration 76, loss = 1.02430982\n",
            "Iteration 77, loss = 1.01792534\n",
            "Iteration 78, loss = 1.04532197\n",
            "Iteration 79, loss = 1.05743192\n",
            "Iteration 80, loss = 1.06558494\n",
            "Iteration 81, loss = 1.03523579\n",
            "Iteration 82, loss = 1.05093571\n",
            "Iteration 83, loss = 1.02889985\n",
            "Iteration 84, loss = 1.06342641\n",
            "Iteration 85, loss = 1.02213639\n",
            "Iteration 86, loss = 1.01936092\n",
            "Iteration 87, loss = 0.98936595\n",
            "Iteration 88, loss = 1.00757360\n",
            "Iteration 89, loss = 1.01851105\n",
            "Iteration 90, loss = 0.99303392\n",
            "Iteration 91, loss = 0.98601685\n",
            "Iteration 92, loss = 0.99183845\n",
            "Iteration 93, loss = 0.99445568\n",
            "Iteration 94, loss = 0.97604261\n",
            "Iteration 95, loss = 0.96945630\n",
            "Iteration 96, loss = 0.96307284\n",
            "Iteration 97, loss = 0.97331819\n",
            "Iteration 98, loss = 0.96522254\n",
            "Iteration 99, loss = 0.95336878\n",
            "Iteration 100, loss = 0.96132393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29842483\n",
            "Iteration 2, loss = 1.24751853\n",
            "Iteration 3, loss = 1.23798351\n",
            "Iteration 4, loss = 1.23028414\n",
            "Iteration 5, loss = 1.22244670\n",
            "Iteration 6, loss = 1.21285109\n",
            "Iteration 7, loss = 1.20811825\n",
            "Iteration 8, loss = 1.21076594\n",
            "Iteration 9, loss = 1.21110111\n",
            "Iteration 10, loss = 1.19961677\n",
            "Iteration 11, loss = 1.19835356\n",
            "Iteration 12, loss = 1.19323223\n",
            "Iteration 13, loss = 1.18474103\n",
            "Iteration 14, loss = 1.18585220\n",
            "Iteration 15, loss = 1.18846972\n",
            "Iteration 16, loss = 1.17772738\n",
            "Iteration 17, loss = 1.17317302\n",
            "Iteration 18, loss = 1.16398202\n",
            "Iteration 19, loss = 1.15788041\n",
            "Iteration 20, loss = 1.16170851\n",
            "Iteration 21, loss = 1.16730792\n",
            "Iteration 22, loss = 1.16362397\n",
            "Iteration 23, loss = 1.16943796\n",
            "Iteration 24, loss = 1.15578272\n",
            "Iteration 25, loss = 1.15992772\n",
            "Iteration 26, loss = 1.16708413\n",
            "Iteration 27, loss = 1.12397806\n",
            "Iteration 28, loss = 1.14539542\n",
            "Iteration 29, loss = 1.12463510\n",
            "Iteration 30, loss = 1.15344596\n",
            "Iteration 31, loss = 1.14871775\n",
            "Iteration 32, loss = 1.14408241\n",
            "Iteration 33, loss = 1.12371549\n",
            "Iteration 34, loss = 1.13007047\n",
            "Iteration 35, loss = 1.10353276\n",
            "Iteration 36, loss = 1.09913526\n",
            "Iteration 37, loss = 1.09849055\n",
            "Iteration 38, loss = 1.13778697\n",
            "Iteration 39, loss = 1.11128056\n",
            "Iteration 40, loss = 1.09526706\n",
            "Iteration 41, loss = 1.09319861\n",
            "Iteration 42, loss = 1.10023721\n",
            "Iteration 43, loss = 1.09338078\n",
            "Iteration 44, loss = 1.09752574\n",
            "Iteration 45, loss = 1.10757934\n",
            "Iteration 46, loss = 1.09846149\n",
            "Iteration 47, loss = 1.07273783\n",
            "Iteration 48, loss = 1.11022705\n",
            "Iteration 49, loss = 1.09851579\n",
            "Iteration 50, loss = 1.07005146\n",
            "Iteration 51, loss = 1.06588593\n",
            "Iteration 52, loss = 1.07229223\n",
            "Iteration 53, loss = 1.05998240\n",
            "Iteration 54, loss = 1.07082416\n",
            "Iteration 55, loss = 1.05405719\n",
            "Iteration 56, loss = 1.05394250\n",
            "Iteration 57, loss = 1.09546945\n",
            "Iteration 58, loss = 1.10973454\n",
            "Iteration 59, loss = 1.09181373\n",
            "Iteration 60, loss = 1.06495275\n",
            "Iteration 61, loss = 1.06488252\n",
            "Iteration 62, loss = 1.04601860\n",
            "Iteration 63, loss = 1.03911907\n",
            "Iteration 64, loss = 1.03740109\n",
            "Iteration 65, loss = 1.03495576\n",
            "Iteration 66, loss = 1.01052208\n",
            "Iteration 67, loss = 1.02112733\n",
            "Iteration 68, loss = 1.00879826\n",
            "Iteration 69, loss = 1.00432024\n",
            "Iteration 70, loss = 0.99947452\n",
            "Iteration 71, loss = 1.00556314\n",
            "Iteration 72, loss = 1.00856674\n",
            "Iteration 73, loss = 1.00177544\n",
            "Iteration 74, loss = 0.97957534\n",
            "Iteration 75, loss = 0.99321608\n",
            "Iteration 76, loss = 0.99540568\n",
            "Iteration 77, loss = 0.97588052\n",
            "Iteration 78, loss = 0.99355425\n",
            "Iteration 79, loss = 1.00237000\n",
            "Iteration 80, loss = 1.00667998\n",
            "Iteration 81, loss = 0.98533042\n",
            "Iteration 82, loss = 0.97636576\n",
            "Iteration 83, loss = 0.96434600\n",
            "Iteration 84, loss = 0.98946573\n",
            "Iteration 85, loss = 0.97418685\n",
            "Iteration 86, loss = 1.01218532\n",
            "Iteration 87, loss = 0.96816956\n",
            "Iteration 88, loss = 1.00330494\n",
            "Iteration 89, loss = 0.97863494\n",
            "Iteration 90, loss = 0.94148486\n",
            "Iteration 91, loss = 0.94251510\n",
            "Iteration 92, loss = 0.92904549\n",
            "Iteration 93, loss = 0.94715850\n",
            "Iteration 94, loss = 0.94670347\n",
            "Iteration 95, loss = 0.94090260\n",
            "Iteration 96, loss = 0.91544391\n",
            "Iteration 97, loss = 0.97909288\n",
            "Iteration 98, loss = 0.95063931\n",
            "Iteration 99, loss = 0.92111809\n",
            "Iteration 100, loss = 0.91948757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28917505\n",
            "Iteration 2, loss = 1.23607745\n",
            "Iteration 3, loss = 1.21925176\n",
            "Iteration 4, loss = 1.21900155\n",
            "Iteration 5, loss = 1.21090097\n",
            "Iteration 6, loss = 1.20728555\n",
            "Iteration 7, loss = 1.20862710\n",
            "Iteration 8, loss = 1.20285849\n",
            "Iteration 9, loss = 1.20062269\n",
            "Iteration 10, loss = 1.19438789\n",
            "Iteration 11, loss = 1.18965618\n",
            "Iteration 12, loss = 1.19140777\n",
            "Iteration 13, loss = 1.19534220\n",
            "Iteration 14, loss = 1.20474876\n",
            "Iteration 15, loss = 1.18970170\n",
            "Iteration 16, loss = 1.17495874\n",
            "Iteration 17, loss = 1.17780828\n",
            "Iteration 18, loss = 1.16633893\n",
            "Iteration 19, loss = 1.15641759\n",
            "Iteration 20, loss = 1.15813319\n",
            "Iteration 21, loss = 1.15602798\n",
            "Iteration 22, loss = 1.17206250\n",
            "Iteration 23, loss = 1.15272830\n",
            "Iteration 24, loss = 1.13555766\n",
            "Iteration 25, loss = 1.13832180\n",
            "Iteration 26, loss = 1.12430246\n",
            "Iteration 27, loss = 1.12578265\n",
            "Iteration 28, loss = 1.12143618\n",
            "Iteration 29, loss = 1.12330447\n",
            "Iteration 30, loss = 1.13712494\n",
            "Iteration 31, loss = 1.11698490\n",
            "Iteration 32, loss = 1.12933546\n",
            "Iteration 33, loss = 1.11423940\n",
            "Iteration 34, loss = 1.09582000\n",
            "Iteration 35, loss = 1.08886531\n",
            "Iteration 36, loss = 1.08850693\n",
            "Iteration 37, loss = 1.09520955\n",
            "Iteration 38, loss = 1.14137249\n",
            "Iteration 39, loss = 1.09886939\n",
            "Iteration 40, loss = 1.08058954\n",
            "Iteration 41, loss = 1.08083161\n",
            "Iteration 42, loss = 1.12000963\n",
            "Iteration 43, loss = 1.13713440\n",
            "Iteration 44, loss = 1.09366659\n",
            "Iteration 45, loss = 1.07022320\n",
            "Iteration 46, loss = 1.06227717\n",
            "Iteration 47, loss = 1.06034327\n",
            "Iteration 48, loss = 1.10724277\n",
            "Iteration 49, loss = 1.11331974\n",
            "Iteration 50, loss = 1.08747235\n",
            "Iteration 51, loss = 1.08262661\n",
            "Iteration 52, loss = 1.08504807\n",
            "Iteration 53, loss = 1.05557501\n",
            "Iteration 54, loss = 1.04861907\n",
            "Iteration 55, loss = 1.04365602\n",
            "Iteration 56, loss = 1.04267117\n",
            "Iteration 57, loss = 1.05085437\n",
            "Iteration 58, loss = 1.07764819\n",
            "Iteration 59, loss = 1.08493593\n",
            "Iteration 60, loss = 1.05165019\n",
            "Iteration 61, loss = 1.03667969\n",
            "Iteration 62, loss = 1.05864097\n",
            "Iteration 63, loss = 1.04337418\n",
            "Iteration 64, loss = 1.05280662\n",
            "Iteration 65, loss = 1.02045776\n",
            "Iteration 66, loss = 1.02815977\n",
            "Iteration 67, loss = 1.01964208\n",
            "Iteration 68, loss = 1.03008498\n",
            "Iteration 69, loss = 1.03083613\n",
            "Iteration 70, loss = 1.01775601\n",
            "Iteration 71, loss = 1.02132822\n",
            "Iteration 72, loss = 1.03097527\n",
            "Iteration 73, loss = 1.02177062\n",
            "Iteration 74, loss = 1.00547776\n",
            "Iteration 75, loss = 1.02882139\n",
            "Iteration 76, loss = 1.02323479\n",
            "Iteration 77, loss = 1.01233993\n",
            "Iteration 78, loss = 1.01827040\n",
            "Iteration 79, loss = 1.03492523\n",
            "Iteration 80, loss = 1.03094192\n",
            "Iteration 81, loss = 1.04673590\n",
            "Iteration 82, loss = 1.04573396\n",
            "Iteration 83, loss = 1.02642634\n",
            "Iteration 84, loss = 1.03727689\n",
            "Iteration 85, loss = 1.00336718\n",
            "Iteration 86, loss = 1.00363763\n",
            "Iteration 87, loss = 0.97898709\n",
            "Iteration 88, loss = 0.97289766\n",
            "Iteration 89, loss = 0.98134727\n",
            "Iteration 90, loss = 0.99472148\n",
            "Iteration 91, loss = 0.96189440\n",
            "Iteration 92, loss = 0.97200725\n",
            "Iteration 93, loss = 0.96522140\n",
            "Iteration 94, loss = 0.95873488\n",
            "Iteration 95, loss = 0.94542337\n",
            "Iteration 96, loss = 0.95402813\n",
            "Iteration 97, loss = 0.96289072\n",
            "Iteration 98, loss = 0.97143028\n",
            "Iteration 99, loss = 0.94058412\n",
            "Iteration 100, loss = 0.92163527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29533414\n",
            "Iteration 2, loss = 1.24143534\n",
            "Iteration 3, loss = 1.22843434\n",
            "Iteration 4, loss = 1.22829544\n",
            "Iteration 5, loss = 1.21241988\n",
            "Iteration 6, loss = 1.21201780\n",
            "Iteration 7, loss = 1.20922846\n",
            "Iteration 8, loss = 1.20965182\n",
            "Iteration 9, loss = 1.21117842\n",
            "Iteration 10, loss = 1.19273133\n",
            "Iteration 11, loss = 1.19244157\n",
            "Iteration 12, loss = 1.18828294\n",
            "Iteration 13, loss = 1.18942700\n",
            "Iteration 14, loss = 1.20321345\n",
            "Iteration 15, loss = 1.18797157\n",
            "Iteration 16, loss = 1.17754712\n",
            "Iteration 17, loss = 1.18406613\n",
            "Iteration 18, loss = 1.17059741\n",
            "Iteration 19, loss = 1.16198474\n",
            "Iteration 20, loss = 1.15839752\n",
            "Iteration 21, loss = 1.15915327\n",
            "Iteration 22, loss = 1.17375156\n",
            "Iteration 23, loss = 1.15471217\n",
            "Iteration 24, loss = 1.14242996\n",
            "Iteration 25, loss = 1.17440097\n",
            "Iteration 26, loss = 1.13663578\n",
            "Iteration 27, loss = 1.12457424\n",
            "Iteration 28, loss = 1.12990998\n",
            "Iteration 29, loss = 1.12589070\n",
            "Iteration 30, loss = 1.12717726\n",
            "Iteration 31, loss = 1.11926874\n",
            "Iteration 32, loss = 1.14178613\n",
            "Iteration 33, loss = 1.12463416\n",
            "Iteration 34, loss = 1.11408575\n",
            "Iteration 35, loss = 1.09594973\n",
            "Iteration 36, loss = 1.09528074\n",
            "Iteration 37, loss = 1.09437382\n",
            "Iteration 38, loss = 1.16736496\n",
            "Iteration 39, loss = 1.13821620\n",
            "Iteration 40, loss = 1.10912051\n",
            "Iteration 41, loss = 1.10272501\n",
            "Iteration 42, loss = 1.10848791\n",
            "Iteration 43, loss = 1.11273458\n",
            "Iteration 44, loss = 1.08571587\n",
            "Iteration 45, loss = 1.08956550\n",
            "Iteration 46, loss = 1.07990096\n",
            "Iteration 47, loss = 1.07927036\n",
            "Iteration 48, loss = 1.14202768\n",
            "Iteration 49, loss = 1.12629621\n",
            "Iteration 50, loss = 1.08150599\n",
            "Iteration 51, loss = 1.07045291\n",
            "Iteration 52, loss = 1.06786959\n",
            "Iteration 53, loss = 1.05571732\n",
            "Iteration 54, loss = 1.05512994\n",
            "Iteration 55, loss = 1.05657800\n",
            "Iteration 56, loss = 1.05354113\n",
            "Iteration 57, loss = 1.04655207\n",
            "Iteration 58, loss = 1.04918121\n",
            "Iteration 59, loss = 1.09470685\n",
            "Iteration 60, loss = 1.09149656\n",
            "Iteration 61, loss = 1.06453776\n",
            "Iteration 62, loss = 1.04518936\n",
            "Iteration 63, loss = 1.03120163\n",
            "Iteration 64, loss = 1.03394944\n",
            "Iteration 65, loss = 1.01987912\n",
            "Iteration 66, loss = 1.04079353\n",
            "Iteration 67, loss = 1.04046333\n",
            "Iteration 68, loss = 1.05973794\n",
            "Iteration 69, loss = 1.02719457\n",
            "Iteration 70, loss = 1.01884556\n",
            "Iteration 71, loss = 1.05395649\n",
            "Iteration 72, loss = 1.04054616\n",
            "Iteration 73, loss = 1.05322622\n",
            "Iteration 74, loss = 1.04631329\n",
            "Iteration 75, loss = 1.04373267\n",
            "Iteration 76, loss = 1.01226108\n",
            "Iteration 77, loss = 1.00308508\n",
            "Iteration 78, loss = 1.01621179\n",
            "Iteration 79, loss = 1.07875959\n",
            "Iteration 80, loss = 1.05755492\n",
            "Iteration 81, loss = 1.04261400\n",
            "Iteration 82, loss = 1.06083831\n",
            "Iteration 83, loss = 1.02274258\n",
            "Iteration 84, loss = 1.03033718\n",
            "Iteration 85, loss = 1.02739318\n",
            "Iteration 86, loss = 1.00367070\n",
            "Iteration 87, loss = 1.03712178\n",
            "Iteration 88, loss = 1.02386348\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.28351936\n",
            "Iteration 2, loss = 1.24769443\n",
            "Iteration 3, loss = 1.23595265\n",
            "Iteration 4, loss = 1.23205761\n",
            "Iteration 5, loss = 1.22546897\n",
            "Iteration 6, loss = 1.22468349\n",
            "Iteration 7, loss = 1.21645511\n",
            "Iteration 8, loss = 1.22316041\n",
            "Iteration 9, loss = 1.20952438\n",
            "Iteration 10, loss = 1.20640513\n",
            "Iteration 11, loss = 1.20963577\n",
            "Iteration 12, loss = 1.20305327\n",
            "Iteration 13, loss = 1.21275664\n",
            "Iteration 14, loss = 1.19787181\n",
            "Iteration 15, loss = 1.20531857\n",
            "Iteration 16, loss = 1.19810237\n",
            "Iteration 17, loss = 1.19844840\n",
            "Iteration 18, loss = 1.18999607\n",
            "Iteration 19, loss = 1.18346446\n",
            "Iteration 20, loss = 1.18037208\n",
            "Iteration 21, loss = 1.18968237\n",
            "Iteration 22, loss = 1.19272881\n",
            "Iteration 23, loss = 1.17722839\n",
            "Iteration 24, loss = 1.18530381\n",
            "Iteration 25, loss = 1.16819410\n",
            "Iteration 26, loss = 1.18079130\n",
            "Iteration 27, loss = 1.16530133\n",
            "Iteration 28, loss = 1.17681862\n",
            "Iteration 29, loss = 1.15779941\n",
            "Iteration 30, loss = 1.17132303\n",
            "Iteration 31, loss = 1.18627719\n",
            "Iteration 32, loss = 1.17122380\n",
            "Iteration 33, loss = 1.15490008\n",
            "Iteration 34, loss = 1.16798261\n",
            "Iteration 35, loss = 1.15269603\n",
            "Iteration 36, loss = 1.16257920\n",
            "Iteration 37, loss = 1.15512449\n",
            "Iteration 38, loss = 1.15055867\n",
            "Iteration 39, loss = 1.15931331\n",
            "Iteration 40, loss = 1.13917701\n",
            "Iteration 41, loss = 1.13790897\n",
            "Iteration 42, loss = 1.15176050\n",
            "Iteration 43, loss = 1.15859875\n",
            "Iteration 44, loss = 1.16254793\n",
            "Iteration 45, loss = 1.14106542\n",
            "Iteration 46, loss = 1.11783748\n",
            "Iteration 47, loss = 1.11131570\n",
            "Iteration 48, loss = 1.11150209\n",
            "Iteration 49, loss = 1.11858588\n",
            "Iteration 50, loss = 1.12136078\n",
            "Iteration 51, loss = 1.10447483\n",
            "Iteration 52, loss = 1.09242363\n",
            "Iteration 53, loss = 1.09076632\n",
            "Iteration 54, loss = 1.08584543\n",
            "Iteration 55, loss = 1.08657643\n",
            "Iteration 56, loss = 1.09153811\n",
            "Iteration 57, loss = 1.10240982\n",
            "Iteration 58, loss = 1.07754103\n",
            "Iteration 59, loss = 1.05815831\n",
            "Iteration 60, loss = 1.07371982\n",
            "Iteration 61, loss = 1.08964911\n",
            "Iteration 62, loss = 1.10727381\n",
            "Iteration 63, loss = 1.05412661\n",
            "Iteration 64, loss = 1.07022091\n",
            "Iteration 65, loss = 1.06733071\n",
            "Iteration 66, loss = 1.10461268\n",
            "Iteration 67, loss = 1.10887387\n",
            "Iteration 68, loss = 1.08389231\n",
            "Iteration 69, loss = 1.06140240\n",
            "Iteration 70, loss = 1.04321835\n",
            "Iteration 71, loss = 1.02748261\n",
            "Iteration 72, loss = 1.02514512\n",
            "Iteration 73, loss = 1.01828718\n",
            "Iteration 74, loss = 1.02600663\n",
            "Iteration 75, loss = 1.03489736\n",
            "Iteration 76, loss = 1.06359906\n",
            "Iteration 77, loss = 1.04218054\n",
            "Iteration 78, loss = 1.07404525\n",
            "Iteration 79, loss = 1.06828545\n",
            "Iteration 80, loss = 1.05514895\n",
            "Iteration 81, loss = 1.04335183\n",
            "Iteration 82, loss = 1.02926916\n",
            "Iteration 83, loss = 1.02725202\n",
            "Iteration 84, loss = 1.02943728\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27631281\n",
            "Iteration 2, loss = 1.24390563\n",
            "Iteration 3, loss = 1.23633404\n",
            "Iteration 4, loss = 1.22429499\n",
            "Iteration 5, loss = 1.21460730\n",
            "Iteration 6, loss = 1.21029160\n",
            "Iteration 7, loss = 1.20436654\n",
            "Iteration 8, loss = 1.20390657\n",
            "Iteration 9, loss = 1.19892190\n",
            "Iteration 10, loss = 1.19641503\n",
            "Iteration 11, loss = 1.19813830\n",
            "Iteration 12, loss = 1.19051814\n",
            "Iteration 13, loss = 1.19158419\n",
            "Iteration 14, loss = 1.18645330\n",
            "Iteration 15, loss = 1.19007847\n",
            "Iteration 16, loss = 1.18631588\n",
            "Iteration 17, loss = 1.18415560\n",
            "Iteration 18, loss = 1.17641635\n",
            "Iteration 19, loss = 1.17687833\n",
            "Iteration 20, loss = 1.17469835\n",
            "Iteration 21, loss = 1.18834491\n",
            "Iteration 22, loss = 1.19787185\n",
            "Iteration 23, loss = 1.17910564\n",
            "Iteration 24, loss = 1.18857351\n",
            "Iteration 25, loss = 1.16284200\n",
            "Iteration 26, loss = 1.16359146\n",
            "Iteration 27, loss = 1.16200962\n",
            "Iteration 28, loss = 1.15900602\n",
            "Iteration 29, loss = 1.15609398\n",
            "Iteration 30, loss = 1.16398142\n",
            "Iteration 31, loss = 1.17366259\n",
            "Iteration 32, loss = 1.16345536\n",
            "Iteration 33, loss = 1.15629359\n",
            "Iteration 34, loss = 1.16191364\n",
            "Iteration 35, loss = 1.15316186\n",
            "Iteration 36, loss = 1.13830616\n",
            "Iteration 37, loss = 1.12551109\n",
            "Iteration 38, loss = 1.11985660\n",
            "Iteration 39, loss = 1.13531959\n",
            "Iteration 40, loss = 1.11660098\n",
            "Iteration 41, loss = 1.11773462\n",
            "Iteration 42, loss = 1.11600872\n",
            "Iteration 43, loss = 1.13009769\n",
            "Iteration 44, loss = 1.10597914\n",
            "Iteration 45, loss = 1.10689313\n",
            "Iteration 46, loss = 1.10828569\n",
            "Iteration 47, loss = 1.09205985\n",
            "Iteration 48, loss = 1.09652013\n",
            "Iteration 49, loss = 1.09105062\n",
            "Iteration 50, loss = 1.08687342\n",
            "Iteration 51, loss = 1.08257821\n",
            "Iteration 52, loss = 1.08096365\n",
            "Iteration 53, loss = 1.07342739\n",
            "Iteration 54, loss = 1.08397613\n",
            "Iteration 55, loss = 1.07047332\n",
            "Iteration 56, loss = 1.07783198\n",
            "Iteration 57, loss = 1.07384912\n",
            "Iteration 58, loss = 1.05618420\n",
            "Iteration 59, loss = 1.04214163\n",
            "Iteration 60, loss = 1.05836130\n",
            "Iteration 61, loss = 1.08035259\n",
            "Iteration 62, loss = 1.09303812\n",
            "Iteration 63, loss = 1.03607882\n",
            "Iteration 64, loss = 1.06680573\n",
            "Iteration 65, loss = 1.05621006\n",
            "Iteration 66, loss = 1.08894861\n",
            "Iteration 67, loss = 1.10106514\n",
            "Iteration 68, loss = 1.04114512\n",
            "Iteration 69, loss = 1.03578786\n",
            "Iteration 70, loss = 1.02139105\n",
            "Iteration 71, loss = 1.00605328\n",
            "Iteration 72, loss = 1.01597796\n",
            "Iteration 73, loss = 1.00597842\n",
            "Iteration 74, loss = 1.01390333\n",
            "Iteration 75, loss = 1.03911804\n",
            "Iteration 76, loss = 1.04083542\n",
            "Iteration 77, loss = 1.04100677\n",
            "Iteration 78, loss = 1.04946445\n",
            "Iteration 79, loss = 1.03268710\n",
            "Iteration 80, loss = 1.02676183\n",
            "Iteration 81, loss = 0.98954879\n",
            "Iteration 82, loss = 0.99137521\n",
            "Iteration 83, loss = 0.99440668\n",
            "Iteration 84, loss = 1.01000029\n",
            "Iteration 85, loss = 0.99999057\n",
            "Iteration 86, loss = 0.98373204\n",
            "Iteration 87, loss = 0.99204957\n",
            "Iteration 88, loss = 1.04007140\n",
            "Iteration 89, loss = 1.00453029\n",
            "Iteration 90, loss = 0.96724539\n",
            "Iteration 91, loss = 0.96271137\n",
            "Iteration 92, loss = 0.95182806\n",
            "Iteration 93, loss = 0.95837270\n",
            "Iteration 94, loss = 0.98413594\n",
            "Iteration 95, loss = 0.95880239\n",
            "Iteration 96, loss = 0.95720348\n",
            "Iteration 97, loss = 0.96826197\n",
            "Iteration 98, loss = 0.97803488\n",
            "Iteration 99, loss = 0.95841788\n",
            "Iteration 100, loss = 0.98596081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27544448\n",
            "Iteration 2, loss = 1.22804551\n",
            "Iteration 3, loss = 1.21573915\n",
            "Iteration 4, loss = 1.22123578\n",
            "Iteration 5, loss = 1.21034886\n",
            "Iteration 6, loss = 1.20117260\n",
            "Iteration 7, loss = 1.21199428\n",
            "Iteration 8, loss = 1.19576532\n",
            "Iteration 9, loss = 1.19503101\n",
            "Iteration 10, loss = 1.18883805\n",
            "Iteration 11, loss = 1.19183227\n",
            "Iteration 12, loss = 1.18696326\n",
            "Iteration 13, loss = 1.18344194\n",
            "Iteration 14, loss = 1.18317520\n",
            "Iteration 15, loss = 1.18538150\n",
            "Iteration 16, loss = 1.17824385\n",
            "Iteration 17, loss = 1.17421129\n",
            "Iteration 18, loss = 1.17504892\n",
            "Iteration 19, loss = 1.16709924\n",
            "Iteration 20, loss = 1.16790579\n",
            "Iteration 21, loss = 1.18401839\n",
            "Iteration 22, loss = 1.18986831\n",
            "Iteration 23, loss = 1.16778971\n",
            "Iteration 24, loss = 1.16535044\n",
            "Iteration 25, loss = 1.16150831\n",
            "Iteration 26, loss = 1.16315162\n",
            "Iteration 27, loss = 1.16692512\n",
            "Iteration 28, loss = 1.14441262\n",
            "Iteration 29, loss = 1.16391934\n",
            "Iteration 30, loss = 1.15143249\n",
            "Iteration 31, loss = 1.17017078\n",
            "Iteration 32, loss = 1.17523897\n",
            "Iteration 33, loss = 1.15128593\n",
            "Iteration 34, loss = 1.16950820\n",
            "Iteration 35, loss = 1.14966429\n",
            "Iteration 36, loss = 1.14448302\n",
            "Iteration 37, loss = 1.13769984\n",
            "Iteration 38, loss = 1.13901259\n",
            "Iteration 39, loss = 1.13067419\n",
            "Iteration 40, loss = 1.12190681\n",
            "Iteration 41, loss = 1.11250269\n",
            "Iteration 42, loss = 1.10885410\n",
            "Iteration 43, loss = 1.12572416\n",
            "Iteration 44, loss = 1.10051458\n",
            "Iteration 45, loss = 1.11294822\n",
            "Iteration 46, loss = 1.09243489\n",
            "Iteration 47, loss = 1.09687884\n",
            "Iteration 48, loss = 1.10327611\n",
            "Iteration 49, loss = 1.09162517\n",
            "Iteration 50, loss = 1.09184913\n",
            "Iteration 51, loss = 1.10391407\n",
            "Iteration 52, loss = 1.07939795\n",
            "Iteration 53, loss = 1.07854546\n",
            "Iteration 54, loss = 1.08185282\n",
            "Iteration 55, loss = 1.05904466\n",
            "Iteration 56, loss = 1.10445956\n",
            "Iteration 57, loss = 1.09602045\n",
            "Iteration 58, loss = 1.07595320\n",
            "Iteration 59, loss = 1.07247610\n",
            "Iteration 60, loss = 1.06111350\n",
            "Iteration 61, loss = 1.05350790\n",
            "Iteration 62, loss = 1.06456244\n",
            "Iteration 63, loss = 1.02506539\n",
            "Iteration 64, loss = 1.05134735\n",
            "Iteration 65, loss = 1.04397428\n",
            "Iteration 66, loss = 1.10470120\n",
            "Iteration 67, loss = 1.09092751\n",
            "Iteration 68, loss = 1.04098489\n",
            "Iteration 69, loss = 1.03488651\n",
            "Iteration 70, loss = 1.02391231\n",
            "Iteration 71, loss = 1.04372315\n",
            "Iteration 72, loss = 1.01585831\n",
            "Iteration 73, loss = 1.00714045\n",
            "Iteration 74, loss = 1.01174567\n",
            "Iteration 75, loss = 1.00440764\n",
            "Iteration 76, loss = 1.00731285\n",
            "Iteration 77, loss = 0.99500051\n",
            "Iteration 78, loss = 0.98480387\n",
            "Iteration 79, loss = 0.99571299\n",
            "Iteration 80, loss = 0.98456843\n",
            "Iteration 81, loss = 0.99122689\n",
            "Iteration 82, loss = 0.99033993\n",
            "Iteration 83, loss = 1.02730402\n",
            "Iteration 84, loss = 1.04979417\n",
            "Iteration 85, loss = 1.00930260\n",
            "Iteration 86, loss = 0.98957270\n",
            "Iteration 87, loss = 0.98042801\n",
            "Iteration 88, loss = 0.98925311\n",
            "Iteration 89, loss = 0.97357149\n",
            "Iteration 90, loss = 0.96079723\n",
            "Iteration 91, loss = 0.96007439\n",
            "Iteration 92, loss = 0.94611049\n",
            "Iteration 93, loss = 0.94483739\n",
            "Iteration 94, loss = 1.01039264\n",
            "Iteration 95, loss = 0.97280890\n",
            "Iteration 96, loss = 0.96417310\n",
            "Iteration 97, loss = 0.94386950\n",
            "Iteration 98, loss = 0.92327792\n",
            "Iteration 99, loss = 0.92136099\n",
            "Iteration 100, loss = 0.97075550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "Iteration 1, loss = 1.32468720\n",
            "Iteration 2, loss = 1.27626250\n",
            "Iteration 3, loss = 1.26271296\n",
            "Iteration 4, loss = 1.26140035\n",
            "Iteration 5, loss = 1.25924263\n",
            "Iteration 6, loss = 1.25768243\n",
            "Iteration 7, loss = 1.25704048\n",
            "Iteration 8, loss = 1.25581332\n",
            "Iteration 9, loss = 1.25476505\n",
            "Iteration 10, loss = 1.25445301\n",
            "Iteration 11, loss = 1.25465169\n",
            "Iteration 12, loss = 1.25406461\n",
            "Iteration 13, loss = 1.25249877\n",
            "Iteration 14, loss = 1.25318764\n",
            "Iteration 15, loss = 1.25172696\n",
            "Iteration 16, loss = 1.25130079\n",
            "Iteration 17, loss = 1.25166850\n",
            "Iteration 18, loss = 1.25159416\n",
            "Iteration 19, loss = 1.25096671\n",
            "Iteration 20, loss = 1.24926272\n",
            "Iteration 21, loss = 1.24829259\n",
            "Iteration 22, loss = 1.24789125\n",
            "Iteration 23, loss = 1.24858663\n",
            "Iteration 24, loss = 1.24731830\n",
            "Iteration 25, loss = 1.24592819\n",
            "Iteration 26, loss = 1.24686799\n",
            "Iteration 27, loss = 1.24602943\n",
            "Iteration 28, loss = 1.24458211\n",
            "Iteration 29, loss = 1.24454826\n",
            "Iteration 30, loss = 1.24430985\n",
            "Iteration 31, loss = 1.24455519\n",
            "Iteration 32, loss = 1.24696003\n",
            "Iteration 33, loss = 1.24586166\n",
            "Iteration 34, loss = 1.24491147\n",
            "Iteration 35, loss = 1.24578670\n",
            "Iteration 36, loss = 1.24448718\n",
            "Iteration 37, loss = 1.24518252\n",
            "Iteration 38, loss = 1.24423737\n",
            "Iteration 39, loss = 1.24272113\n",
            "Iteration 40, loss = 1.24197986\n",
            "Iteration 41, loss = 1.24171478\n",
            "Iteration 42, loss = 1.24128691\n",
            "Iteration 43, loss = 1.24117928\n",
            "Iteration 44, loss = 1.24183620\n",
            "Iteration 45, loss = 1.24040826\n",
            "Iteration 46, loss = 1.24165654\n",
            "Iteration 47, loss = 1.24218806\n",
            "Iteration 48, loss = 1.24093908\n",
            "Iteration 49, loss = 1.24024110\n",
            "Iteration 50, loss = 1.24049683\n",
            "Iteration 51, loss = 1.24011165\n",
            "Iteration 52, loss = 1.24051355\n",
            "Iteration 53, loss = 1.24172594\n",
            "Iteration 54, loss = 1.24064779\n",
            "Iteration 55, loss = 1.24117829\n",
            "Iteration 56, loss = 1.24054786\n",
            "Iteration 57, loss = 1.23969285\n",
            "Iteration 58, loss = 1.23990601\n",
            "Iteration 59, loss = 1.24013241\n",
            "Iteration 60, loss = 1.23992185\n",
            "Iteration 61, loss = 1.24011360\n",
            "Iteration 62, loss = 1.24034211\n",
            "Iteration 63, loss = 1.23954567\n",
            "Iteration 64, loss = 1.23995611\n",
            "Iteration 65, loss = 1.23907499\n",
            "Iteration 66, loss = 1.24029637\n",
            "Iteration 67, loss = 1.24014606\n",
            "Iteration 68, loss = 1.24010128\n",
            "Iteration 69, loss = 1.23940030\n",
            "Iteration 70, loss = 1.23973344\n",
            "Iteration 71, loss = 1.23926432\n",
            "Iteration 72, loss = 1.23892482\n",
            "Iteration 73, loss = 1.23855411\n",
            "Iteration 74, loss = 1.23878528\n",
            "Iteration 75, loss = 1.23897661\n",
            "Iteration 76, loss = 1.24055448\n",
            "Iteration 77, loss = 1.23993388\n",
            "Iteration 78, loss = 1.23823590\n",
            "Iteration 79, loss = 1.23949932\n",
            "Iteration 80, loss = 1.24002698\n",
            "Iteration 81, loss = 1.24055724\n",
            "Iteration 82, loss = 1.23989617\n",
            "Iteration 83, loss = 1.23828376\n",
            "Iteration 84, loss = 1.23875708\n",
            "Iteration 85, loss = 1.23949665\n",
            "Iteration 86, loss = 1.23948936\n",
            "Iteration 87, loss = 1.23817154\n",
            "Iteration 88, loss = 1.23805457\n",
            "Iteration 89, loss = 1.23939879\n",
            "Iteration 90, loss = 1.23932062\n",
            "Iteration 91, loss = 1.23809157\n",
            "Iteration 92, loss = 1.23985442\n",
            "Iteration 93, loss = 1.23786527\n",
            "Iteration 94, loss = 1.23800600\n",
            "Iteration 95, loss = 1.23849856\n",
            "Iteration 96, loss = 1.23806681\n",
            "Iteration 97, loss = 1.23866338\n",
            "Iteration 98, loss = 1.23832030\n",
            "Iteration 99, loss = 1.23795832\n",
            "Iteration 100, loss = 1.23766168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32035961\n",
            "Iteration 2, loss = 1.27221762\n",
            "Iteration 3, loss = 1.25911719\n",
            "Iteration 4, loss = 1.25907149\n",
            "Iteration 5, loss = 1.25760275\n",
            "Iteration 6, loss = 1.25446780\n",
            "Iteration 7, loss = 1.25400069\n",
            "Iteration 8, loss = 1.25324173\n",
            "Iteration 9, loss = 1.25221483\n",
            "Iteration 10, loss = 1.25204151\n",
            "Iteration 11, loss = 1.25189477\n",
            "Iteration 12, loss = 1.25097746\n",
            "Iteration 13, loss = 1.25000117\n",
            "Iteration 14, loss = 1.25048774\n",
            "Iteration 15, loss = 1.24979641\n",
            "Iteration 16, loss = 1.24911020\n",
            "Iteration 17, loss = 1.24923822\n",
            "Iteration 18, loss = 1.24923694\n",
            "Iteration 19, loss = 1.24880321\n",
            "Iteration 20, loss = 1.24791553\n",
            "Iteration 21, loss = 1.24711378\n",
            "Iteration 22, loss = 1.24648255\n",
            "Iteration 23, loss = 1.24640678\n",
            "Iteration 24, loss = 1.24580859\n",
            "Iteration 25, loss = 1.24482151\n",
            "Iteration 26, loss = 1.24486191\n",
            "Iteration 27, loss = 1.24479869\n",
            "Iteration 28, loss = 1.24351974\n",
            "Iteration 29, loss = 1.24422295\n",
            "Iteration 30, loss = 1.24313114\n",
            "Iteration 31, loss = 1.24373700\n",
            "Iteration 32, loss = 1.24469462\n",
            "Iteration 33, loss = 1.24405668\n",
            "Iteration 34, loss = 1.24406881\n",
            "Iteration 35, loss = 1.24461298\n",
            "Iteration 36, loss = 1.24337288\n",
            "Iteration 37, loss = 1.24402785\n",
            "Iteration 38, loss = 1.24398809\n",
            "Iteration 39, loss = 1.24277959\n",
            "Iteration 40, loss = 1.24198318\n",
            "Iteration 41, loss = 1.24098672\n",
            "Iteration 42, loss = 1.24168784\n",
            "Iteration 43, loss = 1.24253605\n",
            "Iteration 44, loss = 1.24251911\n",
            "Iteration 45, loss = 1.24075683\n",
            "Iteration 46, loss = 1.24040532\n",
            "Iteration 47, loss = 1.24109609\n",
            "Iteration 48, loss = 1.24057339\n",
            "Iteration 49, loss = 1.24000479\n",
            "Iteration 50, loss = 1.24113003\n",
            "Iteration 51, loss = 1.24033355\n",
            "Iteration 52, loss = 1.24053841\n",
            "Iteration 53, loss = 1.24044820\n",
            "Iteration 54, loss = 1.24066456\n",
            "Iteration 55, loss = 1.24117630\n",
            "Iteration 56, loss = 1.23997498\n",
            "Iteration 57, loss = 1.23977870\n",
            "Iteration 58, loss = 1.24093829\n",
            "Iteration 59, loss = 1.24015430\n",
            "Iteration 60, loss = 1.23913216\n",
            "Iteration 61, loss = 1.24039564\n",
            "Iteration 62, loss = 1.24083251\n",
            "Iteration 63, loss = 1.23988223\n",
            "Iteration 64, loss = 1.24096250\n",
            "Iteration 65, loss = 1.23949518\n",
            "Iteration 66, loss = 1.24112839\n",
            "Iteration 67, loss = 1.24116844\n",
            "Iteration 68, loss = 1.24065888\n",
            "Iteration 69, loss = 1.23967996\n",
            "Iteration 70, loss = 1.23982116\n",
            "Iteration 71, loss = 1.23999789\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32718803\n",
            "Iteration 2, loss = 1.27834283\n",
            "Iteration 3, loss = 1.26495446\n",
            "Iteration 4, loss = 1.26435411\n",
            "Iteration 5, loss = 1.26451926\n",
            "Iteration 6, loss = 1.26189251\n",
            "Iteration 7, loss = 1.26180924\n",
            "Iteration 8, loss = 1.26127583\n",
            "Iteration 9, loss = 1.25863264\n",
            "Iteration 10, loss = 1.25754897\n",
            "Iteration 11, loss = 1.25734153\n",
            "Iteration 12, loss = 1.25732202\n",
            "Iteration 13, loss = 1.25639267\n",
            "Iteration 14, loss = 1.25569582\n",
            "Iteration 15, loss = 1.25570321\n",
            "Iteration 16, loss = 1.25524016\n",
            "Iteration 17, loss = 1.25432491\n",
            "Iteration 18, loss = 1.25491045\n",
            "Iteration 19, loss = 1.25474800\n",
            "Iteration 20, loss = 1.25391203\n",
            "Iteration 21, loss = 1.25266178\n",
            "Iteration 22, loss = 1.25222190\n",
            "Iteration 23, loss = 1.25230363\n",
            "Iteration 24, loss = 1.25163914\n",
            "Iteration 25, loss = 1.25053564\n",
            "Iteration 26, loss = 1.25025824\n",
            "Iteration 27, loss = 1.25078945\n",
            "Iteration 28, loss = 1.24961863\n",
            "Iteration 29, loss = 1.24989266\n",
            "Iteration 30, loss = 1.24849232\n",
            "Iteration 31, loss = 1.25003079\n",
            "Iteration 32, loss = 1.25125287\n",
            "Iteration 33, loss = 1.24933786\n",
            "Iteration 34, loss = 1.24899861\n",
            "Iteration 35, loss = 1.25108367\n",
            "Iteration 36, loss = 1.25104402\n",
            "Iteration 37, loss = 1.24941410\n",
            "Iteration 38, loss = 1.24762934\n",
            "Iteration 39, loss = 1.24739037\n",
            "Iteration 40, loss = 1.24749295\n",
            "Iteration 41, loss = 1.24726462\n",
            "Iteration 42, loss = 1.24646797\n",
            "Iteration 43, loss = 1.24694315\n",
            "Iteration 44, loss = 1.24719716\n",
            "Iteration 45, loss = 1.24555199\n",
            "Iteration 46, loss = 1.24645629\n",
            "Iteration 47, loss = 1.24667325\n",
            "Iteration 48, loss = 1.24565624\n",
            "Iteration 49, loss = 1.24532583\n",
            "Iteration 50, loss = 1.24583304\n",
            "Iteration 51, loss = 1.24514318\n",
            "Iteration 52, loss = 1.24550557\n",
            "Iteration 53, loss = 1.24696875\n",
            "Iteration 54, loss = 1.24578554\n",
            "Iteration 55, loss = 1.24581033\n",
            "Iteration 56, loss = 1.24543951\n",
            "Iteration 57, loss = 1.24590128\n",
            "Iteration 58, loss = 1.24687507\n",
            "Iteration 59, loss = 1.24587796\n",
            "Iteration 60, loss = 1.24448886\n",
            "Iteration 61, loss = 1.24555333\n",
            "Iteration 62, loss = 1.24607611\n",
            "Iteration 63, loss = 1.24507511\n",
            "Iteration 64, loss = 1.24497515\n",
            "Iteration 65, loss = 1.24388793\n",
            "Iteration 66, loss = 1.24590950\n",
            "Iteration 67, loss = 1.24593602\n",
            "Iteration 68, loss = 1.24573757\n",
            "Iteration 69, loss = 1.24526193\n",
            "Iteration 70, loss = 1.24528238\n",
            "Iteration 71, loss = 1.24471236\n",
            "Iteration 72, loss = 1.24395915\n",
            "Iteration 73, loss = 1.24377131\n",
            "Iteration 74, loss = 1.24382028\n",
            "Iteration 75, loss = 1.24433239\n",
            "Iteration 76, loss = 1.24542757\n",
            "Iteration 77, loss = 1.24500694\n",
            "Iteration 78, loss = 1.24373126\n",
            "Iteration 79, loss = 1.24411966\n",
            "Iteration 80, loss = 1.24464225\n",
            "Iteration 81, loss = 1.24577799\n",
            "Iteration 82, loss = 1.24588604\n",
            "Iteration 83, loss = 1.24388371\n",
            "Iteration 84, loss = 1.24434602\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33019724\n",
            "Iteration 2, loss = 1.28381708\n",
            "Iteration 3, loss = 1.26773603\n",
            "Iteration 4, loss = 1.26745268\n",
            "Iteration 5, loss = 1.26698052\n",
            "Iteration 6, loss = 1.26385322\n",
            "Iteration 7, loss = 1.26425897\n",
            "Iteration 8, loss = 1.26432949\n",
            "Iteration 9, loss = 1.26185224\n",
            "Iteration 10, loss = 1.26093800\n",
            "Iteration 11, loss = 1.26065414\n",
            "Iteration 12, loss = 1.26071304\n",
            "Iteration 13, loss = 1.25996023\n",
            "Iteration 14, loss = 1.25973483\n",
            "Iteration 15, loss = 1.25993907\n",
            "Iteration 16, loss = 1.25860432\n",
            "Iteration 17, loss = 1.25805601\n",
            "Iteration 18, loss = 1.25810964\n",
            "Iteration 19, loss = 1.25784148\n",
            "Iteration 20, loss = 1.25739512\n",
            "Iteration 21, loss = 1.25641825\n",
            "Iteration 22, loss = 1.25620962\n",
            "Iteration 23, loss = 1.25684066\n",
            "Iteration 24, loss = 1.25539914\n",
            "Iteration 25, loss = 1.25429306\n",
            "Iteration 26, loss = 1.25436279\n",
            "Iteration 27, loss = 1.25450207\n",
            "Iteration 28, loss = 1.25260456\n",
            "Iteration 29, loss = 1.25278311\n",
            "Iteration 30, loss = 1.25146278\n",
            "Iteration 31, loss = 1.25216852\n",
            "Iteration 32, loss = 1.25380225\n",
            "Iteration 33, loss = 1.25170382\n",
            "Iteration 34, loss = 1.25195254\n",
            "Iteration 35, loss = 1.25462960\n",
            "Iteration 36, loss = 1.25277246\n",
            "Iteration 37, loss = 1.25166544\n",
            "Iteration 38, loss = 1.25113920\n",
            "Iteration 39, loss = 1.25082384\n",
            "Iteration 40, loss = 1.25023351\n",
            "Iteration 41, loss = 1.25038614\n",
            "Iteration 42, loss = 1.24951467\n",
            "Iteration 43, loss = 1.25021618\n",
            "Iteration 44, loss = 1.24983062\n",
            "Iteration 45, loss = 1.24858967\n",
            "Iteration 46, loss = 1.24939680\n",
            "Iteration 47, loss = 1.24925309\n",
            "Iteration 48, loss = 1.24847399\n",
            "Iteration 49, loss = 1.24893325\n",
            "Iteration 50, loss = 1.24957091\n",
            "Iteration 51, loss = 1.24820936\n",
            "Iteration 52, loss = 1.24822814\n",
            "Iteration 53, loss = 1.24867015\n",
            "Iteration 54, loss = 1.24801506\n",
            "Iteration 55, loss = 1.24886974\n",
            "Iteration 56, loss = 1.24886730\n",
            "Iteration 57, loss = 1.24956518\n",
            "Iteration 58, loss = 1.25011532\n",
            "Iteration 59, loss = 1.24867645\n",
            "Iteration 60, loss = 1.24728841\n",
            "Iteration 61, loss = 1.24928633\n",
            "Iteration 62, loss = 1.24999013\n",
            "Iteration 63, loss = 1.24789757\n",
            "Iteration 64, loss = 1.24702288\n",
            "Iteration 65, loss = 1.24675105\n",
            "Iteration 66, loss = 1.24940485\n",
            "Iteration 67, loss = 1.24839170\n",
            "Iteration 68, loss = 1.24850373\n",
            "Iteration 69, loss = 1.24781598\n",
            "Iteration 70, loss = 1.24715816\n",
            "Iteration 71, loss = 1.24709753\n",
            "Iteration 72, loss = 1.24665473\n",
            "Iteration 73, loss = 1.24609993\n",
            "Iteration 74, loss = 1.24687883\n",
            "Iteration 75, loss = 1.24751116\n",
            "Iteration 76, loss = 1.24792266\n",
            "Iteration 77, loss = 1.24760456\n",
            "Iteration 78, loss = 1.24655781\n",
            "Iteration 79, loss = 1.24602778\n",
            "Iteration 80, loss = 1.24658851\n",
            "Iteration 81, loss = 1.24859183\n",
            "Iteration 82, loss = 1.24743300\n",
            "Iteration 83, loss = 1.24633711\n",
            "Iteration 84, loss = 1.24828039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32340302\n",
            "Iteration 2, loss = 1.27442017\n",
            "Iteration 3, loss = 1.25896810\n",
            "Iteration 4, loss = 1.25938914\n",
            "Iteration 5, loss = 1.25811524\n",
            "Iteration 6, loss = 1.25472017\n",
            "Iteration 7, loss = 1.25550792\n",
            "Iteration 8, loss = 1.25546108\n",
            "Iteration 9, loss = 1.25272076\n",
            "Iteration 10, loss = 1.25173291\n",
            "Iteration 11, loss = 1.25107767\n",
            "Iteration 12, loss = 1.25122461\n",
            "Iteration 13, loss = 1.25173677\n",
            "Iteration 14, loss = 1.25115976\n",
            "Iteration 15, loss = 1.25085945\n",
            "Iteration 16, loss = 1.24958413\n",
            "Iteration 17, loss = 1.24905112\n",
            "Iteration 18, loss = 1.24903666\n",
            "Iteration 19, loss = 1.24810461\n",
            "Iteration 20, loss = 1.24770770\n",
            "Iteration 21, loss = 1.24674695\n",
            "Iteration 22, loss = 1.24774240\n",
            "Iteration 23, loss = 1.24710677\n",
            "Iteration 24, loss = 1.24616189\n",
            "Iteration 25, loss = 1.24510018\n",
            "Iteration 26, loss = 1.24494658\n",
            "Iteration 27, loss = 1.24549318\n",
            "Iteration 28, loss = 1.24429844\n",
            "Iteration 29, loss = 1.24498993\n",
            "Iteration 30, loss = 1.24303151\n",
            "Iteration 31, loss = 1.24316261\n",
            "Iteration 32, loss = 1.24446974\n",
            "Iteration 33, loss = 1.24328375\n",
            "Iteration 34, loss = 1.24237230\n",
            "Iteration 35, loss = 1.24413293\n",
            "Iteration 36, loss = 1.24352945\n",
            "Iteration 37, loss = 1.24227799\n",
            "Iteration 38, loss = 1.24177426\n",
            "Iteration 39, loss = 1.24144519\n",
            "Iteration 40, loss = 1.24074357\n",
            "Iteration 41, loss = 1.24052358\n",
            "Iteration 42, loss = 1.24093553\n",
            "Iteration 43, loss = 1.24178913\n",
            "Iteration 44, loss = 1.24032842\n",
            "Iteration 45, loss = 1.23933818\n",
            "Iteration 46, loss = 1.24004001\n",
            "Iteration 47, loss = 1.24047735\n",
            "Iteration 48, loss = 1.23948213\n",
            "Iteration 49, loss = 1.24018772\n",
            "Iteration 50, loss = 1.24102314\n",
            "Iteration 51, loss = 1.23945109\n",
            "Iteration 52, loss = 1.23889459\n",
            "Iteration 53, loss = 1.23940360\n",
            "Iteration 54, loss = 1.23892002\n",
            "Iteration 55, loss = 1.23893996\n",
            "Iteration 56, loss = 1.23869464\n",
            "Iteration 57, loss = 1.24003299\n",
            "Iteration 58, loss = 1.24078907\n",
            "Iteration 59, loss = 1.23948097\n",
            "Iteration 60, loss = 1.23803642\n",
            "Iteration 61, loss = 1.23915063\n",
            "Iteration 62, loss = 1.23947415\n",
            "Iteration 63, loss = 1.23819436\n",
            "Iteration 64, loss = 1.23790171\n",
            "Iteration 65, loss = 1.23800660\n",
            "Iteration 66, loss = 1.23906129\n",
            "Iteration 67, loss = 1.23837566\n",
            "Iteration 68, loss = 1.23885239\n",
            "Iteration 69, loss = 1.23870108\n",
            "Iteration 70, loss = 1.23773402\n",
            "Iteration 71, loss = 1.23802158\n",
            "Iteration 72, loss = 1.23758168\n",
            "Iteration 73, loss = 1.23715619\n",
            "Iteration 74, loss = 1.23801149\n",
            "Iteration 75, loss = 1.23779960\n",
            "Iteration 76, loss = 1.23758753\n",
            "Iteration 77, loss = 1.23770900\n",
            "Iteration 78, loss = 1.23716890\n",
            "Iteration 79, loss = 1.23718660\n",
            "Iteration 80, loss = 1.23793363\n",
            "Iteration 81, loss = 1.23869156\n",
            "Iteration 82, loss = 1.23811433\n",
            "Iteration 83, loss = 1.23772745\n",
            "Iteration 84, loss = 1.23912923\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32004075\n",
            "Iteration 2, loss = 1.26695893\n",
            "Iteration 3, loss = 1.24946954\n",
            "Iteration 4, loss = 1.24925380\n",
            "Iteration 5, loss = 1.24956158\n",
            "Iteration 6, loss = 1.24399999\n",
            "Iteration 7, loss = 1.24381197\n",
            "Iteration 8, loss = 1.24504202\n",
            "Iteration 9, loss = 1.24299432\n",
            "Iteration 10, loss = 1.24146009\n",
            "Iteration 11, loss = 1.24015103\n",
            "Iteration 12, loss = 1.23991470\n",
            "Iteration 13, loss = 1.24009758\n",
            "Iteration 14, loss = 1.24006232\n",
            "Iteration 15, loss = 1.23914951\n",
            "Iteration 16, loss = 1.23794614\n",
            "Iteration 17, loss = 1.23716526\n",
            "Iteration 18, loss = 1.23701796\n",
            "Iteration 19, loss = 1.23577223\n",
            "Iteration 20, loss = 1.23601743\n",
            "Iteration 21, loss = 1.23497921\n",
            "Iteration 22, loss = 1.23549271\n",
            "Iteration 23, loss = 1.23487515\n",
            "Iteration 24, loss = 1.23380706\n",
            "Iteration 25, loss = 1.23349954\n",
            "Iteration 26, loss = 1.23341721\n",
            "Iteration 27, loss = 1.23402273\n",
            "Iteration 28, loss = 1.23210259\n",
            "Iteration 29, loss = 1.23418317\n",
            "Iteration 30, loss = 1.23267814\n",
            "Iteration 31, loss = 1.23222922\n",
            "Iteration 32, loss = 1.23321662\n",
            "Iteration 33, loss = 1.23116014\n",
            "Iteration 34, loss = 1.23031415\n",
            "Iteration 35, loss = 1.23312401\n",
            "Iteration 36, loss = 1.23164138\n",
            "Iteration 37, loss = 1.23022019\n",
            "Iteration 38, loss = 1.22975435\n",
            "Iteration 39, loss = 1.23046068\n",
            "Iteration 40, loss = 1.23040410\n",
            "Iteration 41, loss = 1.22972262\n",
            "Iteration 42, loss = 1.22917661\n",
            "Iteration 43, loss = 1.23110929\n",
            "Iteration 44, loss = 1.23031924\n",
            "Iteration 45, loss = 1.22891362\n",
            "Iteration 46, loss = 1.22859708\n",
            "Iteration 47, loss = 1.22945437\n",
            "Iteration 48, loss = 1.22923712\n",
            "Iteration 49, loss = 1.22937999\n",
            "Iteration 50, loss = 1.22962930\n",
            "Iteration 51, loss = 1.22824633\n",
            "Iteration 52, loss = 1.22847036\n",
            "Iteration 53, loss = 1.22837695\n",
            "Iteration 54, loss = 1.22845162\n",
            "Iteration 55, loss = 1.22860066\n",
            "Iteration 56, loss = 1.22780621\n",
            "Iteration 57, loss = 1.22794882\n",
            "Iteration 58, loss = 1.22890596\n",
            "Iteration 59, loss = 1.22857029\n",
            "Iteration 60, loss = 1.22731569\n",
            "Iteration 61, loss = 1.22813829\n",
            "Iteration 62, loss = 1.22879538\n",
            "Iteration 63, loss = 1.22736013\n",
            "Iteration 64, loss = 1.22696774\n",
            "Iteration 65, loss = 1.22685525\n",
            "Iteration 66, loss = 1.22742944\n",
            "Iteration 67, loss = 1.22757099\n",
            "Iteration 68, loss = 1.22786584\n",
            "Iteration 69, loss = 1.22783084\n",
            "Iteration 70, loss = 1.22719647\n",
            "Iteration 71, loss = 1.22680062\n",
            "Iteration 72, loss = 1.22673407\n",
            "Iteration 73, loss = 1.22646293\n",
            "Iteration 74, loss = 1.22706585\n",
            "Iteration 75, loss = 1.22649548\n",
            "Iteration 76, loss = 1.22639239\n",
            "Iteration 77, loss = 1.22639151\n",
            "Iteration 78, loss = 1.22626037\n",
            "Iteration 79, loss = 1.22657972\n",
            "Iteration 80, loss = 1.22653563\n",
            "Iteration 81, loss = 1.22712913\n",
            "Iteration 82, loss = 1.22675750\n",
            "Iteration 83, loss = 1.22626299\n",
            "Iteration 84, loss = 1.22769603\n",
            "Iteration 85, loss = 1.22781233\n",
            "Iteration 86, loss = 1.22719687\n",
            "Iteration 87, loss = 1.22612247\n",
            "Iteration 88, loss = 1.22611754\n",
            "Iteration 89, loss = 1.22613644\n",
            "Iteration 90, loss = 1.22636049\n",
            "Iteration 91, loss = 1.22541588\n",
            "Iteration 92, loss = 1.22513433\n",
            "Iteration 93, loss = 1.22655748\n",
            "Iteration 94, loss = 1.22530193\n",
            "Iteration 95, loss = 1.22574191\n",
            "Iteration 96, loss = 1.22590148\n",
            "Iteration 97, loss = 1.22490631\n",
            "Iteration 98, loss = 1.22512830\n",
            "Iteration 99, loss = 1.22431217\n",
            "Iteration 100, loss = 1.22729798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32693977\n",
            "Iteration 2, loss = 1.27478202\n",
            "Iteration 3, loss = 1.25719719\n",
            "Iteration 4, loss = 1.25431266\n",
            "Iteration 5, loss = 1.25328013\n",
            "Iteration 6, loss = 1.24931077\n",
            "Iteration 7, loss = 1.24906683\n",
            "Iteration 8, loss = 1.24985962\n",
            "Iteration 9, loss = 1.24802006\n",
            "Iteration 10, loss = 1.24684198\n",
            "Iteration 11, loss = 1.24560262\n",
            "Iteration 12, loss = 1.24524931\n",
            "Iteration 13, loss = 1.24550676\n",
            "Iteration 14, loss = 1.24563572\n",
            "Iteration 15, loss = 1.24459590\n",
            "Iteration 16, loss = 1.24321489\n",
            "Iteration 17, loss = 1.24270211\n",
            "Iteration 18, loss = 1.24227349\n",
            "Iteration 19, loss = 1.24216618\n",
            "Iteration 20, loss = 1.24319788\n",
            "Iteration 21, loss = 1.24168745\n",
            "Iteration 22, loss = 1.24241170\n",
            "Iteration 23, loss = 1.24142479\n",
            "Iteration 24, loss = 1.24025862\n",
            "Iteration 25, loss = 1.23988614\n",
            "Iteration 26, loss = 1.24027971\n",
            "Iteration 27, loss = 1.24141648\n",
            "Iteration 28, loss = 1.23884541\n",
            "Iteration 29, loss = 1.24233649\n",
            "Iteration 30, loss = 1.23902430\n",
            "Iteration 31, loss = 1.23867688\n",
            "Iteration 32, loss = 1.24022111\n",
            "Iteration 33, loss = 1.23762611\n",
            "Iteration 34, loss = 1.23714391\n",
            "Iteration 35, loss = 1.23943295\n",
            "Iteration 36, loss = 1.23732970\n",
            "Iteration 37, loss = 1.23666087\n",
            "Iteration 38, loss = 1.23673093\n",
            "Iteration 39, loss = 1.23646888\n",
            "Iteration 40, loss = 1.23619947\n",
            "Iteration 41, loss = 1.23583410\n",
            "Iteration 42, loss = 1.23521660\n",
            "Iteration 43, loss = 1.23743609\n",
            "Iteration 44, loss = 1.23659614\n",
            "Iteration 45, loss = 1.23515906\n",
            "Iteration 46, loss = 1.23486198\n",
            "Iteration 47, loss = 1.23489354\n",
            "Iteration 48, loss = 1.23497545\n",
            "Iteration 49, loss = 1.23658843\n",
            "Iteration 50, loss = 1.23637569\n",
            "Iteration 51, loss = 1.23457660\n",
            "Iteration 52, loss = 1.23483226\n",
            "Iteration 53, loss = 1.23468840\n",
            "Iteration 54, loss = 1.23479742\n",
            "Iteration 55, loss = 1.23536400\n",
            "Iteration 56, loss = 1.23416985\n",
            "Iteration 57, loss = 1.23384422\n",
            "Iteration 58, loss = 1.23503112\n",
            "Iteration 59, loss = 1.23555889\n",
            "Iteration 60, loss = 1.23446612\n",
            "Iteration 61, loss = 1.23513431\n",
            "Iteration 62, loss = 1.23530376\n",
            "Iteration 63, loss = 1.23410738\n",
            "Iteration 64, loss = 1.23381743\n",
            "Iteration 65, loss = 1.23394724\n",
            "Iteration 66, loss = 1.23521110\n",
            "Iteration 67, loss = 1.23410355\n",
            "Iteration 68, loss = 1.23602266\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32594079\n",
            "Iteration 2, loss = 1.27528274\n",
            "Iteration 3, loss = 1.26049376\n",
            "Iteration 4, loss = 1.25248937\n",
            "Iteration 5, loss = 1.25002533\n",
            "Iteration 6, loss = 1.24939474\n",
            "Iteration 7, loss = 1.24852922\n",
            "Iteration 8, loss = 1.24649698\n",
            "Iteration 9, loss = 1.24839904\n",
            "Iteration 10, loss = 1.24770283\n",
            "Iteration 11, loss = 1.24480377\n",
            "Iteration 12, loss = 1.24562600\n",
            "Iteration 13, loss = 1.24530941\n",
            "Iteration 14, loss = 1.24333481\n",
            "Iteration 15, loss = 1.24214162\n",
            "Iteration 16, loss = 1.24206051\n",
            "Iteration 17, loss = 1.24257489\n",
            "Iteration 18, loss = 1.24138744\n",
            "Iteration 19, loss = 1.24030299\n",
            "Iteration 20, loss = 1.23990676\n",
            "Iteration 21, loss = 1.23977884\n",
            "Iteration 22, loss = 1.23958106\n",
            "Iteration 23, loss = 1.23817295\n",
            "Iteration 24, loss = 1.23772136\n",
            "Iteration 25, loss = 1.23704791\n",
            "Iteration 26, loss = 1.23622263\n",
            "Iteration 27, loss = 1.23651929\n",
            "Iteration 28, loss = 1.23736966\n",
            "Iteration 29, loss = 1.23595376\n",
            "Iteration 30, loss = 1.23526177\n",
            "Iteration 31, loss = 1.23583487\n",
            "Iteration 32, loss = 1.23399672\n",
            "Iteration 33, loss = 1.23374547\n",
            "Iteration 34, loss = 1.23386923\n",
            "Iteration 35, loss = 1.23419172\n",
            "Iteration 36, loss = 1.23376251\n",
            "Iteration 37, loss = 1.23307333\n",
            "Iteration 38, loss = 1.23339008\n",
            "Iteration 39, loss = 1.23309630\n",
            "Iteration 40, loss = 1.23196395\n",
            "Iteration 41, loss = 1.23313011\n",
            "Iteration 42, loss = 1.23249794\n",
            "Iteration 43, loss = 1.23202814\n",
            "Iteration 44, loss = 1.23189089\n",
            "Iteration 45, loss = 1.23141088\n",
            "Iteration 46, loss = 1.23155907\n",
            "Iteration 47, loss = 1.23084487\n",
            "Iteration 48, loss = 1.23170194\n",
            "Iteration 49, loss = 1.23140749\n",
            "Iteration 50, loss = 1.23087822\n",
            "Iteration 51, loss = 1.23254989\n",
            "Iteration 52, loss = 1.23028647\n",
            "Iteration 53, loss = 1.23145331\n",
            "Iteration 54, loss = 1.23183695\n",
            "Iteration 55, loss = 1.23157209\n",
            "Iteration 56, loss = 1.23132871\n",
            "Iteration 57, loss = 1.23014985\n",
            "Iteration 58, loss = 1.23162196\n",
            "Iteration 59, loss = 1.23135031\n",
            "Iteration 60, loss = 1.23057848\n",
            "Iteration 61, loss = 1.23157117\n",
            "Iteration 62, loss = 1.23000266\n",
            "Iteration 63, loss = 1.23421666\n",
            "Iteration 64, loss = 1.23199705\n",
            "Iteration 65, loss = 1.22985248\n",
            "Iteration 66, loss = 1.23006768\n",
            "Iteration 67, loss = 1.23008631\n",
            "Iteration 68, loss = 1.23114294\n",
            "Iteration 69, loss = 1.22974985\n",
            "Iteration 70, loss = 1.23071277\n",
            "Iteration 71, loss = 1.23009086\n",
            "Iteration 72, loss = 1.23011150\n",
            "Iteration 73, loss = 1.23044708\n",
            "Iteration 74, loss = 1.22965607\n",
            "Iteration 75, loss = 1.22979384\n",
            "Iteration 76, loss = 1.22943872\n",
            "Iteration 77, loss = 1.22993018\n",
            "Iteration 78, loss = 1.22984203\n",
            "Iteration 79, loss = 1.22985407\n",
            "Iteration 80, loss = 1.22923762\n",
            "Iteration 81, loss = 1.22960875\n",
            "Iteration 82, loss = 1.22968510\n",
            "Iteration 83, loss = 1.22889018\n",
            "Iteration 84, loss = 1.22963519\n",
            "Iteration 85, loss = 1.23069038\n",
            "Iteration 86, loss = 1.23052849\n",
            "Iteration 87, loss = 1.23059141\n",
            "Iteration 88, loss = 1.22975390\n",
            "Iteration 89, loss = 1.22861142\n",
            "Iteration 90, loss = 1.22825467\n",
            "Iteration 91, loss = 1.22894870\n",
            "Iteration 92, loss = 1.22888130\n",
            "Iteration 93, loss = 1.22971387\n",
            "Iteration 94, loss = 1.22859417\n",
            "Iteration 95, loss = 1.23031798\n",
            "Iteration 96, loss = 1.23172071\n",
            "Iteration 97, loss = 1.22986116\n",
            "Iteration 98, loss = 1.22873627\n",
            "Iteration 99, loss = 1.22874612\n",
            "Iteration 100, loss = 1.22834997\n",
            "Iteration 1, loss = 1.32362454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.27243884\n",
            "Iteration 3, loss = 1.25709562\n",
            "Iteration 4, loss = 1.24910259\n",
            "Iteration 5, loss = 1.24491612\n",
            "Iteration 6, loss = 1.24380451\n",
            "Iteration 7, loss = 1.24362717\n",
            "Iteration 8, loss = 1.24260854\n",
            "Iteration 9, loss = 1.24394471\n",
            "Iteration 10, loss = 1.24292011\n",
            "Iteration 11, loss = 1.24042875\n",
            "Iteration 12, loss = 1.24174979\n",
            "Iteration 13, loss = 1.24083576\n",
            "Iteration 14, loss = 1.23932292\n",
            "Iteration 15, loss = 1.23827238\n",
            "Iteration 16, loss = 1.23889206\n",
            "Iteration 17, loss = 1.23906966\n",
            "Iteration 18, loss = 1.23790712\n",
            "Iteration 19, loss = 1.23706382\n",
            "Iteration 20, loss = 1.23758717\n",
            "Iteration 21, loss = 1.23688993\n",
            "Iteration 22, loss = 1.23594502\n",
            "Iteration 23, loss = 1.23487606\n",
            "Iteration 24, loss = 1.23537645\n",
            "Iteration 25, loss = 1.23452847\n",
            "Iteration 26, loss = 1.23372765\n",
            "Iteration 27, loss = 1.23411728\n",
            "Iteration 28, loss = 1.23514748\n",
            "Iteration 29, loss = 1.23365610\n",
            "Iteration 30, loss = 1.23302946\n",
            "Iteration 31, loss = 1.23345223\n",
            "Iteration 32, loss = 1.23273278\n",
            "Iteration 33, loss = 1.23202286\n",
            "Iteration 34, loss = 1.23146080\n",
            "Iteration 35, loss = 1.23233024\n",
            "Iteration 36, loss = 1.23168623\n",
            "Iteration 37, loss = 1.23074401\n",
            "Iteration 38, loss = 1.23160449\n",
            "Iteration 39, loss = 1.23116915\n",
            "Iteration 40, loss = 1.23025130\n",
            "Iteration 41, loss = 1.23146479\n",
            "Iteration 42, loss = 1.23057577\n",
            "Iteration 43, loss = 1.22960723\n",
            "Iteration 44, loss = 1.22973010\n",
            "Iteration 45, loss = 1.22964140\n",
            "Iteration 46, loss = 1.22915778\n",
            "Iteration 47, loss = 1.22861751\n",
            "Iteration 48, loss = 1.22982153\n",
            "Iteration 49, loss = 1.22925788\n",
            "Iteration 50, loss = 1.22955675\n",
            "Iteration 51, loss = 1.22996444\n",
            "Iteration 52, loss = 1.22794002\n",
            "Iteration 53, loss = 1.22952329\n",
            "Iteration 54, loss = 1.22962011\n",
            "Iteration 55, loss = 1.22912522\n",
            "Iteration 56, loss = 1.22872702\n",
            "Iteration 57, loss = 1.22802834\n",
            "Iteration 58, loss = 1.22951311\n",
            "Iteration 59, loss = 1.22901847\n",
            "Iteration 60, loss = 1.22836490\n",
            "Iteration 61, loss = 1.22864679\n",
            "Iteration 62, loss = 1.22804190\n",
            "Iteration 63, loss = 1.23117295\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32331785\n",
            "Iteration 2, loss = 1.26619150\n",
            "Iteration 3, loss = 1.24723419\n",
            "Iteration 4, loss = 1.23728050\n",
            "Iteration 5, loss = 1.23259180\n",
            "Iteration 6, loss = 1.23150422\n",
            "Iteration 7, loss = 1.23112029\n",
            "Iteration 8, loss = 1.23017595\n",
            "Iteration 9, loss = 1.23178731\n",
            "Iteration 10, loss = 1.22923508\n",
            "Iteration 11, loss = 1.22775959\n",
            "Iteration 12, loss = 1.22911084\n",
            "Iteration 13, loss = 1.22789270\n",
            "Iteration 14, loss = 1.22695144\n",
            "Iteration 15, loss = 1.22578362\n",
            "Iteration 16, loss = 1.22580160\n",
            "Iteration 17, loss = 1.22617514\n",
            "Iteration 18, loss = 1.22576838\n",
            "Iteration 19, loss = 1.22477106\n",
            "Iteration 20, loss = 1.22538119\n",
            "Iteration 21, loss = 1.22437879\n",
            "Iteration 22, loss = 1.22318071\n",
            "Iteration 23, loss = 1.22314520\n",
            "Iteration 24, loss = 1.22409663\n",
            "Iteration 25, loss = 1.22288895\n",
            "Iteration 26, loss = 1.22270940\n",
            "Iteration 27, loss = 1.22195679\n",
            "Iteration 28, loss = 1.22362120\n",
            "Iteration 29, loss = 1.22286553\n",
            "Iteration 30, loss = 1.22202680\n",
            "Iteration 31, loss = 1.22214367\n",
            "Iteration 32, loss = 1.22101925\n",
            "Iteration 33, loss = 1.22013491\n",
            "Iteration 34, loss = 1.22001184\n",
            "Iteration 35, loss = 1.22057020\n",
            "Iteration 36, loss = 1.22051280\n",
            "Iteration 37, loss = 1.21973367\n",
            "Iteration 38, loss = 1.22046388\n",
            "Iteration 39, loss = 1.21959795\n",
            "Iteration 40, loss = 1.21883989\n",
            "Iteration 41, loss = 1.21923046\n",
            "Iteration 42, loss = 1.21854165\n",
            "Iteration 43, loss = 1.21866657\n",
            "Iteration 44, loss = 1.21877206\n",
            "Iteration 45, loss = 1.21852747\n",
            "Iteration 46, loss = 1.21772315\n",
            "Iteration 47, loss = 1.21750979\n",
            "Iteration 48, loss = 1.21845632\n",
            "Iteration 49, loss = 1.21753684\n",
            "Iteration 50, loss = 1.21833834\n",
            "Iteration 51, loss = 1.22005033\n",
            "Iteration 52, loss = 1.21688454\n",
            "Iteration 53, loss = 1.21892991\n",
            "Iteration 54, loss = 1.21943271\n",
            "Iteration 55, loss = 1.21784020\n",
            "Iteration 56, loss = 1.21738786\n",
            "Iteration 57, loss = 1.21704969\n",
            "Iteration 58, loss = 1.21861849\n",
            "Iteration 59, loss = 1.21812924\n",
            "Iteration 60, loss = 1.21705421\n",
            "Iteration 61, loss = 1.21674147\n",
            "Iteration 62, loss = 1.21746222\n",
            "Iteration 63, loss = 1.21921278\n",
            "Iteration 64, loss = 1.21751376\n",
            "Iteration 65, loss = 1.21687772\n",
            "Iteration 66, loss = 1.21662933\n",
            "Iteration 67, loss = 1.21644148\n",
            "Iteration 68, loss = 1.21714632\n",
            "Iteration 69, loss = 1.21671316\n",
            "Iteration 70, loss = 1.21897767\n",
            "Iteration 71, loss = 1.21639378\n",
            "Iteration 72, loss = 1.21642083\n",
            "Iteration 73, loss = 1.21779442\n",
            "Iteration 74, loss = 1.21649084\n",
            "Iteration 75, loss = 1.21618695\n",
            "Iteration 76, loss = 1.21627109\n",
            "Iteration 77, loss = 1.21655694\n",
            "Iteration 78, loss = 1.21761391\n",
            "Iteration 79, loss = 1.21697033\n",
            "Iteration 80, loss = 1.21608651\n",
            "Iteration 81, loss = 1.21636451\n",
            "Iteration 82, loss = 1.21664224\n",
            "Iteration 83, loss = 1.21635818\n",
            "Iteration 84, loss = 1.21509069\n",
            "Iteration 85, loss = 1.21789728\n",
            "Iteration 86, loss = 1.21813962\n",
            "Iteration 87, loss = 1.21839202\n",
            "Iteration 88, loss = 1.21668128\n",
            "Iteration 89, loss = 1.21525379\n",
            "Iteration 90, loss = 1.21626410\n",
            "Iteration 91, loss = 1.21657929\n",
            "Iteration 92, loss = 1.21560492\n",
            "Iteration 93, loss = 1.21619671\n",
            "Iteration 94, loss = 1.21590606\n",
            "Iteration 95, loss = 1.21711498\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "16\n",
            "Iteration 1, loss = 1.32468716\n",
            "Iteration 2, loss = 1.27626239\n",
            "Iteration 3, loss = 1.26271184\n",
            "Iteration 4, loss = 1.26139887\n",
            "Iteration 5, loss = 1.25924130\n",
            "Iteration 6, loss = 1.25768174\n",
            "Iteration 7, loss = 1.25704035\n",
            "Iteration 8, loss = 1.25581365\n",
            "Iteration 9, loss = 1.25476575\n",
            "Iteration 10, loss = 1.25445351\n",
            "Iteration 11, loss = 1.25465270\n",
            "Iteration 12, loss = 1.25406646\n",
            "Iteration 13, loss = 1.25249680\n",
            "Iteration 14, loss = 1.25321187\n",
            "Iteration 15, loss = 1.25175161\n",
            "Iteration 16, loss = 1.25122785\n",
            "Iteration 17, loss = 1.25166791\n",
            "Iteration 18, loss = 1.25164002\n",
            "Iteration 19, loss = 1.25097073\n",
            "Iteration 20, loss = 1.24920113\n",
            "Iteration 21, loss = 1.24831130\n",
            "Iteration 22, loss = 1.24795580\n",
            "Iteration 23, loss = 1.24852969\n",
            "Iteration 24, loss = 1.24729265\n",
            "Iteration 25, loss = 1.24594837\n",
            "Iteration 26, loss = 1.24668352\n",
            "Iteration 27, loss = 1.24606940\n",
            "Iteration 28, loss = 1.24445535\n",
            "Iteration 29, loss = 1.24468202\n",
            "Iteration 30, loss = 1.24435081\n",
            "Iteration 31, loss = 1.24447660\n",
            "Iteration 32, loss = 1.24678258\n",
            "Iteration 33, loss = 1.24587073\n",
            "Iteration 34, loss = 1.24481113\n",
            "Iteration 35, loss = 1.24562760\n",
            "Iteration 36, loss = 1.24435658\n",
            "Iteration 37, loss = 1.24513235\n",
            "Iteration 38, loss = 1.24427699\n",
            "Iteration 39, loss = 1.24280490\n",
            "Iteration 40, loss = 1.24207724\n",
            "Iteration 41, loss = 1.24180932\n",
            "Iteration 42, loss = 1.24137470\n",
            "Iteration 43, loss = 1.24123422\n",
            "Iteration 44, loss = 1.24194151\n",
            "Iteration 45, loss = 1.24025504\n",
            "Iteration 46, loss = 1.24167155\n",
            "Iteration 47, loss = 1.24242735\n",
            "Iteration 48, loss = 1.24103617\n",
            "Iteration 49, loss = 1.24028772\n",
            "Iteration 50, loss = 1.24052540\n",
            "Iteration 51, loss = 1.24005124\n",
            "Iteration 52, loss = 1.24067843\n",
            "Iteration 53, loss = 1.24187838\n",
            "Iteration 54, loss = 1.24079815\n",
            "Iteration 55, loss = 1.24151609\n",
            "Iteration 56, loss = 1.24067111\n",
            "Iteration 57, loss = 1.23974653\n",
            "Iteration 58, loss = 1.23997778\n",
            "Iteration 59, loss = 1.24027463\n",
            "Iteration 60, loss = 1.24005991\n",
            "Iteration 61, loss = 1.24040645\n",
            "Iteration 62, loss = 1.24054861\n",
            "Iteration 63, loss = 1.23948760\n",
            "Iteration 64, loss = 1.24002287\n",
            "Iteration 65, loss = 1.23891387\n",
            "Iteration 66, loss = 1.24035975\n",
            "Iteration 67, loss = 1.24012435\n",
            "Iteration 68, loss = 1.24007932\n",
            "Iteration 69, loss = 1.23945412\n",
            "Iteration 70, loss = 1.23987330\n",
            "Iteration 71, loss = 1.23920565\n",
            "Iteration 72, loss = 1.23903734\n",
            "Iteration 73, loss = 1.23863170\n",
            "Iteration 74, loss = 1.23882924\n",
            "Iteration 75, loss = 1.23907824\n",
            "Iteration 76, loss = 1.24045820\n",
            "Iteration 77, loss = 1.23973587\n",
            "Iteration 78, loss = 1.23829548\n",
            "Iteration 79, loss = 1.23951979\n",
            "Iteration 80, loss = 1.23989538\n",
            "Iteration 81, loss = 1.24035185\n",
            "Iteration 82, loss = 1.23976629\n",
            "Iteration 83, loss = 1.23819383\n",
            "Iteration 84, loss = 1.23874144\n",
            "Iteration 85, loss = 1.23964634\n",
            "Iteration 86, loss = 1.23956888\n",
            "Iteration 87, loss = 1.23827730\n",
            "Iteration 88, loss = 1.23813895\n",
            "Iteration 89, loss = 1.23951698\n",
            "Iteration 90, loss = 1.23951454\n",
            "Iteration 91, loss = 1.23831615\n",
            "Iteration 92, loss = 1.23977258\n",
            "Iteration 93, loss = 1.23792433\n",
            "Iteration 94, loss = 1.23798250\n",
            "Iteration 95, loss = 1.23855530\n",
            "Iteration 96, loss = 1.23822881\n",
            "Iteration 97, loss = 1.23880017\n",
            "Iteration 98, loss = 1.23842200\n",
            "Iteration 99, loss = 1.23796532\n",
            "Iteration 100, loss = 1.23780604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32035938\n",
            "Iteration 2, loss = 1.27221739\n",
            "Iteration 3, loss = 1.25911630\n",
            "Iteration 4, loss = 1.25907031\n",
            "Iteration 5, loss = 1.25760101\n",
            "Iteration 6, loss = 1.25446523\n",
            "Iteration 7, loss = 1.25400051\n",
            "Iteration 8, loss = 1.25323415\n",
            "Iteration 9, loss = 1.25220276\n",
            "Iteration 10, loss = 1.25201729\n",
            "Iteration 11, loss = 1.25186513\n",
            "Iteration 12, loss = 1.25097362\n",
            "Iteration 13, loss = 1.25001175\n",
            "Iteration 14, loss = 1.25041194\n",
            "Iteration 15, loss = 1.24978489\n",
            "Iteration 16, loss = 1.24914285\n",
            "Iteration 17, loss = 1.24922212\n",
            "Iteration 18, loss = 1.24917527\n",
            "Iteration 19, loss = 1.24881666\n",
            "Iteration 20, loss = 1.24786117\n",
            "Iteration 21, loss = 1.24690406\n",
            "Iteration 22, loss = 1.24638850\n",
            "Iteration 23, loss = 1.24629762\n",
            "Iteration 24, loss = 1.24572591\n",
            "Iteration 25, loss = 1.24476240\n",
            "Iteration 26, loss = 1.24486003\n",
            "Iteration 27, loss = 1.24477629\n",
            "Iteration 28, loss = 1.24340297\n",
            "Iteration 29, loss = 1.24406565\n",
            "Iteration 30, loss = 1.24302721\n",
            "Iteration 31, loss = 1.24378109\n",
            "Iteration 32, loss = 1.24466984\n",
            "Iteration 33, loss = 1.24416860\n",
            "Iteration 34, loss = 1.24410629\n",
            "Iteration 35, loss = 1.24455102\n",
            "Iteration 36, loss = 1.24349276\n",
            "Iteration 37, loss = 1.24418901\n",
            "Iteration 38, loss = 1.24389074\n",
            "Iteration 39, loss = 1.24249661\n",
            "Iteration 40, loss = 1.24175940\n",
            "Iteration 41, loss = 1.24097710\n",
            "Iteration 42, loss = 1.24167159\n",
            "Iteration 43, loss = 1.24235391\n",
            "Iteration 44, loss = 1.24257378\n",
            "Iteration 45, loss = 1.24083737\n",
            "Iteration 46, loss = 1.24040360\n",
            "Iteration 47, loss = 1.24119155\n",
            "Iteration 48, loss = 1.24079936\n",
            "Iteration 49, loss = 1.23995097\n",
            "Iteration 50, loss = 1.24092720\n",
            "Iteration 51, loss = 1.24027351\n",
            "Iteration 52, loss = 1.24045228\n",
            "Iteration 53, loss = 1.24035094\n",
            "Iteration 54, loss = 1.24057907\n",
            "Iteration 55, loss = 1.24108404\n",
            "Iteration 56, loss = 1.23994949\n",
            "Iteration 57, loss = 1.23970789\n",
            "Iteration 58, loss = 1.24089556\n",
            "Iteration 59, loss = 1.24044435\n",
            "Iteration 60, loss = 1.23934022\n",
            "Iteration 61, loss = 1.24038674\n",
            "Iteration 62, loss = 1.24081654\n",
            "Iteration 63, loss = 1.23968203\n",
            "Iteration 64, loss = 1.24062845\n",
            "Iteration 65, loss = 1.23956858\n",
            "Iteration 66, loss = 1.24109104\n",
            "Iteration 67, loss = 1.24075676\n",
            "Iteration 68, loss = 1.24019085\n",
            "Iteration 69, loss = 1.23964644\n",
            "Iteration 70, loss = 1.23980503\n",
            "Iteration 71, loss = 1.23992209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32718779\n",
            "Iteration 2, loss = 1.27834292\n",
            "Iteration 3, loss = 1.26495408\n",
            "Iteration 4, loss = 1.26434877\n",
            "Iteration 5, loss = 1.26447180\n",
            "Iteration 6, loss = 1.26190417\n",
            "Iteration 7, loss = 1.26192122\n",
            "Iteration 8, loss = 1.26138102\n",
            "Iteration 9, loss = 1.25874567\n",
            "Iteration 10, loss = 1.25766410\n",
            "Iteration 11, loss = 1.25729663\n",
            "Iteration 12, loss = 1.25723540\n",
            "Iteration 13, loss = 1.25632499\n",
            "Iteration 14, loss = 1.25568181\n",
            "Iteration 15, loss = 1.25570015\n",
            "Iteration 16, loss = 1.25530081\n",
            "Iteration 17, loss = 1.25434208\n",
            "Iteration 18, loss = 1.25497850\n",
            "Iteration 19, loss = 1.25481117\n",
            "Iteration 20, loss = 1.25396672\n",
            "Iteration 21, loss = 1.25269996\n",
            "Iteration 22, loss = 1.25223388\n",
            "Iteration 23, loss = 1.25222810\n",
            "Iteration 24, loss = 1.25143491\n",
            "Iteration 25, loss = 1.25034317\n",
            "Iteration 26, loss = 1.25031878\n",
            "Iteration 27, loss = 1.25100318\n",
            "Iteration 28, loss = 1.24914379\n",
            "Iteration 29, loss = 1.24969697\n",
            "Iteration 30, loss = 1.24824820\n",
            "Iteration 31, loss = 1.25045862\n",
            "Iteration 32, loss = 1.25130975\n",
            "Iteration 33, loss = 1.24895424\n",
            "Iteration 34, loss = 1.24952744\n",
            "Iteration 35, loss = 1.25135376\n",
            "Iteration 36, loss = 1.25123210\n",
            "Iteration 37, loss = 1.25003823\n",
            "Iteration 38, loss = 1.24813840\n",
            "Iteration 39, loss = 1.24750103\n",
            "Iteration 40, loss = 1.24749549\n",
            "Iteration 41, loss = 1.24730902\n",
            "Iteration 42, loss = 1.24670387\n",
            "Iteration 43, loss = 1.24691154\n",
            "Iteration 44, loss = 1.24713259\n",
            "Iteration 45, loss = 1.24572606\n",
            "Iteration 46, loss = 1.24652342\n",
            "Iteration 47, loss = 1.24681649\n",
            "Iteration 48, loss = 1.24572553\n",
            "Iteration 49, loss = 1.24543314\n",
            "Iteration 50, loss = 1.24578499\n",
            "Iteration 51, loss = 1.24524972\n",
            "Iteration 52, loss = 1.24553553\n",
            "Iteration 53, loss = 1.24718380\n",
            "Iteration 54, loss = 1.24577257\n",
            "Iteration 55, loss = 1.24566047\n",
            "Iteration 56, loss = 1.24540950\n",
            "Iteration 57, loss = 1.24584751\n",
            "Iteration 58, loss = 1.24686019\n",
            "Iteration 59, loss = 1.24601547\n",
            "Iteration 60, loss = 1.24458104\n",
            "Iteration 61, loss = 1.24543079\n",
            "Iteration 62, loss = 1.24597567\n",
            "Iteration 63, loss = 1.24496862\n",
            "Iteration 64, loss = 1.24499988\n",
            "Iteration 65, loss = 1.24412305\n",
            "Iteration 66, loss = 1.24604934\n",
            "Iteration 67, loss = 1.24602342\n",
            "Iteration 68, loss = 1.24598236\n",
            "Iteration 69, loss = 1.24551619\n",
            "Iteration 70, loss = 1.24533182\n",
            "Iteration 71, loss = 1.24474708\n",
            "Iteration 72, loss = 1.24393266\n",
            "Iteration 73, loss = 1.24369026\n",
            "Iteration 74, loss = 1.24384831\n",
            "Iteration 75, loss = 1.24454851\n",
            "Iteration 76, loss = 1.24563967\n",
            "Iteration 77, loss = 1.24510656\n",
            "Iteration 78, loss = 1.24385075\n",
            "Iteration 79, loss = 1.24428652\n",
            "Iteration 80, loss = 1.24462538\n",
            "Iteration 81, loss = 1.24558791\n",
            "Iteration 82, loss = 1.24550813\n",
            "Iteration 83, loss = 1.24422221\n",
            "Iteration 84, loss = 1.24495093\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33019743\n",
            "Iteration 2, loss = 1.28381623\n",
            "Iteration 3, loss = 1.26773479\n",
            "Iteration 4, loss = 1.26746563\n",
            "Iteration 5, loss = 1.26698004\n",
            "Iteration 6, loss = 1.26382810\n",
            "Iteration 7, loss = 1.26423274\n",
            "Iteration 8, loss = 1.26429865\n",
            "Iteration 9, loss = 1.26184518\n",
            "Iteration 10, loss = 1.26093329\n",
            "Iteration 11, loss = 1.26064737\n",
            "Iteration 12, loss = 1.26071759\n",
            "Iteration 13, loss = 1.25997913\n",
            "Iteration 14, loss = 1.25975649\n",
            "Iteration 15, loss = 1.25995475\n",
            "Iteration 16, loss = 1.25864459\n",
            "Iteration 17, loss = 1.25807596\n",
            "Iteration 18, loss = 1.25804286\n",
            "Iteration 19, loss = 1.25773321\n",
            "Iteration 20, loss = 1.25744271\n",
            "Iteration 21, loss = 1.25647873\n",
            "Iteration 22, loss = 1.25614509\n",
            "Iteration 23, loss = 1.25685803\n",
            "Iteration 24, loss = 1.25550104\n",
            "Iteration 25, loss = 1.25436613\n",
            "Iteration 26, loss = 1.25448953\n",
            "Iteration 27, loss = 1.25471635\n",
            "Iteration 28, loss = 1.25288687\n",
            "Iteration 29, loss = 1.25295534\n",
            "Iteration 30, loss = 1.25170194\n",
            "Iteration 31, loss = 1.25228961\n",
            "Iteration 32, loss = 1.25379627\n",
            "Iteration 33, loss = 1.25192451\n",
            "Iteration 34, loss = 1.25186343\n",
            "Iteration 35, loss = 1.25410338\n",
            "Iteration 36, loss = 1.25272930\n",
            "Iteration 37, loss = 1.25155897\n",
            "Iteration 38, loss = 1.25097109\n",
            "Iteration 39, loss = 1.25074965\n",
            "Iteration 40, loss = 1.25013394\n",
            "Iteration 41, loss = 1.25019116\n",
            "Iteration 42, loss = 1.24962224\n",
            "Iteration 43, loss = 1.25045287\n",
            "Iteration 44, loss = 1.24991082\n",
            "Iteration 45, loss = 1.24849680\n",
            "Iteration 46, loss = 1.24928239\n",
            "Iteration 47, loss = 1.24918132\n",
            "Iteration 48, loss = 1.24833594\n",
            "Iteration 49, loss = 1.24877164\n",
            "Iteration 50, loss = 1.24947627\n",
            "Iteration 51, loss = 1.24815905\n",
            "Iteration 52, loss = 1.24830052\n",
            "Iteration 53, loss = 1.24875295\n",
            "Iteration 54, loss = 1.24807704\n",
            "Iteration 55, loss = 1.24887243\n",
            "Iteration 56, loss = 1.24903498\n",
            "Iteration 57, loss = 1.24965076\n",
            "Iteration 58, loss = 1.24975856\n",
            "Iteration 59, loss = 1.24850346\n",
            "Iteration 60, loss = 1.24726794\n",
            "Iteration 61, loss = 1.24896425\n",
            "Iteration 62, loss = 1.24979804\n",
            "Iteration 63, loss = 1.24774027\n",
            "Iteration 64, loss = 1.24697269\n",
            "Iteration 65, loss = 1.24654667\n",
            "Iteration 66, loss = 1.24900944\n",
            "Iteration 67, loss = 1.24817207\n",
            "Iteration 68, loss = 1.24838926\n",
            "Iteration 69, loss = 1.24769828\n",
            "Iteration 70, loss = 1.24712478\n",
            "Iteration 71, loss = 1.24718725\n",
            "Iteration 72, loss = 1.24667568\n",
            "Iteration 73, loss = 1.24614850\n",
            "Iteration 74, loss = 1.24668399\n",
            "Iteration 75, loss = 1.24741093\n",
            "Iteration 76, loss = 1.24771731\n",
            "Iteration 77, loss = 1.24748203\n",
            "Iteration 78, loss = 1.24659027\n",
            "Iteration 79, loss = 1.24589177\n",
            "Iteration 80, loss = 1.24661014\n",
            "Iteration 81, loss = 1.24887037\n",
            "Iteration 82, loss = 1.24781212\n",
            "Iteration 83, loss = 1.24663273\n",
            "Iteration 84, loss = 1.24874223\n",
            "Iteration 85, loss = 1.24911798\n",
            "Iteration 86, loss = 1.24763192\n",
            "Iteration 87, loss = 1.24530955\n",
            "Iteration 88, loss = 1.24603559\n",
            "Iteration 89, loss = 1.24640427\n",
            "Iteration 90, loss = 1.24581662\n",
            "Iteration 91, loss = 1.24591159\n",
            "Iteration 92, loss = 1.24636758\n",
            "Iteration 93, loss = 1.24534363\n",
            "Iteration 94, loss = 1.24479898\n",
            "Iteration 95, loss = 1.24557343\n",
            "Iteration 96, loss = 1.24525241\n",
            "Iteration 97, loss = 1.24505107\n",
            "Iteration 98, loss = 1.24469285\n",
            "Iteration 99, loss = 1.24530649\n",
            "Iteration 100, loss = 1.24778201\n",
            "Iteration 1, loss = 1.32340284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.27442029\n",
            "Iteration 3, loss = 1.25896747\n",
            "Iteration 4, loss = 1.25938792\n",
            "Iteration 5, loss = 1.25811582\n",
            "Iteration 6, loss = 1.25471886\n",
            "Iteration 7, loss = 1.25550997\n",
            "Iteration 8, loss = 1.25546460\n",
            "Iteration 9, loss = 1.25272262\n",
            "Iteration 10, loss = 1.25173062\n",
            "Iteration 11, loss = 1.25107796\n",
            "Iteration 12, loss = 1.25121751\n",
            "Iteration 13, loss = 1.25173041\n",
            "Iteration 14, loss = 1.25115841\n",
            "Iteration 15, loss = 1.25086093\n",
            "Iteration 16, loss = 1.24958845\n",
            "Iteration 17, loss = 1.24905500\n",
            "Iteration 18, loss = 1.24903913\n",
            "Iteration 19, loss = 1.24810282\n",
            "Iteration 20, loss = 1.24770771\n",
            "Iteration 21, loss = 1.24677256\n",
            "Iteration 22, loss = 1.24773960\n",
            "Iteration 23, loss = 1.24716237\n",
            "Iteration 24, loss = 1.24613016\n",
            "Iteration 25, loss = 1.24506256\n",
            "Iteration 26, loss = 1.24496019\n",
            "Iteration 27, loss = 1.24549082\n",
            "Iteration 28, loss = 1.24433136\n",
            "Iteration 29, loss = 1.24504640\n",
            "Iteration 30, loss = 1.24305820\n",
            "Iteration 31, loss = 1.24321514\n",
            "Iteration 32, loss = 1.24462345\n",
            "Iteration 33, loss = 1.24331382\n",
            "Iteration 34, loss = 1.24253570\n",
            "Iteration 35, loss = 1.24406415\n",
            "Iteration 36, loss = 1.24359159\n",
            "Iteration 37, loss = 1.24252499\n",
            "Iteration 38, loss = 1.24215273\n",
            "Iteration 39, loss = 1.24180280\n",
            "Iteration 40, loss = 1.24092332\n",
            "Iteration 41, loss = 1.24047582\n",
            "Iteration 42, loss = 1.24112463\n",
            "Iteration 43, loss = 1.24213367\n",
            "Iteration 44, loss = 1.24046301\n",
            "Iteration 45, loss = 1.23941530\n",
            "Iteration 46, loss = 1.24003466\n",
            "Iteration 47, loss = 1.24053166\n",
            "Iteration 48, loss = 1.23955353\n",
            "Iteration 49, loss = 1.24009363\n",
            "Iteration 50, loss = 1.24061818\n",
            "Iteration 51, loss = 1.23924924\n",
            "Iteration 52, loss = 1.23910938\n",
            "Iteration 53, loss = 1.23940407\n",
            "Iteration 54, loss = 1.23885695\n",
            "Iteration 55, loss = 1.23885653\n",
            "Iteration 56, loss = 1.23862943\n",
            "Iteration 57, loss = 1.23964810\n",
            "Iteration 58, loss = 1.24039326\n",
            "Iteration 59, loss = 1.23931875\n",
            "Iteration 60, loss = 1.23784187\n",
            "Iteration 61, loss = 1.23886037\n",
            "Iteration 62, loss = 1.23941797\n",
            "Iteration 63, loss = 1.23807495\n",
            "Iteration 64, loss = 1.23773781\n",
            "Iteration 65, loss = 1.23798517\n",
            "Iteration 66, loss = 1.23890658\n",
            "Iteration 67, loss = 1.23823122\n",
            "Iteration 68, loss = 1.23914319\n",
            "Iteration 69, loss = 1.23901413\n",
            "Iteration 70, loss = 1.23785515\n",
            "Iteration 71, loss = 1.23822624\n",
            "Iteration 72, loss = 1.23771045\n",
            "Iteration 73, loss = 1.23729719\n",
            "Iteration 74, loss = 1.23801490\n",
            "Iteration 75, loss = 1.23761347\n",
            "Iteration 76, loss = 1.23744137\n",
            "Iteration 77, loss = 1.23748354\n",
            "Iteration 78, loss = 1.23706759\n",
            "Iteration 79, loss = 1.23723437\n",
            "Iteration 80, loss = 1.23808215\n",
            "Iteration 81, loss = 1.23863466\n",
            "Iteration 82, loss = 1.23804340\n",
            "Iteration 83, loss = 1.23786814\n",
            "Iteration 84, loss = 1.23899523\n",
            "Iteration 85, loss = 1.23881664\n",
            "Iteration 86, loss = 1.23844404\n",
            "Iteration 87, loss = 1.23623139\n",
            "Iteration 88, loss = 1.23648871\n",
            "Iteration 89, loss = 1.23723275\n",
            "Iteration 90, loss = 1.23691150\n",
            "Iteration 91, loss = 1.23626542\n",
            "Iteration 92, loss = 1.23704766\n",
            "Iteration 93, loss = 1.23759526\n",
            "Iteration 94, loss = 1.23634051\n",
            "Iteration 95, loss = 1.23636747\n",
            "Iteration 96, loss = 1.23627869\n",
            "Iteration 97, loss = 1.23571537\n",
            "Iteration 98, loss = 1.23622996\n",
            "Iteration 99, loss = 1.23724398\n",
            "Iteration 100, loss = 1.23789486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32004033\n",
            "Iteration 2, loss = 1.26695942\n",
            "Iteration 3, loss = 1.24947019\n",
            "Iteration 4, loss = 1.24925154\n",
            "Iteration 5, loss = 1.24955839\n",
            "Iteration 6, loss = 1.24399776\n",
            "Iteration 7, loss = 1.24381157\n",
            "Iteration 8, loss = 1.24504400\n",
            "Iteration 9, loss = 1.24299715\n",
            "Iteration 10, loss = 1.24146315\n",
            "Iteration 11, loss = 1.24015844\n",
            "Iteration 12, loss = 1.23993552\n",
            "Iteration 13, loss = 1.24012662\n",
            "Iteration 14, loss = 1.24009280\n",
            "Iteration 15, loss = 1.23918833\n",
            "Iteration 16, loss = 1.23797420\n",
            "Iteration 17, loss = 1.23718406\n",
            "Iteration 18, loss = 1.23702517\n",
            "Iteration 19, loss = 1.23576465\n",
            "Iteration 20, loss = 1.23601323\n",
            "Iteration 21, loss = 1.23497683\n",
            "Iteration 22, loss = 1.23547725\n",
            "Iteration 23, loss = 1.23488748\n",
            "Iteration 24, loss = 1.23381715\n",
            "Iteration 25, loss = 1.23342563\n",
            "Iteration 26, loss = 1.23334775\n",
            "Iteration 27, loss = 1.23390666\n",
            "Iteration 28, loss = 1.23208874\n",
            "Iteration 29, loss = 1.23384140\n",
            "Iteration 30, loss = 1.23228070\n",
            "Iteration 31, loss = 1.23201036\n",
            "Iteration 32, loss = 1.23280713\n",
            "Iteration 33, loss = 1.23074910\n",
            "Iteration 34, loss = 1.23026623\n",
            "Iteration 35, loss = 1.23240477\n",
            "Iteration 36, loss = 1.23115355\n",
            "Iteration 37, loss = 1.23002977\n",
            "Iteration 38, loss = 1.22964563\n",
            "Iteration 39, loss = 1.23023660\n",
            "Iteration 40, loss = 1.23019288\n",
            "Iteration 41, loss = 1.22940229\n",
            "Iteration 42, loss = 1.22887489\n",
            "Iteration 43, loss = 1.23030343\n",
            "Iteration 44, loss = 1.22987318\n",
            "Iteration 45, loss = 1.22852526\n",
            "Iteration 46, loss = 1.22821235\n",
            "Iteration 47, loss = 1.22942038\n",
            "Iteration 48, loss = 1.22894404\n",
            "Iteration 49, loss = 1.22930756\n",
            "Iteration 50, loss = 1.22977061\n",
            "Iteration 51, loss = 1.22774990\n",
            "Iteration 52, loss = 1.22805264\n",
            "Iteration 53, loss = 1.22812660\n",
            "Iteration 54, loss = 1.22790402\n",
            "Iteration 55, loss = 1.22781793\n",
            "Iteration 56, loss = 1.22736415\n",
            "Iteration 57, loss = 1.22744765\n",
            "Iteration 58, loss = 1.22857333\n",
            "Iteration 59, loss = 1.22819795\n",
            "Iteration 60, loss = 1.22720473\n",
            "Iteration 61, loss = 1.22788876\n",
            "Iteration 62, loss = 1.22839226\n",
            "Iteration 63, loss = 1.22686742\n",
            "Iteration 64, loss = 1.22683508\n",
            "Iteration 65, loss = 1.22668443\n",
            "Iteration 66, loss = 1.22722664\n",
            "Iteration 67, loss = 1.22757574\n",
            "Iteration 68, loss = 1.22736531\n",
            "Iteration 69, loss = 1.22728370\n",
            "Iteration 70, loss = 1.22703005\n",
            "Iteration 71, loss = 1.22657503\n",
            "Iteration 72, loss = 1.22642254\n",
            "Iteration 73, loss = 1.22633262\n",
            "Iteration 74, loss = 1.22689111\n",
            "Iteration 75, loss = 1.22636361\n",
            "Iteration 76, loss = 1.22619972\n",
            "Iteration 77, loss = 1.22613679\n",
            "Iteration 78, loss = 1.22614530\n",
            "Iteration 79, loss = 1.22634484\n",
            "Iteration 80, loss = 1.22630336\n",
            "Iteration 81, loss = 1.22689366\n",
            "Iteration 82, loss = 1.22661505\n",
            "Iteration 83, loss = 1.22606809\n",
            "Iteration 84, loss = 1.22741022\n",
            "Iteration 85, loss = 1.22747521\n",
            "Iteration 86, loss = 1.22719713\n",
            "Iteration 87, loss = 1.22617092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32693972\n",
            "Iteration 2, loss = 1.27478173\n",
            "Iteration 3, loss = 1.25719300\n",
            "Iteration 4, loss = 1.25430809\n",
            "Iteration 5, loss = 1.25326874\n",
            "Iteration 6, loss = 1.24929473\n",
            "Iteration 7, loss = 1.24906335\n",
            "Iteration 8, loss = 1.24985532\n",
            "Iteration 9, loss = 1.24786240\n",
            "Iteration 10, loss = 1.24675332\n",
            "Iteration 11, loss = 1.24561484\n",
            "Iteration 12, loss = 1.24518203\n",
            "Iteration 13, loss = 1.24567906\n",
            "Iteration 14, loss = 1.24599787\n",
            "Iteration 15, loss = 1.24494395\n",
            "Iteration 16, loss = 1.24333895\n",
            "Iteration 17, loss = 1.24268401\n",
            "Iteration 18, loss = 1.24245797\n",
            "Iteration 19, loss = 1.24215738\n",
            "Iteration 20, loss = 1.24300161\n",
            "Iteration 21, loss = 1.24135896\n",
            "Iteration 22, loss = 1.24241772\n",
            "Iteration 23, loss = 1.24164936\n",
            "Iteration 24, loss = 1.24009643\n",
            "Iteration 25, loss = 1.23968864\n",
            "Iteration 26, loss = 1.24012130\n",
            "Iteration 27, loss = 1.24134186\n",
            "Iteration 28, loss = 1.23886141\n",
            "Iteration 29, loss = 1.24234462\n",
            "Iteration 30, loss = 1.23945185\n",
            "Iteration 31, loss = 1.23864483\n",
            "Iteration 32, loss = 1.24022412\n",
            "Iteration 33, loss = 1.23805353\n",
            "Iteration 34, loss = 1.23728022\n",
            "Iteration 35, loss = 1.23958169\n",
            "Iteration 36, loss = 1.23788352\n",
            "Iteration 37, loss = 1.23688546\n",
            "Iteration 38, loss = 1.23653901\n",
            "Iteration 39, loss = 1.23665397\n",
            "Iteration 40, loss = 1.23651685\n",
            "Iteration 41, loss = 1.23626241\n",
            "Iteration 42, loss = 1.23541900\n",
            "Iteration 43, loss = 1.23748327\n",
            "Iteration 44, loss = 1.23661219\n",
            "Iteration 45, loss = 1.23525414\n",
            "Iteration 46, loss = 1.23511380\n",
            "Iteration 47, loss = 1.23523676\n",
            "Iteration 48, loss = 1.23500726\n",
            "Iteration 49, loss = 1.23698297\n",
            "Iteration 50, loss = 1.23683177\n",
            "Iteration 51, loss = 1.23475553\n",
            "Iteration 52, loss = 1.23499327\n",
            "Iteration 53, loss = 1.23505039\n",
            "Iteration 54, loss = 1.23482832\n",
            "Iteration 55, loss = 1.23502416\n",
            "Iteration 56, loss = 1.23420011\n",
            "Iteration 57, loss = 1.23387294\n",
            "Iteration 58, loss = 1.23504641\n",
            "Iteration 59, loss = 1.23539665\n",
            "Iteration 60, loss = 1.23452377\n",
            "Iteration 61, loss = 1.23527562\n",
            "Iteration 62, loss = 1.23554588\n",
            "Iteration 63, loss = 1.23425758\n",
            "Iteration 64, loss = 1.23385312\n",
            "Iteration 65, loss = 1.23388074\n",
            "Iteration 66, loss = 1.23532284\n",
            "Iteration 67, loss = 1.23452462\n",
            "Iteration 68, loss = 1.23579948\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32594103\n",
            "Iteration 2, loss = 1.27528188\n",
            "Iteration 3, loss = 1.26048111\n",
            "Iteration 4, loss = 1.25247874\n",
            "Iteration 5, loss = 1.24999128\n",
            "Iteration 6, loss = 1.24937760\n",
            "Iteration 7, loss = 1.24854588\n",
            "Iteration 8, loss = 1.24645276\n",
            "Iteration 9, loss = 1.24839050\n",
            "Iteration 10, loss = 1.24766351\n",
            "Iteration 11, loss = 1.24466079\n",
            "Iteration 12, loss = 1.24562869\n",
            "Iteration 13, loss = 1.24520399\n",
            "Iteration 14, loss = 1.24315952\n",
            "Iteration 15, loss = 1.24186878\n",
            "Iteration 16, loss = 1.24183285\n",
            "Iteration 17, loss = 1.24238144\n",
            "Iteration 18, loss = 1.24117185\n",
            "Iteration 19, loss = 1.23993345\n",
            "Iteration 20, loss = 1.23998987\n",
            "Iteration 21, loss = 1.24034679\n",
            "Iteration 22, loss = 1.23972346\n",
            "Iteration 23, loss = 1.23792098\n",
            "Iteration 24, loss = 1.23768875\n",
            "Iteration 25, loss = 1.23692827\n",
            "Iteration 26, loss = 1.23598752\n",
            "Iteration 27, loss = 1.23671336\n",
            "Iteration 28, loss = 1.23765802\n",
            "Iteration 29, loss = 1.23574886\n",
            "Iteration 30, loss = 1.23512346\n",
            "Iteration 31, loss = 1.23578866\n",
            "Iteration 32, loss = 1.23428594\n",
            "Iteration 33, loss = 1.23389949\n",
            "Iteration 34, loss = 1.23383474\n",
            "Iteration 35, loss = 1.23409291\n",
            "Iteration 36, loss = 1.23367019\n",
            "Iteration 37, loss = 1.23311272\n",
            "Iteration 38, loss = 1.23353065\n",
            "Iteration 39, loss = 1.23302120\n",
            "Iteration 40, loss = 1.23193356\n",
            "Iteration 41, loss = 1.23279267\n",
            "Iteration 42, loss = 1.23248438\n",
            "Iteration 43, loss = 1.23193559\n",
            "Iteration 44, loss = 1.23159370\n",
            "Iteration 45, loss = 1.23123061\n",
            "Iteration 46, loss = 1.23144729\n",
            "Iteration 47, loss = 1.23082171\n",
            "Iteration 48, loss = 1.23158941\n",
            "Iteration 49, loss = 1.23108531\n",
            "Iteration 50, loss = 1.23090188\n",
            "Iteration 51, loss = 1.23246135\n",
            "Iteration 52, loss = 1.23006521\n",
            "Iteration 53, loss = 1.23121467\n",
            "Iteration 54, loss = 1.23130582\n",
            "Iteration 55, loss = 1.23143801\n",
            "Iteration 56, loss = 1.23111481\n",
            "Iteration 57, loss = 1.22997574\n",
            "Iteration 58, loss = 1.23119623\n",
            "Iteration 59, loss = 1.23118130\n",
            "Iteration 60, loss = 1.23081863\n",
            "Iteration 61, loss = 1.23152132\n",
            "Iteration 62, loss = 1.22975226\n",
            "Iteration 63, loss = 1.23446042\n",
            "Iteration 64, loss = 1.23185388\n",
            "Iteration 65, loss = 1.22951748\n",
            "Iteration 66, loss = 1.22985103\n",
            "Iteration 67, loss = 1.22993050\n",
            "Iteration 68, loss = 1.23104627\n",
            "Iteration 69, loss = 1.22974206\n",
            "Iteration 70, loss = 1.23042852\n",
            "Iteration 71, loss = 1.22971934\n",
            "Iteration 72, loss = 1.23005561\n",
            "Iteration 73, loss = 1.23013020\n",
            "Iteration 74, loss = 1.22930931\n",
            "Iteration 75, loss = 1.22940648\n",
            "Iteration 76, loss = 1.22892990\n",
            "Iteration 77, loss = 1.22921590\n",
            "Iteration 78, loss = 1.22935175\n",
            "Iteration 79, loss = 1.22944359\n",
            "Iteration 80, loss = 1.22889021\n",
            "Iteration 81, loss = 1.22925305\n",
            "Iteration 82, loss = 1.22927009\n",
            "Iteration 83, loss = 1.22857172\n",
            "Iteration 84, loss = 1.22902525\n",
            "Iteration 85, loss = 1.22997939\n",
            "Iteration 86, loss = 1.22999388\n",
            "Iteration 87, loss = 1.22987212\n",
            "Iteration 88, loss = 1.22893444\n",
            "Iteration 89, loss = 1.22805836\n",
            "Iteration 90, loss = 1.22768372\n",
            "Iteration 91, loss = 1.22837552\n",
            "Iteration 92, loss = 1.22828383\n",
            "Iteration 93, loss = 1.22887509\n",
            "Iteration 94, loss = 1.22837050\n",
            "Iteration 95, loss = 1.22972307\n",
            "Iteration 96, loss = 1.23020756\n",
            "Iteration 97, loss = 1.22890472\n",
            "Iteration 98, loss = 1.22796037\n",
            "Iteration 99, loss = 1.22778052\n",
            "Iteration 100, loss = 1.22745124\n",
            "Iteration 1, loss = 1.32362421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.27246915\n",
            "Iteration 3, loss = 1.25712082\n",
            "Iteration 4, loss = 1.24896628\n",
            "Iteration 5, loss = 1.24483839\n",
            "Iteration 6, loss = 1.24377344\n",
            "Iteration 7, loss = 1.24362803\n",
            "Iteration 8, loss = 1.24260044\n",
            "Iteration 9, loss = 1.24399525\n",
            "Iteration 10, loss = 1.24292566\n",
            "Iteration 11, loss = 1.24047196\n",
            "Iteration 12, loss = 1.24183137\n",
            "Iteration 13, loss = 1.24094472\n",
            "Iteration 14, loss = 1.23936205\n",
            "Iteration 15, loss = 1.23831171\n",
            "Iteration 16, loss = 1.23900889\n",
            "Iteration 17, loss = 1.23910409\n",
            "Iteration 18, loss = 1.23784875\n",
            "Iteration 19, loss = 1.23705914\n",
            "Iteration 20, loss = 1.23758479\n",
            "Iteration 21, loss = 1.23685088\n",
            "Iteration 22, loss = 1.23597775\n",
            "Iteration 23, loss = 1.23477713\n",
            "Iteration 24, loss = 1.23529337\n",
            "Iteration 25, loss = 1.23440416\n",
            "Iteration 26, loss = 1.23359274\n",
            "Iteration 27, loss = 1.23432204\n",
            "Iteration 28, loss = 1.23536863\n",
            "Iteration 29, loss = 1.23377490\n",
            "Iteration 30, loss = 1.23313539\n",
            "Iteration 31, loss = 1.23381333\n",
            "Iteration 32, loss = 1.23318358\n",
            "Iteration 33, loss = 1.23209117\n",
            "Iteration 34, loss = 1.23140965\n",
            "Iteration 35, loss = 1.23212223\n",
            "Iteration 36, loss = 1.23156063\n",
            "Iteration 37, loss = 1.23080089\n",
            "Iteration 38, loss = 1.23168742\n",
            "Iteration 39, loss = 1.23116763\n",
            "Iteration 40, loss = 1.23013263\n",
            "Iteration 41, loss = 1.23116741\n",
            "Iteration 42, loss = 1.23028463\n",
            "Iteration 43, loss = 1.22947109\n",
            "Iteration 44, loss = 1.22960727\n",
            "Iteration 45, loss = 1.22959554\n",
            "Iteration 46, loss = 1.22905987\n",
            "Iteration 47, loss = 1.22862055\n",
            "Iteration 48, loss = 1.22981237\n",
            "Iteration 49, loss = 1.22914863\n",
            "Iteration 50, loss = 1.22959979\n",
            "Iteration 51, loss = 1.23007807\n",
            "Iteration 52, loss = 1.22798388\n",
            "Iteration 53, loss = 1.22948191\n",
            "Iteration 54, loss = 1.22975575\n",
            "Iteration 55, loss = 1.22917052\n",
            "Iteration 56, loss = 1.22859148\n",
            "Iteration 57, loss = 1.22799182\n",
            "Iteration 58, loss = 1.22978824\n",
            "Iteration 59, loss = 1.22908656\n",
            "Iteration 60, loss = 1.22840946\n",
            "Iteration 61, loss = 1.22875597\n",
            "Iteration 62, loss = 1.22813758\n",
            "Iteration 63, loss = 1.23110107\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32331740\n",
            "Iteration 2, loss = 1.26619121\n",
            "Iteration 3, loss = 1.24723187\n",
            "Iteration 4, loss = 1.23728884\n",
            "Iteration 5, loss = 1.23261071\n",
            "Iteration 6, loss = 1.23152714\n",
            "Iteration 7, loss = 1.23112812\n",
            "Iteration 8, loss = 1.23018145\n",
            "Iteration 9, loss = 1.23181852\n",
            "Iteration 10, loss = 1.22923265\n",
            "Iteration 11, loss = 1.22776324\n",
            "Iteration 12, loss = 1.22911569\n",
            "Iteration 13, loss = 1.22791099\n",
            "Iteration 14, loss = 1.22695127\n",
            "Iteration 15, loss = 1.22581877\n",
            "Iteration 16, loss = 1.22580948\n",
            "Iteration 17, loss = 1.22614742\n",
            "Iteration 18, loss = 1.22573570\n",
            "Iteration 19, loss = 1.22477518\n",
            "Iteration 20, loss = 1.22545667\n",
            "Iteration 21, loss = 1.22446201\n",
            "Iteration 22, loss = 1.22318790\n",
            "Iteration 23, loss = 1.22322543\n",
            "Iteration 24, loss = 1.22422989\n",
            "Iteration 25, loss = 1.22301234\n",
            "Iteration 26, loss = 1.22290973\n",
            "Iteration 27, loss = 1.22212306\n",
            "Iteration 28, loss = 1.22351246\n",
            "Iteration 29, loss = 1.22298549\n",
            "Iteration 30, loss = 1.22227872\n",
            "Iteration 31, loss = 1.22229484\n",
            "Iteration 32, loss = 1.22112801\n",
            "Iteration 33, loss = 1.22024764\n",
            "Iteration 34, loss = 1.22001864\n",
            "Iteration 35, loss = 1.22078135\n",
            "Iteration 36, loss = 1.22076792\n",
            "Iteration 37, loss = 1.21991222\n",
            "Iteration 38, loss = 1.22059432\n",
            "Iteration 39, loss = 1.21970105\n",
            "Iteration 40, loss = 1.21908113\n",
            "Iteration 41, loss = 1.21941942\n",
            "Iteration 42, loss = 1.21886555\n",
            "Iteration 43, loss = 1.21883470\n",
            "Iteration 44, loss = 1.21887899\n",
            "Iteration 45, loss = 1.21868068\n",
            "Iteration 46, loss = 1.21784288\n",
            "Iteration 47, loss = 1.21767428\n",
            "Iteration 48, loss = 1.21830161\n",
            "Iteration 49, loss = 1.21764841\n",
            "Iteration 50, loss = 1.21844109\n",
            "Iteration 51, loss = 1.22009226\n",
            "Iteration 52, loss = 1.21705594\n",
            "Iteration 53, loss = 1.21931654\n",
            "Iteration 54, loss = 1.21966008\n",
            "Iteration 55, loss = 1.21801597\n",
            "Iteration 56, loss = 1.21763023\n",
            "Iteration 57, loss = 1.21722946\n",
            "Iteration 58, loss = 1.21878703\n",
            "Iteration 59, loss = 1.21832982\n",
            "Iteration 60, loss = 1.21744617\n",
            "Iteration 61, loss = 1.21683778\n",
            "Iteration 62, loss = 1.21747710\n",
            "Iteration 63, loss = 1.21946311\n",
            "Iteration 64, loss = 1.21759535\n",
            "Iteration 65, loss = 1.21684200\n",
            "Iteration 66, loss = 1.21674592\n",
            "Iteration 67, loss = 1.21664440\n",
            "Iteration 68, loss = 1.21733746\n",
            "Iteration 69, loss = 1.21700077\n",
            "Iteration 70, loss = 1.21911749\n",
            "Iteration 71, loss = 1.21661050\n",
            "Iteration 72, loss = 1.21659717\n",
            "Iteration 73, loss = 1.21787021\n",
            "Iteration 74, loss = 1.21658397\n",
            "Iteration 75, loss = 1.21634396\n",
            "Iteration 76, loss = 1.21655159\n",
            "Iteration 77, loss = 1.21673886\n",
            "Iteration 78, loss = 1.21791955\n",
            "Iteration 79, loss = 1.21744352\n",
            "Iteration 80, loss = 1.21645021\n",
            "Iteration 81, loss = 1.21651720\n",
            "Iteration 82, loss = 1.21676415\n",
            "Iteration 83, loss = 1.21656898\n",
            "Iteration 84, loss = 1.21522260\n",
            "Iteration 85, loss = 1.21795009\n",
            "Iteration 86, loss = 1.21822293\n",
            "Iteration 87, loss = 1.21838242\n",
            "Iteration 88, loss = 1.21707756\n",
            "Iteration 89, loss = 1.21543937\n",
            "Iteration 90, loss = 1.21620262\n",
            "Iteration 91, loss = 1.21646488\n",
            "Iteration 92, loss = 1.21584747\n",
            "Iteration 93, loss = 1.21636126\n",
            "Iteration 94, loss = 1.21603351\n",
            "Iteration 95, loss = 1.21726109\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "17\n",
            "Iteration 1, loss = 1.32468746\n",
            "Iteration 2, loss = 1.27626215\n",
            "Iteration 3, loss = 1.26271186\n",
            "Iteration 4, loss = 1.26139890\n",
            "Iteration 5, loss = 1.25924144\n",
            "Iteration 6, loss = 1.25768188\n",
            "Iteration 7, loss = 1.25704047\n",
            "Iteration 8, loss = 1.25581355\n",
            "Iteration 9, loss = 1.25476576\n",
            "Iteration 10, loss = 1.25445341\n",
            "Iteration 11, loss = 1.25465270\n",
            "Iteration 12, loss = 1.25406635\n",
            "Iteration 13, loss = 1.25249694\n",
            "Iteration 14, loss = 1.25321208\n",
            "Iteration 15, loss = 1.25175216\n",
            "Iteration 16, loss = 1.25122864\n",
            "Iteration 17, loss = 1.25166881\n",
            "Iteration 18, loss = 1.25163464\n",
            "Iteration 19, loss = 1.25097277\n",
            "Iteration 20, loss = 1.24920622\n",
            "Iteration 21, loss = 1.24830437\n",
            "Iteration 22, loss = 1.24794888\n",
            "Iteration 23, loss = 1.24853283\n",
            "Iteration 24, loss = 1.24730193\n",
            "Iteration 25, loss = 1.24595016\n",
            "Iteration 26, loss = 1.24670721\n",
            "Iteration 27, loss = 1.24609201\n",
            "Iteration 28, loss = 1.24449056\n",
            "Iteration 29, loss = 1.24461249\n",
            "Iteration 30, loss = 1.24419692\n",
            "Iteration 31, loss = 1.24459455\n",
            "Iteration 32, loss = 1.24692710\n",
            "Iteration 33, loss = 1.24589933\n",
            "Iteration 34, loss = 1.24492967\n",
            "Iteration 35, loss = 1.24596564\n",
            "Iteration 36, loss = 1.24466668\n",
            "Iteration 37, loss = 1.24519196\n",
            "Iteration 38, loss = 1.24440806\n",
            "Iteration 39, loss = 1.24289165\n",
            "Iteration 40, loss = 1.24212779\n",
            "Iteration 41, loss = 1.24191591\n",
            "Iteration 42, loss = 1.24150257\n",
            "Iteration 43, loss = 1.24138444\n",
            "Iteration 44, loss = 1.24185183\n",
            "Iteration 45, loss = 1.24042895\n",
            "Iteration 46, loss = 1.24180735\n",
            "Iteration 47, loss = 1.24249186\n",
            "Iteration 48, loss = 1.24107096\n",
            "Iteration 49, loss = 1.24030422\n",
            "Iteration 50, loss = 1.24051476\n",
            "Iteration 51, loss = 1.24005958\n",
            "Iteration 52, loss = 1.24050675\n",
            "Iteration 53, loss = 1.24179514\n",
            "Iteration 54, loss = 1.24071730\n",
            "Iteration 55, loss = 1.24144795\n",
            "Iteration 56, loss = 1.24056583\n",
            "Iteration 57, loss = 1.23975455\n",
            "Iteration 58, loss = 1.24003157\n",
            "Iteration 59, loss = 1.24021100\n",
            "Iteration 60, loss = 1.24002134\n",
            "Iteration 61, loss = 1.24020701\n",
            "Iteration 62, loss = 1.24037454\n",
            "Iteration 63, loss = 1.23955758\n",
            "Iteration 64, loss = 1.24007193\n",
            "Iteration 65, loss = 1.23901860\n",
            "Iteration 66, loss = 1.24029423\n",
            "Iteration 67, loss = 1.24000921\n",
            "Iteration 68, loss = 1.23998248\n",
            "Iteration 69, loss = 1.23940288\n",
            "Iteration 70, loss = 1.23981514\n",
            "Iteration 71, loss = 1.23926653\n",
            "Iteration 72, loss = 1.23909430\n",
            "Iteration 73, loss = 1.23862566\n",
            "Iteration 74, loss = 1.23879938\n",
            "Iteration 75, loss = 1.23902682\n",
            "Iteration 76, loss = 1.24049014\n",
            "Iteration 77, loss = 1.23982338\n",
            "Iteration 78, loss = 1.23837190\n",
            "Iteration 79, loss = 1.23961441\n",
            "Iteration 80, loss = 1.23985995\n",
            "Iteration 81, loss = 1.24043494\n",
            "Iteration 82, loss = 1.23991126\n",
            "Iteration 83, loss = 1.23813165\n",
            "Iteration 84, loss = 1.23867401\n",
            "Iteration 85, loss = 1.23948235\n",
            "Iteration 86, loss = 1.23947963\n",
            "Iteration 87, loss = 1.23820747\n",
            "Iteration 88, loss = 1.23818240\n",
            "Iteration 89, loss = 1.23949013\n",
            "Iteration 90, loss = 1.23938652\n",
            "Iteration 91, loss = 1.23810098\n",
            "Iteration 92, loss = 1.24021795\n",
            "Iteration 93, loss = 1.23814245\n",
            "Iteration 94, loss = 1.23833889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32035967\n",
            "Iteration 2, loss = 1.27221715\n",
            "Iteration 3, loss = 1.25911638\n",
            "Iteration 4, loss = 1.25907032\n",
            "Iteration 5, loss = 1.25760145\n",
            "Iteration 6, loss = 1.25446591\n",
            "Iteration 7, loss = 1.25400026\n",
            "Iteration 8, loss = 1.25323393\n",
            "Iteration 9, loss = 1.25220273\n",
            "Iteration 10, loss = 1.25201522\n",
            "Iteration 11, loss = 1.25186692\n",
            "Iteration 12, loss = 1.25097640\n",
            "Iteration 13, loss = 1.25001519\n",
            "Iteration 14, loss = 1.25041369\n",
            "Iteration 15, loss = 1.24979543\n",
            "Iteration 16, loss = 1.24915273\n",
            "Iteration 17, loss = 1.24923834\n",
            "Iteration 18, loss = 1.24919655\n",
            "Iteration 19, loss = 1.24883600\n",
            "Iteration 20, loss = 1.24786591\n",
            "Iteration 21, loss = 1.24690954\n",
            "Iteration 22, loss = 1.24639351\n",
            "Iteration 23, loss = 1.24632432\n",
            "Iteration 24, loss = 1.24573040\n",
            "Iteration 25, loss = 1.24475398\n",
            "Iteration 26, loss = 1.24485980\n",
            "Iteration 27, loss = 1.24477845\n",
            "Iteration 28, loss = 1.24346740\n",
            "Iteration 29, loss = 1.24407083\n",
            "Iteration 30, loss = 1.24304952\n",
            "Iteration 31, loss = 1.24375056\n",
            "Iteration 32, loss = 1.24459656\n",
            "Iteration 33, loss = 1.24406069\n",
            "Iteration 34, loss = 1.24414538\n",
            "Iteration 35, loss = 1.24470293\n",
            "Iteration 36, loss = 1.24341277\n",
            "Iteration 37, loss = 1.24400229\n",
            "Iteration 38, loss = 1.24392592\n",
            "Iteration 39, loss = 1.24255652\n",
            "Iteration 40, loss = 1.24175048\n",
            "Iteration 41, loss = 1.24089891\n",
            "Iteration 42, loss = 1.24163970\n",
            "Iteration 43, loss = 1.24242702\n",
            "Iteration 44, loss = 1.24246879\n",
            "Iteration 45, loss = 1.24078538\n",
            "Iteration 46, loss = 1.24046432\n",
            "Iteration 47, loss = 1.24106211\n",
            "Iteration 48, loss = 1.24070864\n",
            "Iteration 49, loss = 1.24005237\n",
            "Iteration 50, loss = 1.24110293\n",
            "Iteration 51, loss = 1.24034926\n",
            "Iteration 52, loss = 1.24039035\n",
            "Iteration 53, loss = 1.24029864\n",
            "Iteration 54, loss = 1.24056223\n",
            "Iteration 55, loss = 1.24105573\n",
            "Iteration 56, loss = 1.23985790\n",
            "Iteration 57, loss = 1.23971343\n",
            "Iteration 58, loss = 1.24101303\n",
            "Iteration 59, loss = 1.24051259\n",
            "Iteration 60, loss = 1.23939045\n",
            "Iteration 61, loss = 1.24052766\n",
            "Iteration 62, loss = 1.24094938\n",
            "Iteration 63, loss = 1.23968363\n",
            "Iteration 64, loss = 1.24058623\n",
            "Iteration 65, loss = 1.23965550\n",
            "Iteration 66, loss = 1.24100504\n",
            "Iteration 67, loss = 1.24071997\n",
            "Iteration 68, loss = 1.24023744\n",
            "Iteration 69, loss = 1.23966133\n",
            "Iteration 70, loss = 1.23981227\n",
            "Iteration 71, loss = 1.23992113\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32718808\n",
            "Iteration 2, loss = 1.27834270\n",
            "Iteration 3, loss = 1.26495409\n",
            "Iteration 4, loss = 1.26434877\n",
            "Iteration 5, loss = 1.26447176\n",
            "Iteration 6, loss = 1.26190432\n",
            "Iteration 7, loss = 1.26191539\n",
            "Iteration 8, loss = 1.26137471\n",
            "Iteration 9, loss = 1.25874727\n",
            "Iteration 10, loss = 1.25766211\n",
            "Iteration 11, loss = 1.25729818\n",
            "Iteration 12, loss = 1.25724976\n",
            "Iteration 13, loss = 1.25634102\n",
            "Iteration 14, loss = 1.25571727\n",
            "Iteration 15, loss = 1.25570465\n",
            "Iteration 16, loss = 1.25527119\n",
            "Iteration 17, loss = 1.25431912\n",
            "Iteration 18, loss = 1.25496020\n",
            "Iteration 19, loss = 1.25482419\n",
            "Iteration 20, loss = 1.25399211\n",
            "Iteration 21, loss = 1.25271607\n",
            "Iteration 22, loss = 1.25217765\n",
            "Iteration 23, loss = 1.25226243\n",
            "Iteration 24, loss = 1.25157884\n",
            "Iteration 25, loss = 1.25042305\n",
            "Iteration 26, loss = 1.25031328\n",
            "Iteration 27, loss = 1.25108762\n",
            "Iteration 28, loss = 1.24948227\n",
            "Iteration 29, loss = 1.25001657\n",
            "Iteration 30, loss = 1.24827397\n",
            "Iteration 31, loss = 1.25062255\n",
            "Iteration 32, loss = 1.25189140\n",
            "Iteration 33, loss = 1.24919859\n",
            "Iteration 34, loss = 1.24952776\n",
            "Iteration 35, loss = 1.25162968\n",
            "Iteration 36, loss = 1.25138290\n",
            "Iteration 37, loss = 1.24987423\n",
            "Iteration 38, loss = 1.24801410\n",
            "Iteration 39, loss = 1.24754203\n",
            "Iteration 40, loss = 1.24744400\n",
            "Iteration 41, loss = 1.24711569\n",
            "Iteration 42, loss = 1.24649576\n",
            "Iteration 43, loss = 1.24699868\n",
            "Iteration 44, loss = 1.24731346\n",
            "Iteration 45, loss = 1.24556121\n",
            "Iteration 46, loss = 1.24645233\n",
            "Iteration 47, loss = 1.24675837\n",
            "Iteration 48, loss = 1.24560096\n",
            "Iteration 49, loss = 1.24554683\n",
            "Iteration 50, loss = 1.24600657\n",
            "Iteration 51, loss = 1.24531106\n",
            "Iteration 52, loss = 1.24564546\n",
            "Iteration 53, loss = 1.24708268\n",
            "Iteration 54, loss = 1.24585053\n",
            "Iteration 55, loss = 1.24598974\n",
            "Iteration 56, loss = 1.24574627\n",
            "Iteration 57, loss = 1.24597589\n",
            "Iteration 58, loss = 1.24672050\n",
            "Iteration 59, loss = 1.24593239\n",
            "Iteration 60, loss = 1.24465959\n",
            "Iteration 61, loss = 1.24548330\n",
            "Iteration 62, loss = 1.24602858\n",
            "Iteration 63, loss = 1.24507036\n",
            "Iteration 64, loss = 1.24514803\n",
            "Iteration 65, loss = 1.24408297\n",
            "Iteration 66, loss = 1.24591780\n",
            "Iteration 67, loss = 1.24595244\n",
            "Iteration 68, loss = 1.24585272\n",
            "Iteration 69, loss = 1.24546638\n",
            "Iteration 70, loss = 1.24538391\n",
            "Iteration 71, loss = 1.24479047\n",
            "Iteration 72, loss = 1.24396839\n",
            "Iteration 73, loss = 1.24377105\n",
            "Iteration 74, loss = 1.24399372\n",
            "Iteration 75, loss = 1.24461894\n",
            "Iteration 76, loss = 1.24581922\n",
            "Iteration 77, loss = 1.24529780\n",
            "Iteration 78, loss = 1.24395481\n",
            "Iteration 79, loss = 1.24433486\n",
            "Iteration 80, loss = 1.24489541\n",
            "Iteration 81, loss = 1.24600930\n",
            "Iteration 82, loss = 1.24581184\n",
            "Iteration 83, loss = 1.24401658\n",
            "Iteration 84, loss = 1.24470759\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33019773\n",
            "Iteration 2, loss = 1.28381552\n",
            "Iteration 3, loss = 1.26773299\n",
            "Iteration 4, loss = 1.26744428\n",
            "Iteration 5, loss = 1.26697802\n",
            "Iteration 6, loss = 1.26385382\n",
            "Iteration 7, loss = 1.26426130\n",
            "Iteration 8, loss = 1.26429637\n",
            "Iteration 9, loss = 1.26180492\n",
            "Iteration 10, loss = 1.26091910\n",
            "Iteration 11, loss = 1.26063662\n",
            "Iteration 12, loss = 1.26068148\n",
            "Iteration 13, loss = 1.25992069\n",
            "Iteration 14, loss = 1.25967493\n",
            "Iteration 15, loss = 1.25994032\n",
            "Iteration 16, loss = 1.25864235\n",
            "Iteration 17, loss = 1.25808091\n",
            "Iteration 18, loss = 1.25811150\n",
            "Iteration 19, loss = 1.25778897\n",
            "Iteration 20, loss = 1.25736493\n",
            "Iteration 21, loss = 1.25643697\n",
            "Iteration 22, loss = 1.25619577\n",
            "Iteration 23, loss = 1.25691582\n",
            "Iteration 24, loss = 1.25547595\n",
            "Iteration 25, loss = 1.25427005\n",
            "Iteration 26, loss = 1.25442470\n",
            "Iteration 27, loss = 1.25471047\n",
            "Iteration 28, loss = 1.25284799\n",
            "Iteration 29, loss = 1.25299183\n",
            "Iteration 30, loss = 1.25172119\n",
            "Iteration 31, loss = 1.25218494\n",
            "Iteration 32, loss = 1.25363428\n",
            "Iteration 33, loss = 1.25183694\n",
            "Iteration 34, loss = 1.25191199\n",
            "Iteration 35, loss = 1.25441833\n",
            "Iteration 36, loss = 1.25284166\n",
            "Iteration 37, loss = 1.25154725\n",
            "Iteration 38, loss = 1.25103693\n",
            "Iteration 39, loss = 1.25078024\n",
            "Iteration 40, loss = 1.25004608\n",
            "Iteration 41, loss = 1.25013371\n",
            "Iteration 42, loss = 1.24955018\n",
            "Iteration 43, loss = 1.25044761\n",
            "Iteration 44, loss = 1.24988273\n",
            "Iteration 45, loss = 1.24845040\n",
            "Iteration 46, loss = 1.24926357\n",
            "Iteration 47, loss = 1.24923569\n",
            "Iteration 48, loss = 1.24831660\n",
            "Iteration 49, loss = 1.24875396\n",
            "Iteration 50, loss = 1.24943888\n",
            "Iteration 51, loss = 1.24813930\n",
            "Iteration 52, loss = 1.24828098\n",
            "Iteration 53, loss = 1.24878245\n",
            "Iteration 54, loss = 1.24809773\n",
            "Iteration 55, loss = 1.24913167\n",
            "Iteration 56, loss = 1.24897804\n",
            "Iteration 57, loss = 1.24933197\n",
            "Iteration 58, loss = 1.24970855\n",
            "Iteration 59, loss = 1.24863765\n",
            "Iteration 60, loss = 1.24737084\n",
            "Iteration 61, loss = 1.24914430\n",
            "Iteration 62, loss = 1.24997203\n",
            "Iteration 63, loss = 1.24786132\n",
            "Iteration 64, loss = 1.24707650\n",
            "Iteration 65, loss = 1.24664408\n",
            "Iteration 66, loss = 1.24918523\n",
            "Iteration 67, loss = 1.24823690\n",
            "Iteration 68, loss = 1.24846908\n",
            "Iteration 69, loss = 1.24779973\n",
            "Iteration 70, loss = 1.24730168\n",
            "Iteration 71, loss = 1.24724898\n",
            "Iteration 72, loss = 1.24667300\n",
            "Iteration 73, loss = 1.24615961\n",
            "Iteration 74, loss = 1.24677411\n",
            "Iteration 75, loss = 1.24752862\n",
            "Iteration 76, loss = 1.24787196\n",
            "Iteration 77, loss = 1.24751067\n",
            "Iteration 78, loss = 1.24658755\n",
            "Iteration 79, loss = 1.24589098\n",
            "Iteration 80, loss = 1.24639497\n",
            "Iteration 81, loss = 1.24869046\n",
            "Iteration 82, loss = 1.24756028\n",
            "Iteration 83, loss = 1.24636208\n",
            "Iteration 84, loss = 1.24831280\n",
            "Iteration 85, loss = 1.24869220\n",
            "Iteration 86, loss = 1.24763501\n",
            "Iteration 87, loss = 1.24546940\n",
            "Iteration 88, loss = 1.24563850\n",
            "Iteration 89, loss = 1.24634453\n",
            "Iteration 90, loss = 1.24618717\n",
            "Iteration 91, loss = 1.24621895\n",
            "Iteration 92, loss = 1.24653136\n",
            "Iteration 93, loss = 1.24537807\n",
            "Iteration 94, loss = 1.24484148\n",
            "Iteration 95, loss = 1.24561702\n",
            "Iteration 96, loss = 1.24537579\n",
            "Iteration 97, loss = 1.24489039\n",
            "Iteration 98, loss = 1.24461521\n",
            "Iteration 99, loss = 1.24506448\n",
            "Iteration 100, loss = 1.24727602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32340314\n",
            "Iteration 2, loss = 1.27442003\n",
            "Iteration 3, loss = 1.25896747\n",
            "Iteration 4, loss = 1.25938789\n",
            "Iteration 5, loss = 1.25811565\n",
            "Iteration 6, loss = 1.25471880\n",
            "Iteration 7, loss = 1.25550994\n",
            "Iteration 8, loss = 1.25546447\n",
            "Iteration 9, loss = 1.25272225\n",
            "Iteration 10, loss = 1.25173409\n",
            "Iteration 11, loss = 1.25107980\n",
            "Iteration 12, loss = 1.25122882\n",
            "Iteration 13, loss = 1.25174042\n",
            "Iteration 14, loss = 1.25116377\n",
            "Iteration 15, loss = 1.25086392\n",
            "Iteration 16, loss = 1.24958913\n",
            "Iteration 17, loss = 1.24904786\n",
            "Iteration 18, loss = 1.24903342\n",
            "Iteration 19, loss = 1.24810442\n",
            "Iteration 20, loss = 1.24768422\n",
            "Iteration 21, loss = 1.24678351\n",
            "Iteration 22, loss = 1.24781593\n",
            "Iteration 23, loss = 1.24720385\n",
            "Iteration 24, loss = 1.24613564\n",
            "Iteration 25, loss = 1.24510139\n",
            "Iteration 26, loss = 1.24498721\n",
            "Iteration 27, loss = 1.24552593\n",
            "Iteration 28, loss = 1.24430675\n",
            "Iteration 29, loss = 1.24500801\n",
            "Iteration 30, loss = 1.24319117\n",
            "Iteration 31, loss = 1.24340409\n",
            "Iteration 32, loss = 1.24461396\n",
            "Iteration 33, loss = 1.24324201\n",
            "Iteration 34, loss = 1.24242562\n",
            "Iteration 35, loss = 1.24409730\n",
            "Iteration 36, loss = 1.24362400\n",
            "Iteration 37, loss = 1.24262926\n",
            "Iteration 38, loss = 1.24200531\n",
            "Iteration 39, loss = 1.24161988\n",
            "Iteration 40, loss = 1.24073554\n",
            "Iteration 41, loss = 1.24052234\n",
            "Iteration 42, loss = 1.24097891\n",
            "Iteration 43, loss = 1.24200131\n",
            "Iteration 44, loss = 1.24049188\n",
            "Iteration 45, loss = 1.23941891\n",
            "Iteration 46, loss = 1.24002017\n",
            "Iteration 47, loss = 1.24041400\n",
            "Iteration 48, loss = 1.23953120\n",
            "Iteration 49, loss = 1.24019008\n",
            "Iteration 50, loss = 1.24075928\n",
            "Iteration 51, loss = 1.23932717\n",
            "Iteration 52, loss = 1.23888872\n",
            "Iteration 53, loss = 1.23936837\n",
            "Iteration 54, loss = 1.23893543\n",
            "Iteration 55, loss = 1.23910954\n",
            "Iteration 56, loss = 1.23852194\n",
            "Iteration 57, loss = 1.23948505\n",
            "Iteration 58, loss = 1.24035320\n",
            "Iteration 59, loss = 1.23956759\n",
            "Iteration 60, loss = 1.23812985\n",
            "Iteration 61, loss = 1.23900983\n",
            "Iteration 62, loss = 1.23956792\n",
            "Iteration 63, loss = 1.23824312\n",
            "Iteration 64, loss = 1.23791629\n",
            "Iteration 65, loss = 1.23787019\n",
            "Iteration 66, loss = 1.23920495\n",
            "Iteration 67, loss = 1.23857408\n",
            "Iteration 68, loss = 1.23908693\n",
            "Iteration 69, loss = 1.23905652\n",
            "Iteration 70, loss = 1.23786020\n",
            "Iteration 71, loss = 1.23801645\n",
            "Iteration 72, loss = 1.23763191\n",
            "Iteration 73, loss = 1.23712269\n",
            "Iteration 74, loss = 1.23776777\n",
            "Iteration 75, loss = 1.23752020\n",
            "Iteration 76, loss = 1.23753110\n",
            "Iteration 77, loss = 1.23756908\n",
            "Iteration 78, loss = 1.23696693\n",
            "Iteration 79, loss = 1.23710929\n",
            "Iteration 80, loss = 1.23799180\n",
            "Iteration 81, loss = 1.23869075\n",
            "Iteration 82, loss = 1.23819418\n",
            "Iteration 83, loss = 1.23813306\n",
            "Iteration 84, loss = 1.23947618\n",
            "Iteration 85, loss = 1.23957091\n",
            "Iteration 86, loss = 1.23886252\n",
            "Iteration 87, loss = 1.23650137\n",
            "Iteration 88, loss = 1.23677578\n",
            "Iteration 89, loss = 1.23756962\n",
            "Iteration 90, loss = 1.23719249\n",
            "Iteration 91, loss = 1.23640411\n",
            "Iteration 92, loss = 1.23708008\n",
            "Iteration 93, loss = 1.23746509\n",
            "Iteration 94, loss = 1.23636979\n",
            "Iteration 95, loss = 1.23661510\n",
            "Iteration 96, loss = 1.23654409\n",
            "Iteration 97, loss = 1.23605611\n",
            "Iteration 98, loss = 1.23657383\n",
            "Iteration 99, loss = 1.23738893\n",
            "Iteration 100, loss = 1.23839399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32004068\n",
            "Iteration 2, loss = 1.26695968\n",
            "Iteration 3, loss = 1.24947094\n",
            "Iteration 4, loss = 1.24925354\n",
            "Iteration 5, loss = 1.24956141\n",
            "Iteration 6, loss = 1.24400358\n",
            "Iteration 7, loss = 1.24380507\n",
            "Iteration 8, loss = 1.24503810\n",
            "Iteration 9, loss = 1.24299226\n",
            "Iteration 10, loss = 1.24145079\n",
            "Iteration 11, loss = 1.24015868\n",
            "Iteration 12, loss = 1.23993234\n",
            "Iteration 13, loss = 1.24012505\n",
            "Iteration 14, loss = 1.24009510\n",
            "Iteration 15, loss = 1.23923235\n",
            "Iteration 16, loss = 1.23798599\n",
            "Iteration 17, loss = 1.23714681\n",
            "Iteration 18, loss = 1.23700771\n",
            "Iteration 19, loss = 1.23575029\n",
            "Iteration 20, loss = 1.23600888\n",
            "Iteration 21, loss = 1.23497392\n",
            "Iteration 22, loss = 1.23537805\n",
            "Iteration 23, loss = 1.23481273\n",
            "Iteration 24, loss = 1.23374974\n",
            "Iteration 25, loss = 1.23337996\n",
            "Iteration 26, loss = 1.23321480\n",
            "Iteration 27, loss = 1.23383991\n",
            "Iteration 28, loss = 1.23206983\n",
            "Iteration 29, loss = 1.23427287\n",
            "Iteration 30, loss = 1.23256378\n",
            "Iteration 31, loss = 1.23207205\n",
            "Iteration 32, loss = 1.23310319\n",
            "Iteration 33, loss = 1.23133481\n",
            "Iteration 34, loss = 1.23050640\n",
            "Iteration 35, loss = 1.23304824\n",
            "Iteration 36, loss = 1.23198812\n",
            "Iteration 37, loss = 1.23057826\n",
            "Iteration 38, loss = 1.22983841\n",
            "Iteration 39, loss = 1.23069749\n",
            "Iteration 40, loss = 1.23072950\n",
            "Iteration 41, loss = 1.22962931\n",
            "Iteration 42, loss = 1.22895056\n",
            "Iteration 43, loss = 1.23077857\n",
            "Iteration 44, loss = 1.23026685\n",
            "Iteration 45, loss = 1.22886057\n",
            "Iteration 46, loss = 1.22842640\n",
            "Iteration 47, loss = 1.22953897\n",
            "Iteration 48, loss = 1.22905827\n",
            "Iteration 49, loss = 1.22952294\n",
            "Iteration 50, loss = 1.23008431\n",
            "Iteration 51, loss = 1.22814421\n",
            "Iteration 52, loss = 1.22836685\n",
            "Iteration 53, loss = 1.22848579\n",
            "Iteration 54, loss = 1.22821889\n",
            "Iteration 55, loss = 1.22824387\n",
            "Iteration 56, loss = 1.22786839\n",
            "Iteration 57, loss = 1.22818284\n",
            "Iteration 58, loss = 1.22894496\n",
            "Iteration 59, loss = 1.22838322\n",
            "Iteration 60, loss = 1.22748176\n",
            "Iteration 61, loss = 1.22850178\n",
            "Iteration 62, loss = 1.22891081\n",
            "Iteration 63, loss = 1.22717459\n",
            "Iteration 64, loss = 1.22706613\n",
            "Iteration 65, loss = 1.22698563\n",
            "Iteration 66, loss = 1.22745162\n",
            "Iteration 67, loss = 1.22767963\n",
            "Iteration 68, loss = 1.22770080\n",
            "Iteration 69, loss = 1.22760309\n",
            "Iteration 70, loss = 1.22728214\n",
            "Iteration 71, loss = 1.22677805\n",
            "Iteration 72, loss = 1.22677124\n",
            "Iteration 73, loss = 1.22651252\n",
            "Iteration 74, loss = 1.22708287\n",
            "Iteration 75, loss = 1.22658042\n",
            "Iteration 76, loss = 1.22643172\n",
            "Iteration 77, loss = 1.22636285\n",
            "Iteration 78, loss = 1.22633300\n",
            "Iteration 79, loss = 1.22659779\n",
            "Iteration 80, loss = 1.22652503\n",
            "Iteration 81, loss = 1.22701923\n",
            "Iteration 82, loss = 1.22677318\n",
            "Iteration 83, loss = 1.22633318\n",
            "Iteration 84, loss = 1.22769446\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32694016\n",
            "Iteration 2, loss = 1.27478102\n",
            "Iteration 3, loss = 1.25719159\n",
            "Iteration 4, loss = 1.25431478\n",
            "Iteration 5, loss = 1.25328384\n",
            "Iteration 6, loss = 1.24929243\n",
            "Iteration 7, loss = 1.24902578\n",
            "Iteration 8, loss = 1.24983490\n",
            "Iteration 9, loss = 1.24787289\n",
            "Iteration 10, loss = 1.24669960\n",
            "Iteration 11, loss = 1.24550243\n",
            "Iteration 12, loss = 1.24518993\n",
            "Iteration 13, loss = 1.24555066\n",
            "Iteration 14, loss = 1.24587408\n",
            "Iteration 15, loss = 1.24488377\n",
            "Iteration 16, loss = 1.24319281\n",
            "Iteration 17, loss = 1.24251716\n",
            "Iteration 18, loss = 1.24238962\n",
            "Iteration 19, loss = 1.24202411\n",
            "Iteration 20, loss = 1.24289775\n",
            "Iteration 21, loss = 1.24148678\n",
            "Iteration 22, loss = 1.24255486\n",
            "Iteration 23, loss = 1.24171879\n",
            "Iteration 24, loss = 1.24016816\n",
            "Iteration 25, loss = 1.23957721\n",
            "Iteration 26, loss = 1.24000354\n",
            "Iteration 27, loss = 1.24108448\n",
            "Iteration 28, loss = 1.23899044\n",
            "Iteration 29, loss = 1.24241115\n",
            "Iteration 30, loss = 1.23892656\n",
            "Iteration 31, loss = 1.23882232\n",
            "Iteration 32, loss = 1.24053636\n",
            "Iteration 33, loss = 1.23770830\n",
            "Iteration 34, loss = 1.23734581\n",
            "Iteration 35, loss = 1.23976890\n",
            "Iteration 36, loss = 1.23748576\n",
            "Iteration 37, loss = 1.23669270\n",
            "Iteration 38, loss = 1.23668585\n",
            "Iteration 39, loss = 1.23644680\n",
            "Iteration 40, loss = 1.23618245\n",
            "Iteration 41, loss = 1.23598436\n",
            "Iteration 42, loss = 1.23549331\n",
            "Iteration 43, loss = 1.23745654\n",
            "Iteration 44, loss = 1.23660305\n",
            "Iteration 45, loss = 1.23518825\n",
            "Iteration 46, loss = 1.23505818\n",
            "Iteration 47, loss = 1.23525147\n",
            "Iteration 48, loss = 1.23497376\n",
            "Iteration 49, loss = 1.23678206\n",
            "Iteration 50, loss = 1.23662728\n",
            "Iteration 51, loss = 1.23447735\n",
            "Iteration 52, loss = 1.23494811\n",
            "Iteration 53, loss = 1.23484806\n",
            "Iteration 54, loss = 1.23491470\n",
            "Iteration 55, loss = 1.23518417\n",
            "Iteration 56, loss = 1.23397124\n",
            "Iteration 57, loss = 1.23368111\n",
            "Iteration 58, loss = 1.23486010\n",
            "Iteration 59, loss = 1.23500103\n",
            "Iteration 60, loss = 1.23450678\n",
            "Iteration 61, loss = 1.23514539\n",
            "Iteration 62, loss = 1.23522044\n",
            "Iteration 63, loss = 1.23402427\n",
            "Iteration 64, loss = 1.23366614\n",
            "Iteration 65, loss = 1.23374494\n",
            "Iteration 66, loss = 1.23512380\n",
            "Iteration 67, loss = 1.23420470\n",
            "Iteration 68, loss = 1.23571994\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32594111\n",
            "Iteration 2, loss = 1.27528283\n",
            "Iteration 3, loss = 1.26048590\n",
            "Iteration 4, loss = 1.25248830\n",
            "Iteration 5, loss = 1.25000615\n",
            "Iteration 6, loss = 1.24939627\n",
            "Iteration 7, loss = 1.24853204\n",
            "Iteration 8, loss = 1.24647432\n",
            "Iteration 9, loss = 1.24848447\n",
            "Iteration 10, loss = 1.24768172\n",
            "Iteration 11, loss = 1.24469690\n",
            "Iteration 12, loss = 1.24557299\n",
            "Iteration 13, loss = 1.24506045\n",
            "Iteration 14, loss = 1.24309600\n",
            "Iteration 15, loss = 1.24204904\n",
            "Iteration 16, loss = 1.24206407\n",
            "Iteration 17, loss = 1.24247205\n",
            "Iteration 18, loss = 1.24126008\n",
            "Iteration 19, loss = 1.24014472\n",
            "Iteration 20, loss = 1.23997290\n",
            "Iteration 21, loss = 1.23990973\n",
            "Iteration 22, loss = 1.23954094\n",
            "Iteration 23, loss = 1.23821235\n",
            "Iteration 24, loss = 1.23779910\n",
            "Iteration 25, loss = 1.23710235\n",
            "Iteration 26, loss = 1.23619583\n",
            "Iteration 27, loss = 1.23664123\n",
            "Iteration 28, loss = 1.23745751\n",
            "Iteration 29, loss = 1.23581410\n",
            "Iteration 30, loss = 1.23518662\n",
            "Iteration 31, loss = 1.23588961\n",
            "Iteration 32, loss = 1.23427976\n",
            "Iteration 33, loss = 1.23397627\n",
            "Iteration 34, loss = 1.23404610\n",
            "Iteration 35, loss = 1.23425270\n",
            "Iteration 36, loss = 1.23369757\n",
            "Iteration 37, loss = 1.23321304\n",
            "Iteration 38, loss = 1.23348919\n",
            "Iteration 39, loss = 1.23318612\n",
            "Iteration 40, loss = 1.23213092\n",
            "Iteration 41, loss = 1.23313318\n",
            "Iteration 42, loss = 1.23254722\n",
            "Iteration 43, loss = 1.23216014\n",
            "Iteration 44, loss = 1.23188165\n",
            "Iteration 45, loss = 1.23129103\n",
            "Iteration 46, loss = 1.23151060\n",
            "Iteration 47, loss = 1.23095102\n",
            "Iteration 48, loss = 1.23164919\n",
            "Iteration 49, loss = 1.23129066\n",
            "Iteration 50, loss = 1.23086952\n",
            "Iteration 51, loss = 1.23252572\n",
            "Iteration 52, loss = 1.23027215\n",
            "Iteration 53, loss = 1.23137315\n",
            "Iteration 54, loss = 1.23157289\n",
            "Iteration 55, loss = 1.23146849\n",
            "Iteration 56, loss = 1.23133129\n",
            "Iteration 57, loss = 1.23028326\n",
            "Iteration 58, loss = 1.23163104\n",
            "Iteration 59, loss = 1.23123966\n",
            "Iteration 60, loss = 1.23063584\n",
            "Iteration 61, loss = 1.23150809\n",
            "Iteration 62, loss = 1.22979959\n",
            "Iteration 63, loss = 1.23354629\n",
            "Iteration 64, loss = 1.23173595\n",
            "Iteration 65, loss = 1.22993092\n",
            "Iteration 66, loss = 1.22984723\n",
            "Iteration 67, loss = 1.22980957\n",
            "Iteration 68, loss = 1.23121637\n",
            "Iteration 69, loss = 1.22969837\n",
            "Iteration 70, loss = 1.23044701\n",
            "Iteration 71, loss = 1.22998500\n",
            "Iteration 72, loss = 1.23001963\n",
            "Iteration 73, loss = 1.23007494\n",
            "Iteration 74, loss = 1.22941348\n",
            "Iteration 75, loss = 1.22970672\n",
            "Iteration 76, loss = 1.22891440\n",
            "Iteration 77, loss = 1.22960803\n",
            "Iteration 78, loss = 1.22967463\n",
            "Iteration 79, loss = 1.22954281\n",
            "Iteration 80, loss = 1.22912952\n",
            "Iteration 81, loss = 1.22932232\n",
            "Iteration 82, loss = 1.22937303\n",
            "Iteration 83, loss = 1.22848316\n",
            "Iteration 84, loss = 1.22943248\n",
            "Iteration 85, loss = 1.23037894\n",
            "Iteration 86, loss = 1.22995801\n",
            "Iteration 87, loss = 1.22997117\n",
            "Iteration 88, loss = 1.22929405\n",
            "Iteration 89, loss = 1.22825427\n",
            "Iteration 90, loss = 1.22787369\n",
            "Iteration 91, loss = 1.22855293\n",
            "Iteration 92, loss = 1.22849274\n",
            "Iteration 93, loss = 1.22896683\n",
            "Iteration 94, loss = 1.22820245\n",
            "Iteration 95, loss = 1.22978587\n",
            "Iteration 96, loss = 1.23082019\n",
            "Iteration 97, loss = 1.22931452\n",
            "Iteration 98, loss = 1.22832870\n",
            "Iteration 99, loss = 1.22828231\n",
            "Iteration 100, loss = 1.22780900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32362457\n",
            "Iteration 2, loss = 1.27246873\n",
            "Iteration 3, loss = 1.25712066\n",
            "Iteration 4, loss = 1.24896620\n",
            "Iteration 5, loss = 1.24483848\n",
            "Iteration 6, loss = 1.24377357\n",
            "Iteration 7, loss = 1.24362816\n",
            "Iteration 8, loss = 1.24260064\n",
            "Iteration 9, loss = 1.24399523\n",
            "Iteration 10, loss = 1.24292556\n",
            "Iteration 11, loss = 1.24047206\n",
            "Iteration 12, loss = 1.24183143\n",
            "Iteration 13, loss = 1.24094473\n",
            "Iteration 14, loss = 1.23936234\n",
            "Iteration 15, loss = 1.23831203\n",
            "Iteration 16, loss = 1.23900244\n",
            "Iteration 17, loss = 1.23909835\n",
            "Iteration 18, loss = 1.23786222\n",
            "Iteration 19, loss = 1.23706917\n",
            "Iteration 20, loss = 1.23761762\n",
            "Iteration 21, loss = 1.23690897\n",
            "Iteration 22, loss = 1.23599372\n",
            "Iteration 23, loss = 1.23477318\n",
            "Iteration 24, loss = 1.23526022\n",
            "Iteration 25, loss = 1.23439394\n",
            "Iteration 26, loss = 1.23361157\n",
            "Iteration 27, loss = 1.23430211\n",
            "Iteration 28, loss = 1.23541146\n",
            "Iteration 29, loss = 1.23388174\n",
            "Iteration 30, loss = 1.23308106\n",
            "Iteration 31, loss = 1.23355064\n",
            "Iteration 32, loss = 1.23298646\n",
            "Iteration 33, loss = 1.23197543\n",
            "Iteration 34, loss = 1.23138286\n",
            "Iteration 35, loss = 1.23231810\n",
            "Iteration 36, loss = 1.23161012\n",
            "Iteration 37, loss = 1.23066996\n",
            "Iteration 38, loss = 1.23164065\n",
            "Iteration 39, loss = 1.23119195\n",
            "Iteration 40, loss = 1.23017117\n",
            "Iteration 41, loss = 1.23125750\n",
            "Iteration 42, loss = 1.23039376\n",
            "Iteration 43, loss = 1.22956235\n",
            "Iteration 44, loss = 1.22977870\n",
            "Iteration 45, loss = 1.22966897\n",
            "Iteration 46, loss = 1.22912507\n",
            "Iteration 47, loss = 1.22872123\n",
            "Iteration 48, loss = 1.23008843\n",
            "Iteration 49, loss = 1.22935513\n",
            "Iteration 50, loss = 1.22981074\n",
            "Iteration 51, loss = 1.23010505\n",
            "Iteration 52, loss = 1.22807265\n",
            "Iteration 53, loss = 1.22957606\n",
            "Iteration 54, loss = 1.22975178\n",
            "Iteration 55, loss = 1.22918552\n",
            "Iteration 56, loss = 1.22868321\n",
            "Iteration 57, loss = 1.22810467\n",
            "Iteration 58, loss = 1.22959703\n",
            "Iteration 59, loss = 1.22915374\n",
            "Iteration 60, loss = 1.22859569\n",
            "Iteration 61, loss = 1.22871986\n",
            "Iteration 62, loss = 1.22824928\n",
            "Iteration 63, loss = 1.23122016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32331630\n",
            "Iteration 2, loss = 1.26618979\n",
            "Iteration 3, loss = 1.24723209\n",
            "Iteration 4, loss = 1.23728943\n",
            "Iteration 5, loss = 1.23261128\n",
            "Iteration 6, loss = 1.23152911\n",
            "Iteration 7, loss = 1.23113285\n",
            "Iteration 8, loss = 1.23018572\n",
            "Iteration 9, loss = 1.23181672\n",
            "Iteration 10, loss = 1.22923182\n",
            "Iteration 11, loss = 1.22776309\n",
            "Iteration 12, loss = 1.22911233\n",
            "Iteration 13, loss = 1.22791167\n",
            "Iteration 14, loss = 1.22694301\n",
            "Iteration 15, loss = 1.22579932\n",
            "Iteration 16, loss = 1.22584095\n",
            "Iteration 17, loss = 1.22616801\n",
            "Iteration 18, loss = 1.22571456\n",
            "Iteration 19, loss = 1.22490169\n",
            "Iteration 20, loss = 1.22530319\n",
            "Iteration 21, loss = 1.22425341\n",
            "Iteration 22, loss = 1.22324019\n",
            "Iteration 23, loss = 1.22335022\n",
            "Iteration 24, loss = 1.22428680\n",
            "Iteration 25, loss = 1.22312503\n",
            "Iteration 26, loss = 1.22292656\n",
            "Iteration 27, loss = 1.22226322\n",
            "Iteration 28, loss = 1.22409392\n",
            "Iteration 29, loss = 1.22320023\n",
            "Iteration 30, loss = 1.22210174\n",
            "Iteration 31, loss = 1.22251555\n",
            "Iteration 32, loss = 1.22139662\n",
            "Iteration 33, loss = 1.22039772\n",
            "Iteration 34, loss = 1.22033866\n",
            "Iteration 35, loss = 1.22092055\n",
            "Iteration 36, loss = 1.22080593\n",
            "Iteration 37, loss = 1.21996317\n",
            "Iteration 38, loss = 1.22068466\n",
            "Iteration 39, loss = 1.21980125\n",
            "Iteration 40, loss = 1.21910826\n",
            "Iteration 41, loss = 1.21944482\n",
            "Iteration 42, loss = 1.21893093\n",
            "Iteration 43, loss = 1.21890727\n",
            "Iteration 44, loss = 1.21898111\n",
            "Iteration 45, loss = 1.21875109\n",
            "Iteration 46, loss = 1.21797872\n",
            "Iteration 47, loss = 1.21778963\n",
            "Iteration 48, loss = 1.21836197\n",
            "Iteration 49, loss = 1.21772319\n",
            "Iteration 50, loss = 1.21839160\n",
            "Iteration 51, loss = 1.22001989\n",
            "Iteration 52, loss = 1.21708468\n",
            "Iteration 53, loss = 1.21931541\n",
            "Iteration 54, loss = 1.21974427\n",
            "Iteration 55, loss = 1.21814350\n",
            "Iteration 56, loss = 1.21764366\n",
            "Iteration 57, loss = 1.21724024\n",
            "Iteration 58, loss = 1.21892503\n",
            "Iteration 59, loss = 1.21847172\n",
            "Iteration 60, loss = 1.21743387\n",
            "Iteration 61, loss = 1.21676632\n",
            "Iteration 62, loss = 1.21745658\n",
            "Iteration 63, loss = 1.21946222\n",
            "Iteration 64, loss = 1.21771160\n",
            "Iteration 65, loss = 1.21694286\n",
            "Iteration 66, loss = 1.21668567\n",
            "Iteration 67, loss = 1.21651162\n",
            "Iteration 68, loss = 1.21734718\n",
            "Iteration 69, loss = 1.21697061\n",
            "Iteration 70, loss = 1.21910189\n",
            "Iteration 71, loss = 1.21655504\n",
            "Iteration 72, loss = 1.21674028\n",
            "Iteration 73, loss = 1.21805300\n",
            "Iteration 74, loss = 1.21670577\n",
            "Iteration 75, loss = 1.21649227\n",
            "Iteration 76, loss = 1.21654349\n",
            "Iteration 77, loss = 1.21673214\n",
            "Iteration 78, loss = 1.21782263\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "18\n",
            "Iteration 1, loss = 1.28751515\n",
            "Iteration 2, loss = 1.25555368\n",
            "Iteration 3, loss = 1.25023764\n",
            "Iteration 4, loss = 1.23504965\n",
            "Iteration 5, loss = 1.22732603\n",
            "Iteration 6, loss = 1.21569213\n",
            "Iteration 7, loss = 1.20060527\n",
            "Iteration 8, loss = 1.18530816\n",
            "Iteration 9, loss = 1.17394499\n",
            "Iteration 10, loss = 1.16064247\n",
            "Iteration 11, loss = 1.13969498\n",
            "Iteration 12, loss = 1.12477279\n",
            "Iteration 13, loss = 1.10469596\n",
            "Iteration 14, loss = 1.09271205\n",
            "Iteration 15, loss = 1.07652346\n",
            "Iteration 16, loss = 1.05723870\n",
            "Iteration 17, loss = 1.04892903\n",
            "Iteration 18, loss = 1.04152260\n",
            "Iteration 19, loss = 1.02334644\n",
            "Iteration 20, loss = 1.01305170\n",
            "Iteration 21, loss = 1.01917983\n",
            "Iteration 22, loss = 1.00539937\n",
            "Iteration 23, loss = 0.99973195\n",
            "Iteration 24, loss = 0.99640460\n",
            "Iteration 25, loss = 0.99691104\n",
            "Iteration 26, loss = 0.99867629\n",
            "Iteration 27, loss = 1.00197449\n",
            "Iteration 28, loss = 0.99292471\n",
            "Iteration 29, loss = 0.99759580\n",
            "Iteration 30, loss = 0.99100967\n",
            "Iteration 31, loss = 0.98740653\n",
            "Iteration 32, loss = 0.98169604\n",
            "Iteration 33, loss = 0.98174816\n",
            "Iteration 34, loss = 0.97498142\n",
            "Iteration 35, loss = 0.97913631\n",
            "Iteration 36, loss = 0.97439917\n",
            "Iteration 37, loss = 0.97886683\n",
            "Iteration 38, loss = 0.97600045\n",
            "Iteration 39, loss = 0.97158994\n",
            "Iteration 40, loss = 0.96855818\n",
            "Iteration 41, loss = 0.96987811\n",
            "Iteration 42, loss = 0.96881454\n",
            "Iteration 43, loss = 0.97263936\n",
            "Iteration 44, loss = 0.96461327\n",
            "Iteration 45, loss = 0.96752186\n",
            "Iteration 46, loss = 0.96391321\n",
            "Iteration 47, loss = 0.96275942\n",
            "Iteration 48, loss = 0.96496077\n",
            "Iteration 49, loss = 0.96205006\n",
            "Iteration 50, loss = 0.95826082\n",
            "Iteration 51, loss = 0.96089596\n",
            "Iteration 52, loss = 0.96413944\n",
            "Iteration 53, loss = 0.95336716\n",
            "Iteration 54, loss = 0.96020149\n",
            "Iteration 55, loss = 0.95946880\n",
            "Iteration 56, loss = 0.95074783\n",
            "Iteration 57, loss = 0.96322531\n",
            "Iteration 58, loss = 0.95969522\n",
            "Iteration 59, loss = 0.95621580\n",
            "Iteration 60, loss = 0.94920603\n",
            "Iteration 61, loss = 0.95903620\n",
            "Iteration 62, loss = 0.95705790\n",
            "Iteration 63, loss = 0.96386171\n",
            "Iteration 64, loss = 0.94836525\n",
            "Iteration 65, loss = 0.95410792\n",
            "Iteration 66, loss = 0.95000969\n",
            "Iteration 67, loss = 0.94592794\n",
            "Iteration 68, loss = 0.94400471\n",
            "Iteration 69, loss = 0.95428719\n",
            "Iteration 70, loss = 0.94413292\n",
            "Iteration 71, loss = 0.94293961\n",
            "Iteration 72, loss = 0.93872813\n",
            "Iteration 73, loss = 0.93850633\n",
            "Iteration 74, loss = 0.94297654\n",
            "Iteration 75, loss = 0.93470118\n",
            "Iteration 76, loss = 0.94106275\n",
            "Iteration 77, loss = 0.93519890\n",
            "Iteration 78, loss = 0.93566905\n",
            "Iteration 79, loss = 0.93298033\n",
            "Iteration 80, loss = 0.93105689\n",
            "Iteration 81, loss = 0.93846554\n",
            "Iteration 82, loss = 0.94522744\n",
            "Iteration 83, loss = 0.93629794\n",
            "Iteration 84, loss = 0.93528610\n",
            "Iteration 85, loss = 0.93588353\n",
            "Iteration 86, loss = 0.93330723\n",
            "Iteration 87, loss = 0.92993414\n",
            "Iteration 88, loss = 0.92728237\n",
            "Iteration 89, loss = 0.92637704\n",
            "Iteration 90, loss = 0.92299140\n",
            "Iteration 91, loss = 0.92127452\n",
            "Iteration 92, loss = 0.92518432\n",
            "Iteration 93, loss = 0.92017102\n",
            "Iteration 94, loss = 0.91979741\n",
            "Iteration 95, loss = 0.92228639\n",
            "Iteration 96, loss = 0.92344990\n",
            "Iteration 97, loss = 0.91485840\n",
            "Iteration 98, loss = 0.91920720\n",
            "Iteration 99, loss = 0.91676115\n",
            "Iteration 100, loss = 0.91470650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28704261\n",
            "Iteration 2, loss = 1.25204193\n",
            "Iteration 3, loss = 1.24912685\n",
            "Iteration 4, loss = 1.23344315\n",
            "Iteration 5, loss = 1.22508036\n",
            "Iteration 6, loss = 1.21661935\n",
            "Iteration 7, loss = 1.20117588\n",
            "Iteration 8, loss = 1.18953601\n",
            "Iteration 9, loss = 1.17437704\n",
            "Iteration 10, loss = 1.16316415\n",
            "Iteration 11, loss = 1.14175518\n",
            "Iteration 12, loss = 1.12801345\n",
            "Iteration 13, loss = 1.10793501\n",
            "Iteration 14, loss = 1.09217165\n",
            "Iteration 15, loss = 1.07817351\n",
            "Iteration 16, loss = 1.05740937\n",
            "Iteration 17, loss = 1.04540788\n",
            "Iteration 18, loss = 1.04154628\n",
            "Iteration 19, loss = 1.02374101\n",
            "Iteration 20, loss = 1.01478608\n",
            "Iteration 21, loss = 1.02034027\n",
            "Iteration 22, loss = 1.00991422\n",
            "Iteration 23, loss = 1.00347364\n",
            "Iteration 24, loss = 1.00098882\n",
            "Iteration 25, loss = 1.00324203\n",
            "Iteration 26, loss = 1.00470959\n",
            "Iteration 27, loss = 1.00393440\n",
            "Iteration 28, loss = 0.99620387\n",
            "Iteration 29, loss = 0.99366249\n",
            "Iteration 30, loss = 0.98807828\n",
            "Iteration 31, loss = 0.98921422\n",
            "Iteration 32, loss = 0.98514962\n",
            "Iteration 33, loss = 0.98327826\n",
            "Iteration 34, loss = 0.97970125\n",
            "Iteration 35, loss = 0.97789338\n",
            "Iteration 36, loss = 0.97559389\n",
            "Iteration 37, loss = 0.98079582\n",
            "Iteration 38, loss = 0.97610881\n",
            "Iteration 39, loss = 0.97557652\n",
            "Iteration 40, loss = 0.97089731\n",
            "Iteration 41, loss = 0.97134177\n",
            "Iteration 42, loss = 0.97096201\n",
            "Iteration 43, loss = 0.97177483\n",
            "Iteration 44, loss = 0.96972142\n",
            "Iteration 45, loss = 0.96927513\n",
            "Iteration 46, loss = 0.96532994\n",
            "Iteration 47, loss = 0.96662251\n",
            "Iteration 48, loss = 0.96801813\n",
            "Iteration 49, loss = 0.96428627\n",
            "Iteration 50, loss = 0.96042663\n",
            "Iteration 51, loss = 0.96498778\n",
            "Iteration 52, loss = 0.96717075\n",
            "Iteration 53, loss = 0.95746775\n",
            "Iteration 54, loss = 0.96401987\n",
            "Iteration 55, loss = 0.96024946\n",
            "Iteration 56, loss = 0.95320717\n",
            "Iteration 57, loss = 0.96175202\n",
            "Iteration 58, loss = 0.95750809\n",
            "Iteration 59, loss = 0.95339868\n",
            "Iteration 60, loss = 0.95051668\n",
            "Iteration 61, loss = 0.95708281\n",
            "Iteration 62, loss = 0.96195654\n",
            "Iteration 63, loss = 0.95912704\n",
            "Iteration 64, loss = 0.95161506\n",
            "Iteration 65, loss = 0.94956092\n",
            "Iteration 66, loss = 0.94725482\n",
            "Iteration 67, loss = 0.94538781\n",
            "Iteration 68, loss = 0.94389020\n",
            "Iteration 69, loss = 0.95247840\n",
            "Iteration 70, loss = 0.94904528\n",
            "Iteration 71, loss = 0.94378477\n",
            "Iteration 72, loss = 0.94074369\n",
            "Iteration 73, loss = 0.93715121\n",
            "Iteration 74, loss = 0.94354282\n",
            "Iteration 75, loss = 0.93594371\n",
            "Iteration 76, loss = 0.94035300\n",
            "Iteration 77, loss = 0.93565494\n",
            "Iteration 78, loss = 0.93439359\n",
            "Iteration 79, loss = 0.93362850\n",
            "Iteration 80, loss = 0.93497044\n",
            "Iteration 81, loss = 0.93852453\n",
            "Iteration 82, loss = 0.94006546\n",
            "Iteration 83, loss = 0.94029920\n",
            "Iteration 84, loss = 0.93816284\n",
            "Iteration 85, loss = 0.93073407\n",
            "Iteration 86, loss = 0.92931428\n",
            "Iteration 87, loss = 0.92951557\n",
            "Iteration 88, loss = 0.92555564\n",
            "Iteration 89, loss = 0.92485563\n",
            "Iteration 90, loss = 0.92100595\n",
            "Iteration 91, loss = 0.91955092\n",
            "Iteration 92, loss = 0.92833064\n",
            "Iteration 93, loss = 0.92654924\n",
            "Iteration 94, loss = 0.92209166\n",
            "Iteration 95, loss = 0.92286026\n",
            "Iteration 96, loss = 0.92462021\n",
            "Iteration 97, loss = 0.91824763\n",
            "Iteration 98, loss = 0.92154741\n",
            "Iteration 99, loss = 0.91650445\n",
            "Iteration 100, loss = 0.91380949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28909884\n",
            "Iteration 2, loss = 1.25783833\n",
            "Iteration 3, loss = 1.25323474\n",
            "Iteration 4, loss = 1.24117024\n",
            "Iteration 5, loss = 1.22797336\n",
            "Iteration 6, loss = 1.21843165\n",
            "Iteration 7, loss = 1.20300749\n",
            "Iteration 8, loss = 1.18940432\n",
            "Iteration 9, loss = 1.17450646\n",
            "Iteration 10, loss = 1.16230330\n",
            "Iteration 11, loss = 1.14117172\n",
            "Iteration 12, loss = 1.12780782\n",
            "Iteration 13, loss = 1.10659500\n",
            "Iteration 14, loss = 1.09298488\n",
            "Iteration 15, loss = 1.07804951\n",
            "Iteration 16, loss = 1.05708099\n",
            "Iteration 17, loss = 1.04590392\n",
            "Iteration 18, loss = 1.03792563\n",
            "Iteration 19, loss = 1.02371685\n",
            "Iteration 20, loss = 1.01811643\n",
            "Iteration 21, loss = 1.01424706\n",
            "Iteration 22, loss = 1.00547762\n",
            "Iteration 23, loss = 1.00080521\n",
            "Iteration 24, loss = 0.99499898\n",
            "Iteration 25, loss = 0.99325057\n",
            "Iteration 26, loss = 0.99569694\n",
            "Iteration 27, loss = 0.99822641\n",
            "Iteration 28, loss = 0.99075777\n",
            "Iteration 29, loss = 0.99090396\n",
            "Iteration 30, loss = 0.98438939\n",
            "Iteration 31, loss = 0.98624112\n",
            "Iteration 32, loss = 0.98584767\n",
            "Iteration 33, loss = 0.97730725\n",
            "Iteration 34, loss = 0.97574956\n",
            "Iteration 35, loss = 0.97725540\n",
            "Iteration 36, loss = 0.97204250\n",
            "Iteration 37, loss = 0.96899224\n",
            "Iteration 38, loss = 0.97214433\n",
            "Iteration 39, loss = 0.96830416\n",
            "Iteration 40, loss = 0.96577622\n",
            "Iteration 41, loss = 0.96436212\n",
            "Iteration 42, loss = 0.96640206\n",
            "Iteration 43, loss = 0.96617792\n",
            "Iteration 44, loss = 0.96301932\n",
            "Iteration 45, loss = 0.96086459\n",
            "Iteration 46, loss = 0.95807289\n",
            "Iteration 47, loss = 0.96372576\n",
            "Iteration 48, loss = 0.96091712\n",
            "Iteration 49, loss = 0.96046567\n",
            "Iteration 50, loss = 0.95807084\n",
            "Iteration 51, loss = 0.95257039\n",
            "Iteration 52, loss = 0.96037434\n",
            "Iteration 53, loss = 0.94945306\n",
            "Iteration 54, loss = 0.95479945\n",
            "Iteration 55, loss = 0.95064750\n",
            "Iteration 56, loss = 0.94452650\n",
            "Iteration 57, loss = 0.95853709\n",
            "Iteration 58, loss = 0.96045092\n",
            "Iteration 59, loss = 0.95068421\n",
            "Iteration 60, loss = 0.94274431\n",
            "Iteration 61, loss = 0.95045764\n",
            "Iteration 62, loss = 0.95429880\n",
            "Iteration 63, loss = 0.94855860\n",
            "Iteration 64, loss = 0.93882797\n",
            "Iteration 65, loss = 0.94285624\n",
            "Iteration 66, loss = 0.93926429\n",
            "Iteration 67, loss = 0.93496286\n",
            "Iteration 68, loss = 0.93427447\n",
            "Iteration 69, loss = 0.93988789\n",
            "Iteration 70, loss = 0.93735269\n",
            "Iteration 71, loss = 0.93941557\n",
            "Iteration 72, loss = 0.93612196\n",
            "Iteration 73, loss = 0.92622002\n",
            "Iteration 74, loss = 0.93253457\n",
            "Iteration 75, loss = 0.93310611\n",
            "Iteration 76, loss = 0.92623450\n",
            "Iteration 77, loss = 0.92175803\n",
            "Iteration 78, loss = 0.92246138\n",
            "Iteration 79, loss = 0.92274235\n",
            "Iteration 80, loss = 0.92360872\n",
            "Iteration 81, loss = 0.92473570\n",
            "Iteration 82, loss = 0.92883538\n",
            "Iteration 83, loss = 0.92085138\n",
            "Iteration 84, loss = 0.91941792\n",
            "Iteration 85, loss = 0.91568301\n",
            "Iteration 86, loss = 0.91476011\n",
            "Iteration 87, loss = 0.91387289\n",
            "Iteration 88, loss = 0.91094848\n",
            "Iteration 89, loss = 0.91184795\n",
            "Iteration 90, loss = 0.90710684\n",
            "Iteration 91, loss = 0.91164563\n",
            "Iteration 92, loss = 0.91985329\n",
            "Iteration 93, loss = 0.91558158\n",
            "Iteration 94, loss = 0.91747118\n",
            "Iteration 95, loss = 0.90961392\n",
            "Iteration 96, loss = 0.91635535\n",
            "Iteration 97, loss = 0.90231955\n",
            "Iteration 98, loss = 0.90254840\n",
            "Iteration 99, loss = 0.90190187\n",
            "Iteration 100, loss = 0.90273226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28479041\n",
            "Iteration 2, loss = 1.26179432\n",
            "Iteration 3, loss = 1.25688448\n",
            "Iteration 4, loss = 1.24408610\n",
            "Iteration 5, loss = 1.23319119\n",
            "Iteration 6, loss = 1.22475292\n",
            "Iteration 7, loss = 1.21520193\n",
            "Iteration 8, loss = 1.20423312\n",
            "Iteration 9, loss = 1.18844309\n",
            "Iteration 10, loss = 1.17787163\n",
            "Iteration 11, loss = 1.15998923\n",
            "Iteration 12, loss = 1.14587173\n",
            "Iteration 13, loss = 1.12926635\n",
            "Iteration 14, loss = 1.10949072\n",
            "Iteration 15, loss = 1.09276514\n",
            "Iteration 16, loss = 1.07535186\n",
            "Iteration 17, loss = 1.06292524\n",
            "Iteration 18, loss = 1.05633916\n",
            "Iteration 19, loss = 1.04124713\n",
            "Iteration 20, loss = 1.03721505\n",
            "Iteration 21, loss = 1.03310844\n",
            "Iteration 22, loss = 1.02438673\n",
            "Iteration 23, loss = 1.01817526\n",
            "Iteration 24, loss = 1.01247041\n",
            "Iteration 25, loss = 1.01390632\n",
            "Iteration 26, loss = 1.01197398\n",
            "Iteration 27, loss = 1.01327840\n",
            "Iteration 28, loss = 1.01660127\n",
            "Iteration 29, loss = 1.01551016\n",
            "Iteration 30, loss = 1.00388247\n",
            "Iteration 31, loss = 1.00707639\n",
            "Iteration 32, loss = 1.00568850\n",
            "Iteration 33, loss = 0.99544141\n",
            "Iteration 34, loss = 0.99683749\n",
            "Iteration 35, loss = 0.99554449\n",
            "Iteration 36, loss = 0.99586214\n",
            "Iteration 37, loss = 0.99095283\n",
            "Iteration 38, loss = 0.99038499\n",
            "Iteration 39, loss = 0.99265630\n",
            "Iteration 40, loss = 0.98726482\n",
            "Iteration 41, loss = 0.98833555\n",
            "Iteration 42, loss = 0.99070888\n",
            "Iteration 43, loss = 0.98525294\n",
            "Iteration 44, loss = 0.98480841\n",
            "Iteration 45, loss = 0.98187354\n",
            "Iteration 46, loss = 0.98853573\n",
            "Iteration 47, loss = 0.99502020\n",
            "Iteration 48, loss = 0.98801160\n",
            "Iteration 49, loss = 0.98751830\n",
            "Iteration 50, loss = 0.98004617\n",
            "Iteration 51, loss = 0.97578389\n",
            "Iteration 52, loss = 0.98351907\n",
            "Iteration 53, loss = 0.97281883\n",
            "Iteration 54, loss = 0.97881507\n",
            "Iteration 55, loss = 0.97418597\n",
            "Iteration 56, loss = 0.97411111\n",
            "Iteration 57, loss = 0.98783152\n",
            "Iteration 58, loss = 0.98775748\n",
            "Iteration 59, loss = 0.97902307\n",
            "Iteration 60, loss = 0.97405898\n",
            "Iteration 61, loss = 0.97927451\n",
            "Iteration 62, loss = 0.98102451\n",
            "Iteration 63, loss = 0.97401183\n",
            "Iteration 64, loss = 0.97216762\n",
            "Iteration 65, loss = 0.97040872\n",
            "Iteration 66, loss = 0.96925548\n",
            "Iteration 67, loss = 0.96323218\n",
            "Iteration 68, loss = 0.96081181\n",
            "Iteration 69, loss = 0.96881592\n",
            "Iteration 70, loss = 0.96529897\n",
            "Iteration 71, loss = 0.96208663\n",
            "Iteration 72, loss = 0.96038093\n",
            "Iteration 73, loss = 0.95519223\n",
            "Iteration 74, loss = 0.96451598\n",
            "Iteration 75, loss = 0.95447206\n",
            "Iteration 76, loss = 0.95805666\n",
            "Iteration 77, loss = 0.95459675\n",
            "Iteration 78, loss = 0.95197268\n",
            "Iteration 79, loss = 0.95251256\n",
            "Iteration 80, loss = 0.95066939\n",
            "Iteration 81, loss = 0.95895850\n",
            "Iteration 82, loss = 0.96036462\n",
            "Iteration 83, loss = 0.95881254\n",
            "Iteration 84, loss = 0.95501469\n",
            "Iteration 85, loss = 0.94513495\n",
            "Iteration 86, loss = 0.94806531\n",
            "Iteration 87, loss = 0.94336837\n",
            "Iteration 88, loss = 0.94449951\n",
            "Iteration 89, loss = 0.94376704\n",
            "Iteration 90, loss = 0.94092062\n",
            "Iteration 91, loss = 0.94277376\n",
            "Iteration 92, loss = 0.95189824\n",
            "Iteration 93, loss = 0.94461865\n",
            "Iteration 94, loss = 0.94705098\n",
            "Iteration 95, loss = 0.94112114\n",
            "Iteration 96, loss = 0.94878295\n",
            "Iteration 97, loss = 0.93475526\n",
            "Iteration 98, loss = 0.93404483\n",
            "Iteration 99, loss = 0.93483058\n",
            "Iteration 100, loss = 0.93639451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28334623\n",
            "Iteration 2, loss = 1.25210061\n",
            "Iteration 3, loss = 1.24643882\n",
            "Iteration 4, loss = 1.23398038\n",
            "Iteration 5, loss = 1.22350995\n",
            "Iteration 6, loss = 1.21699321\n",
            "Iteration 7, loss = 1.20285554\n",
            "Iteration 8, loss = 1.19270190\n",
            "Iteration 9, loss = 1.17470231\n",
            "Iteration 10, loss = 1.16107713\n",
            "Iteration 11, loss = 1.14308624\n",
            "Iteration 12, loss = 1.12623723\n",
            "Iteration 13, loss = 1.11179952\n",
            "Iteration 14, loss = 1.08762324\n",
            "Iteration 15, loss = 1.07086979\n",
            "Iteration 16, loss = 1.05389744\n",
            "Iteration 17, loss = 1.04244203\n",
            "Iteration 18, loss = 1.03325825\n",
            "Iteration 19, loss = 1.02075638\n",
            "Iteration 20, loss = 1.01464706\n",
            "Iteration 21, loss = 1.01002727\n",
            "Iteration 22, loss = 1.00363502\n",
            "Iteration 23, loss = 0.99723916\n",
            "Iteration 24, loss = 0.99480972\n",
            "Iteration 25, loss = 0.99439147\n",
            "Iteration 26, loss = 0.99380780\n",
            "Iteration 27, loss = 0.99303536\n",
            "Iteration 28, loss = 0.99832133\n",
            "Iteration 29, loss = 0.99571668\n",
            "Iteration 30, loss = 0.98380220\n",
            "Iteration 31, loss = 0.98405092\n",
            "Iteration 32, loss = 0.98324650\n",
            "Iteration 33, loss = 0.97424914\n",
            "Iteration 34, loss = 0.97856346\n",
            "Iteration 35, loss = 0.97508683\n",
            "Iteration 36, loss = 0.97277790\n",
            "Iteration 37, loss = 0.97035770\n",
            "Iteration 38, loss = 0.97038074\n",
            "Iteration 39, loss = 0.97580306\n",
            "Iteration 40, loss = 0.96927912\n",
            "Iteration 41, loss = 0.96691079\n",
            "Iteration 42, loss = 0.97065457\n",
            "Iteration 43, loss = 0.96264671\n",
            "Iteration 44, loss = 0.97127275\n",
            "Iteration 45, loss = 0.96208183\n",
            "Iteration 46, loss = 0.97051330\n",
            "Iteration 47, loss = 0.97374667\n",
            "Iteration 48, loss = 0.96701282\n",
            "Iteration 49, loss = 0.96425633\n",
            "Iteration 50, loss = 0.95997107\n",
            "Iteration 51, loss = 0.95584659\n",
            "Iteration 52, loss = 0.95892478\n",
            "Iteration 53, loss = 0.95114979\n",
            "Iteration 54, loss = 0.95712499\n",
            "Iteration 55, loss = 0.95355579\n",
            "Iteration 56, loss = 0.95127340\n",
            "Iteration 57, loss = 0.95345523\n",
            "Iteration 58, loss = 0.96256881\n",
            "Iteration 59, loss = 0.95321260\n",
            "Iteration 60, loss = 0.94707355\n",
            "Iteration 61, loss = 0.95382288\n",
            "Iteration 62, loss = 0.96009427\n",
            "Iteration 63, loss = 0.94577378\n",
            "Iteration 64, loss = 0.94374840\n",
            "Iteration 65, loss = 0.94691232\n",
            "Iteration 66, loss = 0.94355832\n",
            "Iteration 67, loss = 0.94307734\n",
            "Iteration 68, loss = 0.94343751\n",
            "Iteration 69, loss = 0.93964864\n",
            "Iteration 70, loss = 0.94778661\n",
            "Iteration 71, loss = 0.94751876\n",
            "Iteration 72, loss = 0.94581019\n",
            "Iteration 73, loss = 0.93418901\n",
            "Iteration 74, loss = 0.94476424\n",
            "Iteration 75, loss = 0.94161488\n",
            "Iteration 76, loss = 0.93402667\n",
            "Iteration 77, loss = 0.93119711\n",
            "Iteration 78, loss = 0.92853279\n",
            "Iteration 79, loss = 0.93580489\n",
            "Iteration 80, loss = 0.93056693\n",
            "Iteration 81, loss = 0.94033870\n",
            "Iteration 82, loss = 0.93657611\n",
            "Iteration 83, loss = 0.94455797\n",
            "Iteration 84, loss = 0.93451011\n",
            "Iteration 85, loss = 0.92865499\n",
            "Iteration 86, loss = 0.92721987\n",
            "Iteration 87, loss = 0.92152347\n",
            "Iteration 88, loss = 0.92664825\n",
            "Iteration 89, loss = 0.92243372\n",
            "Iteration 90, loss = 0.92207385\n",
            "Iteration 91, loss = 0.92088632\n",
            "Iteration 92, loss = 0.92295818\n",
            "Iteration 93, loss = 0.92491857\n",
            "Iteration 94, loss = 0.92650828\n",
            "Iteration 95, loss = 0.91816315\n",
            "Iteration 96, loss = 0.92110075\n",
            "Iteration 97, loss = 0.91550033\n",
            "Iteration 98, loss = 0.91473052\n",
            "Iteration 99, loss = 0.91412983\n",
            "Iteration 100, loss = 0.91777707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27860073\n",
            "Iteration 2, loss = 1.24190189\n",
            "Iteration 3, loss = 1.23625441\n",
            "Iteration 4, loss = 1.22355436\n",
            "Iteration 5, loss = 1.21223584\n",
            "Iteration 6, loss = 1.20353745\n",
            "Iteration 7, loss = 1.18939226\n",
            "Iteration 8, loss = 1.17699861\n",
            "Iteration 9, loss = 1.16032842\n",
            "Iteration 10, loss = 1.14714486\n",
            "Iteration 11, loss = 1.12731341\n",
            "Iteration 12, loss = 1.10975596\n",
            "Iteration 13, loss = 1.09429571\n",
            "Iteration 14, loss = 1.06852323\n",
            "Iteration 15, loss = 1.05108446\n",
            "Iteration 16, loss = 1.03489769\n",
            "Iteration 17, loss = 1.02311658\n",
            "Iteration 18, loss = 1.01172788\n",
            "Iteration 19, loss = 1.00162087\n",
            "Iteration 20, loss = 0.99352650\n",
            "Iteration 21, loss = 0.98827508\n",
            "Iteration 22, loss = 0.98644584\n",
            "Iteration 23, loss = 0.97783481\n",
            "Iteration 24, loss = 0.97597250\n",
            "Iteration 25, loss = 0.97407069\n",
            "Iteration 26, loss = 0.98512482\n",
            "Iteration 27, loss = 0.98372286\n",
            "Iteration 28, loss = 0.97862712\n",
            "Iteration 29, loss = 0.97144291\n",
            "Iteration 30, loss = 0.96754076\n",
            "Iteration 31, loss = 0.96864994\n",
            "Iteration 32, loss = 0.97724997\n",
            "Iteration 33, loss = 0.96036909\n",
            "Iteration 34, loss = 0.96291507\n",
            "Iteration 35, loss = 0.95667319\n",
            "Iteration 36, loss = 0.96078012\n",
            "Iteration 37, loss = 0.95210719\n",
            "Iteration 38, loss = 0.95508170\n",
            "Iteration 39, loss = 0.95176464\n",
            "Iteration 40, loss = 0.94966711\n",
            "Iteration 41, loss = 0.94659930\n",
            "Iteration 42, loss = 0.95142523\n",
            "Iteration 43, loss = 0.94729018\n",
            "Iteration 44, loss = 0.95407494\n",
            "Iteration 45, loss = 0.94943144\n",
            "Iteration 46, loss = 0.95504017\n",
            "Iteration 47, loss = 0.95088596\n",
            "Iteration 48, loss = 0.94814639\n",
            "Iteration 49, loss = 0.94884385\n",
            "Iteration 50, loss = 0.94357903\n",
            "Iteration 51, loss = 0.94406046\n",
            "Iteration 52, loss = 0.94362339\n",
            "Iteration 53, loss = 0.94221896\n",
            "Iteration 54, loss = 0.94136945\n",
            "Iteration 55, loss = 0.93750549\n",
            "Iteration 56, loss = 0.94201651\n",
            "Iteration 57, loss = 0.94102820\n",
            "Iteration 58, loss = 0.94016028\n",
            "Iteration 59, loss = 0.94100641\n",
            "Iteration 60, loss = 0.93446006\n",
            "Iteration 61, loss = 0.94244109\n",
            "Iteration 62, loss = 0.94250354\n",
            "Iteration 63, loss = 0.93415966\n",
            "Iteration 64, loss = 0.93069555\n",
            "Iteration 65, loss = 0.92963582\n",
            "Iteration 66, loss = 0.92773798\n",
            "Iteration 67, loss = 0.92902481\n",
            "Iteration 68, loss = 0.92572748\n",
            "Iteration 69, loss = 0.93095949\n",
            "Iteration 70, loss = 0.93593401\n",
            "Iteration 71, loss = 0.93164821\n",
            "Iteration 72, loss = 0.93834261\n",
            "Iteration 73, loss = 0.92470075\n",
            "Iteration 74, loss = 0.93064731\n",
            "Iteration 75, loss = 0.93719670\n",
            "Iteration 76, loss = 0.92219450\n",
            "Iteration 77, loss = 0.92225074\n",
            "Iteration 78, loss = 0.91767047\n",
            "Iteration 79, loss = 0.91930348\n",
            "Iteration 80, loss = 0.91485682\n",
            "Iteration 81, loss = 0.92487336\n",
            "Iteration 82, loss = 0.91822796\n",
            "Iteration 83, loss = 0.92168030\n",
            "Iteration 84, loss = 0.91704672\n",
            "Iteration 85, loss = 0.90924129\n",
            "Iteration 86, loss = 0.91134118\n",
            "Iteration 87, loss = 0.90707983\n",
            "Iteration 88, loss = 0.91147889\n",
            "Iteration 89, loss = 0.91275917\n",
            "Iteration 90, loss = 0.91067374\n",
            "Iteration 91, loss = 0.90914007\n",
            "Iteration 92, loss = 0.91032318\n",
            "Iteration 93, loss = 0.90773523\n",
            "Iteration 94, loss = 0.91648404\n",
            "Iteration 95, loss = 0.90914523\n",
            "Iteration 96, loss = 0.91102984\n",
            "Iteration 97, loss = 0.90131230\n",
            "Iteration 98, loss = 0.90203581\n",
            "Iteration 99, loss = 0.89906390\n",
            "Iteration 100, loss = 0.90491814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28083566\n",
            "Iteration 2, loss = 1.24723331\n",
            "Iteration 3, loss = 1.24065224\n",
            "Iteration 4, loss = 1.22949305\n",
            "Iteration 5, loss = 1.21871178\n",
            "Iteration 6, loss = 1.20914673\n",
            "Iteration 7, loss = 1.19615115\n",
            "Iteration 8, loss = 1.18575027\n",
            "Iteration 9, loss = 1.16951266\n",
            "Iteration 10, loss = 1.15540184\n",
            "Iteration 11, loss = 1.13794496\n",
            "Iteration 12, loss = 1.12190219\n",
            "Iteration 13, loss = 1.10456083\n",
            "Iteration 14, loss = 1.08387147\n",
            "Iteration 15, loss = 1.06606831\n",
            "Iteration 16, loss = 1.04967823\n",
            "Iteration 17, loss = 1.03934713\n",
            "Iteration 18, loss = 1.02633312\n",
            "Iteration 19, loss = 1.02278398\n",
            "Iteration 20, loss = 1.01041842\n",
            "Iteration 21, loss = 1.00742173\n",
            "Iteration 22, loss = 1.00830391\n",
            "Iteration 23, loss = 0.99735895\n",
            "Iteration 24, loss = 0.99417298\n",
            "Iteration 25, loss = 0.99469592\n",
            "Iteration 26, loss = 0.99577882\n",
            "Iteration 27, loss = 0.99284487\n",
            "Iteration 28, loss = 1.00092154\n",
            "Iteration 29, loss = 0.99335665\n",
            "Iteration 30, loss = 0.98619753\n",
            "Iteration 31, loss = 0.98548048\n",
            "Iteration 32, loss = 1.00683099\n",
            "Iteration 33, loss = 0.98225923\n",
            "Iteration 34, loss = 0.98318390\n",
            "Iteration 35, loss = 0.97522815\n",
            "Iteration 36, loss = 0.97822831\n",
            "Iteration 37, loss = 0.96880510\n",
            "Iteration 38, loss = 0.97330760\n",
            "Iteration 39, loss = 0.96728030\n",
            "Iteration 40, loss = 0.96602250\n",
            "Iteration 41, loss = 0.96110037\n",
            "Iteration 42, loss = 0.96436019\n",
            "Iteration 43, loss = 0.96018103\n",
            "Iteration 44, loss = 0.96490955\n",
            "Iteration 45, loss = 0.96319751\n",
            "Iteration 46, loss = 0.97474654\n",
            "Iteration 47, loss = 0.96453662\n",
            "Iteration 48, loss = 0.96552795\n",
            "Iteration 49, loss = 0.96368081\n",
            "Iteration 50, loss = 0.95453698\n",
            "Iteration 51, loss = 0.95311475\n",
            "Iteration 52, loss = 0.96218546\n",
            "Iteration 53, loss = 0.95587504\n",
            "Iteration 54, loss = 0.95511028\n",
            "Iteration 55, loss = 0.95401965\n",
            "Iteration 56, loss = 0.95299706\n",
            "Iteration 57, loss = 0.94608971\n",
            "Iteration 58, loss = 0.95271790\n",
            "Iteration 59, loss = 0.95109303\n",
            "Iteration 60, loss = 0.94270224\n",
            "Iteration 61, loss = 0.94441054\n",
            "Iteration 62, loss = 0.94555063\n",
            "Iteration 63, loss = 0.94402350\n",
            "Iteration 64, loss = 0.94872764\n",
            "Iteration 65, loss = 0.94256307\n",
            "Iteration 66, loss = 0.93939621\n",
            "Iteration 67, loss = 0.93549580\n",
            "Iteration 68, loss = 0.93504660\n",
            "Iteration 69, loss = 0.93785972\n",
            "Iteration 70, loss = 0.93847787\n",
            "Iteration 71, loss = 0.93849069\n",
            "Iteration 72, loss = 0.94209830\n",
            "Iteration 73, loss = 0.93553694\n",
            "Iteration 74, loss = 0.93780621\n",
            "Iteration 75, loss = 0.94352004\n",
            "Iteration 76, loss = 0.93119245\n",
            "Iteration 77, loss = 0.92726219\n",
            "Iteration 78, loss = 0.93005111\n",
            "Iteration 79, loss = 0.93041478\n",
            "Iteration 80, loss = 0.92429847\n",
            "Iteration 81, loss = 0.93398924\n",
            "Iteration 82, loss = 0.92424210\n",
            "Iteration 83, loss = 0.93386012\n",
            "Iteration 84, loss = 0.92016926\n",
            "Iteration 85, loss = 0.92525083\n",
            "Iteration 86, loss = 0.92313422\n",
            "Iteration 87, loss = 0.91565646\n",
            "Iteration 88, loss = 0.91910102\n",
            "Iteration 89, loss = 0.91824001\n",
            "Iteration 90, loss = 0.91613153\n",
            "Iteration 91, loss = 0.91464208\n",
            "Iteration 92, loss = 0.91440516\n",
            "Iteration 93, loss = 0.91128180\n",
            "Iteration 94, loss = 0.91763229\n",
            "Iteration 95, loss = 0.91711899\n",
            "Iteration 96, loss = 0.91198077\n",
            "Iteration 97, loss = 0.90738482\n",
            "Iteration 98, loss = 0.90964227\n",
            "Iteration 99, loss = 0.90585939\n",
            "Iteration 100, loss = 0.91060547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28164271\n",
            "Iteration 2, loss = 1.24643256\n",
            "Iteration 3, loss = 1.24335264\n",
            "Iteration 4, loss = 1.23172846\n",
            "Iteration 5, loss = 1.22297678\n",
            "Iteration 6, loss = 1.21597948\n",
            "Iteration 7, loss = 1.20536170\n",
            "Iteration 8, loss = 1.19043919\n",
            "Iteration 9, loss = 1.18044549\n",
            "Iteration 10, loss = 1.16847756\n",
            "Iteration 11, loss = 1.15185777\n",
            "Iteration 12, loss = 1.13448117\n",
            "Iteration 13, loss = 1.12293614\n",
            "Iteration 14, loss = 1.10371576\n",
            "Iteration 15, loss = 1.08966444\n",
            "Iteration 16, loss = 1.07263836\n",
            "Iteration 17, loss = 1.05864449\n",
            "Iteration 18, loss = 1.03972554\n",
            "Iteration 19, loss = 1.03540221\n",
            "Iteration 20, loss = 1.02224625\n",
            "Iteration 21, loss = 1.01473100\n",
            "Iteration 22, loss = 1.02411472\n",
            "Iteration 23, loss = 1.00316935\n",
            "Iteration 24, loss = 1.00361294\n",
            "Iteration 25, loss = 1.00053251\n",
            "Iteration 26, loss = 0.99728936\n",
            "Iteration 27, loss = 0.99335419\n",
            "Iteration 28, loss = 0.98928807\n",
            "Iteration 29, loss = 0.99181542\n",
            "Iteration 30, loss = 0.98950915\n",
            "Iteration 31, loss = 0.98419597\n",
            "Iteration 32, loss = 0.97986805\n",
            "Iteration 33, loss = 0.98008011\n",
            "Iteration 34, loss = 0.97766256\n",
            "Iteration 35, loss = 0.98881783\n",
            "Iteration 36, loss = 0.98084737\n",
            "Iteration 37, loss = 0.97397158\n",
            "Iteration 38, loss = 0.97949644\n",
            "Iteration 39, loss = 0.99153471\n",
            "Iteration 40, loss = 0.97889849\n",
            "Iteration 41, loss = 0.97609984\n",
            "Iteration 42, loss = 0.98356329\n",
            "Iteration 43, loss = 0.96483411\n",
            "Iteration 44, loss = 0.97785373\n",
            "Iteration 45, loss = 0.96225695\n",
            "Iteration 46, loss = 0.96153187\n",
            "Iteration 47, loss = 0.95956319\n",
            "Iteration 48, loss = 0.95787287\n",
            "Iteration 49, loss = 0.96047046\n",
            "Iteration 50, loss = 0.95522244\n",
            "Iteration 51, loss = 0.95923574\n",
            "Iteration 52, loss = 0.95782324\n",
            "Iteration 53, loss = 0.95332483\n",
            "Iteration 54, loss = 0.95130997\n",
            "Iteration 55, loss = 0.95062659\n",
            "Iteration 56, loss = 0.95259169\n",
            "Iteration 57, loss = 0.95152158\n",
            "Iteration 58, loss = 0.94922124\n",
            "Iteration 59, loss = 0.95247922\n",
            "Iteration 60, loss = 0.94859034\n",
            "Iteration 61, loss = 0.94864140\n",
            "Iteration 62, loss = 0.95190059\n",
            "Iteration 63, loss = 0.94468648\n",
            "Iteration 64, loss = 0.94688740\n",
            "Iteration 65, loss = 0.94288888\n",
            "Iteration 66, loss = 0.94761503\n",
            "Iteration 67, loss = 0.94106721\n",
            "Iteration 68, loss = 0.93719726\n",
            "Iteration 69, loss = 0.93910150\n",
            "Iteration 70, loss = 0.93824961\n",
            "Iteration 71, loss = 0.93478050\n",
            "Iteration 72, loss = 0.93360504\n",
            "Iteration 73, loss = 0.93905469\n",
            "Iteration 74, loss = 0.93203328\n",
            "Iteration 75, loss = 0.93300058\n",
            "Iteration 76, loss = 0.93749948\n",
            "Iteration 77, loss = 0.92936750\n",
            "Iteration 78, loss = 0.92751868\n",
            "Iteration 79, loss = 0.93010067\n",
            "Iteration 80, loss = 0.94880634\n",
            "Iteration 81, loss = 0.92907584\n",
            "Iteration 82, loss = 0.94034865\n",
            "Iteration 83, loss = 0.92753867\n",
            "Iteration 84, loss = 0.92962518\n",
            "Iteration 85, loss = 0.92388612\n",
            "Iteration 86, loss = 0.92418712\n",
            "Iteration 87, loss = 0.92240996\n",
            "Iteration 88, loss = 0.92033588\n",
            "Iteration 89, loss = 0.92101206\n",
            "Iteration 90, loss = 0.91615104\n",
            "Iteration 91, loss = 0.92404147\n",
            "Iteration 92, loss = 0.91671548\n",
            "Iteration 93, loss = 0.91295486\n",
            "Iteration 94, loss = 0.91363226\n",
            "Iteration 95, loss = 0.91292577\n",
            "Iteration 96, loss = 0.91247865\n",
            "Iteration 97, loss = 0.91198648\n",
            "Iteration 98, loss = 0.91984686\n",
            "Iteration 99, loss = 0.91360981\n",
            "Iteration 100, loss = 0.91638138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.28096069\n",
            "Iteration 2, loss = 1.24014465\n",
            "Iteration 3, loss = 1.23910023\n",
            "Iteration 4, loss = 1.22978696\n",
            "Iteration 5, loss = 1.22106790\n",
            "Iteration 6, loss = 1.21433583\n",
            "Iteration 7, loss = 1.20617142\n",
            "Iteration 8, loss = 1.19254280\n",
            "Iteration 9, loss = 1.18134688\n",
            "Iteration 10, loss = 1.17156179\n",
            "Iteration 11, loss = 1.15836811\n",
            "Iteration 12, loss = 1.13844669\n",
            "Iteration 13, loss = 1.13055580\n",
            "Iteration 14, loss = 1.11038138\n",
            "Iteration 15, loss = 1.09848512\n",
            "Iteration 16, loss = 1.07882598\n",
            "Iteration 17, loss = 1.06551575\n",
            "Iteration 18, loss = 1.04966112\n",
            "Iteration 19, loss = 1.03765327\n",
            "Iteration 20, loss = 1.02604934\n",
            "Iteration 21, loss = 1.01911375\n",
            "Iteration 22, loss = 1.02541758\n",
            "Iteration 23, loss = 1.00845890\n",
            "Iteration 24, loss = 1.00196099\n",
            "Iteration 25, loss = 1.00599820\n",
            "Iteration 26, loss = 0.99754945\n",
            "Iteration 27, loss = 0.99673179\n",
            "Iteration 28, loss = 0.99148480\n",
            "Iteration 29, loss = 0.99662703\n",
            "Iteration 30, loss = 0.99195773\n",
            "Iteration 31, loss = 0.98443142\n",
            "Iteration 32, loss = 0.98168542\n",
            "Iteration 33, loss = 0.98119982\n",
            "Iteration 34, loss = 0.97486413\n",
            "Iteration 35, loss = 0.98010638\n",
            "Iteration 36, loss = 0.98023432\n",
            "Iteration 37, loss = 0.97039300\n",
            "Iteration 38, loss = 0.98011513\n",
            "Iteration 39, loss = 0.98335786\n",
            "Iteration 40, loss = 0.99019688\n",
            "Iteration 41, loss = 0.97003008\n",
            "Iteration 42, loss = 0.99033237\n",
            "Iteration 43, loss = 0.96483635\n",
            "Iteration 44, loss = 0.97705964\n",
            "Iteration 45, loss = 0.96508512\n",
            "Iteration 46, loss = 0.96397009\n",
            "Iteration 47, loss = 0.95904668\n",
            "Iteration 48, loss = 0.95893498\n",
            "Iteration 49, loss = 0.95932161\n",
            "Iteration 50, loss = 0.95418119\n",
            "Iteration 51, loss = 0.96045518\n",
            "Iteration 52, loss = 0.95853157\n",
            "Iteration 53, loss = 0.95402771\n",
            "Iteration 54, loss = 0.95224448\n",
            "Iteration 55, loss = 0.95021873\n",
            "Iteration 56, loss = 0.94962149\n",
            "Iteration 57, loss = 0.94918750\n",
            "Iteration 58, loss = 0.94580509\n",
            "Iteration 59, loss = 0.94759813\n",
            "Iteration 60, loss = 0.94657417\n",
            "Iteration 61, loss = 0.94421537\n",
            "Iteration 62, loss = 0.94930174\n",
            "Iteration 63, loss = 0.94458164\n",
            "Iteration 64, loss = 0.94408736\n",
            "Iteration 65, loss = 0.94219564\n",
            "Iteration 66, loss = 0.94334336\n",
            "Iteration 67, loss = 0.93918915\n",
            "Iteration 68, loss = 0.93446390\n",
            "Iteration 69, loss = 0.93659938\n",
            "Iteration 70, loss = 0.93812421\n",
            "Iteration 71, loss = 0.93505164\n",
            "Iteration 72, loss = 0.93915053\n",
            "Iteration 73, loss = 0.93564112\n",
            "Iteration 74, loss = 0.93276801\n",
            "Iteration 75, loss = 0.93146861\n",
            "Iteration 76, loss = 0.93412638\n",
            "Iteration 77, loss = 0.92730602\n",
            "Iteration 78, loss = 0.92814805\n",
            "Iteration 79, loss = 0.92674978\n",
            "Iteration 80, loss = 0.94006593\n",
            "Iteration 81, loss = 0.92574810\n",
            "Iteration 82, loss = 0.93371002\n",
            "Iteration 83, loss = 0.92144676\n",
            "Iteration 84, loss = 0.93026518\n",
            "Iteration 85, loss = 0.91843526\n",
            "Iteration 86, loss = 0.92074390\n",
            "Iteration 87, loss = 0.91825534\n",
            "Iteration 88, loss = 0.91679975\n",
            "Iteration 89, loss = 0.91578653\n",
            "Iteration 90, loss = 0.91435888\n",
            "Iteration 91, loss = 0.92033646\n",
            "Iteration 92, loss = 0.91547813\n",
            "Iteration 93, loss = 0.91335039\n",
            "Iteration 94, loss = 0.91189850\n",
            "Iteration 95, loss = 0.90974927\n",
            "Iteration 96, loss = 0.90819744\n",
            "Iteration 97, loss = 0.91040391\n",
            "Iteration 98, loss = 0.92067561\n",
            "Iteration 99, loss = 0.91223534\n",
            "Iteration 100, loss = 0.91467059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.27268788\n",
            "Iteration 2, loss = 1.22825878\n",
            "Iteration 3, loss = 1.22816928\n",
            "Iteration 4, loss = 1.22092252\n",
            "Iteration 5, loss = 1.21194187\n",
            "Iteration 6, loss = 1.20407660\n",
            "Iteration 7, loss = 1.19795468\n",
            "Iteration 8, loss = 1.18813723\n",
            "Iteration 9, loss = 1.17655504\n",
            "Iteration 10, loss = 1.16822808\n",
            "Iteration 11, loss = 1.15873508\n",
            "Iteration 12, loss = 1.14387054\n",
            "Iteration 13, loss = 1.13851686\n",
            "Iteration 14, loss = 1.11638758\n",
            "Iteration 15, loss = 1.10395486\n",
            "Iteration 16, loss = 1.08529976\n",
            "Iteration 17, loss = 1.07300202\n",
            "Iteration 18, loss = 1.05593155\n",
            "Iteration 19, loss = 1.04404191\n",
            "Iteration 20, loss = 1.03178866\n",
            "Iteration 21, loss = 1.02799918\n",
            "Iteration 22, loss = 1.02535035\n",
            "Iteration 23, loss = 1.01395592\n",
            "Iteration 24, loss = 1.00763723\n",
            "Iteration 25, loss = 1.00563400\n",
            "Iteration 26, loss = 1.00078157\n",
            "Iteration 27, loss = 0.99464052\n",
            "Iteration 28, loss = 0.99210672\n",
            "Iteration 29, loss = 0.99694597\n",
            "Iteration 30, loss = 0.99064202\n",
            "Iteration 31, loss = 0.98003927\n",
            "Iteration 32, loss = 0.98126010\n",
            "Iteration 33, loss = 0.97842882\n",
            "Iteration 34, loss = 0.97352484\n",
            "Iteration 35, loss = 0.97580989\n",
            "Iteration 36, loss = 0.97875494\n",
            "Iteration 37, loss = 0.97033584\n",
            "Iteration 38, loss = 0.97553775\n",
            "Iteration 39, loss = 0.98152841\n",
            "Iteration 40, loss = 0.98577698\n",
            "Iteration 41, loss = 0.96986697\n",
            "Iteration 42, loss = 0.98194214\n",
            "Iteration 43, loss = 0.95944161\n",
            "Iteration 44, loss = 0.97590760\n",
            "Iteration 45, loss = 0.96570202\n",
            "Iteration 46, loss = 0.95956475\n",
            "Iteration 47, loss = 0.95545219\n",
            "Iteration 48, loss = 0.95646852\n",
            "Iteration 49, loss = 0.95539275\n",
            "Iteration 50, loss = 0.95447825\n",
            "Iteration 51, loss = 0.95679578\n",
            "Iteration 52, loss = 0.95462852\n",
            "Iteration 53, loss = 0.95109927\n",
            "Iteration 54, loss = 0.94887182\n",
            "Iteration 55, loss = 0.94663625\n",
            "Iteration 56, loss = 0.94629704\n",
            "Iteration 57, loss = 0.94573366\n",
            "Iteration 58, loss = 0.94334315\n",
            "Iteration 59, loss = 0.94634959\n",
            "Iteration 60, loss = 0.94263601\n",
            "Iteration 61, loss = 0.93912503\n",
            "Iteration 62, loss = 0.94635981\n",
            "Iteration 63, loss = 0.93778092\n",
            "Iteration 64, loss = 0.93703589\n",
            "Iteration 65, loss = 0.93622910\n",
            "Iteration 66, loss = 0.93568513\n",
            "Iteration 67, loss = 0.93531123\n",
            "Iteration 68, loss = 0.93124307\n",
            "Iteration 69, loss = 0.93178109\n",
            "Iteration 70, loss = 0.93449235\n",
            "Iteration 71, loss = 0.93238946\n",
            "Iteration 72, loss = 0.93369065\n",
            "Iteration 73, loss = 0.93205315\n",
            "Iteration 74, loss = 0.93079708\n",
            "Iteration 75, loss = 0.92975605\n",
            "Iteration 76, loss = 0.92750270\n",
            "Iteration 77, loss = 0.92298080\n",
            "Iteration 78, loss = 0.92070106\n",
            "Iteration 79, loss = 0.92209781\n",
            "Iteration 80, loss = 0.93797594\n",
            "Iteration 81, loss = 0.92416225\n",
            "Iteration 82, loss = 0.92384896\n",
            "Iteration 83, loss = 0.91442044\n",
            "Iteration 84, loss = 0.92687794\n",
            "Iteration 85, loss = 0.91501796\n",
            "Iteration 86, loss = 0.91697478\n",
            "Iteration 87, loss = 0.91448754\n",
            "Iteration 88, loss = 0.91363664\n",
            "Iteration 89, loss = 0.91191060\n",
            "Iteration 90, loss = 0.90908544\n",
            "Iteration 91, loss = 0.91187540\n",
            "Iteration 92, loss = 0.90879743\n",
            "Iteration 93, loss = 0.91166170\n",
            "Iteration 94, loss = 0.90795512\n",
            "Iteration 95, loss = 0.90451238\n",
            "Iteration 96, loss = 0.90568324\n",
            "Iteration 97, loss = 0.90896185\n",
            "Iteration 98, loss = 0.90706940\n",
            "Iteration 99, loss = 0.90155322\n",
            "Iteration 100, loss = 0.90277461\n",
            "19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32468769\n",
            "Iteration 2, loss = 1.27626213\n",
            "Iteration 3, loss = 1.26271145\n",
            "Iteration 4, loss = 1.26139842\n",
            "Iteration 5, loss = 1.25924083\n",
            "Iteration 6, loss = 1.25768150\n",
            "Iteration 7, loss = 1.25704041\n",
            "Iteration 8, loss = 1.25581424\n",
            "Iteration 9, loss = 1.25476655\n",
            "Iteration 10, loss = 1.25445383\n",
            "Iteration 11, loss = 1.25465271\n",
            "Iteration 12, loss = 1.25406632\n",
            "Iteration 13, loss = 1.25249917\n",
            "Iteration 14, loss = 1.25319201\n",
            "Iteration 15, loss = 1.25173672\n",
            "Iteration 16, loss = 1.25127163\n",
            "Iteration 17, loss = 1.25168708\n",
            "Iteration 18, loss = 1.25162852\n",
            "Iteration 19, loss = 1.25092255\n",
            "Iteration 20, loss = 1.24919687\n",
            "Iteration 21, loss = 1.24830116\n",
            "Iteration 22, loss = 1.24796552\n",
            "Iteration 23, loss = 1.24850196\n",
            "Iteration 24, loss = 1.24734233\n",
            "Iteration 25, loss = 1.24597741\n",
            "Iteration 26, loss = 1.24686396\n",
            "Iteration 27, loss = 1.24609923\n",
            "Iteration 28, loss = 1.24443354\n",
            "Iteration 29, loss = 1.24465416\n",
            "Iteration 30, loss = 1.24433931\n",
            "Iteration 31, loss = 1.24451371\n",
            "Iteration 32, loss = 1.24688981\n",
            "Iteration 33, loss = 1.24587974\n",
            "Iteration 34, loss = 1.24493249\n",
            "Iteration 35, loss = 1.24579665\n",
            "Iteration 36, loss = 1.24444234\n",
            "Iteration 37, loss = 1.24513554\n",
            "Iteration 38, loss = 1.24423309\n",
            "Iteration 39, loss = 1.24266135\n",
            "Iteration 40, loss = 1.24190096\n",
            "Iteration 41, loss = 1.24166175\n",
            "Iteration 42, loss = 1.24129647\n",
            "Iteration 43, loss = 1.24120544\n",
            "Iteration 44, loss = 1.24177355\n",
            "Iteration 45, loss = 1.24032842\n",
            "Iteration 46, loss = 1.24140361\n",
            "Iteration 47, loss = 1.24198832\n",
            "Iteration 48, loss = 1.24075234\n",
            "Iteration 49, loss = 1.24017332\n",
            "Iteration 50, loss = 1.24046623\n",
            "Iteration 51, loss = 1.23995243\n",
            "Iteration 52, loss = 1.24047286\n",
            "Iteration 53, loss = 1.24175828\n",
            "Iteration 54, loss = 1.24075569\n",
            "Iteration 55, loss = 1.24164187\n",
            "Iteration 56, loss = 1.24063143\n",
            "Iteration 57, loss = 1.23972605\n",
            "Iteration 58, loss = 1.23993748\n",
            "Iteration 59, loss = 1.24016166\n",
            "Iteration 60, loss = 1.23984301\n",
            "Iteration 61, loss = 1.24022513\n",
            "Iteration 62, loss = 1.24055863\n",
            "Iteration 63, loss = 1.23948554\n",
            "Iteration 64, loss = 1.23993951\n",
            "Iteration 65, loss = 1.23888369\n",
            "Iteration 66, loss = 1.24019342\n",
            "Iteration 67, loss = 1.24003431\n",
            "Iteration 68, loss = 1.24004423\n",
            "Iteration 69, loss = 1.23938474\n",
            "Iteration 70, loss = 1.23979140\n",
            "Iteration 71, loss = 1.23926846\n",
            "Iteration 72, loss = 1.23889216\n",
            "Iteration 73, loss = 1.23856819\n",
            "Iteration 74, loss = 1.23882883\n",
            "Iteration 75, loss = 1.23896944\n",
            "Iteration 76, loss = 1.24038921\n",
            "Iteration 77, loss = 1.23972465\n",
            "Iteration 78, loss = 1.23831580\n",
            "Iteration 79, loss = 1.23939974\n",
            "Iteration 80, loss = 1.23957805\n",
            "Iteration 81, loss = 1.24023213\n",
            "Iteration 82, loss = 1.23983011\n",
            "Iteration 83, loss = 1.23813291\n",
            "Iteration 84, loss = 1.23893110\n",
            "Iteration 85, loss = 1.23968634\n",
            "Iteration 86, loss = 1.23936325\n",
            "Iteration 87, loss = 1.23807145\n",
            "Iteration 88, loss = 1.23815859\n",
            "Iteration 89, loss = 1.23937665\n",
            "Iteration 90, loss = 1.23934578\n",
            "Iteration 91, loss = 1.23808401\n",
            "Iteration 92, loss = 1.24000067\n",
            "Iteration 93, loss = 1.23798243\n",
            "Iteration 94, loss = 1.23819698\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32035981\n",
            "Iteration 2, loss = 1.27221701\n",
            "Iteration 3, loss = 1.25911594\n",
            "Iteration 4, loss = 1.25906991\n",
            "Iteration 5, loss = 1.25760050\n",
            "Iteration 6, loss = 1.25446539\n",
            "Iteration 7, loss = 1.25400055\n",
            "Iteration 8, loss = 1.25323853\n",
            "Iteration 9, loss = 1.25220907\n",
            "Iteration 10, loss = 1.25202438\n",
            "Iteration 11, loss = 1.25188072\n",
            "Iteration 12, loss = 1.25097335\n",
            "Iteration 13, loss = 1.24998960\n",
            "Iteration 14, loss = 1.25045293\n",
            "Iteration 15, loss = 1.24980475\n",
            "Iteration 16, loss = 1.24909134\n",
            "Iteration 17, loss = 1.24919571\n",
            "Iteration 18, loss = 1.24920443\n",
            "Iteration 19, loss = 1.24876404\n",
            "Iteration 20, loss = 1.24784245\n",
            "Iteration 21, loss = 1.24697708\n",
            "Iteration 22, loss = 1.24635463\n",
            "Iteration 23, loss = 1.24620580\n",
            "Iteration 24, loss = 1.24570011\n",
            "Iteration 25, loss = 1.24482166\n",
            "Iteration 26, loss = 1.24481995\n",
            "Iteration 27, loss = 1.24476143\n",
            "Iteration 28, loss = 1.24357856\n",
            "Iteration 29, loss = 1.24418035\n",
            "Iteration 30, loss = 1.24309292\n",
            "Iteration 31, loss = 1.24377957\n",
            "Iteration 32, loss = 1.24469871\n",
            "Iteration 33, loss = 1.24413639\n",
            "Iteration 34, loss = 1.24421997\n",
            "Iteration 35, loss = 1.24472322\n",
            "Iteration 36, loss = 1.24338312\n",
            "Iteration 37, loss = 1.24405289\n",
            "Iteration 38, loss = 1.24398749\n",
            "Iteration 39, loss = 1.24269374\n",
            "Iteration 40, loss = 1.24187225\n",
            "Iteration 41, loss = 1.24098045\n",
            "Iteration 42, loss = 1.24167904\n",
            "Iteration 43, loss = 1.24268271\n",
            "Iteration 44, loss = 1.24263612\n",
            "Iteration 45, loss = 1.24075195\n",
            "Iteration 46, loss = 1.24039535\n",
            "Iteration 47, loss = 1.24105763\n",
            "Iteration 48, loss = 1.24075181\n",
            "Iteration 49, loss = 1.24001689\n",
            "Iteration 50, loss = 1.24117751\n",
            "Iteration 51, loss = 1.24047182\n",
            "Iteration 52, loss = 1.24042445\n",
            "Iteration 53, loss = 1.24036994\n",
            "Iteration 54, loss = 1.24076318\n",
            "Iteration 55, loss = 1.24114794\n",
            "Iteration 56, loss = 1.23994141\n",
            "Iteration 57, loss = 1.23982867\n",
            "Iteration 58, loss = 1.24117536\n",
            "Iteration 59, loss = 1.24060105\n",
            "Iteration 60, loss = 1.23936335\n",
            "Iteration 61, loss = 1.24059583\n",
            "Iteration 62, loss = 1.24103111\n",
            "Iteration 63, loss = 1.23970462\n",
            "Iteration 64, loss = 1.24075059\n",
            "Iteration 65, loss = 1.23970371\n",
            "Iteration 66, loss = 1.24107169\n",
            "Iteration 67, loss = 1.24090758\n",
            "Iteration 68, loss = 1.24034940\n",
            "Iteration 69, loss = 1.23965859\n",
            "Iteration 70, loss = 1.23983345\n",
            "Iteration 71, loss = 1.24003563\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32718814\n",
            "Iteration 2, loss = 1.27834236\n",
            "Iteration 3, loss = 1.26495344\n",
            "Iteration 4, loss = 1.26434846\n",
            "Iteration 5, loss = 1.26447378\n",
            "Iteration 6, loss = 1.26190361\n",
            "Iteration 7, loss = 1.26192800\n",
            "Iteration 8, loss = 1.26139212\n",
            "Iteration 9, loss = 1.25875946\n",
            "Iteration 10, loss = 1.25770877\n",
            "Iteration 11, loss = 1.25733186\n",
            "Iteration 12, loss = 1.25729206\n",
            "Iteration 13, loss = 1.25637590\n",
            "Iteration 14, loss = 1.25575730\n",
            "Iteration 15, loss = 1.25578608\n",
            "Iteration 16, loss = 1.25529760\n",
            "Iteration 17, loss = 1.25439948\n",
            "Iteration 18, loss = 1.25496717\n",
            "Iteration 19, loss = 1.25479881\n",
            "Iteration 20, loss = 1.25400274\n",
            "Iteration 21, loss = 1.25272048\n",
            "Iteration 22, loss = 1.25223357\n",
            "Iteration 23, loss = 1.25233116\n",
            "Iteration 24, loss = 1.25164639\n",
            "Iteration 25, loss = 1.25051352\n",
            "Iteration 26, loss = 1.25026027\n",
            "Iteration 27, loss = 1.25108814\n",
            "Iteration 28, loss = 1.24953561\n",
            "Iteration 29, loss = 1.24984335\n",
            "Iteration 30, loss = 1.24824771\n",
            "Iteration 31, loss = 1.25054705\n",
            "Iteration 32, loss = 1.25158278\n",
            "Iteration 33, loss = 1.24915648\n",
            "Iteration 34, loss = 1.24949454\n",
            "Iteration 35, loss = 1.25144347\n",
            "Iteration 36, loss = 1.25114757\n",
            "Iteration 37, loss = 1.24995536\n",
            "Iteration 38, loss = 1.24832311\n",
            "Iteration 39, loss = 1.24764559\n",
            "Iteration 40, loss = 1.24746624\n",
            "Iteration 41, loss = 1.24722923\n",
            "Iteration 42, loss = 1.24662504\n",
            "Iteration 43, loss = 1.24701693\n",
            "Iteration 44, loss = 1.24723961\n",
            "Iteration 45, loss = 1.24566566\n",
            "Iteration 46, loss = 1.24637123\n",
            "Iteration 47, loss = 1.24658084\n",
            "Iteration 48, loss = 1.24567725\n",
            "Iteration 49, loss = 1.24547017\n",
            "Iteration 50, loss = 1.24599191\n",
            "Iteration 51, loss = 1.24537331\n",
            "Iteration 52, loss = 1.24556379\n",
            "Iteration 53, loss = 1.24706628\n",
            "Iteration 54, loss = 1.24589617\n",
            "Iteration 55, loss = 1.24593013\n",
            "Iteration 56, loss = 1.24560915\n",
            "Iteration 57, loss = 1.24591273\n",
            "Iteration 58, loss = 1.24672207\n",
            "Iteration 59, loss = 1.24600403\n",
            "Iteration 60, loss = 1.24465940\n",
            "Iteration 61, loss = 1.24538157\n",
            "Iteration 62, loss = 1.24590705\n",
            "Iteration 63, loss = 1.24498060\n",
            "Iteration 64, loss = 1.24508523\n",
            "Iteration 65, loss = 1.24398167\n",
            "Iteration 66, loss = 1.24588961\n",
            "Iteration 67, loss = 1.24585498\n",
            "Iteration 68, loss = 1.24580117\n",
            "Iteration 69, loss = 1.24526405\n",
            "Iteration 70, loss = 1.24497221\n",
            "Iteration 71, loss = 1.24447322\n",
            "Iteration 72, loss = 1.24391000\n",
            "Iteration 73, loss = 1.24378745\n",
            "Iteration 74, loss = 1.24381914\n",
            "Iteration 75, loss = 1.24427410\n",
            "Iteration 76, loss = 1.24557228\n",
            "Iteration 77, loss = 1.24511319\n",
            "Iteration 78, loss = 1.24367863\n",
            "Iteration 79, loss = 1.24400309\n",
            "Iteration 80, loss = 1.24461115\n",
            "Iteration 81, loss = 1.24569473\n",
            "Iteration 82, loss = 1.24546013\n",
            "Iteration 83, loss = 1.24393997\n",
            "Iteration 84, loss = 1.24471097\n",
            "Iteration 85, loss = 1.24522031\n",
            "Iteration 86, loss = 1.24475673\n",
            "Iteration 87, loss = 1.24336186\n",
            "Iteration 88, loss = 1.24341191\n",
            "Iteration 89, loss = 1.24347115\n",
            "Iteration 90, loss = 1.24322442\n",
            "Iteration 91, loss = 1.24317203\n",
            "Iteration 92, loss = 1.24355855\n",
            "Iteration 93, loss = 1.24281888\n",
            "Iteration 94, loss = 1.24260762\n",
            "Iteration 95, loss = 1.24328873\n",
            "Iteration 96, loss = 1.24359081\n",
            "Iteration 97, loss = 1.24314876\n",
            "Iteration 98, loss = 1.24235544\n",
            "Iteration 99, loss = 1.24295422\n",
            "Iteration 100, loss = 1.24394159\n",
            "Iteration 1, loss = 1.33019787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.28381582\n",
            "Iteration 3, loss = 1.26773404\n",
            "Iteration 4, loss = 1.26746485\n",
            "Iteration 5, loss = 1.26697916\n",
            "Iteration 6, loss = 1.26382769\n",
            "Iteration 7, loss = 1.26423328\n",
            "Iteration 8, loss = 1.26429604\n",
            "Iteration 9, loss = 1.26184050\n",
            "Iteration 10, loss = 1.26092609\n",
            "Iteration 11, loss = 1.26065198\n",
            "Iteration 12, loss = 1.26073836\n",
            "Iteration 13, loss = 1.25999468\n",
            "Iteration 14, loss = 1.25974500\n",
            "Iteration 15, loss = 1.25995109\n",
            "Iteration 16, loss = 1.25864272\n",
            "Iteration 17, loss = 1.25806203\n",
            "Iteration 18, loss = 1.25807443\n",
            "Iteration 19, loss = 1.25781629\n",
            "Iteration 20, loss = 1.25771961\n",
            "Iteration 21, loss = 1.25660249\n",
            "Iteration 22, loss = 1.25605175\n",
            "Iteration 23, loss = 1.25675674\n",
            "Iteration 24, loss = 1.25558295\n",
            "Iteration 25, loss = 1.25439149\n",
            "Iteration 26, loss = 1.25464441\n",
            "Iteration 27, loss = 1.25492160\n",
            "Iteration 28, loss = 1.25314915\n",
            "Iteration 29, loss = 1.25322515\n",
            "Iteration 30, loss = 1.25197158\n",
            "Iteration 31, loss = 1.25262623\n",
            "Iteration 32, loss = 1.25412742\n",
            "Iteration 33, loss = 1.25208974\n",
            "Iteration 34, loss = 1.25212347\n",
            "Iteration 35, loss = 1.25461266\n",
            "Iteration 36, loss = 1.25295201\n",
            "Iteration 37, loss = 1.25164336\n",
            "Iteration 38, loss = 1.25120890\n",
            "Iteration 39, loss = 1.25104500\n",
            "Iteration 40, loss = 1.25052268\n",
            "Iteration 41, loss = 1.25069155\n",
            "Iteration 42, loss = 1.24971152\n",
            "Iteration 43, loss = 1.25050295\n",
            "Iteration 44, loss = 1.25014051\n",
            "Iteration 45, loss = 1.24865825\n",
            "Iteration 46, loss = 1.24935384\n",
            "Iteration 47, loss = 1.24938983\n",
            "Iteration 48, loss = 1.24865160\n",
            "Iteration 49, loss = 1.24893856\n",
            "Iteration 50, loss = 1.24950452\n",
            "Iteration 51, loss = 1.24825711\n",
            "Iteration 52, loss = 1.24834710\n",
            "Iteration 53, loss = 1.24882336\n",
            "Iteration 54, loss = 1.24811626\n",
            "Iteration 55, loss = 1.24910288\n",
            "Iteration 56, loss = 1.24899850\n",
            "Iteration 57, loss = 1.24959416\n",
            "Iteration 58, loss = 1.25001762\n",
            "Iteration 59, loss = 1.24883793\n",
            "Iteration 60, loss = 1.24738287\n",
            "Iteration 61, loss = 1.24894227\n",
            "Iteration 62, loss = 1.24993275\n",
            "Iteration 63, loss = 1.24812250\n",
            "Iteration 64, loss = 1.24736681\n",
            "Iteration 65, loss = 1.24662352\n",
            "Iteration 66, loss = 1.24906417\n",
            "Iteration 67, loss = 1.24831872\n",
            "Iteration 68, loss = 1.24855820\n",
            "Iteration 69, loss = 1.24801013\n",
            "Iteration 70, loss = 1.24752358\n",
            "Iteration 71, loss = 1.24749807\n",
            "Iteration 72, loss = 1.24684748\n",
            "Iteration 73, loss = 1.24632660\n",
            "Iteration 74, loss = 1.24692351\n",
            "Iteration 75, loss = 1.24786765\n",
            "Iteration 76, loss = 1.24784355\n",
            "Iteration 77, loss = 1.24741211\n",
            "Iteration 78, loss = 1.24684737\n",
            "Iteration 79, loss = 1.24591885\n",
            "Iteration 80, loss = 1.24640043\n",
            "Iteration 81, loss = 1.24871704\n",
            "Iteration 82, loss = 1.24777172\n",
            "Iteration 83, loss = 1.24670280\n",
            "Iteration 84, loss = 1.24855751\n",
            "Iteration 85, loss = 1.24888156\n",
            "Iteration 86, loss = 1.24794614\n",
            "Iteration 87, loss = 1.24583303\n",
            "Iteration 88, loss = 1.24602075\n",
            "Iteration 89, loss = 1.24666040\n",
            "Iteration 90, loss = 1.24613968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32340315\n",
            "Iteration 2, loss = 1.27441972\n",
            "Iteration 3, loss = 1.25896690\n",
            "Iteration 4, loss = 1.25938716\n",
            "Iteration 5, loss = 1.25811502\n",
            "Iteration 6, loss = 1.25471814\n",
            "Iteration 7, loss = 1.25550938\n",
            "Iteration 8, loss = 1.25546401\n",
            "Iteration 9, loss = 1.25272120\n",
            "Iteration 10, loss = 1.25173288\n",
            "Iteration 11, loss = 1.25107875\n",
            "Iteration 12, loss = 1.25122667\n",
            "Iteration 13, loss = 1.25174156\n",
            "Iteration 14, loss = 1.25116092\n",
            "Iteration 15, loss = 1.25086788\n",
            "Iteration 16, loss = 1.24959582\n",
            "Iteration 17, loss = 1.24904331\n",
            "Iteration 18, loss = 1.24902641\n",
            "Iteration 19, loss = 1.24809310\n",
            "Iteration 20, loss = 1.24768045\n",
            "Iteration 21, loss = 1.24676484\n",
            "Iteration 22, loss = 1.24774734\n",
            "Iteration 23, loss = 1.24715426\n",
            "Iteration 24, loss = 1.24612582\n",
            "Iteration 25, loss = 1.24505881\n",
            "Iteration 26, loss = 1.24495528\n",
            "Iteration 27, loss = 1.24555853\n",
            "Iteration 28, loss = 1.24433423\n",
            "Iteration 29, loss = 1.24490967\n",
            "Iteration 30, loss = 1.24315208\n",
            "Iteration 31, loss = 1.24361228\n",
            "Iteration 32, loss = 1.24481272\n",
            "Iteration 33, loss = 1.24321486\n",
            "Iteration 34, loss = 1.24230949\n",
            "Iteration 35, loss = 1.24420355\n",
            "Iteration 36, loss = 1.24375752\n",
            "Iteration 37, loss = 1.24254626\n",
            "Iteration 38, loss = 1.24198285\n",
            "Iteration 39, loss = 1.24191589\n",
            "Iteration 40, loss = 1.24113987\n",
            "Iteration 41, loss = 1.24080863\n",
            "Iteration 42, loss = 1.24129286\n",
            "Iteration 43, loss = 1.24218970\n",
            "Iteration 44, loss = 1.24074179\n",
            "Iteration 45, loss = 1.23965125\n",
            "Iteration 46, loss = 1.24020525\n",
            "Iteration 47, loss = 1.24075242\n",
            "Iteration 48, loss = 1.23992691\n",
            "Iteration 49, loss = 1.24004884\n",
            "Iteration 50, loss = 1.24099017\n",
            "Iteration 51, loss = 1.23945032\n",
            "Iteration 52, loss = 1.23881709\n",
            "Iteration 53, loss = 1.23931826\n",
            "Iteration 54, loss = 1.23887582\n",
            "Iteration 55, loss = 1.23869737\n",
            "Iteration 56, loss = 1.23844872\n",
            "Iteration 57, loss = 1.23939012\n",
            "Iteration 58, loss = 1.24044494\n",
            "Iteration 59, loss = 1.23944419\n",
            "Iteration 60, loss = 1.23802399\n",
            "Iteration 61, loss = 1.23887591\n",
            "Iteration 62, loss = 1.23940579\n",
            "Iteration 63, loss = 1.23808815\n",
            "Iteration 64, loss = 1.23788384\n",
            "Iteration 65, loss = 1.23803342\n",
            "Iteration 66, loss = 1.23905937\n",
            "Iteration 67, loss = 1.23825300\n",
            "Iteration 68, loss = 1.23909753\n",
            "Iteration 69, loss = 1.23901726\n",
            "Iteration 70, loss = 1.23776952\n",
            "Iteration 71, loss = 1.23789657\n",
            "Iteration 72, loss = 1.23750366\n",
            "Iteration 73, loss = 1.23702734\n",
            "Iteration 74, loss = 1.23762035\n",
            "Iteration 75, loss = 1.23737877\n",
            "Iteration 76, loss = 1.23728811\n",
            "Iteration 77, loss = 1.23732243\n",
            "Iteration 78, loss = 1.23685370\n",
            "Iteration 79, loss = 1.23712977\n",
            "Iteration 80, loss = 1.23794628\n",
            "Iteration 81, loss = 1.23848864\n",
            "Iteration 82, loss = 1.23782617\n",
            "Iteration 83, loss = 1.23761857\n",
            "Iteration 84, loss = 1.23921104\n",
            "Iteration 85, loss = 1.23902355\n",
            "Iteration 86, loss = 1.23824773\n",
            "Iteration 87, loss = 1.23608859\n",
            "Iteration 88, loss = 1.23645326\n",
            "Iteration 89, loss = 1.23704255\n",
            "Iteration 90, loss = 1.23672922\n",
            "Iteration 91, loss = 1.23614737\n",
            "Iteration 92, loss = 1.23700386\n",
            "Iteration 93, loss = 1.23733311\n",
            "Iteration 94, loss = 1.23617761\n",
            "Iteration 95, loss = 1.23624543\n",
            "Iteration 96, loss = 1.23607676\n",
            "Iteration 97, loss = 1.23560629\n",
            "Iteration 98, loss = 1.23604239\n",
            "Iteration 99, loss = 1.23698318\n",
            "Iteration 100, loss = 1.23799629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32004071\n",
            "Iteration 2, loss = 1.26695845\n",
            "Iteration 3, loss = 1.24946881\n",
            "Iteration 4, loss = 1.24924939\n",
            "Iteration 5, loss = 1.24956036\n",
            "Iteration 6, loss = 1.24399963\n",
            "Iteration 7, loss = 1.24380984\n",
            "Iteration 8, loss = 1.24504478\n",
            "Iteration 9, loss = 1.24299730\n",
            "Iteration 10, loss = 1.24145964\n",
            "Iteration 11, loss = 1.24016253\n",
            "Iteration 12, loss = 1.23994307\n",
            "Iteration 13, loss = 1.24012265\n",
            "Iteration 14, loss = 1.24008532\n",
            "Iteration 15, loss = 1.23921575\n",
            "Iteration 16, loss = 1.23800118\n",
            "Iteration 17, loss = 1.23719146\n",
            "Iteration 18, loss = 1.23704284\n",
            "Iteration 19, loss = 1.23577832\n",
            "Iteration 20, loss = 1.23599441\n",
            "Iteration 21, loss = 1.23495291\n",
            "Iteration 22, loss = 1.23539186\n",
            "Iteration 23, loss = 1.23485462\n",
            "Iteration 24, loss = 1.23379531\n",
            "Iteration 25, loss = 1.23335518\n",
            "Iteration 26, loss = 1.23318273\n",
            "Iteration 27, loss = 1.23372515\n",
            "Iteration 28, loss = 1.23204798\n",
            "Iteration 29, loss = 1.23394098\n",
            "Iteration 30, loss = 1.23231392\n",
            "Iteration 31, loss = 1.23195351\n",
            "Iteration 32, loss = 1.23271630\n",
            "Iteration 33, loss = 1.23097546\n",
            "Iteration 34, loss = 1.23032097\n",
            "Iteration 35, loss = 1.23255577\n",
            "Iteration 36, loss = 1.23141617\n",
            "Iteration 37, loss = 1.23010352\n",
            "Iteration 38, loss = 1.22950542\n",
            "Iteration 39, loss = 1.23030010\n",
            "Iteration 40, loss = 1.23036784\n",
            "Iteration 41, loss = 1.22931966\n",
            "Iteration 42, loss = 1.22883309\n",
            "Iteration 43, loss = 1.23061064\n",
            "Iteration 44, loss = 1.23014236\n",
            "Iteration 45, loss = 1.22863458\n",
            "Iteration 46, loss = 1.22823048\n",
            "Iteration 47, loss = 1.22928715\n",
            "Iteration 48, loss = 1.22882776\n",
            "Iteration 49, loss = 1.22939485\n",
            "Iteration 50, loss = 1.22957189\n",
            "Iteration 51, loss = 1.22775803\n",
            "Iteration 52, loss = 1.22803637\n",
            "Iteration 53, loss = 1.22808597\n",
            "Iteration 54, loss = 1.22793382\n",
            "Iteration 55, loss = 1.22784840\n",
            "Iteration 56, loss = 1.22732599\n",
            "Iteration 57, loss = 1.22746537\n",
            "Iteration 58, loss = 1.22855357\n",
            "Iteration 59, loss = 1.22817723\n",
            "Iteration 60, loss = 1.22718470\n",
            "Iteration 61, loss = 1.22781508\n",
            "Iteration 62, loss = 1.22840472\n",
            "Iteration 63, loss = 1.22706369\n",
            "Iteration 64, loss = 1.22696188\n",
            "Iteration 65, loss = 1.22675858\n",
            "Iteration 66, loss = 1.22737298\n",
            "Iteration 67, loss = 1.22759788\n",
            "Iteration 68, loss = 1.22770361\n",
            "Iteration 69, loss = 1.22765686\n",
            "Iteration 70, loss = 1.22709411\n",
            "Iteration 71, loss = 1.22658392\n",
            "Iteration 72, loss = 1.22649169\n",
            "Iteration 73, loss = 1.22631747\n",
            "Iteration 74, loss = 1.22694044\n",
            "Iteration 75, loss = 1.22636079\n",
            "Iteration 76, loss = 1.22619241\n",
            "Iteration 77, loss = 1.22613117\n",
            "Iteration 78, loss = 1.22613947\n",
            "Iteration 79, loss = 1.22637433\n",
            "Iteration 80, loss = 1.22645351\n",
            "Iteration 81, loss = 1.22708617\n",
            "Iteration 82, loss = 1.22666335\n",
            "Iteration 83, loss = 1.22609990\n",
            "Iteration 84, loss = 1.22758872\n",
            "Iteration 85, loss = 1.22764806\n",
            "Iteration 86, loss = 1.22717630\n",
            "Iteration 87, loss = 1.22611107\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32694007\n",
            "Iteration 2, loss = 1.27478111\n",
            "Iteration 3, loss = 1.25719520\n",
            "Iteration 4, loss = 1.25430982\n",
            "Iteration 5, loss = 1.25327799\n",
            "Iteration 6, loss = 1.24930899\n",
            "Iteration 7, loss = 1.24906391\n",
            "Iteration 8, loss = 1.24985816\n",
            "Iteration 9, loss = 1.24799265\n",
            "Iteration 10, loss = 1.24679744\n",
            "Iteration 11, loss = 1.24560584\n",
            "Iteration 12, loss = 1.24516536\n",
            "Iteration 13, loss = 1.24526228\n",
            "Iteration 14, loss = 1.24557714\n",
            "Iteration 15, loss = 1.24465090\n",
            "Iteration 16, loss = 1.24332274\n",
            "Iteration 17, loss = 1.24266231\n",
            "Iteration 18, loss = 1.24234517\n",
            "Iteration 19, loss = 1.24214810\n",
            "Iteration 20, loss = 1.24314290\n",
            "Iteration 21, loss = 1.24173159\n",
            "Iteration 22, loss = 1.24279132\n",
            "Iteration 23, loss = 1.24187612\n",
            "Iteration 24, loss = 1.24032885\n",
            "Iteration 25, loss = 1.23973307\n",
            "Iteration 26, loss = 1.24028784\n",
            "Iteration 27, loss = 1.24145878\n",
            "Iteration 28, loss = 1.23908712\n",
            "Iteration 29, loss = 1.24229821\n",
            "Iteration 30, loss = 1.23918293\n",
            "Iteration 31, loss = 1.23884692\n",
            "Iteration 32, loss = 1.24029645\n",
            "Iteration 33, loss = 1.23762139\n",
            "Iteration 34, loss = 1.23775848\n",
            "Iteration 35, loss = 1.24010953\n",
            "Iteration 36, loss = 1.23750819\n",
            "Iteration 37, loss = 1.23689762\n",
            "Iteration 38, loss = 1.23678615\n",
            "Iteration 39, loss = 1.23655211\n",
            "Iteration 40, loss = 1.23624053\n",
            "Iteration 41, loss = 1.23597375\n",
            "Iteration 42, loss = 1.23562678\n",
            "Iteration 43, loss = 1.23768003\n",
            "Iteration 44, loss = 1.23658229\n",
            "Iteration 45, loss = 1.23525768\n",
            "Iteration 46, loss = 1.23524019\n",
            "Iteration 47, loss = 1.23513662\n",
            "Iteration 48, loss = 1.23509779\n",
            "Iteration 49, loss = 1.23683433\n",
            "Iteration 50, loss = 1.23641489\n",
            "Iteration 51, loss = 1.23464409\n",
            "Iteration 52, loss = 1.23487365\n",
            "Iteration 53, loss = 1.23474963\n",
            "Iteration 54, loss = 1.23478163\n",
            "Iteration 55, loss = 1.23499149\n",
            "Iteration 56, loss = 1.23409145\n",
            "Iteration 57, loss = 1.23371929\n",
            "Iteration 58, loss = 1.23515942\n",
            "Iteration 59, loss = 1.23543941\n",
            "Iteration 60, loss = 1.23438355\n",
            "Iteration 61, loss = 1.23526917\n",
            "Iteration 62, loss = 1.23531009\n",
            "Iteration 63, loss = 1.23404055\n",
            "Iteration 64, loss = 1.23371281\n",
            "Iteration 65, loss = 1.23410064\n",
            "Iteration 66, loss = 1.23510047\n",
            "Iteration 67, loss = 1.23385031\n",
            "Iteration 68, loss = 1.23599920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32594135\n",
            "Iteration 2, loss = 1.27528147\n",
            "Iteration 3, loss = 1.26048084\n",
            "Iteration 4, loss = 1.25247816\n",
            "Iteration 5, loss = 1.24999083\n",
            "Iteration 6, loss = 1.24937512\n",
            "Iteration 7, loss = 1.24854213\n",
            "Iteration 8, loss = 1.24645811\n",
            "Iteration 9, loss = 1.24831614\n",
            "Iteration 10, loss = 1.24760053\n",
            "Iteration 11, loss = 1.24464635\n",
            "Iteration 12, loss = 1.24544729\n",
            "Iteration 13, loss = 1.24484504\n",
            "Iteration 14, loss = 1.24323532\n",
            "Iteration 15, loss = 1.24225608\n",
            "Iteration 16, loss = 1.24216727\n",
            "Iteration 17, loss = 1.24250611\n",
            "Iteration 18, loss = 1.24127535\n",
            "Iteration 19, loss = 1.24018666\n",
            "Iteration 20, loss = 1.24002831\n",
            "Iteration 21, loss = 1.24000083\n",
            "Iteration 22, loss = 1.23960106\n",
            "Iteration 23, loss = 1.23826514\n",
            "Iteration 24, loss = 1.23783922\n",
            "Iteration 25, loss = 1.23720848\n",
            "Iteration 26, loss = 1.23633577\n",
            "Iteration 27, loss = 1.23666996\n",
            "Iteration 28, loss = 1.23746935\n",
            "Iteration 29, loss = 1.23602525\n",
            "Iteration 30, loss = 1.23554662\n",
            "Iteration 31, loss = 1.23604005\n",
            "Iteration 32, loss = 1.23425723\n",
            "Iteration 33, loss = 1.23414926\n",
            "Iteration 34, loss = 1.23420355\n",
            "Iteration 35, loss = 1.23454140\n",
            "Iteration 36, loss = 1.23388297\n",
            "Iteration 37, loss = 1.23334303\n",
            "Iteration 38, loss = 1.23373645\n",
            "Iteration 39, loss = 1.23345188\n",
            "Iteration 40, loss = 1.23241906\n",
            "Iteration 41, loss = 1.23333681\n",
            "Iteration 42, loss = 1.23277796\n",
            "Iteration 43, loss = 1.23249626\n",
            "Iteration 44, loss = 1.23221843\n",
            "Iteration 45, loss = 1.23155370\n",
            "Iteration 46, loss = 1.23175452\n",
            "Iteration 47, loss = 1.23129139\n",
            "Iteration 48, loss = 1.23186732\n",
            "Iteration 49, loss = 1.23134979\n",
            "Iteration 50, loss = 1.23106348\n",
            "Iteration 51, loss = 1.23263452\n",
            "Iteration 52, loss = 1.23036226\n",
            "Iteration 53, loss = 1.23133320\n",
            "Iteration 54, loss = 1.23163847\n",
            "Iteration 55, loss = 1.23148627\n",
            "Iteration 56, loss = 1.23135775\n",
            "Iteration 57, loss = 1.23034849\n",
            "Iteration 58, loss = 1.23160594\n",
            "Iteration 59, loss = 1.23122638\n",
            "Iteration 60, loss = 1.23067835\n",
            "Iteration 61, loss = 1.23150222\n",
            "Iteration 62, loss = 1.22992410\n",
            "Iteration 63, loss = 1.23378420\n",
            "Iteration 64, loss = 1.23214360\n",
            "Iteration 65, loss = 1.23014413\n",
            "Iteration 66, loss = 1.22988504\n",
            "Iteration 67, loss = 1.22993097\n",
            "Iteration 68, loss = 1.23136632\n",
            "Iteration 69, loss = 1.22978113\n",
            "Iteration 70, loss = 1.23060055\n",
            "Iteration 71, loss = 1.23015042\n",
            "Iteration 72, loss = 1.22998461\n",
            "Iteration 73, loss = 1.23049063\n",
            "Iteration 74, loss = 1.22963816\n",
            "Iteration 75, loss = 1.22961577\n",
            "Iteration 76, loss = 1.22941355\n",
            "Iteration 77, loss = 1.23027661\n",
            "Iteration 78, loss = 1.22989482\n",
            "Iteration 79, loss = 1.22966621\n",
            "Iteration 80, loss = 1.22931208\n",
            "Iteration 81, loss = 1.22952920\n",
            "Iteration 82, loss = 1.22967380\n",
            "Iteration 83, loss = 1.22865528\n",
            "Iteration 84, loss = 1.22918576\n",
            "Iteration 85, loss = 1.23043177\n",
            "Iteration 86, loss = 1.23047263\n",
            "Iteration 87, loss = 1.23031283\n",
            "Iteration 88, loss = 1.22919390\n",
            "Iteration 89, loss = 1.22840480\n",
            "Iteration 90, loss = 1.22818818\n",
            "Iteration 91, loss = 1.22868960\n",
            "Iteration 92, loss = 1.22855054\n",
            "Iteration 93, loss = 1.22935600\n",
            "Iteration 94, loss = 1.22833426\n",
            "Iteration 95, loss = 1.23013390\n",
            "Iteration 96, loss = 1.23120470\n",
            "Iteration 97, loss = 1.22936873\n",
            "Iteration 98, loss = 1.22825711\n",
            "Iteration 99, loss = 1.22814759\n",
            "Iteration 100, loss = 1.22778264\n",
            "Iteration 1, loss = 1.32362459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.27246863\n",
            "Iteration 3, loss = 1.25712053\n",
            "Iteration 4, loss = 1.24895629\n",
            "Iteration 5, loss = 1.24482329\n",
            "Iteration 6, loss = 1.24376847\n",
            "Iteration 7, loss = 1.24364590\n",
            "Iteration 8, loss = 1.24261822\n",
            "Iteration 9, loss = 1.24398090\n",
            "Iteration 10, loss = 1.24298851\n",
            "Iteration 11, loss = 1.24048759\n",
            "Iteration 12, loss = 1.24184038\n",
            "Iteration 13, loss = 1.24102725\n",
            "Iteration 14, loss = 1.23951958\n",
            "Iteration 15, loss = 1.23847226\n",
            "Iteration 16, loss = 1.23905000\n",
            "Iteration 17, loss = 1.23919270\n",
            "Iteration 18, loss = 1.23803997\n",
            "Iteration 19, loss = 1.23718560\n",
            "Iteration 20, loss = 1.23765910\n",
            "Iteration 21, loss = 1.23694400\n",
            "Iteration 22, loss = 1.23604606\n",
            "Iteration 23, loss = 1.23484615\n",
            "Iteration 24, loss = 1.23538966\n",
            "Iteration 25, loss = 1.23458011\n",
            "Iteration 26, loss = 1.23378339\n",
            "Iteration 27, loss = 1.23436053\n",
            "Iteration 28, loss = 1.23546926\n",
            "Iteration 29, loss = 1.23408863\n",
            "Iteration 30, loss = 1.23324840\n",
            "Iteration 31, loss = 1.23340440\n",
            "Iteration 32, loss = 1.23264740\n",
            "Iteration 33, loss = 1.23176974\n",
            "Iteration 34, loss = 1.23127444\n",
            "Iteration 35, loss = 1.23200693\n",
            "Iteration 36, loss = 1.23130605\n",
            "Iteration 37, loss = 1.23041787\n",
            "Iteration 38, loss = 1.23142997\n",
            "Iteration 39, loss = 1.23108226\n",
            "Iteration 40, loss = 1.23004067\n",
            "Iteration 41, loss = 1.23111627\n",
            "Iteration 42, loss = 1.23035079\n",
            "Iteration 43, loss = 1.22960683\n",
            "Iteration 44, loss = 1.22971788\n",
            "Iteration 45, loss = 1.22956265\n",
            "Iteration 46, loss = 1.22903755\n",
            "Iteration 47, loss = 1.22861481\n",
            "Iteration 48, loss = 1.23007441\n",
            "Iteration 49, loss = 1.22930940\n",
            "Iteration 50, loss = 1.22958030\n",
            "Iteration 51, loss = 1.22996698\n",
            "Iteration 52, loss = 1.22798313\n",
            "Iteration 53, loss = 1.22931816\n",
            "Iteration 54, loss = 1.22951859\n",
            "Iteration 55, loss = 1.22909502\n",
            "Iteration 56, loss = 1.22865718\n",
            "Iteration 57, loss = 1.22804936\n",
            "Iteration 58, loss = 1.22954781\n",
            "Iteration 59, loss = 1.22911444\n",
            "Iteration 60, loss = 1.22851263\n",
            "Iteration 61, loss = 1.22870612\n",
            "Iteration 62, loss = 1.22811000\n",
            "Iteration 63, loss = 1.23111715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32331779\n",
            "Iteration 2, loss = 1.26619098\n",
            "Iteration 3, loss = 1.24723430\n",
            "Iteration 4, loss = 1.23729051\n",
            "Iteration 5, loss = 1.23260675\n",
            "Iteration 6, loss = 1.23151592\n",
            "Iteration 7, loss = 1.23112677\n",
            "Iteration 8, loss = 1.23018788\n",
            "Iteration 9, loss = 1.23181537\n",
            "Iteration 10, loss = 1.22922426\n",
            "Iteration 11, loss = 1.22775725\n",
            "Iteration 12, loss = 1.22910595\n",
            "Iteration 13, loss = 1.22789936\n",
            "Iteration 14, loss = 1.22696176\n",
            "Iteration 15, loss = 1.22579647\n",
            "Iteration 16, loss = 1.22578810\n",
            "Iteration 17, loss = 1.22607942\n",
            "Iteration 18, loss = 1.22569185\n",
            "Iteration 19, loss = 1.22474219\n",
            "Iteration 20, loss = 1.22538853\n",
            "Iteration 21, loss = 1.22439569\n",
            "Iteration 22, loss = 1.22312489\n",
            "Iteration 23, loss = 1.22306691\n",
            "Iteration 24, loss = 1.22393595\n",
            "Iteration 25, loss = 1.22298693\n",
            "Iteration 26, loss = 1.22281181\n",
            "Iteration 27, loss = 1.22202520\n",
            "Iteration 28, loss = 1.22352795\n",
            "Iteration 29, loss = 1.22269056\n",
            "Iteration 30, loss = 1.22197631\n",
            "Iteration 31, loss = 1.22190135\n",
            "Iteration 32, loss = 1.22060533\n",
            "Iteration 33, loss = 1.22002323\n",
            "Iteration 34, loss = 1.21989020\n",
            "Iteration 35, loss = 1.22078280\n",
            "Iteration 36, loss = 1.22087000\n",
            "Iteration 37, loss = 1.21991037\n",
            "Iteration 38, loss = 1.22041998\n",
            "Iteration 39, loss = 1.21941000\n",
            "Iteration 40, loss = 1.21886629\n",
            "Iteration 41, loss = 1.21917039\n",
            "Iteration 42, loss = 1.21868136\n",
            "Iteration 43, loss = 1.21852834\n",
            "Iteration 44, loss = 1.21852722\n",
            "Iteration 45, loss = 1.21844499\n",
            "Iteration 46, loss = 1.21762838\n",
            "Iteration 47, loss = 1.21742783\n",
            "Iteration 48, loss = 1.21814134\n",
            "Iteration 49, loss = 1.21746292\n",
            "Iteration 50, loss = 1.21830738\n",
            "Iteration 51, loss = 1.22000703\n",
            "Iteration 52, loss = 1.21668709\n",
            "Iteration 53, loss = 1.21915943\n",
            "Iteration 54, loss = 1.21943781\n",
            "Iteration 55, loss = 1.21778718\n",
            "Iteration 56, loss = 1.21752966\n",
            "Iteration 57, loss = 1.21723410\n",
            "Iteration 58, loss = 1.21851126\n",
            "Iteration 59, loss = 1.21814646\n",
            "Iteration 60, loss = 1.21724344\n",
            "Iteration 61, loss = 1.21658266\n",
            "Iteration 62, loss = 1.21720129\n",
            "Iteration 63, loss = 1.21927213\n",
            "Iteration 64, loss = 1.21749816\n",
            "Iteration 65, loss = 1.21683188\n",
            "Iteration 66, loss = 1.21650292\n",
            "Iteration 67, loss = 1.21627962\n",
            "Iteration 68, loss = 1.21700975\n",
            "Iteration 69, loss = 1.21665669\n",
            "Iteration 70, loss = 1.21897602\n",
            "Iteration 71, loss = 1.21629297\n",
            "Iteration 72, loss = 1.21667171\n",
            "Iteration 73, loss = 1.21763805\n",
            "Iteration 74, loss = 1.21634142\n",
            "Iteration 75, loss = 1.21617893\n",
            "Iteration 76, loss = 1.21624685\n",
            "Iteration 77, loss = 1.21673224\n",
            "Iteration 78, loss = 1.21770290\n",
            "Iteration 79, loss = 1.21728386\n",
            "Iteration 80, loss = 1.21639667\n",
            "Iteration 81, loss = 1.21635097\n",
            "Iteration 82, loss = 1.21662358\n",
            "Iteration 83, loss = 1.21669540\n",
            "Iteration 84, loss = 1.21519548\n",
            "Iteration 85, loss = 1.21773558\n",
            "Iteration 86, loss = 1.21776962\n",
            "Iteration 87, loss = 1.21803543\n",
            "Iteration 88, loss = 1.21688138\n",
            "Iteration 89, loss = 1.21535080\n",
            "Iteration 90, loss = 1.21628703\n",
            "Iteration 91, loss = 1.21644693\n",
            "Iteration 92, loss = 1.21571655\n",
            "Iteration 93, loss = 1.21611832\n",
            "Iteration 94, loss = 1.21569513\n",
            "Iteration 95, loss = 1.21680953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "20\n",
            "Iteration 1, loss = 1.35664583\n",
            "Iteration 2, loss = 1.30013731\n",
            "Iteration 3, loss = 1.27166867\n",
            "Iteration 4, loss = 1.26042945\n",
            "Iteration 5, loss = 1.25446629\n",
            "Iteration 6, loss = 1.24815084\n",
            "Iteration 7, loss = 1.24556719\n",
            "Iteration 8, loss = 1.23903407\n",
            "Iteration 9, loss = 1.23469899\n",
            "Iteration 10, loss = 1.23042320\n",
            "Iteration 11, loss = 1.22144547\n",
            "Iteration 12, loss = 1.21390162\n",
            "Iteration 13, loss = 1.20840701\n",
            "Iteration 14, loss = 1.19800000\n",
            "Iteration 15, loss = 1.18681022\n",
            "Iteration 16, loss = 1.17484122\n",
            "Iteration 17, loss = 1.16613698\n",
            "Iteration 18, loss = 1.15369067\n",
            "Iteration 19, loss = 1.14552012\n",
            "Iteration 20, loss = 1.13404348\n",
            "Iteration 21, loss = 1.13244131\n",
            "Iteration 22, loss = 1.11844318\n",
            "Iteration 23, loss = 1.11065204\n",
            "Iteration 24, loss = 1.11117491\n",
            "Iteration 25, loss = 1.10145267\n",
            "Iteration 26, loss = 1.09674255\n",
            "Iteration 27, loss = 1.09778643\n",
            "Iteration 28, loss = 1.10756509\n",
            "Iteration 29, loss = 1.09421558\n",
            "Iteration 30, loss = 1.08964111\n",
            "Iteration 31, loss = 1.08530265\n",
            "Iteration 32, loss = 1.08952599\n",
            "Iteration 33, loss = 1.08183493\n",
            "Iteration 34, loss = 1.07858611\n",
            "Iteration 35, loss = 1.07650901\n",
            "Iteration 36, loss = 1.08287018\n",
            "Iteration 37, loss = 1.07640013\n",
            "Iteration 38, loss = 1.07029088\n",
            "Iteration 39, loss = 1.07864542\n",
            "Iteration 40, loss = 1.07014582\n",
            "Iteration 41, loss = 1.07217276\n",
            "Iteration 42, loss = 1.06556635\n",
            "Iteration 43, loss = 1.06408031\n",
            "Iteration 44, loss = 1.06250682\n",
            "Iteration 45, loss = 1.06219922\n",
            "Iteration 46, loss = 1.06296215\n",
            "Iteration 47, loss = 1.05812247\n",
            "Iteration 48, loss = 1.06323747\n",
            "Iteration 49, loss = 1.06042515\n",
            "Iteration 50, loss = 1.06224344\n",
            "Iteration 51, loss = 1.06352938\n",
            "Iteration 52, loss = 1.05807780\n",
            "Iteration 53, loss = 1.05648713\n",
            "Iteration 54, loss = 1.05689330\n",
            "Iteration 55, loss = 1.05994027\n",
            "Iteration 56, loss = 1.05902455\n",
            "Iteration 57, loss = 1.05308338\n",
            "Iteration 58, loss = 1.05810545\n",
            "Iteration 59, loss = 1.05385449\n",
            "Iteration 60, loss = 1.05454383\n",
            "Iteration 61, loss = 1.06322316\n",
            "Iteration 62, loss = 1.05235911\n",
            "Iteration 63, loss = 1.05417240\n",
            "Iteration 64, loss = 1.05927283\n",
            "Iteration 65, loss = 1.04945256\n",
            "Iteration 66, loss = 1.05142471\n",
            "Iteration 67, loss = 1.04883576\n",
            "Iteration 68, loss = 1.04811320\n",
            "Iteration 69, loss = 1.04913442\n",
            "Iteration 70, loss = 1.05446043\n",
            "Iteration 71, loss = 1.04628302\n",
            "Iteration 72, loss = 1.05048547\n",
            "Iteration 73, loss = 1.04375077\n",
            "Iteration 74, loss = 1.04409553\n",
            "Iteration 75, loss = 1.05018992\n",
            "Iteration 76, loss = 1.05497887\n",
            "Iteration 77, loss = 1.05054706\n",
            "Iteration 78, loss = 1.04694343\n",
            "Iteration 79, loss = 1.04483508\n",
            "Iteration 80, loss = 1.04251481\n",
            "Iteration 81, loss = 1.04280099\n",
            "Iteration 82, loss = 1.04218053\n",
            "Iteration 83, loss = 1.04139917\n",
            "Iteration 84, loss = 1.04127773\n",
            "Iteration 85, loss = 1.03974667\n",
            "Iteration 86, loss = 1.04114229\n",
            "Iteration 87, loss = 1.03803316\n",
            "Iteration 88, loss = 1.03740752\n",
            "Iteration 89, loss = 1.03901800\n",
            "Iteration 90, loss = 1.04278805\n",
            "Iteration 91, loss = 1.04155546\n",
            "Iteration 92, loss = 1.04582366\n",
            "Iteration 93, loss = 1.03655379\n",
            "Iteration 94, loss = 1.04059083\n",
            "Iteration 95, loss = 1.04157608\n",
            "Iteration 96, loss = 1.03914237\n",
            "Iteration 97, loss = 1.03607851\n",
            "Iteration 98, loss = 1.03313288\n",
            "Iteration 99, loss = 1.03543166\n",
            "Iteration 100, loss = 1.03315971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35242788\n",
            "Iteration 2, loss = 1.29277678\n",
            "Iteration 3, loss = 1.26624967\n",
            "Iteration 4, loss = 1.25723116\n",
            "Iteration 5, loss = 1.24970323\n",
            "Iteration 6, loss = 1.24569518\n",
            "Iteration 7, loss = 1.24302026\n",
            "Iteration 8, loss = 1.23534548\n",
            "Iteration 9, loss = 1.23082250\n",
            "Iteration 10, loss = 1.22768983\n",
            "Iteration 11, loss = 1.21906338\n",
            "Iteration 12, loss = 1.21150954\n",
            "Iteration 13, loss = 1.20704380\n",
            "Iteration 14, loss = 1.19507059\n",
            "Iteration 15, loss = 1.18454583\n",
            "Iteration 16, loss = 1.17211621\n",
            "Iteration 17, loss = 1.16253741\n",
            "Iteration 18, loss = 1.15210382\n",
            "Iteration 19, loss = 1.14154010\n",
            "Iteration 20, loss = 1.13650642\n",
            "Iteration 21, loss = 1.12144433\n",
            "Iteration 22, loss = 1.11852735\n",
            "Iteration 23, loss = 1.10882100\n",
            "Iteration 24, loss = 1.10620449\n",
            "Iteration 25, loss = 1.09589134\n",
            "Iteration 26, loss = 1.09138224\n",
            "Iteration 27, loss = 1.08700127\n",
            "Iteration 28, loss = 1.09113243\n",
            "Iteration 29, loss = 1.08491991\n",
            "Iteration 30, loss = 1.08133190\n",
            "Iteration 31, loss = 1.07893896\n",
            "Iteration 32, loss = 1.08241747\n",
            "Iteration 33, loss = 1.07347402\n",
            "Iteration 34, loss = 1.07110615\n",
            "Iteration 35, loss = 1.06961824\n",
            "Iteration 36, loss = 1.07816788\n",
            "Iteration 37, loss = 1.06920789\n",
            "Iteration 38, loss = 1.06000456\n",
            "Iteration 39, loss = 1.06737130\n",
            "Iteration 40, loss = 1.06097182\n",
            "Iteration 41, loss = 1.06241838\n",
            "Iteration 42, loss = 1.05286724\n",
            "Iteration 43, loss = 1.05242263\n",
            "Iteration 44, loss = 1.04925788\n",
            "Iteration 45, loss = 1.05005961\n",
            "Iteration 46, loss = 1.04758520\n",
            "Iteration 47, loss = 1.04521014\n",
            "Iteration 48, loss = 1.04433747\n",
            "Iteration 49, loss = 1.04341367\n",
            "Iteration 50, loss = 1.04606320\n",
            "Iteration 51, loss = 1.05219085\n",
            "Iteration 52, loss = 1.04495346\n",
            "Iteration 53, loss = 1.04134736\n",
            "Iteration 54, loss = 1.04423679\n",
            "Iteration 55, loss = 1.04476943\n",
            "Iteration 56, loss = 1.04797897\n",
            "Iteration 57, loss = 1.04205830\n",
            "Iteration 58, loss = 1.04102120\n",
            "Iteration 59, loss = 1.04685782\n",
            "Iteration 60, loss = 1.03563466\n",
            "Iteration 61, loss = 1.04315758\n",
            "Iteration 62, loss = 1.03671207\n",
            "Iteration 63, loss = 1.03878864\n",
            "Iteration 64, loss = 1.04056057\n",
            "Iteration 65, loss = 1.03827496\n",
            "Iteration 66, loss = 1.03782226\n",
            "Iteration 67, loss = 1.03589969\n",
            "Iteration 68, loss = 1.03397157\n",
            "Iteration 69, loss = 1.03687827\n",
            "Iteration 70, loss = 1.03748782\n",
            "Iteration 71, loss = 1.03431477\n",
            "Iteration 72, loss = 1.03317075\n",
            "Iteration 73, loss = 1.03011501\n",
            "Iteration 74, loss = 1.03059380\n",
            "Iteration 75, loss = 1.03378076\n",
            "Iteration 76, loss = 1.04017236\n",
            "Iteration 77, loss = 1.04195825\n",
            "Iteration 78, loss = 1.03413257\n",
            "Iteration 79, loss = 1.03263004\n",
            "Iteration 80, loss = 1.02993618\n",
            "Iteration 81, loss = 1.02720117\n",
            "Iteration 82, loss = 1.02974038\n",
            "Iteration 83, loss = 1.02963016\n",
            "Iteration 84, loss = 1.03143657\n",
            "Iteration 85, loss = 1.02847285\n",
            "Iteration 86, loss = 1.02948591\n",
            "Iteration 87, loss = 1.02474691\n",
            "Iteration 88, loss = 1.02495463\n",
            "Iteration 89, loss = 1.02629269\n",
            "Iteration 90, loss = 1.02670777\n",
            "Iteration 91, loss = 1.03213446\n",
            "Iteration 92, loss = 1.03282936\n",
            "Iteration 93, loss = 1.02358584\n",
            "Iteration 94, loss = 1.03106615\n",
            "Iteration 95, loss = 1.03185115\n",
            "Iteration 96, loss = 1.02775826\n",
            "Iteration 97, loss = 1.02385507\n",
            "Iteration 98, loss = 1.02126706\n",
            "Iteration 99, loss = 1.02388942\n",
            "Iteration 100, loss = 1.02024622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36058225\n",
            "Iteration 2, loss = 1.29920248\n",
            "Iteration 3, loss = 1.27116687\n",
            "Iteration 4, loss = 1.26451551\n",
            "Iteration 5, loss = 1.25679250\n",
            "Iteration 6, loss = 1.25151196\n",
            "Iteration 7, loss = 1.24823096\n",
            "Iteration 8, loss = 1.24097881\n",
            "Iteration 9, loss = 1.23642546\n",
            "Iteration 10, loss = 1.23121396\n",
            "Iteration 11, loss = 1.22341703\n",
            "Iteration 12, loss = 1.21597436\n",
            "Iteration 13, loss = 1.20785850\n",
            "Iteration 14, loss = 1.19533838\n",
            "Iteration 15, loss = 1.18829113\n",
            "Iteration 16, loss = 1.17522360\n",
            "Iteration 17, loss = 1.16820434\n",
            "Iteration 18, loss = 1.16164296\n",
            "Iteration 19, loss = 1.14869479\n",
            "Iteration 20, loss = 1.14401329\n",
            "Iteration 21, loss = 1.12936224\n",
            "Iteration 22, loss = 1.12721496\n",
            "Iteration 23, loss = 1.11920116\n",
            "Iteration 24, loss = 1.11747535\n",
            "Iteration 25, loss = 1.10910354\n",
            "Iteration 26, loss = 1.10281842\n",
            "Iteration 27, loss = 1.10076101\n",
            "Iteration 28, loss = 1.10738252\n",
            "Iteration 29, loss = 1.09763814\n",
            "Iteration 30, loss = 1.09421535\n",
            "Iteration 31, loss = 1.08936529\n",
            "Iteration 32, loss = 1.09079913\n",
            "Iteration 33, loss = 1.08371495\n",
            "Iteration 34, loss = 1.08016844\n",
            "Iteration 35, loss = 1.08363361\n",
            "Iteration 36, loss = 1.09478482\n",
            "Iteration 37, loss = 1.08888925\n",
            "Iteration 38, loss = 1.07486752\n",
            "Iteration 39, loss = 1.08155968\n",
            "Iteration 40, loss = 1.07148686\n",
            "Iteration 41, loss = 1.06674753\n",
            "Iteration 42, loss = 1.06503200\n",
            "Iteration 43, loss = 1.06128954\n",
            "Iteration 44, loss = 1.06036576\n",
            "Iteration 45, loss = 1.05922694\n",
            "Iteration 46, loss = 1.05886881\n",
            "Iteration 47, loss = 1.05586045\n",
            "Iteration 48, loss = 1.05624017\n",
            "Iteration 49, loss = 1.05558452\n",
            "Iteration 50, loss = 1.05902876\n",
            "Iteration 51, loss = 1.05689528\n",
            "Iteration 52, loss = 1.05312153\n",
            "Iteration 53, loss = 1.05100088\n",
            "Iteration 54, loss = 1.05121484\n",
            "Iteration 55, loss = 1.05034187\n",
            "Iteration 56, loss = 1.05244257\n",
            "Iteration 57, loss = 1.05010316\n",
            "Iteration 58, loss = 1.04782776\n",
            "Iteration 59, loss = 1.05259514\n",
            "Iteration 60, loss = 1.04325027\n",
            "Iteration 61, loss = 1.05180174\n",
            "Iteration 62, loss = 1.04435081\n",
            "Iteration 63, loss = 1.04769871\n",
            "Iteration 64, loss = 1.04706619\n",
            "Iteration 65, loss = 1.05120399\n",
            "Iteration 66, loss = 1.04799978\n",
            "Iteration 67, loss = 1.04154998\n",
            "Iteration 68, loss = 1.04139771\n",
            "Iteration 69, loss = 1.04174198\n",
            "Iteration 70, loss = 1.04135816\n",
            "Iteration 71, loss = 1.03838421\n",
            "Iteration 72, loss = 1.03811286\n",
            "Iteration 73, loss = 1.03718625\n",
            "Iteration 74, loss = 1.03471795\n",
            "Iteration 75, loss = 1.03754912\n",
            "Iteration 76, loss = 1.04169085\n",
            "Iteration 77, loss = 1.04833895\n",
            "Iteration 78, loss = 1.03612965\n",
            "Iteration 79, loss = 1.03755161\n",
            "Iteration 80, loss = 1.03325658\n",
            "Iteration 81, loss = 1.03097274\n",
            "Iteration 82, loss = 1.03525448\n",
            "Iteration 83, loss = 1.03420937\n",
            "Iteration 84, loss = 1.03613325\n",
            "Iteration 85, loss = 1.03383664\n",
            "Iteration 86, loss = 1.03474231\n",
            "Iteration 87, loss = 1.02832938\n",
            "Iteration 88, loss = 1.03019859\n",
            "Iteration 89, loss = 1.03016185\n",
            "Iteration 90, loss = 1.03443293\n",
            "Iteration 91, loss = 1.03657598\n",
            "Iteration 92, loss = 1.04722587\n",
            "Iteration 93, loss = 1.02948291\n",
            "Iteration 94, loss = 1.03329133\n",
            "Iteration 95, loss = 1.03803136\n",
            "Iteration 96, loss = 1.03255934\n",
            "Iteration 97, loss = 1.02682091\n",
            "Iteration 98, loss = 1.02653648\n",
            "Iteration 99, loss = 1.02643359\n",
            "Iteration 100, loss = 1.02257821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36985387\n",
            "Iteration 2, loss = 1.30623774\n",
            "Iteration 3, loss = 1.27660812\n",
            "Iteration 4, loss = 1.26769238\n",
            "Iteration 5, loss = 1.25954026\n",
            "Iteration 6, loss = 1.25385963\n",
            "Iteration 7, loss = 1.25227710\n",
            "Iteration 8, loss = 1.24528183\n",
            "Iteration 9, loss = 1.24080759\n",
            "Iteration 10, loss = 1.23482033\n",
            "Iteration 11, loss = 1.22720148\n",
            "Iteration 12, loss = 1.21892238\n",
            "Iteration 13, loss = 1.21068117\n",
            "Iteration 14, loss = 1.20010500\n",
            "Iteration 15, loss = 1.19145238\n",
            "Iteration 16, loss = 1.17860697\n",
            "Iteration 17, loss = 1.17246243\n",
            "Iteration 18, loss = 1.16804517\n",
            "Iteration 19, loss = 1.15548774\n",
            "Iteration 20, loss = 1.15096308\n",
            "Iteration 21, loss = 1.13409235\n",
            "Iteration 22, loss = 1.13854635\n",
            "Iteration 23, loss = 1.12677358\n",
            "Iteration 24, loss = 1.12456341\n",
            "Iteration 25, loss = 1.11780253\n",
            "Iteration 26, loss = 1.11251163\n",
            "Iteration 27, loss = 1.10961750\n",
            "Iteration 28, loss = 1.12246378\n",
            "Iteration 29, loss = 1.10508587\n",
            "Iteration 30, loss = 1.10004153\n",
            "Iteration 31, loss = 1.10477008\n",
            "Iteration 32, loss = 1.09705272\n",
            "Iteration 33, loss = 1.09396571\n",
            "Iteration 34, loss = 1.09499266\n",
            "Iteration 35, loss = 1.09249461\n",
            "Iteration 36, loss = 1.09616224\n",
            "Iteration 37, loss = 1.09287012\n",
            "Iteration 38, loss = 1.08476637\n",
            "Iteration 39, loss = 1.08700450\n",
            "Iteration 40, loss = 1.08180013\n",
            "Iteration 41, loss = 1.07793605\n",
            "Iteration 42, loss = 1.07504350\n",
            "Iteration 43, loss = 1.07430622\n",
            "Iteration 44, loss = 1.07235275\n",
            "Iteration 45, loss = 1.07618306\n",
            "Iteration 46, loss = 1.06994071\n",
            "Iteration 47, loss = 1.06914332\n",
            "Iteration 48, loss = 1.06888360\n",
            "Iteration 49, loss = 1.07476012\n",
            "Iteration 50, loss = 1.08206033\n",
            "Iteration 51, loss = 1.07657994\n",
            "Iteration 52, loss = 1.07165549\n",
            "Iteration 53, loss = 1.06626544\n",
            "Iteration 54, loss = 1.06870914\n",
            "Iteration 55, loss = 1.06609219\n",
            "Iteration 56, loss = 1.06898378\n",
            "Iteration 57, loss = 1.06711797\n",
            "Iteration 58, loss = 1.06433508\n",
            "Iteration 59, loss = 1.07403353\n",
            "Iteration 60, loss = 1.06058750\n",
            "Iteration 61, loss = 1.06947845\n",
            "Iteration 62, loss = 1.06237198\n",
            "Iteration 63, loss = 1.06240723\n",
            "Iteration 64, loss = 1.06611700\n",
            "Iteration 65, loss = 1.07180941\n",
            "Iteration 66, loss = 1.06820293\n",
            "Iteration 67, loss = 1.05999549\n",
            "Iteration 68, loss = 1.06667536\n",
            "Iteration 69, loss = 1.06094411\n",
            "Iteration 70, loss = 1.05890664\n",
            "Iteration 71, loss = 1.05874617\n",
            "Iteration 72, loss = 1.05591165\n",
            "Iteration 73, loss = 1.05555622\n",
            "Iteration 74, loss = 1.05510387\n",
            "Iteration 75, loss = 1.05908356\n",
            "Iteration 76, loss = 1.05832095\n",
            "Iteration 77, loss = 1.06588223\n",
            "Iteration 78, loss = 1.05414468\n",
            "Iteration 79, loss = 1.05880175\n",
            "Iteration 80, loss = 1.05273719\n",
            "Iteration 81, loss = 1.05418936\n",
            "Iteration 82, loss = 1.05479235\n",
            "Iteration 83, loss = 1.05721594\n",
            "Iteration 84, loss = 1.06349911\n",
            "Iteration 85, loss = 1.06018601\n",
            "Iteration 86, loss = 1.05929994\n",
            "Iteration 87, loss = 1.05168373\n",
            "Iteration 88, loss = 1.05289952\n",
            "Iteration 89, loss = 1.05036119\n",
            "Iteration 90, loss = 1.05628616\n",
            "Iteration 91, loss = 1.06276874\n",
            "Iteration 92, loss = 1.06305031\n",
            "Iteration 93, loss = 1.05021895\n",
            "Iteration 94, loss = 1.05970109\n",
            "Iteration 95, loss = 1.06077945\n",
            "Iteration 96, loss = 1.05621418\n",
            "Iteration 97, loss = 1.04926259\n",
            "Iteration 98, loss = 1.05425924\n",
            "Iteration 99, loss = 1.05002493\n",
            "Iteration 100, loss = 1.04475320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35685640\n",
            "Iteration 2, loss = 1.29399723\n",
            "Iteration 3, loss = 1.26533618\n",
            "Iteration 4, loss = 1.25638631\n",
            "Iteration 5, loss = 1.24904250\n",
            "Iteration 6, loss = 1.24388200\n",
            "Iteration 7, loss = 1.24011109\n",
            "Iteration 8, loss = 1.23234100\n",
            "Iteration 9, loss = 1.22744571\n",
            "Iteration 10, loss = 1.22200281\n",
            "Iteration 11, loss = 1.21386236\n",
            "Iteration 12, loss = 1.20489143\n",
            "Iteration 13, loss = 1.19372196\n",
            "Iteration 14, loss = 1.18412090\n",
            "Iteration 15, loss = 1.17487607\n",
            "Iteration 16, loss = 1.16252167\n",
            "Iteration 17, loss = 1.15364817\n",
            "Iteration 18, loss = 1.14778907\n",
            "Iteration 19, loss = 1.13802561\n",
            "Iteration 20, loss = 1.13223647\n",
            "Iteration 21, loss = 1.12032805\n",
            "Iteration 22, loss = 1.12329882\n",
            "Iteration 23, loss = 1.11267715\n",
            "Iteration 24, loss = 1.10952472\n",
            "Iteration 25, loss = 1.10392035\n",
            "Iteration 26, loss = 1.09906203\n",
            "Iteration 27, loss = 1.09666566\n",
            "Iteration 28, loss = 1.10074480\n",
            "Iteration 29, loss = 1.09166798\n",
            "Iteration 30, loss = 1.09314039\n",
            "Iteration 31, loss = 1.08819264\n",
            "Iteration 32, loss = 1.08606429\n",
            "Iteration 33, loss = 1.08121569\n",
            "Iteration 34, loss = 1.08586546\n",
            "Iteration 35, loss = 1.08249303\n",
            "Iteration 36, loss = 1.08341798\n",
            "Iteration 37, loss = 1.07765114\n",
            "Iteration 38, loss = 1.07330601\n",
            "Iteration 39, loss = 1.07619598\n",
            "Iteration 40, loss = 1.07099175\n",
            "Iteration 41, loss = 1.06794705\n",
            "Iteration 42, loss = 1.06569453\n",
            "Iteration 43, loss = 1.06263140\n",
            "Iteration 44, loss = 1.05974600\n",
            "Iteration 45, loss = 1.06045303\n",
            "Iteration 46, loss = 1.05780619\n",
            "Iteration 47, loss = 1.05840662\n",
            "Iteration 48, loss = 1.05738164\n",
            "Iteration 49, loss = 1.05865217\n",
            "Iteration 50, loss = 1.06620322\n",
            "Iteration 51, loss = 1.06728982\n",
            "Iteration 52, loss = 1.05726605\n",
            "Iteration 53, loss = 1.05309714\n",
            "Iteration 54, loss = 1.05716847\n",
            "Iteration 55, loss = 1.05152485\n",
            "Iteration 56, loss = 1.05654000\n",
            "Iteration 57, loss = 1.05466687\n",
            "Iteration 58, loss = 1.05269975\n",
            "Iteration 59, loss = 1.06199218\n",
            "Iteration 60, loss = 1.04597287\n",
            "Iteration 61, loss = 1.05616548\n",
            "Iteration 62, loss = 1.04923522\n",
            "Iteration 63, loss = 1.04930533\n",
            "Iteration 64, loss = 1.04838188\n",
            "Iteration 65, loss = 1.05403587\n",
            "Iteration 66, loss = 1.05081653\n",
            "Iteration 67, loss = 1.04560132\n",
            "Iteration 68, loss = 1.04884690\n",
            "Iteration 69, loss = 1.04682827\n",
            "Iteration 70, loss = 1.04447823\n",
            "Iteration 71, loss = 1.04408776\n",
            "Iteration 72, loss = 1.04167882\n",
            "Iteration 73, loss = 1.04281259\n",
            "Iteration 74, loss = 1.04262538\n",
            "Iteration 75, loss = 1.04859432\n",
            "Iteration 76, loss = 1.04447429\n",
            "Iteration 77, loss = 1.05084889\n",
            "Iteration 78, loss = 1.03889864\n",
            "Iteration 79, loss = 1.04383410\n",
            "Iteration 80, loss = 1.03847271\n",
            "Iteration 81, loss = 1.03885070\n",
            "Iteration 82, loss = 1.03968111\n",
            "Iteration 83, loss = 1.04047955\n",
            "Iteration 84, loss = 1.04552176\n",
            "Iteration 85, loss = 1.04424485\n",
            "Iteration 86, loss = 1.04183152\n",
            "Iteration 87, loss = 1.03844860\n",
            "Iteration 88, loss = 1.03870828\n",
            "Iteration 89, loss = 1.03820948\n",
            "Iteration 90, loss = 1.04167504\n",
            "Iteration 91, loss = 1.05075372\n",
            "Iteration 92, loss = 1.04803696\n",
            "Iteration 93, loss = 1.03858270\n",
            "Iteration 94, loss = 1.04178967\n",
            "Iteration 95, loss = 1.04504418\n",
            "Iteration 96, loss = 1.04277557\n",
            "Iteration 97, loss = 1.03489043\n",
            "Iteration 98, loss = 1.03769072\n",
            "Iteration 99, loss = 1.03022331\n",
            "Iteration 100, loss = 1.03094760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35260273\n",
            "Iteration 2, loss = 1.28769549\n",
            "Iteration 3, loss = 1.25567899\n",
            "Iteration 4, loss = 1.24675586\n",
            "Iteration 5, loss = 1.23989367\n",
            "Iteration 6, loss = 1.23436532\n",
            "Iteration 7, loss = 1.23169334\n",
            "Iteration 8, loss = 1.22369003\n",
            "Iteration 9, loss = 1.21700420\n",
            "Iteration 10, loss = 1.21127692\n",
            "Iteration 11, loss = 1.20050175\n",
            "Iteration 12, loss = 1.19135013\n",
            "Iteration 13, loss = 1.17971699\n",
            "Iteration 14, loss = 1.16726955\n",
            "Iteration 15, loss = 1.15866429\n",
            "Iteration 16, loss = 1.14519159\n",
            "Iteration 17, loss = 1.13611791\n",
            "Iteration 18, loss = 1.12506456\n",
            "Iteration 19, loss = 1.11485991\n",
            "Iteration 20, loss = 1.10927839\n",
            "Iteration 21, loss = 1.09400693\n",
            "Iteration 22, loss = 1.09611934\n",
            "Iteration 23, loss = 1.08732965\n",
            "Iteration 24, loss = 1.07942587\n",
            "Iteration 25, loss = 1.07891138\n",
            "Iteration 26, loss = 1.07130543\n",
            "Iteration 27, loss = 1.07018752\n",
            "Iteration 28, loss = 1.07322052\n",
            "Iteration 29, loss = 1.06237229\n",
            "Iteration 30, loss = 1.06156918\n",
            "Iteration 31, loss = 1.05858339\n",
            "Iteration 32, loss = 1.05276705\n",
            "Iteration 33, loss = 1.05109159\n",
            "Iteration 34, loss = 1.05416903\n",
            "Iteration 35, loss = 1.04764859\n",
            "Iteration 36, loss = 1.04564981\n",
            "Iteration 37, loss = 1.04726013\n",
            "Iteration 38, loss = 1.04085935\n",
            "Iteration 39, loss = 1.04177632\n",
            "Iteration 40, loss = 1.03830909\n",
            "Iteration 41, loss = 1.03629028\n",
            "Iteration 42, loss = 1.03823769\n",
            "Iteration 43, loss = 1.03446856\n",
            "Iteration 44, loss = 1.03338237\n",
            "Iteration 45, loss = 1.02908142\n",
            "Iteration 46, loss = 1.03044529\n",
            "Iteration 47, loss = 1.03105823\n",
            "Iteration 48, loss = 1.03157809\n",
            "Iteration 49, loss = 1.02708647\n",
            "Iteration 50, loss = 1.03223624\n",
            "Iteration 51, loss = 1.04201085\n",
            "Iteration 52, loss = 1.02453614\n",
            "Iteration 53, loss = 1.02526415\n",
            "Iteration 54, loss = 1.02875488\n",
            "Iteration 55, loss = 1.02306872\n",
            "Iteration 56, loss = 1.02805775\n",
            "Iteration 57, loss = 1.02353864\n",
            "Iteration 58, loss = 1.02072290\n",
            "Iteration 59, loss = 1.02906313\n",
            "Iteration 60, loss = 1.02095578\n",
            "Iteration 61, loss = 1.02273482\n",
            "Iteration 62, loss = 1.01932243\n",
            "Iteration 63, loss = 1.01871369\n",
            "Iteration 64, loss = 1.01779186\n",
            "Iteration 65, loss = 1.02477749\n",
            "Iteration 66, loss = 1.02211215\n",
            "Iteration 67, loss = 1.01437537\n",
            "Iteration 68, loss = 1.01807807\n",
            "Iteration 69, loss = 1.01845603\n",
            "Iteration 70, loss = 1.01483538\n",
            "Iteration 71, loss = 1.01451486\n",
            "Iteration 72, loss = 1.01529598\n",
            "Iteration 73, loss = 1.01614210\n",
            "Iteration 74, loss = 1.01353440\n",
            "Iteration 75, loss = 1.01839479\n",
            "Iteration 76, loss = 1.01756991\n",
            "Iteration 77, loss = 1.02058376\n",
            "Iteration 78, loss = 1.01336317\n",
            "Iteration 79, loss = 1.01347677\n",
            "Iteration 80, loss = 1.01252116\n",
            "Iteration 81, loss = 1.01247805\n",
            "Iteration 82, loss = 1.01177577\n",
            "Iteration 83, loss = 1.01288958\n",
            "Iteration 84, loss = 1.01595281\n",
            "Iteration 85, loss = 1.01494045\n",
            "Iteration 86, loss = 1.01010398\n",
            "Iteration 87, loss = 1.01487410\n",
            "Iteration 88, loss = 1.00932772\n",
            "Iteration 89, loss = 1.00938108\n",
            "Iteration 90, loss = 1.01689724\n",
            "Iteration 91, loss = 1.01274696\n",
            "Iteration 92, loss = 1.01186861\n",
            "Iteration 93, loss = 1.00757602\n",
            "Iteration 94, loss = 1.00629709\n",
            "Iteration 95, loss = 1.00609052\n",
            "Iteration 96, loss = 1.00803534\n",
            "Iteration 97, loss = 1.00728754\n",
            "Iteration 98, loss = 1.00674580\n",
            "Iteration 99, loss = 1.00871993\n",
            "Iteration 100, loss = 1.00891065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36384771\n",
            "Iteration 2, loss = 1.29810419\n",
            "Iteration 3, loss = 1.26428678\n",
            "Iteration 4, loss = 1.25341226\n",
            "Iteration 5, loss = 1.24365259\n",
            "Iteration 6, loss = 1.24102180\n",
            "Iteration 7, loss = 1.23842709\n",
            "Iteration 8, loss = 1.23135069\n",
            "Iteration 9, loss = 1.22406603\n",
            "Iteration 10, loss = 1.21847453\n",
            "Iteration 11, loss = 1.20927965\n",
            "Iteration 12, loss = 1.20035278\n",
            "Iteration 13, loss = 1.18922610\n",
            "Iteration 14, loss = 1.17976920\n",
            "Iteration 15, loss = 1.16932906\n",
            "Iteration 16, loss = 1.15794680\n",
            "Iteration 17, loss = 1.14810707\n",
            "Iteration 18, loss = 1.13859814\n",
            "Iteration 19, loss = 1.13291481\n",
            "Iteration 20, loss = 1.12171796\n",
            "Iteration 21, loss = 1.11140776\n",
            "Iteration 22, loss = 1.11706642\n",
            "Iteration 23, loss = 1.10658432\n",
            "Iteration 24, loss = 1.09784478\n",
            "Iteration 25, loss = 1.09986076\n",
            "Iteration 26, loss = 1.09104657\n",
            "Iteration 27, loss = 1.08956713\n",
            "Iteration 28, loss = 1.09090923\n",
            "Iteration 29, loss = 1.08383228\n",
            "Iteration 30, loss = 1.08068873\n",
            "Iteration 31, loss = 1.08195659\n",
            "Iteration 32, loss = 1.07503670\n",
            "Iteration 33, loss = 1.07420675\n",
            "Iteration 34, loss = 1.07135777\n",
            "Iteration 35, loss = 1.06781865\n",
            "Iteration 36, loss = 1.06615034\n",
            "Iteration 37, loss = 1.06487190\n",
            "Iteration 38, loss = 1.06596650\n",
            "Iteration 39, loss = 1.06432842\n",
            "Iteration 40, loss = 1.06176282\n",
            "Iteration 41, loss = 1.05815290\n",
            "Iteration 42, loss = 1.05861779\n",
            "Iteration 43, loss = 1.05655768\n",
            "Iteration 44, loss = 1.05301794\n",
            "Iteration 45, loss = 1.05246045\n",
            "Iteration 46, loss = 1.05921485\n",
            "Iteration 47, loss = 1.05127493\n",
            "Iteration 48, loss = 1.05217883\n",
            "Iteration 49, loss = 1.04716174\n",
            "Iteration 50, loss = 1.05312168\n",
            "Iteration 51, loss = 1.06267424\n",
            "Iteration 52, loss = 1.04603063\n",
            "Iteration 53, loss = 1.04639267\n",
            "Iteration 54, loss = 1.05019864\n",
            "Iteration 55, loss = 1.04552334\n",
            "Iteration 56, loss = 1.05246114\n",
            "Iteration 57, loss = 1.04060386\n",
            "Iteration 58, loss = 1.04279631\n",
            "Iteration 59, loss = 1.04764739\n",
            "Iteration 60, loss = 1.03747782\n",
            "Iteration 61, loss = 1.04379000\n",
            "Iteration 62, loss = 1.04141414\n",
            "Iteration 63, loss = 1.03576661\n",
            "Iteration 64, loss = 1.03568649\n",
            "Iteration 65, loss = 1.03948009\n",
            "Iteration 66, loss = 1.03945167\n",
            "Iteration 67, loss = 1.03438103\n",
            "Iteration 68, loss = 1.03899220\n",
            "Iteration 69, loss = 1.03486852\n",
            "Iteration 70, loss = 1.03263281\n",
            "Iteration 71, loss = 1.03184562\n",
            "Iteration 72, loss = 1.03281939\n",
            "Iteration 73, loss = 1.03168546\n",
            "Iteration 74, loss = 1.03185682\n",
            "Iteration 75, loss = 1.03311747\n",
            "Iteration 76, loss = 1.03330289\n",
            "Iteration 77, loss = 1.03336074\n",
            "Iteration 78, loss = 1.03111954\n",
            "Iteration 79, loss = 1.02940321\n",
            "Iteration 80, loss = 1.03322787\n",
            "Iteration 81, loss = 1.03176902\n",
            "Iteration 82, loss = 1.02928784\n",
            "Iteration 83, loss = 1.02903049\n",
            "Iteration 84, loss = 1.03144064\n",
            "Iteration 85, loss = 1.03018752\n",
            "Iteration 86, loss = 1.02599040\n",
            "Iteration 87, loss = 1.03501022\n",
            "Iteration 88, loss = 1.03054576\n",
            "Iteration 89, loss = 1.02511765\n",
            "Iteration 90, loss = 1.03353106\n",
            "Iteration 91, loss = 1.03275868\n",
            "Iteration 92, loss = 1.03061243\n",
            "Iteration 93, loss = 1.02783881\n",
            "Iteration 94, loss = 1.02260918\n",
            "Iteration 95, loss = 1.02234214\n",
            "Iteration 96, loss = 1.02996833\n",
            "Iteration 97, loss = 1.02469995\n",
            "Iteration 98, loss = 1.02406147\n",
            "Iteration 99, loss = 1.02413790\n",
            "Iteration 100, loss = 1.02358428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35493948\n",
            "Iteration 2, loss = 1.29327092\n",
            "Iteration 3, loss = 1.26053180\n",
            "Iteration 4, loss = 1.24836618\n",
            "Iteration 5, loss = 1.24439909\n",
            "Iteration 6, loss = 1.24210542\n",
            "Iteration 7, loss = 1.23637480\n",
            "Iteration 8, loss = 1.22971521\n",
            "Iteration 9, loss = 1.22544663\n",
            "Iteration 10, loss = 1.21823086\n",
            "Iteration 11, loss = 1.21331467\n",
            "Iteration 12, loss = 1.20562875\n",
            "Iteration 13, loss = 1.19962434\n",
            "Iteration 14, loss = 1.18842567\n",
            "Iteration 15, loss = 1.17909840\n",
            "Iteration 16, loss = 1.17125213\n",
            "Iteration 17, loss = 1.16016870\n",
            "Iteration 18, loss = 1.15691509\n",
            "Iteration 19, loss = 1.14531633\n",
            "Iteration 20, loss = 1.13528969\n",
            "Iteration 21, loss = 1.12743811\n",
            "Iteration 22, loss = 1.12358332\n",
            "Iteration 23, loss = 1.11335612\n",
            "Iteration 24, loss = 1.10947729\n",
            "Iteration 25, loss = 1.10241288\n",
            "Iteration 26, loss = 1.09958600\n",
            "Iteration 27, loss = 1.09887555\n",
            "Iteration 28, loss = 1.09948322\n",
            "Iteration 29, loss = 1.09169104\n",
            "Iteration 30, loss = 1.09026406\n",
            "Iteration 31, loss = 1.08787395\n",
            "Iteration 32, loss = 1.08354715\n",
            "Iteration 33, loss = 1.08426805\n",
            "Iteration 34, loss = 1.08177200\n",
            "Iteration 35, loss = 1.07920856\n",
            "Iteration 36, loss = 1.07534554\n",
            "Iteration 37, loss = 1.07271616\n",
            "Iteration 38, loss = 1.07264443\n",
            "Iteration 39, loss = 1.07407013\n",
            "Iteration 40, loss = 1.06831654\n",
            "Iteration 41, loss = 1.06656060\n",
            "Iteration 42, loss = 1.06748189\n",
            "Iteration 43, loss = 1.06487245\n",
            "Iteration 44, loss = 1.06415127\n",
            "Iteration 45, loss = 1.06329915\n",
            "Iteration 46, loss = 1.05934550\n",
            "Iteration 47, loss = 1.05807717\n",
            "Iteration 48, loss = 1.05995837\n",
            "Iteration 49, loss = 1.06683988\n",
            "Iteration 50, loss = 1.05938510\n",
            "Iteration 51, loss = 1.05626850\n",
            "Iteration 52, loss = 1.05170149\n",
            "Iteration 53, loss = 1.05408493\n",
            "Iteration 54, loss = 1.05217527\n",
            "Iteration 55, loss = 1.05653108\n",
            "Iteration 56, loss = 1.05253041\n",
            "Iteration 57, loss = 1.05194302\n",
            "Iteration 58, loss = 1.05033634\n",
            "Iteration 59, loss = 1.05155004\n",
            "Iteration 60, loss = 1.04558483\n",
            "Iteration 61, loss = 1.04617741\n",
            "Iteration 62, loss = 1.04703583\n",
            "Iteration 63, loss = 1.04592501\n",
            "Iteration 64, loss = 1.04870370\n",
            "Iteration 65, loss = 1.04607220\n",
            "Iteration 66, loss = 1.04574029\n",
            "Iteration 67, loss = 1.04568648\n",
            "Iteration 68, loss = 1.04362193\n",
            "Iteration 69, loss = 1.04393881\n",
            "Iteration 70, loss = 1.05209398\n",
            "Iteration 71, loss = 1.07296305\n",
            "Iteration 72, loss = 1.04703790\n",
            "Iteration 73, loss = 1.05748635\n",
            "Iteration 74, loss = 1.05088739\n",
            "Iteration 75, loss = 1.04781807\n",
            "Iteration 76, loss = 1.05173389\n",
            "Iteration 77, loss = 1.04509694\n",
            "Iteration 78, loss = 1.04240612\n",
            "Iteration 79, loss = 1.04216904\n",
            "Iteration 80, loss = 1.04909256\n",
            "Iteration 81, loss = 1.05434344\n",
            "Iteration 82, loss = 1.04567014\n",
            "Iteration 83, loss = 1.05262958\n",
            "Iteration 84, loss = 1.05045515\n",
            "Iteration 85, loss = 1.04586837\n",
            "Iteration 86, loss = 1.04088650\n",
            "Iteration 87, loss = 1.04131035\n",
            "Iteration 88, loss = 1.03507127\n",
            "Iteration 89, loss = 1.04253736\n",
            "Iteration 90, loss = 1.03691436\n",
            "Iteration 91, loss = 1.04012273\n",
            "Iteration 92, loss = 1.04332927\n",
            "Iteration 93, loss = 1.03792396\n",
            "Iteration 94, loss = 1.03760239\n",
            "Iteration 95, loss = 1.03494038\n",
            "Iteration 96, loss = 1.03249954\n",
            "Iteration 97, loss = 1.03488480\n",
            "Iteration 98, loss = 1.03583839\n",
            "Iteration 99, loss = 1.03449599\n",
            "Iteration 100, loss = 1.03204014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35143551\n",
            "Iteration 2, loss = 1.29124228\n",
            "Iteration 3, loss = 1.25920688\n",
            "Iteration 4, loss = 1.24531809\n",
            "Iteration 5, loss = 1.23951214\n",
            "Iteration 6, loss = 1.23730979\n",
            "Iteration 7, loss = 1.23462165\n",
            "Iteration 8, loss = 1.22694355\n",
            "Iteration 9, loss = 1.22326413\n",
            "Iteration 10, loss = 1.21811805\n",
            "Iteration 11, loss = 1.21160187\n",
            "Iteration 12, loss = 1.20762377\n",
            "Iteration 13, loss = 1.19978420\n",
            "Iteration 14, loss = 1.18983278\n",
            "Iteration 15, loss = 1.17894824\n",
            "Iteration 16, loss = 1.17229074\n",
            "Iteration 17, loss = 1.16286266\n",
            "Iteration 18, loss = 1.15445539\n",
            "Iteration 19, loss = 1.14354080\n",
            "Iteration 20, loss = 1.13529874\n",
            "Iteration 21, loss = 1.12572861\n",
            "Iteration 22, loss = 1.11770641\n",
            "Iteration 23, loss = 1.11307791\n",
            "Iteration 24, loss = 1.10905693\n",
            "Iteration 25, loss = 1.10348056\n",
            "Iteration 26, loss = 1.09696386\n",
            "Iteration 27, loss = 1.09385063\n",
            "Iteration 28, loss = 1.09336329\n",
            "Iteration 29, loss = 1.08906923\n",
            "Iteration 30, loss = 1.08523816\n",
            "Iteration 31, loss = 1.08205520\n",
            "Iteration 32, loss = 1.08028038\n",
            "Iteration 33, loss = 1.08203772\n",
            "Iteration 34, loss = 1.07831559\n",
            "Iteration 35, loss = 1.07691640\n",
            "Iteration 36, loss = 1.07051340\n",
            "Iteration 37, loss = 1.06735390\n",
            "Iteration 38, loss = 1.06672321\n",
            "Iteration 39, loss = 1.07022978\n",
            "Iteration 40, loss = 1.06568329\n",
            "Iteration 41, loss = 1.06203397\n",
            "Iteration 42, loss = 1.06334817\n",
            "Iteration 43, loss = 1.06029802\n",
            "Iteration 44, loss = 1.05832909\n",
            "Iteration 45, loss = 1.05739870\n",
            "Iteration 46, loss = 1.05348935\n",
            "Iteration 47, loss = 1.05505788\n",
            "Iteration 48, loss = 1.05947826\n",
            "Iteration 49, loss = 1.06485677\n",
            "Iteration 50, loss = 1.05400953\n",
            "Iteration 51, loss = 1.05742877\n",
            "Iteration 52, loss = 1.05249470\n",
            "Iteration 53, loss = 1.05396786\n",
            "Iteration 54, loss = 1.04860213\n",
            "Iteration 55, loss = 1.05025891\n",
            "Iteration 56, loss = 1.04912722\n",
            "Iteration 57, loss = 1.04592159\n",
            "Iteration 58, loss = 1.04511939\n",
            "Iteration 59, loss = 1.04813202\n",
            "Iteration 60, loss = 1.03910435\n",
            "Iteration 61, loss = 1.04462904\n",
            "Iteration 62, loss = 1.04288005\n",
            "Iteration 63, loss = 1.04042039\n",
            "Iteration 64, loss = 1.04035296\n",
            "Iteration 65, loss = 1.03771394\n",
            "Iteration 66, loss = 1.03596252\n",
            "Iteration 67, loss = 1.03561556\n",
            "Iteration 68, loss = 1.03479323\n",
            "Iteration 69, loss = 1.03841046\n",
            "Iteration 70, loss = 1.05627445\n",
            "Iteration 71, loss = 1.06422568\n",
            "Iteration 72, loss = 1.04727629\n",
            "Iteration 73, loss = 1.04957152\n",
            "Iteration 74, loss = 1.03446708\n",
            "Iteration 75, loss = 1.03650674\n",
            "Iteration 76, loss = 1.03936844\n",
            "Iteration 77, loss = 1.03191732\n",
            "Iteration 78, loss = 1.03080178\n",
            "Iteration 79, loss = 1.03372761\n",
            "Iteration 80, loss = 1.03333773\n",
            "Iteration 81, loss = 1.03452557\n",
            "Iteration 82, loss = 1.03635302\n",
            "Iteration 83, loss = 1.03560433\n",
            "Iteration 84, loss = 1.03986723\n",
            "Iteration 85, loss = 1.03476208\n",
            "Iteration 86, loss = 1.02828008\n",
            "Iteration 87, loss = 1.03328310\n",
            "Iteration 88, loss = 1.02775052\n",
            "Iteration 89, loss = 1.02717418\n",
            "Iteration 90, loss = 1.02855605\n",
            "Iteration 91, loss = 1.02681036\n",
            "Iteration 92, loss = 1.02982642\n",
            "Iteration 93, loss = 1.02640367\n",
            "Iteration 94, loss = 1.02805642\n",
            "Iteration 95, loss = 1.02545973\n",
            "Iteration 96, loss = 1.02566085\n",
            "Iteration 97, loss = 1.02509725\n",
            "Iteration 98, loss = 1.02528989\n",
            "Iteration 99, loss = 1.02255304\n",
            "Iteration 100, loss = 1.02121870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35434509\n",
            "Iteration 2, loss = 1.28871175\n",
            "Iteration 3, loss = 1.25380235\n",
            "Iteration 4, loss = 1.23607880\n",
            "Iteration 5, loss = 1.22757836\n",
            "Iteration 6, loss = 1.22553177\n",
            "Iteration 7, loss = 1.22330500\n",
            "Iteration 8, loss = 1.21649599\n",
            "Iteration 9, loss = 1.21330699\n",
            "Iteration 10, loss = 1.20884069\n",
            "Iteration 11, loss = 1.20165732\n",
            "Iteration 12, loss = 1.20148602\n",
            "Iteration 13, loss = 1.19343180\n",
            "Iteration 14, loss = 1.18346109\n",
            "Iteration 15, loss = 1.17594042\n",
            "Iteration 16, loss = 1.17028083\n",
            "Iteration 17, loss = 1.15829366\n",
            "Iteration 18, loss = 1.15022804\n",
            "Iteration 19, loss = 1.14063389\n",
            "Iteration 20, loss = 1.13131977\n",
            "Iteration 21, loss = 1.12367063\n",
            "Iteration 22, loss = 1.11895176\n",
            "Iteration 23, loss = 1.11286725\n",
            "Iteration 24, loss = 1.10784792\n",
            "Iteration 25, loss = 1.10176977\n",
            "Iteration 26, loss = 1.09923443\n",
            "Iteration 27, loss = 1.09018346\n",
            "Iteration 28, loss = 1.08728179\n",
            "Iteration 29, loss = 1.08275149\n",
            "Iteration 30, loss = 1.07976016\n",
            "Iteration 31, loss = 1.07960832\n",
            "Iteration 32, loss = 1.07422869\n",
            "Iteration 33, loss = 1.07741548\n",
            "Iteration 34, loss = 1.07015085\n",
            "Iteration 35, loss = 1.07074334\n",
            "Iteration 36, loss = 1.06460094\n",
            "Iteration 37, loss = 1.06400798\n",
            "Iteration 38, loss = 1.06105448\n",
            "Iteration 39, loss = 1.06472441\n",
            "Iteration 40, loss = 1.05788595\n",
            "Iteration 41, loss = 1.05972770\n",
            "Iteration 42, loss = 1.05845404\n",
            "Iteration 43, loss = 1.05529964\n",
            "Iteration 44, loss = 1.05192631\n",
            "Iteration 45, loss = 1.05161689\n",
            "Iteration 46, loss = 1.04955467\n",
            "Iteration 47, loss = 1.04990579\n",
            "Iteration 48, loss = 1.05108336\n",
            "Iteration 49, loss = 1.05003420\n",
            "Iteration 50, loss = 1.05022419\n",
            "Iteration 51, loss = 1.05608751\n",
            "Iteration 52, loss = 1.04767597\n",
            "Iteration 53, loss = 1.04838951\n",
            "Iteration 54, loss = 1.04157061\n",
            "Iteration 55, loss = 1.04154715\n",
            "Iteration 56, loss = 1.04045668\n",
            "Iteration 57, loss = 1.04218139\n",
            "Iteration 58, loss = 1.03963207\n",
            "Iteration 59, loss = 1.04465862\n",
            "Iteration 60, loss = 1.03620435\n",
            "Iteration 61, loss = 1.03910448\n",
            "Iteration 62, loss = 1.04119570\n",
            "Iteration 63, loss = 1.03362848\n",
            "Iteration 64, loss = 1.03451554\n",
            "Iteration 65, loss = 1.03173716\n",
            "Iteration 66, loss = 1.03048484\n",
            "Iteration 67, loss = 1.03282571\n",
            "Iteration 68, loss = 1.02998152\n",
            "Iteration 69, loss = 1.03156714\n",
            "Iteration 70, loss = 1.04075466\n",
            "Iteration 71, loss = 1.05507371\n",
            "Iteration 72, loss = 1.03595820\n",
            "Iteration 73, loss = 1.03178785\n",
            "Iteration 74, loss = 1.02706316\n",
            "Iteration 75, loss = 1.03065997\n",
            "Iteration 76, loss = 1.03027972\n",
            "Iteration 77, loss = 1.02423569\n",
            "Iteration 78, loss = 1.02937408\n",
            "Iteration 79, loss = 1.02563667\n",
            "Iteration 80, loss = 1.02482817\n",
            "Iteration 81, loss = 1.02850086\n",
            "Iteration 82, loss = 1.03751008\n",
            "Iteration 83, loss = 1.02561881\n",
            "Iteration 84, loss = 1.04157155\n",
            "Iteration 85, loss = 1.03554665\n",
            "Iteration 86, loss = 1.02413088\n",
            "Iteration 87, loss = 1.02915553\n",
            "Iteration 88, loss = 1.02767363\n",
            "Iteration 89, loss = 1.02157404\n",
            "Iteration 90, loss = 1.02841338\n",
            "Iteration 91, loss = 1.02176362\n",
            "Iteration 92, loss = 1.02473452\n",
            "Iteration 93, loss = 1.02031560\n",
            "Iteration 94, loss = 1.02208641\n",
            "Iteration 95, loss = 1.02111488\n",
            "Iteration 96, loss = 1.02008315\n",
            "Iteration 97, loss = 1.02148595\n",
            "Iteration 98, loss = 1.01854882\n",
            "Iteration 99, loss = 1.01655027\n",
            "Iteration 100, loss = 1.01634968\n",
            "21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31588869\n",
            "Iteration 2, loss = 1.25752245\n",
            "Iteration 3, loss = 1.25848475\n",
            "Iteration 4, loss = 1.25107239\n",
            "Iteration 5, loss = 1.24902685\n",
            "Iteration 6, loss = 1.24250122\n",
            "Iteration 7, loss = 1.23869119\n",
            "Iteration 8, loss = 1.23381971\n",
            "Iteration 9, loss = 1.23054470\n",
            "Iteration 10, loss = 1.22466740\n",
            "Iteration 11, loss = 1.22154051\n",
            "Iteration 12, loss = 1.21407607\n",
            "Iteration 13, loss = 1.21607057\n",
            "Iteration 14, loss = 1.20286185\n",
            "Iteration 15, loss = 1.19588930\n",
            "Iteration 16, loss = 1.19443635\n",
            "Iteration 17, loss = 1.19075657\n",
            "Iteration 18, loss = 1.18083927\n",
            "Iteration 19, loss = 1.17015859\n",
            "Iteration 20, loss = 1.16176783\n",
            "Iteration 21, loss = 1.16754282\n",
            "Iteration 22, loss = 1.14845086\n",
            "Iteration 23, loss = 1.15264855\n",
            "Iteration 24, loss = 1.17630679\n",
            "Iteration 25, loss = 1.16856676\n",
            "Iteration 26, loss = 1.16284771\n",
            "Iteration 27, loss = 1.14421932\n",
            "Iteration 28, loss = 1.12814575\n",
            "Iteration 29, loss = 1.12089713\n",
            "Iteration 30, loss = 1.11328189\n",
            "Iteration 31, loss = 1.11097845\n",
            "Iteration 32, loss = 1.10297345\n",
            "Iteration 33, loss = 1.10248427\n",
            "Iteration 34, loss = 1.09647007\n",
            "Iteration 35, loss = 1.10626861\n",
            "Iteration 36, loss = 1.09708452\n",
            "Iteration 37, loss = 1.08144714\n",
            "Iteration 38, loss = 1.08198984\n",
            "Iteration 39, loss = 1.08331996\n",
            "Iteration 40, loss = 1.07456368\n",
            "Iteration 41, loss = 1.10131542\n",
            "Iteration 42, loss = 1.06638563\n",
            "Iteration 43, loss = 1.07383101\n",
            "Iteration 44, loss = 1.06446330\n",
            "Iteration 45, loss = 1.05812205\n",
            "Iteration 46, loss = 1.04684268\n",
            "Iteration 47, loss = 1.04835870\n",
            "Iteration 48, loss = 1.04807573\n",
            "Iteration 49, loss = 1.04047531\n",
            "Iteration 50, loss = 1.04451889\n",
            "Iteration 51, loss = 1.03962541\n",
            "Iteration 52, loss = 1.02815082\n",
            "Iteration 53, loss = 1.03918917\n",
            "Iteration 54, loss = 1.03414834\n",
            "Iteration 55, loss = 1.02066312\n",
            "Iteration 56, loss = 1.03835083\n",
            "Iteration 57, loss = 1.03171947\n",
            "Iteration 58, loss = 1.01772597\n",
            "Iteration 59, loss = 1.01904919\n",
            "Iteration 60, loss = 1.01414145\n",
            "Iteration 61, loss = 1.00561684\n",
            "Iteration 62, loss = 1.01172712\n",
            "Iteration 63, loss = 0.99792780\n",
            "Iteration 64, loss = 0.99657400\n",
            "Iteration 65, loss = 0.99317110\n",
            "Iteration 66, loss = 0.98980771\n",
            "Iteration 67, loss = 0.99087912\n",
            "Iteration 68, loss = 0.98334757\n",
            "Iteration 69, loss = 0.99001940\n",
            "Iteration 70, loss = 0.97292620\n",
            "Iteration 71, loss = 0.98451499\n",
            "Iteration 72, loss = 1.00748007\n",
            "Iteration 73, loss = 1.00731415\n",
            "Iteration 74, loss = 0.97176353\n",
            "Iteration 75, loss = 0.96955776\n",
            "Iteration 76, loss = 0.95830183\n",
            "Iteration 77, loss = 0.96499939\n",
            "Iteration 78, loss = 0.95780757\n",
            "Iteration 79, loss = 0.95651275\n",
            "Iteration 80, loss = 0.95124522\n",
            "Iteration 81, loss = 0.94773860\n",
            "Iteration 82, loss = 0.94252252\n",
            "Iteration 83, loss = 0.94372066\n",
            "Iteration 84, loss = 0.94253197\n",
            "Iteration 85, loss = 0.94026265\n",
            "Iteration 86, loss = 0.97522422\n",
            "Iteration 87, loss = 0.95606476\n",
            "Iteration 88, loss = 0.95879664\n",
            "Iteration 89, loss = 0.94770197\n",
            "Iteration 90, loss = 0.92961943\n",
            "Iteration 91, loss = 0.93228178\n",
            "Iteration 92, loss = 0.91590838\n",
            "Iteration 93, loss = 0.91020036\n",
            "Iteration 94, loss = 0.90382072\n",
            "Iteration 95, loss = 0.90511399\n",
            "Iteration 96, loss = 0.90911266\n",
            "Iteration 97, loss = 0.89796475\n",
            "Iteration 98, loss = 0.90069068\n",
            "Iteration 99, loss = 0.89887082\n",
            "Iteration 100, loss = 0.89294409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31303502\n",
            "Iteration 2, loss = 1.25366889\n",
            "Iteration 3, loss = 1.25700440\n",
            "Iteration 4, loss = 1.24734553\n",
            "Iteration 5, loss = 1.24458220\n",
            "Iteration 6, loss = 1.24049057\n",
            "Iteration 7, loss = 1.23633973\n",
            "Iteration 8, loss = 1.23311546\n",
            "Iteration 9, loss = 1.22874073\n",
            "Iteration 10, loss = 1.22573827\n",
            "Iteration 11, loss = 1.21984487\n",
            "Iteration 12, loss = 1.21548351\n",
            "Iteration 13, loss = 1.21337435\n",
            "Iteration 14, loss = 1.20746775\n",
            "Iteration 15, loss = 1.19882018\n",
            "Iteration 16, loss = 1.19931224\n",
            "Iteration 17, loss = 1.19537235\n",
            "Iteration 18, loss = 1.18344857\n",
            "Iteration 19, loss = 1.17896787\n",
            "Iteration 20, loss = 1.16759165\n",
            "Iteration 21, loss = 1.17228825\n",
            "Iteration 22, loss = 1.15126356\n",
            "Iteration 23, loss = 1.17153656\n",
            "Iteration 24, loss = 1.16926067\n",
            "Iteration 25, loss = 1.16874530\n",
            "Iteration 26, loss = 1.16569682\n",
            "Iteration 27, loss = 1.14853537\n",
            "Iteration 28, loss = 1.14136865\n",
            "Iteration 29, loss = 1.13161642\n",
            "Iteration 30, loss = 1.12245300\n",
            "Iteration 31, loss = 1.12204828\n",
            "Iteration 32, loss = 1.11542090\n",
            "Iteration 33, loss = 1.10871797\n",
            "Iteration 34, loss = 1.10833416\n",
            "Iteration 35, loss = 1.10439596\n",
            "Iteration 36, loss = 1.10041063\n",
            "Iteration 37, loss = 1.09229612\n",
            "Iteration 38, loss = 1.08937802\n",
            "Iteration 39, loss = 1.08802028\n",
            "Iteration 40, loss = 1.08373422\n",
            "Iteration 41, loss = 1.09624539\n",
            "Iteration 42, loss = 1.07004653\n",
            "Iteration 43, loss = 1.08653848\n",
            "Iteration 44, loss = 1.06899839\n",
            "Iteration 45, loss = 1.06392918\n",
            "Iteration 46, loss = 1.05586011\n",
            "Iteration 47, loss = 1.05519159\n",
            "Iteration 48, loss = 1.05486919\n",
            "Iteration 49, loss = 1.04703752\n",
            "Iteration 50, loss = 1.05378975\n",
            "Iteration 51, loss = 1.05087167\n",
            "Iteration 52, loss = 1.03739396\n",
            "Iteration 53, loss = 1.03997367\n",
            "Iteration 54, loss = 1.03506879\n",
            "Iteration 55, loss = 1.02852311\n",
            "Iteration 56, loss = 1.04142808\n",
            "Iteration 57, loss = 1.03429308\n",
            "Iteration 58, loss = 1.02943423\n",
            "Iteration 59, loss = 1.03014732\n",
            "Iteration 60, loss = 1.01572742\n",
            "Iteration 61, loss = 1.01409334\n",
            "Iteration 62, loss = 1.01178186\n",
            "Iteration 63, loss = 1.00865633\n",
            "Iteration 64, loss = 1.00573405\n",
            "Iteration 65, loss = 1.00869407\n",
            "Iteration 66, loss = 1.00403404\n",
            "Iteration 67, loss = 1.00693872\n",
            "Iteration 68, loss = 0.99890690\n",
            "Iteration 69, loss = 1.00484593\n",
            "Iteration 70, loss = 0.98512865\n",
            "Iteration 71, loss = 0.99268563\n",
            "Iteration 72, loss = 0.99977485\n",
            "Iteration 73, loss = 1.00375241\n",
            "Iteration 74, loss = 0.98550181\n",
            "Iteration 75, loss = 0.97727196\n",
            "Iteration 76, loss = 0.97689877\n",
            "Iteration 77, loss = 0.97952833\n",
            "Iteration 78, loss = 0.98098701\n",
            "Iteration 79, loss = 0.97006657\n",
            "Iteration 80, loss = 0.95949583\n",
            "Iteration 81, loss = 0.96093584\n",
            "Iteration 82, loss = 0.96442222\n",
            "Iteration 83, loss = 0.96825991\n",
            "Iteration 84, loss = 0.96511392\n",
            "Iteration 85, loss = 0.95851520\n",
            "Iteration 86, loss = 0.97062437\n",
            "Iteration 87, loss = 0.96644087\n",
            "Iteration 88, loss = 0.97222576\n",
            "Iteration 89, loss = 0.96940976\n",
            "Iteration 90, loss = 0.95215675\n",
            "Iteration 91, loss = 0.96797426\n",
            "Iteration 92, loss = 0.94299468\n",
            "Iteration 93, loss = 0.93468003\n",
            "Iteration 94, loss = 0.92989391\n",
            "Iteration 95, loss = 0.92223837\n",
            "Iteration 96, loss = 0.93222711\n",
            "Iteration 97, loss = 0.92081276\n",
            "Iteration 98, loss = 0.92689102\n",
            "Iteration 99, loss = 0.92971693\n",
            "Iteration 100, loss = 0.92607793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32046927\n",
            "Iteration 2, loss = 1.25955345\n",
            "Iteration 3, loss = 1.26235400\n",
            "Iteration 4, loss = 1.25271653\n",
            "Iteration 5, loss = 1.24922339\n",
            "Iteration 6, loss = 1.24595361\n",
            "Iteration 7, loss = 1.24158768\n",
            "Iteration 8, loss = 1.23825694\n",
            "Iteration 9, loss = 1.23392018\n",
            "Iteration 10, loss = 1.23272639\n",
            "Iteration 11, loss = 1.22517556\n",
            "Iteration 12, loss = 1.22139439\n",
            "Iteration 13, loss = 1.21776563\n",
            "Iteration 14, loss = 1.21041249\n",
            "Iteration 15, loss = 1.20183516\n",
            "Iteration 16, loss = 1.21354346\n",
            "Iteration 17, loss = 1.20025740\n",
            "Iteration 18, loss = 1.19653862\n",
            "Iteration 19, loss = 1.18687326\n",
            "Iteration 20, loss = 1.18157916\n",
            "Iteration 21, loss = 1.17155509\n",
            "Iteration 22, loss = 1.16682380\n",
            "Iteration 23, loss = 1.18126598\n",
            "Iteration 24, loss = 1.17959146\n",
            "Iteration 25, loss = 1.17025708\n",
            "Iteration 26, loss = 1.16274230\n",
            "Iteration 27, loss = 1.15778813\n",
            "Iteration 28, loss = 1.15046309\n",
            "Iteration 29, loss = 1.13587333\n",
            "Iteration 30, loss = 1.13398836\n",
            "Iteration 31, loss = 1.14417787\n",
            "Iteration 32, loss = 1.12565354\n",
            "Iteration 33, loss = 1.11617942\n",
            "Iteration 34, loss = 1.11563516\n",
            "Iteration 35, loss = 1.11197465\n",
            "Iteration 36, loss = 1.10715402\n",
            "Iteration 37, loss = 1.10036120\n",
            "Iteration 38, loss = 1.09401357\n",
            "Iteration 39, loss = 1.09555966\n",
            "Iteration 40, loss = 1.09200562\n",
            "Iteration 41, loss = 1.09934356\n",
            "Iteration 42, loss = 1.07690248\n",
            "Iteration 43, loss = 1.08875856\n",
            "Iteration 44, loss = 1.08545783\n",
            "Iteration 45, loss = 1.07354018\n",
            "Iteration 46, loss = 1.06962171\n",
            "Iteration 47, loss = 1.06287105\n",
            "Iteration 48, loss = 1.05909341\n",
            "Iteration 49, loss = 1.05748147\n",
            "Iteration 50, loss = 1.06848355\n",
            "Iteration 51, loss = 1.05813795\n",
            "Iteration 52, loss = 1.04981780\n",
            "Iteration 53, loss = 1.05431581\n",
            "Iteration 54, loss = 1.04402495\n",
            "Iteration 55, loss = 1.04591869\n",
            "Iteration 56, loss = 1.05531972\n",
            "Iteration 57, loss = 1.05575557\n",
            "Iteration 58, loss = 1.04603424\n",
            "Iteration 59, loss = 1.03595324\n",
            "Iteration 60, loss = 1.03201791\n",
            "Iteration 61, loss = 1.02218720\n",
            "Iteration 62, loss = 1.02221279\n",
            "Iteration 63, loss = 1.01820003\n",
            "Iteration 64, loss = 1.01461672\n",
            "Iteration 65, loss = 1.01778373\n",
            "Iteration 66, loss = 1.00541865\n",
            "Iteration 67, loss = 1.00244074\n",
            "Iteration 68, loss = 1.00516102\n",
            "Iteration 69, loss = 1.00140484\n",
            "Iteration 70, loss = 0.99087711\n",
            "Iteration 71, loss = 0.99725506\n",
            "Iteration 72, loss = 1.00872128\n",
            "Iteration 73, loss = 1.00764445\n",
            "Iteration 74, loss = 0.99719168\n",
            "Iteration 75, loss = 0.98379485\n",
            "Iteration 76, loss = 0.98059735\n",
            "Iteration 77, loss = 0.98388361\n",
            "Iteration 78, loss = 0.98735764\n",
            "Iteration 79, loss = 0.97969951\n",
            "Iteration 80, loss = 0.96618363\n",
            "Iteration 81, loss = 0.97299040\n",
            "Iteration 82, loss = 0.96493069\n",
            "Iteration 83, loss = 0.96977774\n",
            "Iteration 84, loss = 0.98745562\n",
            "Iteration 85, loss = 0.98025317\n",
            "Iteration 86, loss = 0.98262821\n",
            "Iteration 87, loss = 0.96680410\n",
            "Iteration 88, loss = 0.96423893\n",
            "Iteration 89, loss = 0.96535388\n",
            "Iteration 90, loss = 0.94982618\n",
            "Iteration 91, loss = 0.95375373\n",
            "Iteration 92, loss = 0.94092347\n",
            "Iteration 93, loss = 0.93847743\n",
            "Iteration 94, loss = 0.92986965\n",
            "Iteration 95, loss = 0.94254659\n",
            "Iteration 96, loss = 0.95643547\n",
            "Iteration 97, loss = 0.94951981\n",
            "Iteration 98, loss = 0.95020001\n",
            "Iteration 99, loss = 0.94519078\n",
            "Iteration 100, loss = 0.91909134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32463730\n",
            "Iteration 2, loss = 1.26452439\n",
            "Iteration 3, loss = 1.26752422\n",
            "Iteration 4, loss = 1.25577326\n",
            "Iteration 5, loss = 1.25237131\n",
            "Iteration 6, loss = 1.24929234\n",
            "Iteration 7, loss = 1.24576912\n",
            "Iteration 8, loss = 1.24118816\n",
            "Iteration 9, loss = 1.23745729\n",
            "Iteration 10, loss = 1.23583799\n",
            "Iteration 11, loss = 1.22972670\n",
            "Iteration 12, loss = 1.22585223\n",
            "Iteration 13, loss = 1.22187441\n",
            "Iteration 14, loss = 1.21595030\n",
            "Iteration 15, loss = 1.21055320\n",
            "Iteration 16, loss = 1.21335946\n",
            "Iteration 17, loss = 1.20584869\n",
            "Iteration 18, loss = 1.20635550\n",
            "Iteration 19, loss = 1.19009872\n",
            "Iteration 20, loss = 1.18839838\n",
            "Iteration 21, loss = 1.17636697\n",
            "Iteration 22, loss = 1.16936408\n",
            "Iteration 23, loss = 1.17202025\n",
            "Iteration 24, loss = 1.16738758\n",
            "Iteration 25, loss = 1.15701828\n",
            "Iteration 26, loss = 1.15542472\n",
            "Iteration 27, loss = 1.15616672\n",
            "Iteration 28, loss = 1.14964099\n",
            "Iteration 29, loss = 1.13695868\n",
            "Iteration 30, loss = 1.13120885\n",
            "Iteration 31, loss = 1.12130035\n",
            "Iteration 32, loss = 1.12275858\n",
            "Iteration 33, loss = 1.10892680\n",
            "Iteration 34, loss = 1.10733199\n",
            "Iteration 35, loss = 1.10215258\n",
            "Iteration 36, loss = 1.09148835\n",
            "Iteration 37, loss = 1.09520051\n",
            "Iteration 38, loss = 1.10113116\n",
            "Iteration 39, loss = 1.07988426\n",
            "Iteration 40, loss = 1.08445206\n",
            "Iteration 41, loss = 1.09205097\n",
            "Iteration 42, loss = 1.07259099\n",
            "Iteration 43, loss = 1.07721863\n",
            "Iteration 44, loss = 1.08805983\n",
            "Iteration 45, loss = 1.05999509\n",
            "Iteration 46, loss = 1.06559603\n",
            "Iteration 47, loss = 1.05244461\n",
            "Iteration 48, loss = 1.05122702\n",
            "Iteration 49, loss = 1.05200721\n",
            "Iteration 50, loss = 1.04867887\n",
            "Iteration 51, loss = 1.05023679\n",
            "Iteration 52, loss = 1.03287499\n",
            "Iteration 53, loss = 1.05712379\n",
            "Iteration 54, loss = 1.03114148\n",
            "Iteration 55, loss = 1.03914919\n",
            "Iteration 56, loss = 1.05197855\n",
            "Iteration 57, loss = 1.04608623\n",
            "Iteration 58, loss = 1.03451317\n",
            "Iteration 59, loss = 1.03105345\n",
            "Iteration 60, loss = 1.04080478\n",
            "Iteration 61, loss = 1.01525552\n",
            "Iteration 62, loss = 1.01634593\n",
            "Iteration 63, loss = 1.00938476\n",
            "Iteration 64, loss = 1.00766782\n",
            "Iteration 65, loss = 1.00515271\n",
            "Iteration 66, loss = 1.00028579\n",
            "Iteration 67, loss = 0.99865082\n",
            "Iteration 68, loss = 0.99994111\n",
            "Iteration 69, loss = 0.98819355\n",
            "Iteration 70, loss = 0.98775321\n",
            "Iteration 71, loss = 0.98775311\n",
            "Iteration 72, loss = 0.98784226\n",
            "Iteration 73, loss = 0.98789499\n",
            "Iteration 74, loss = 0.99236494\n",
            "Iteration 75, loss = 0.97694594\n",
            "Iteration 76, loss = 0.97318693\n",
            "Iteration 77, loss = 0.97665380\n",
            "Iteration 78, loss = 0.97527543\n",
            "Iteration 79, loss = 0.95895367\n",
            "Iteration 80, loss = 0.97704609\n",
            "Iteration 81, loss = 0.98292043\n",
            "Iteration 82, loss = 0.96542287\n",
            "Iteration 83, loss = 0.97533858\n",
            "Iteration 84, loss = 0.98682831\n",
            "Iteration 85, loss = 0.97667904\n",
            "Iteration 86, loss = 0.95941299\n",
            "Iteration 87, loss = 0.94523058\n",
            "Iteration 88, loss = 0.96665272\n",
            "Iteration 89, loss = 0.97000784\n",
            "Iteration 90, loss = 0.94951517\n",
            "Iteration 91, loss = 0.94700425\n",
            "Iteration 92, loss = 0.93589971\n",
            "Iteration 93, loss = 0.93080612\n",
            "Iteration 94, loss = 0.93983279\n",
            "Iteration 95, loss = 0.92860550\n",
            "Iteration 96, loss = 0.93087284\n",
            "Iteration 97, loss = 0.93082159\n",
            "Iteration 98, loss = 0.92555261\n",
            "Iteration 99, loss = 0.92190625\n",
            "Iteration 100, loss = 0.90597095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31580865\n",
            "Iteration 2, loss = 1.25409299\n",
            "Iteration 3, loss = 1.25644379\n",
            "Iteration 4, loss = 1.24498946\n",
            "Iteration 5, loss = 1.24349357\n",
            "Iteration 6, loss = 1.24076180\n",
            "Iteration 7, loss = 1.23519003\n",
            "Iteration 8, loss = 1.23058487\n",
            "Iteration 9, loss = 1.22631937\n",
            "Iteration 10, loss = 1.22500753\n",
            "Iteration 11, loss = 1.21830530\n",
            "Iteration 12, loss = 1.21657816\n",
            "Iteration 13, loss = 1.21048905\n",
            "Iteration 14, loss = 1.21079987\n",
            "Iteration 15, loss = 1.19853319\n",
            "Iteration 16, loss = 1.21033425\n",
            "Iteration 17, loss = 1.19446946\n",
            "Iteration 18, loss = 1.19222819\n",
            "Iteration 19, loss = 1.18531492\n",
            "Iteration 20, loss = 1.18030303\n",
            "Iteration 21, loss = 1.16562750\n",
            "Iteration 22, loss = 1.16112570\n",
            "Iteration 23, loss = 1.17265842\n",
            "Iteration 24, loss = 1.16101588\n",
            "Iteration 25, loss = 1.16163651\n",
            "Iteration 26, loss = 1.14690719\n",
            "Iteration 27, loss = 1.15088268\n",
            "Iteration 28, loss = 1.15307032\n",
            "Iteration 29, loss = 1.13054645\n",
            "Iteration 30, loss = 1.12605401\n",
            "Iteration 31, loss = 1.12380284\n",
            "Iteration 32, loss = 1.11513897\n",
            "Iteration 33, loss = 1.11024685\n",
            "Iteration 34, loss = 1.10404065\n",
            "Iteration 35, loss = 1.09363816\n",
            "Iteration 36, loss = 1.09963972\n",
            "Iteration 37, loss = 1.09216952\n",
            "Iteration 38, loss = 1.08485139\n",
            "Iteration 39, loss = 1.07936398\n",
            "Iteration 40, loss = 1.08358225\n",
            "Iteration 41, loss = 1.09591070\n",
            "Iteration 42, loss = 1.06626032\n",
            "Iteration 43, loss = 1.07489284\n",
            "Iteration 44, loss = 1.06982005\n",
            "Iteration 45, loss = 1.05593411\n",
            "Iteration 46, loss = 1.06115930\n",
            "Iteration 47, loss = 1.05299040\n",
            "Iteration 48, loss = 1.05148846\n",
            "Iteration 49, loss = 1.05128988\n",
            "Iteration 50, loss = 1.04939330\n",
            "Iteration 51, loss = 1.03934776\n",
            "Iteration 52, loss = 1.03914151\n",
            "Iteration 53, loss = 1.04572802\n",
            "Iteration 54, loss = 1.03085326\n",
            "Iteration 55, loss = 1.03655769\n",
            "Iteration 56, loss = 1.03731653\n",
            "Iteration 57, loss = 1.04069521\n",
            "Iteration 58, loss = 1.02727624\n",
            "Iteration 59, loss = 1.02783412\n",
            "Iteration 60, loss = 1.03707993\n",
            "Iteration 61, loss = 1.01773652\n",
            "Iteration 62, loss = 1.01379996\n",
            "Iteration 63, loss = 1.00760245\n",
            "Iteration 64, loss = 1.01261722\n",
            "Iteration 65, loss = 1.00026015\n",
            "Iteration 66, loss = 1.00100090\n",
            "Iteration 67, loss = 0.99517952\n",
            "Iteration 68, loss = 0.99795139\n",
            "Iteration 69, loss = 0.99316417\n",
            "Iteration 70, loss = 0.99168872\n",
            "Iteration 71, loss = 0.99255254\n",
            "Iteration 72, loss = 0.99820446\n",
            "Iteration 73, loss = 0.98651529\n",
            "Iteration 74, loss = 0.98640867\n",
            "Iteration 75, loss = 0.97845264\n",
            "Iteration 76, loss = 0.97178462\n",
            "Iteration 77, loss = 0.97080224\n",
            "Iteration 78, loss = 0.97199935\n",
            "Iteration 79, loss = 0.96472480\n",
            "Iteration 80, loss = 0.98037406\n",
            "Iteration 81, loss = 1.00280528\n",
            "Iteration 82, loss = 0.98388822\n",
            "Iteration 83, loss = 0.98609685\n",
            "Iteration 84, loss = 0.98414824\n",
            "Iteration 85, loss = 0.99707850\n",
            "Iteration 86, loss = 0.96913396\n",
            "Iteration 87, loss = 0.95144069\n",
            "Iteration 88, loss = 0.97423636\n",
            "Iteration 89, loss = 0.96745794\n",
            "Iteration 90, loss = 0.96573295\n",
            "Iteration 91, loss = 0.94356414\n",
            "Iteration 92, loss = 0.94013366\n",
            "Iteration 93, loss = 0.93579030\n",
            "Iteration 94, loss = 0.93574200\n",
            "Iteration 95, loss = 0.92877420\n",
            "Iteration 96, loss = 0.93996488\n",
            "Iteration 97, loss = 0.92632257\n",
            "Iteration 98, loss = 0.92680293\n",
            "Iteration 99, loss = 0.92586254\n",
            "Iteration 100, loss = 0.91908749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31079629\n",
            "Iteration 2, loss = 1.24324928\n",
            "Iteration 3, loss = 1.24371957\n",
            "Iteration 4, loss = 1.23431009\n",
            "Iteration 5, loss = 1.23209396\n",
            "Iteration 6, loss = 1.22833405\n",
            "Iteration 7, loss = 1.22396818\n",
            "Iteration 8, loss = 1.21948824\n",
            "Iteration 9, loss = 1.21629481\n",
            "Iteration 10, loss = 1.21534395\n",
            "Iteration 11, loss = 1.20697184\n",
            "Iteration 12, loss = 1.20234434\n",
            "Iteration 13, loss = 1.19957858\n",
            "Iteration 14, loss = 1.19516308\n",
            "Iteration 15, loss = 1.18881522\n",
            "Iteration 16, loss = 1.19473879\n",
            "Iteration 17, loss = 1.17995583\n",
            "Iteration 18, loss = 1.17991561\n",
            "Iteration 19, loss = 1.17176577\n",
            "Iteration 20, loss = 1.16634527\n",
            "Iteration 21, loss = 1.15304108\n",
            "Iteration 22, loss = 1.15105349\n",
            "Iteration 23, loss = 1.16616628\n",
            "Iteration 24, loss = 1.14952708\n",
            "Iteration 25, loss = 1.15184098\n",
            "Iteration 26, loss = 1.13991329\n",
            "Iteration 27, loss = 1.13813132\n",
            "Iteration 28, loss = 1.13422186\n",
            "Iteration 29, loss = 1.12150616\n",
            "Iteration 30, loss = 1.11648035\n",
            "Iteration 31, loss = 1.10822619\n",
            "Iteration 32, loss = 1.11154061\n",
            "Iteration 33, loss = 1.10971752\n",
            "Iteration 34, loss = 1.09605642\n",
            "Iteration 35, loss = 1.09803295\n",
            "Iteration 36, loss = 1.08526483\n",
            "Iteration 37, loss = 1.08875131\n",
            "Iteration 38, loss = 1.08731579\n",
            "Iteration 39, loss = 1.07361366\n",
            "Iteration 40, loss = 1.08948212\n",
            "Iteration 41, loss = 1.10468566\n",
            "Iteration 42, loss = 1.06591826\n",
            "Iteration 43, loss = 1.06429578\n",
            "Iteration 44, loss = 1.05856990\n",
            "Iteration 45, loss = 1.06187631\n",
            "Iteration 46, loss = 1.05835482\n",
            "Iteration 47, loss = 1.05467604\n",
            "Iteration 48, loss = 1.05140442\n",
            "Iteration 49, loss = 1.04496900\n",
            "Iteration 50, loss = 1.03841650\n",
            "Iteration 51, loss = 1.03728866\n",
            "Iteration 52, loss = 1.02974828\n",
            "Iteration 53, loss = 1.03779220\n",
            "Iteration 54, loss = 1.02540617\n",
            "Iteration 55, loss = 1.02672680\n",
            "Iteration 56, loss = 1.03661277\n",
            "Iteration 57, loss = 1.03779807\n",
            "Iteration 58, loss = 1.02456539\n",
            "Iteration 59, loss = 1.01415365\n",
            "Iteration 60, loss = 1.03011347\n",
            "Iteration 61, loss = 1.00724595\n",
            "Iteration 62, loss = 1.00732416\n",
            "Iteration 63, loss = 1.00302612\n",
            "Iteration 64, loss = 1.00431247\n",
            "Iteration 65, loss = 0.99745882\n",
            "Iteration 66, loss = 0.99186781\n",
            "Iteration 67, loss = 0.98653689\n",
            "Iteration 68, loss = 0.98747318\n",
            "Iteration 69, loss = 0.99176060\n",
            "Iteration 70, loss = 0.99186857\n",
            "Iteration 71, loss = 0.98127840\n",
            "Iteration 72, loss = 0.97766802\n",
            "Iteration 73, loss = 0.96980068\n",
            "Iteration 74, loss = 0.98780675\n",
            "Iteration 75, loss = 0.96128104\n",
            "Iteration 76, loss = 0.95727188\n",
            "Iteration 77, loss = 0.95524420\n",
            "Iteration 78, loss = 0.96376665\n",
            "Iteration 79, loss = 0.95665763\n",
            "Iteration 80, loss = 0.96024580\n",
            "Iteration 81, loss = 0.98391483\n",
            "Iteration 82, loss = 0.98213224\n",
            "Iteration 83, loss = 0.95037776\n",
            "Iteration 84, loss = 0.93882101\n",
            "Iteration 85, loss = 0.94318066\n",
            "Iteration 86, loss = 0.93935368\n",
            "Iteration 87, loss = 0.93031533\n",
            "Iteration 88, loss = 0.93625509\n",
            "Iteration 89, loss = 0.92847827\n",
            "Iteration 90, loss = 0.92798847\n",
            "Iteration 91, loss = 0.93104662\n",
            "Iteration 92, loss = 0.91054779\n",
            "Iteration 93, loss = 0.90978228\n",
            "Iteration 94, loss = 0.90361425\n",
            "Iteration 95, loss = 0.90928390\n",
            "Iteration 96, loss = 0.91951026\n",
            "Iteration 97, loss = 0.91060238\n",
            "Iteration 98, loss = 0.89335306\n",
            "Iteration 99, loss = 0.88804623\n",
            "Iteration 100, loss = 0.88444876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31944888\n",
            "Iteration 2, loss = 1.25295762\n",
            "Iteration 3, loss = 1.24902810\n",
            "Iteration 4, loss = 1.24044738\n",
            "Iteration 5, loss = 1.23947634\n",
            "Iteration 6, loss = 1.23527328\n",
            "Iteration 7, loss = 1.22967433\n",
            "Iteration 8, loss = 1.22544530\n",
            "Iteration 9, loss = 1.22289876\n",
            "Iteration 10, loss = 1.21611669\n",
            "Iteration 11, loss = 1.21312749\n",
            "Iteration 12, loss = 1.20748325\n",
            "Iteration 13, loss = 1.20150508\n",
            "Iteration 14, loss = 1.19885380\n",
            "Iteration 15, loss = 1.18997680\n",
            "Iteration 16, loss = 1.19751696\n",
            "Iteration 17, loss = 1.17954994\n",
            "Iteration 18, loss = 1.17895532\n",
            "Iteration 19, loss = 1.17275096\n",
            "Iteration 20, loss = 1.16027761\n",
            "Iteration 21, loss = 1.14767880\n",
            "Iteration 22, loss = 1.14469726\n",
            "Iteration 23, loss = 1.15790127\n",
            "Iteration 24, loss = 1.14545520\n",
            "Iteration 25, loss = 1.13884602\n",
            "Iteration 26, loss = 1.12868417\n",
            "Iteration 27, loss = 1.12761574\n",
            "Iteration 28, loss = 1.12498511\n",
            "Iteration 29, loss = 1.12269109\n",
            "Iteration 30, loss = 1.11536956\n",
            "Iteration 31, loss = 1.09865654\n",
            "Iteration 32, loss = 1.10303425\n",
            "Iteration 33, loss = 1.12055162\n",
            "Iteration 34, loss = 1.11990381\n",
            "Iteration 35, loss = 1.09917750\n",
            "Iteration 36, loss = 1.08608222\n",
            "Iteration 37, loss = 1.09465898\n",
            "Iteration 38, loss = 1.09807732\n",
            "Iteration 39, loss = 1.07633839\n",
            "Iteration 40, loss = 1.06951258\n",
            "Iteration 41, loss = 1.07031737\n",
            "Iteration 42, loss = 1.05925290\n",
            "Iteration 43, loss = 1.05653042\n",
            "Iteration 44, loss = 1.05477881\n",
            "Iteration 45, loss = 1.05223477\n",
            "Iteration 46, loss = 1.05197354\n",
            "Iteration 47, loss = 1.04312575\n",
            "Iteration 48, loss = 1.04571850\n",
            "Iteration 49, loss = 1.04570554\n",
            "Iteration 50, loss = 1.03408765\n",
            "Iteration 51, loss = 1.02867649\n",
            "Iteration 52, loss = 1.03380862\n",
            "Iteration 53, loss = 1.04771657\n",
            "Iteration 54, loss = 1.01825805\n",
            "Iteration 55, loss = 1.03224024\n",
            "Iteration 56, loss = 1.03472231\n",
            "Iteration 57, loss = 1.03030667\n",
            "Iteration 58, loss = 1.01457773\n",
            "Iteration 59, loss = 1.00968736\n",
            "Iteration 60, loss = 1.02337296\n",
            "Iteration 61, loss = 1.00791026\n",
            "Iteration 62, loss = 1.00535146\n",
            "Iteration 63, loss = 0.99691282\n",
            "Iteration 64, loss = 1.00035999\n",
            "Iteration 65, loss = 1.00255643\n",
            "Iteration 66, loss = 0.99496692\n",
            "Iteration 67, loss = 0.98766835\n",
            "Iteration 68, loss = 0.98818944\n",
            "Iteration 69, loss = 0.99648191\n",
            "Iteration 70, loss = 0.99386855\n",
            "Iteration 71, loss = 0.98989756\n",
            "Iteration 72, loss = 0.97939559\n",
            "Iteration 73, loss = 0.97069053\n",
            "Iteration 74, loss = 0.97789764\n",
            "Iteration 75, loss = 0.96239415\n",
            "Iteration 76, loss = 0.96081425\n",
            "Iteration 77, loss = 0.95661795\n",
            "Iteration 78, loss = 0.96231476\n",
            "Iteration 79, loss = 0.95378833\n",
            "Iteration 80, loss = 0.95862712\n",
            "Iteration 81, loss = 0.98061128\n",
            "Iteration 82, loss = 0.97502080\n",
            "Iteration 83, loss = 0.95551827\n",
            "Iteration 84, loss = 0.94525671\n",
            "Iteration 85, loss = 0.94328301\n",
            "Iteration 86, loss = 0.93820118\n",
            "Iteration 87, loss = 0.93826589\n",
            "Iteration 88, loss = 0.94500942\n",
            "Iteration 89, loss = 0.93428643\n",
            "Iteration 90, loss = 0.93015378\n",
            "Iteration 91, loss = 0.93591303\n",
            "Iteration 92, loss = 0.92229655\n",
            "Iteration 93, loss = 0.91928736\n",
            "Iteration 94, loss = 0.91556569\n",
            "Iteration 95, loss = 0.92348577\n",
            "Iteration 96, loss = 0.94857053\n",
            "Iteration 97, loss = 0.92461459\n",
            "Iteration 98, loss = 0.90989863\n",
            "Iteration 99, loss = 0.90681480\n",
            "Iteration 100, loss = 0.89464885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31220137\n",
            "Iteration 2, loss = 1.25015385\n",
            "Iteration 3, loss = 1.24832754\n",
            "Iteration 4, loss = 1.24338747\n",
            "Iteration 5, loss = 1.23773780\n",
            "Iteration 6, loss = 1.23415749\n",
            "Iteration 7, loss = 1.22955375\n",
            "Iteration 8, loss = 1.22362926\n",
            "Iteration 9, loss = 1.22054845\n",
            "Iteration 10, loss = 1.21380234\n",
            "Iteration 11, loss = 1.20879777\n",
            "Iteration 12, loss = 1.20549264\n",
            "Iteration 13, loss = 1.19940490\n",
            "Iteration 14, loss = 1.19108381\n",
            "Iteration 15, loss = 1.18285019\n",
            "Iteration 16, loss = 1.17782354\n",
            "Iteration 17, loss = 1.17110962\n",
            "Iteration 18, loss = 1.17647509\n",
            "Iteration 19, loss = 1.16957338\n",
            "Iteration 20, loss = 1.15533146\n",
            "Iteration 21, loss = 1.14823525\n",
            "Iteration 22, loss = 1.14173380\n",
            "Iteration 23, loss = 1.13712959\n",
            "Iteration 24, loss = 1.14645542\n",
            "Iteration 25, loss = 1.12892456\n",
            "Iteration 26, loss = 1.13455726\n",
            "Iteration 27, loss = 1.13352060\n",
            "Iteration 28, loss = 1.10995574\n",
            "Iteration 29, loss = 1.10697708\n",
            "Iteration 30, loss = 1.15307766\n",
            "Iteration 31, loss = 1.13473952\n",
            "Iteration 32, loss = 1.12285991\n",
            "Iteration 33, loss = 1.11052193\n",
            "Iteration 34, loss = 1.09881242\n",
            "Iteration 35, loss = 1.10589322\n",
            "Iteration 36, loss = 1.08458702\n",
            "Iteration 37, loss = 1.07843533\n",
            "Iteration 38, loss = 1.09356397\n",
            "Iteration 39, loss = 1.10796624\n",
            "Iteration 40, loss = 1.10232860\n",
            "Iteration 41, loss = 1.08672319\n",
            "Iteration 42, loss = 1.07085302\n",
            "Iteration 43, loss = 1.06666410\n",
            "Iteration 44, loss = 1.06611303\n",
            "Iteration 45, loss = 1.06052870\n",
            "Iteration 46, loss = 1.06383044\n",
            "Iteration 47, loss = 1.06657050\n",
            "Iteration 48, loss = 1.06010228\n",
            "Iteration 49, loss = 1.04796272\n",
            "Iteration 50, loss = 1.04664233\n",
            "Iteration 51, loss = 1.04001161\n",
            "Iteration 52, loss = 1.04086856\n",
            "Iteration 53, loss = 1.04139507\n",
            "Iteration 54, loss = 1.03645580\n",
            "Iteration 55, loss = 1.03737037\n",
            "Iteration 56, loss = 1.03661013\n",
            "Iteration 57, loss = 1.03945302\n",
            "Iteration 58, loss = 1.02509150\n",
            "Iteration 59, loss = 1.05289130\n",
            "Iteration 60, loss = 1.05134242\n",
            "Iteration 61, loss = 1.03381188\n",
            "Iteration 62, loss = 1.03862245\n",
            "Iteration 63, loss = 1.03647245\n",
            "Iteration 64, loss = 1.03888436\n",
            "Iteration 65, loss = 1.03169706\n",
            "Iteration 66, loss = 1.01121042\n",
            "Iteration 67, loss = 1.00817206\n",
            "Iteration 68, loss = 1.00300327\n",
            "Iteration 69, loss = 1.00013962\n",
            "Iteration 70, loss = 1.00531581\n",
            "Iteration 71, loss = 1.01106325\n",
            "Iteration 72, loss = 1.00474589\n",
            "Iteration 73, loss = 0.99343869\n",
            "Iteration 74, loss = 0.98927991\n",
            "Iteration 75, loss = 0.99862573\n",
            "Iteration 76, loss = 0.98978023\n",
            "Iteration 77, loss = 0.98058681\n",
            "Iteration 78, loss = 0.99844624\n",
            "Iteration 79, loss = 0.99593752\n",
            "Iteration 80, loss = 0.99844404\n",
            "Iteration 81, loss = 1.00199088\n",
            "Iteration 82, loss = 0.98085943\n",
            "Iteration 83, loss = 0.98060514\n",
            "Iteration 84, loss = 0.98405042\n",
            "Iteration 85, loss = 0.97470421\n",
            "Iteration 86, loss = 0.96403274\n",
            "Iteration 87, loss = 0.95614220\n",
            "Iteration 88, loss = 0.96056007\n",
            "Iteration 89, loss = 0.95734866\n",
            "Iteration 90, loss = 0.95001454\n",
            "Iteration 91, loss = 0.96441481\n",
            "Iteration 92, loss = 0.95002830\n",
            "Iteration 93, loss = 0.93994195\n",
            "Iteration 94, loss = 0.95022288\n",
            "Iteration 95, loss = 0.94601728\n",
            "Iteration 96, loss = 0.95037130\n",
            "Iteration 97, loss = 0.92847773\n",
            "Iteration 98, loss = 0.93009948\n",
            "Iteration 99, loss = 0.92156115\n",
            "Iteration 100, loss = 0.91904558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30619002\n",
            "Iteration 2, loss = 1.24503450\n",
            "Iteration 3, loss = 1.24428295\n",
            "Iteration 4, loss = 1.23840790\n",
            "Iteration 5, loss = 1.23263445\n",
            "Iteration 6, loss = 1.22813714\n",
            "Iteration 7, loss = 1.22343917\n",
            "Iteration 8, loss = 1.21871641\n",
            "Iteration 9, loss = 1.21553707\n",
            "Iteration 10, loss = 1.20958233\n",
            "Iteration 11, loss = 1.20454171\n",
            "Iteration 12, loss = 1.20284808\n",
            "Iteration 13, loss = 1.19343830\n",
            "Iteration 14, loss = 1.18590124\n",
            "Iteration 15, loss = 1.17790813\n",
            "Iteration 16, loss = 1.17191030\n",
            "Iteration 17, loss = 1.16356707\n",
            "Iteration 18, loss = 1.16265874\n",
            "Iteration 19, loss = 1.16189446\n",
            "Iteration 20, loss = 1.15312842\n",
            "Iteration 21, loss = 1.14626653\n",
            "Iteration 22, loss = 1.13777970\n",
            "Iteration 23, loss = 1.12948969\n",
            "Iteration 24, loss = 1.16044837\n",
            "Iteration 25, loss = 1.12448212\n",
            "Iteration 26, loss = 1.12266093\n",
            "Iteration 27, loss = 1.11425521\n",
            "Iteration 28, loss = 1.10658364\n",
            "Iteration 29, loss = 1.09985413\n",
            "Iteration 30, loss = 1.11519181\n",
            "Iteration 31, loss = 1.11127308\n",
            "Iteration 32, loss = 1.09886363\n",
            "Iteration 33, loss = 1.09863735\n",
            "Iteration 34, loss = 1.08726882\n",
            "Iteration 35, loss = 1.10277823\n",
            "Iteration 36, loss = 1.07961431\n",
            "Iteration 37, loss = 1.07792010\n",
            "Iteration 38, loss = 1.06631190\n",
            "Iteration 39, loss = 1.06774642\n",
            "Iteration 40, loss = 1.06764098\n",
            "Iteration 41, loss = 1.06209431\n",
            "Iteration 42, loss = 1.05879459\n",
            "Iteration 43, loss = 1.05517756\n",
            "Iteration 44, loss = 1.06384857\n",
            "Iteration 45, loss = 1.05013678\n",
            "Iteration 46, loss = 1.04830320\n",
            "Iteration 47, loss = 1.06456492\n",
            "Iteration 48, loss = 1.04831111\n",
            "Iteration 49, loss = 1.03455423\n",
            "Iteration 50, loss = 1.03044589\n",
            "Iteration 51, loss = 1.02893446\n",
            "Iteration 52, loss = 1.03380621\n",
            "Iteration 53, loss = 1.02495016\n",
            "Iteration 54, loss = 1.02357305\n",
            "Iteration 55, loss = 1.01795605\n",
            "Iteration 56, loss = 1.01219749\n",
            "Iteration 57, loss = 1.01416045\n",
            "Iteration 58, loss = 1.01427790\n",
            "Iteration 59, loss = 1.02866559\n",
            "Iteration 60, loss = 1.04768161\n",
            "Iteration 61, loss = 1.01276064\n",
            "Iteration 62, loss = 1.02654812\n",
            "Iteration 63, loss = 1.01629178\n",
            "Iteration 64, loss = 1.03602265\n",
            "Iteration 65, loss = 1.03072988\n",
            "Iteration 66, loss = 1.01601518\n",
            "Iteration 67, loss = 1.00967950\n",
            "Iteration 68, loss = 0.99820052\n",
            "Iteration 69, loss = 0.99874749\n",
            "Iteration 70, loss = 0.99607608\n",
            "Iteration 71, loss = 0.99594110\n",
            "Iteration 72, loss = 0.99501779\n",
            "Iteration 73, loss = 0.99417267\n",
            "Iteration 74, loss = 0.98265161\n",
            "Iteration 75, loss = 0.98787985\n",
            "Iteration 76, loss = 0.97893277\n",
            "Iteration 77, loss = 0.97532696\n",
            "Iteration 78, loss = 0.99326290\n",
            "Iteration 79, loss = 0.98069881\n",
            "Iteration 80, loss = 0.97760579\n",
            "Iteration 81, loss = 0.97474891\n",
            "Iteration 82, loss = 0.96938586\n",
            "Iteration 83, loss = 0.97332800\n",
            "Iteration 84, loss = 0.97326741\n",
            "Iteration 85, loss = 0.96469898\n",
            "Iteration 86, loss = 0.95523791\n",
            "Iteration 87, loss = 0.94708740\n",
            "Iteration 88, loss = 0.95993305\n",
            "Iteration 89, loss = 0.95423747\n",
            "Iteration 90, loss = 0.95322965\n",
            "Iteration 91, loss = 0.94266423\n",
            "Iteration 92, loss = 0.94179428\n",
            "Iteration 93, loss = 0.93636870\n",
            "Iteration 94, loss = 0.94586266\n",
            "Iteration 95, loss = 0.95409825\n",
            "Iteration 96, loss = 0.94044548\n",
            "Iteration 97, loss = 0.94118930\n",
            "Iteration 98, loss = 0.93224356\n",
            "Iteration 99, loss = 0.93157340\n",
            "Iteration 100, loss = 0.92612318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30600531\n",
            "Iteration 2, loss = 1.23335092\n",
            "Iteration 3, loss = 1.23171065\n",
            "Iteration 4, loss = 1.22777426\n",
            "Iteration 5, loss = 1.22086356\n",
            "Iteration 6, loss = 1.21586777\n",
            "Iteration 7, loss = 1.21461615\n",
            "Iteration 8, loss = 1.20696358\n",
            "Iteration 9, loss = 1.20571452\n",
            "Iteration 10, loss = 1.19921874\n",
            "Iteration 11, loss = 1.19529761\n",
            "Iteration 12, loss = 1.19183774\n",
            "Iteration 13, loss = 1.18595277\n",
            "Iteration 14, loss = 1.17966506\n",
            "Iteration 15, loss = 1.17165433\n",
            "Iteration 16, loss = 1.16755147\n",
            "Iteration 17, loss = 1.16171395\n",
            "Iteration 18, loss = 1.16021818\n",
            "Iteration 19, loss = 1.15761099\n",
            "Iteration 20, loss = 1.15122732\n",
            "Iteration 21, loss = 1.13945605\n",
            "Iteration 22, loss = 1.13723358\n",
            "Iteration 23, loss = 1.13438967\n",
            "Iteration 24, loss = 1.16201906\n",
            "Iteration 25, loss = 1.12408742\n",
            "Iteration 26, loss = 1.12090030\n",
            "Iteration 27, loss = 1.11652912\n",
            "Iteration 28, loss = 1.10716126\n",
            "Iteration 29, loss = 1.10804415\n",
            "Iteration 30, loss = 1.13443684\n",
            "Iteration 31, loss = 1.11048350\n",
            "Iteration 32, loss = 1.10810579\n",
            "Iteration 33, loss = 1.08650154\n",
            "Iteration 34, loss = 1.09467144\n",
            "Iteration 35, loss = 1.09190823\n",
            "Iteration 36, loss = 1.07879267\n",
            "Iteration 37, loss = 1.07205790\n",
            "Iteration 38, loss = 1.08075110\n",
            "Iteration 39, loss = 1.07511209\n",
            "Iteration 40, loss = 1.08673962\n",
            "Iteration 41, loss = 1.05936621\n",
            "Iteration 42, loss = 1.05876003\n",
            "Iteration 43, loss = 1.05455207\n",
            "Iteration 44, loss = 1.05643693\n",
            "Iteration 45, loss = 1.06725125\n",
            "Iteration 46, loss = 1.06798387\n",
            "Iteration 47, loss = 1.06739444\n",
            "Iteration 48, loss = 1.07074024\n",
            "Iteration 49, loss = 1.04847815\n",
            "Iteration 50, loss = 1.04312717\n",
            "Iteration 51, loss = 1.03240029\n",
            "Iteration 52, loss = 1.02997185\n",
            "Iteration 53, loss = 1.02756851\n",
            "Iteration 54, loss = 1.03297955\n",
            "Iteration 55, loss = 1.02106151\n",
            "Iteration 56, loss = 1.01688925\n",
            "Iteration 57, loss = 1.01698414\n",
            "Iteration 58, loss = 1.01980246\n",
            "Iteration 59, loss = 1.02717023\n",
            "Iteration 60, loss = 1.03061464\n",
            "Iteration 61, loss = 1.01697425\n",
            "Iteration 62, loss = 1.02524339\n",
            "Iteration 63, loss = 1.01190299\n",
            "Iteration 64, loss = 1.03247919\n",
            "Iteration 65, loss = 1.01497880\n",
            "Iteration 66, loss = 1.00297554\n",
            "Iteration 67, loss = 0.99372387\n",
            "Iteration 68, loss = 0.99388251\n",
            "Iteration 69, loss = 1.00228410\n",
            "Iteration 70, loss = 1.00955546\n",
            "Iteration 71, loss = 1.00497491\n",
            "Iteration 72, loss = 0.99076058\n",
            "Iteration 73, loss = 1.00160844\n",
            "Iteration 74, loss = 0.98217878\n",
            "Iteration 75, loss = 0.97994609\n",
            "Iteration 76, loss = 0.98416273\n",
            "Iteration 77, loss = 0.98386946\n",
            "Iteration 78, loss = 0.99662366\n",
            "Iteration 79, loss = 0.98338339\n",
            "Iteration 80, loss = 0.98066449\n",
            "Iteration 81, loss = 0.98015565\n",
            "Iteration 82, loss = 0.97549888\n",
            "Iteration 83, loss = 0.97241406\n",
            "Iteration 84, loss = 0.96707433\n",
            "Iteration 85, loss = 0.96245487\n",
            "Iteration 86, loss = 0.95707077\n",
            "Iteration 87, loss = 0.95487533\n",
            "Iteration 88, loss = 0.96676251\n",
            "Iteration 89, loss = 0.95318990\n",
            "Iteration 90, loss = 0.95236042\n",
            "Iteration 91, loss = 0.95540107\n",
            "Iteration 92, loss = 0.95189014\n",
            "Iteration 93, loss = 0.95179902\n",
            "Iteration 94, loss = 0.95294269\n",
            "Iteration 95, loss = 0.96628162\n",
            "Iteration 96, loss = 0.95738319\n",
            "Iteration 97, loss = 0.94994807\n",
            "Iteration 98, loss = 0.93426724\n",
            "Iteration 99, loss = 0.92586765\n",
            "Iteration 100, loss = 0.92437106\n",
            "22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31082252\n",
            "Iteration 2, loss = 1.25705520\n",
            "Iteration 3, loss = 1.24128645\n",
            "Iteration 4, loss = 1.22326521\n",
            "Iteration 5, loss = 1.21765148\n",
            "Iteration 6, loss = 1.20538083\n",
            "Iteration 7, loss = 1.18940753\n",
            "Iteration 8, loss = 1.17844691\n",
            "Iteration 9, loss = 1.15940131\n",
            "Iteration 10, loss = 1.13960857\n",
            "Iteration 11, loss = 1.12021718\n",
            "Iteration 12, loss = 1.09469011\n",
            "Iteration 13, loss = 1.07026352\n",
            "Iteration 14, loss = 1.04045573\n",
            "Iteration 15, loss = 1.00887922\n",
            "Iteration 16, loss = 0.97547541\n",
            "Iteration 17, loss = 0.94614978\n",
            "Iteration 18, loss = 0.90288329\n",
            "Iteration 19, loss = 0.86884798\n",
            "Iteration 20, loss = 0.83670601\n",
            "Iteration 21, loss = 0.80559747\n",
            "Iteration 22, loss = 0.78272716\n",
            "Iteration 23, loss = 0.75848032\n",
            "Iteration 24, loss = 0.73569016\n",
            "Iteration 25, loss = 0.71274994\n",
            "Iteration 26, loss = 0.70175165\n",
            "Iteration 27, loss = 0.68589114\n",
            "Iteration 28, loss = 0.68101722\n",
            "Iteration 29, loss = 0.66891416\n",
            "Iteration 30, loss = 0.65979004\n",
            "Iteration 31, loss = 0.64826288\n",
            "Iteration 32, loss = 0.63972569\n",
            "Iteration 33, loss = 0.63446160\n",
            "Iteration 34, loss = 0.62914983\n",
            "Iteration 35, loss = 0.61868234\n",
            "Iteration 36, loss = 0.61666829\n",
            "Iteration 37, loss = 0.61188680\n",
            "Iteration 38, loss = 0.60745453\n",
            "Iteration 39, loss = 0.60116019\n",
            "Iteration 40, loss = 0.60314051\n",
            "Iteration 41, loss = 0.59643137\n",
            "Iteration 42, loss = 0.59404389\n",
            "Iteration 43, loss = 0.58693782\n",
            "Iteration 44, loss = 0.59509403\n",
            "Iteration 45, loss = 0.58226516\n",
            "Iteration 46, loss = 0.58053625\n",
            "Iteration 47, loss = 0.57589203\n",
            "Iteration 48, loss = 0.56952830\n",
            "Iteration 49, loss = 0.56802018\n",
            "Iteration 50, loss = 0.56197112\n",
            "Iteration 51, loss = 0.55735536\n",
            "Iteration 52, loss = 0.54696655\n",
            "Iteration 53, loss = 0.54867850\n",
            "Iteration 54, loss = 0.54142323\n",
            "Iteration 55, loss = 0.54925898\n",
            "Iteration 56, loss = 0.53832842\n",
            "Iteration 57, loss = 0.53050999\n",
            "Iteration 58, loss = 0.53118382\n",
            "Iteration 59, loss = 0.52398977\n",
            "Iteration 60, loss = 0.52216616\n",
            "Iteration 61, loss = 0.52641997\n",
            "Iteration 62, loss = 0.51240880\n",
            "Iteration 63, loss = 0.51476418\n",
            "Iteration 64, loss = 0.50381695\n",
            "Iteration 65, loss = 0.50621215\n",
            "Iteration 66, loss = 0.49908136\n",
            "Iteration 67, loss = 0.49806483\n",
            "Iteration 68, loss = 0.49433396\n",
            "Iteration 69, loss = 0.49449921\n",
            "Iteration 70, loss = 0.49435688\n",
            "Iteration 71, loss = 0.50355700\n",
            "Iteration 72, loss = 0.51102636\n",
            "Iteration 73, loss = 0.49093137\n",
            "Iteration 74, loss = 0.48290186\n",
            "Iteration 75, loss = 0.47862116\n",
            "Iteration 76, loss = 0.47361368\n",
            "Iteration 77, loss = 0.46973544\n",
            "Iteration 78, loss = 0.48248802\n",
            "Iteration 79, loss = 0.47537835\n",
            "Iteration 80, loss = 0.46092540\n",
            "Iteration 81, loss = 0.46396011\n",
            "Iteration 82, loss = 0.45619756\n",
            "Iteration 83, loss = 0.45536398\n",
            "Iteration 84, loss = 0.45434414\n",
            "Iteration 85, loss = 0.45504313\n",
            "Iteration 86, loss = 0.44956732\n",
            "Iteration 87, loss = 0.45010991\n",
            "Iteration 88, loss = 0.45124434\n",
            "Iteration 89, loss = 0.44452148\n",
            "Iteration 90, loss = 0.44152874\n",
            "Iteration 91, loss = 0.43708808\n",
            "Iteration 92, loss = 0.44383462\n",
            "Iteration 93, loss = 0.43162177\n",
            "Iteration 94, loss = 0.43931713\n",
            "Iteration 95, loss = 0.43049584\n",
            "Iteration 96, loss = 0.42799126\n",
            "Iteration 97, loss = 0.42981465\n",
            "Iteration 98, loss = 0.42441467\n",
            "Iteration 99, loss = 0.42168618\n",
            "Iteration 100, loss = 0.42039799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31034453\n",
            "Iteration 2, loss = 1.25553450\n",
            "Iteration 3, loss = 1.23876080\n",
            "Iteration 4, loss = 1.22062625\n",
            "Iteration 5, loss = 1.21343042\n",
            "Iteration 6, loss = 1.19988041\n",
            "Iteration 7, loss = 1.18511128\n",
            "Iteration 8, loss = 1.17081710\n",
            "Iteration 9, loss = 1.15277634\n",
            "Iteration 10, loss = 1.13186860\n",
            "Iteration 11, loss = 1.11295212\n",
            "Iteration 12, loss = 1.08292793\n",
            "Iteration 13, loss = 1.05888602\n",
            "Iteration 14, loss = 1.02464498\n",
            "Iteration 15, loss = 0.99216472\n",
            "Iteration 16, loss = 0.95495825\n",
            "Iteration 17, loss = 0.92306847\n",
            "Iteration 18, loss = 0.88371738\n",
            "Iteration 19, loss = 0.84811767\n",
            "Iteration 20, loss = 0.82047193\n",
            "Iteration 21, loss = 0.78939915\n",
            "Iteration 22, loss = 0.77383914\n",
            "Iteration 23, loss = 0.75276383\n",
            "Iteration 24, loss = 0.73002977\n",
            "Iteration 25, loss = 0.70548943\n",
            "Iteration 26, loss = 0.69209075\n",
            "Iteration 27, loss = 0.68325855\n",
            "Iteration 28, loss = 0.67793619\n",
            "Iteration 29, loss = 0.65973635\n",
            "Iteration 30, loss = 0.64958374\n",
            "Iteration 31, loss = 0.64334192\n",
            "Iteration 32, loss = 0.63441425\n",
            "Iteration 33, loss = 0.62828484\n",
            "Iteration 34, loss = 0.62225751\n",
            "Iteration 35, loss = 0.61566155\n",
            "Iteration 36, loss = 0.61686167\n",
            "Iteration 37, loss = 0.61683770\n",
            "Iteration 38, loss = 0.60579354\n",
            "Iteration 39, loss = 0.60243870\n",
            "Iteration 40, loss = 0.60270095\n",
            "Iteration 41, loss = 0.59526427\n",
            "Iteration 42, loss = 0.59350576\n",
            "Iteration 43, loss = 0.58455538\n",
            "Iteration 44, loss = 0.58220813\n",
            "Iteration 45, loss = 0.57660686\n",
            "Iteration 46, loss = 0.57434678\n",
            "Iteration 47, loss = 0.57516059\n",
            "Iteration 48, loss = 0.56836375\n",
            "Iteration 49, loss = 0.56990643\n",
            "Iteration 50, loss = 0.56220490\n",
            "Iteration 51, loss = 0.55896630\n",
            "Iteration 52, loss = 0.55242340\n",
            "Iteration 53, loss = 0.54723337\n",
            "Iteration 54, loss = 0.54720021\n",
            "Iteration 55, loss = 0.55381517\n",
            "Iteration 56, loss = 0.54045030\n",
            "Iteration 57, loss = 0.53562225\n",
            "Iteration 58, loss = 0.53405303\n",
            "Iteration 59, loss = 0.53405836\n",
            "Iteration 60, loss = 0.53069912\n",
            "Iteration 61, loss = 0.53015333\n",
            "Iteration 62, loss = 0.52161329\n",
            "Iteration 63, loss = 0.52526373\n",
            "Iteration 64, loss = 0.51477909\n",
            "Iteration 65, loss = 0.51538144\n",
            "Iteration 66, loss = 0.51053910\n",
            "Iteration 67, loss = 0.50496629\n",
            "Iteration 68, loss = 0.50267492\n",
            "Iteration 69, loss = 0.50569283\n",
            "Iteration 70, loss = 0.50586893\n",
            "Iteration 71, loss = 0.50433022\n",
            "Iteration 72, loss = 0.50035850\n",
            "Iteration 73, loss = 0.48930899\n",
            "Iteration 74, loss = 0.48559771\n",
            "Iteration 75, loss = 0.48232745\n",
            "Iteration 76, loss = 0.48318431\n",
            "Iteration 77, loss = 0.47931129\n",
            "Iteration 78, loss = 0.49137207\n",
            "Iteration 79, loss = 0.47493517\n",
            "Iteration 80, loss = 0.47680655\n",
            "Iteration 81, loss = 0.47661370\n",
            "Iteration 82, loss = 0.47324957\n",
            "Iteration 83, loss = 0.47066371\n",
            "Iteration 84, loss = 0.46325306\n",
            "Iteration 85, loss = 0.46360083\n",
            "Iteration 86, loss = 0.45774630\n",
            "Iteration 87, loss = 0.46148802\n",
            "Iteration 88, loss = 0.45524986\n",
            "Iteration 89, loss = 0.45292831\n",
            "Iteration 90, loss = 0.45348119\n",
            "Iteration 91, loss = 0.44830338\n",
            "Iteration 92, loss = 0.45027253\n",
            "Iteration 93, loss = 0.44135184\n",
            "Iteration 94, loss = 0.44548358\n",
            "Iteration 95, loss = 0.44250891\n",
            "Iteration 96, loss = 0.43482526\n",
            "Iteration 97, loss = 0.43684725\n",
            "Iteration 98, loss = 0.43635817\n",
            "Iteration 99, loss = 0.43226312\n",
            "Iteration 100, loss = 0.43055257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31332566\n",
            "Iteration 2, loss = 1.25909509\n",
            "Iteration 3, loss = 1.24266419\n",
            "Iteration 4, loss = 1.22715761\n",
            "Iteration 5, loss = 1.21742340\n",
            "Iteration 6, loss = 1.20280208\n",
            "Iteration 7, loss = 1.18867431\n",
            "Iteration 8, loss = 1.17239787\n",
            "Iteration 9, loss = 1.15455233\n",
            "Iteration 10, loss = 1.13408308\n",
            "Iteration 11, loss = 1.11465746\n",
            "Iteration 12, loss = 1.08615647\n",
            "Iteration 13, loss = 1.06109694\n",
            "Iteration 14, loss = 1.02876319\n",
            "Iteration 15, loss = 0.99916843\n",
            "Iteration 16, loss = 0.96229233\n",
            "Iteration 17, loss = 0.92765923\n",
            "Iteration 18, loss = 0.89346139\n",
            "Iteration 19, loss = 0.86068122\n",
            "Iteration 20, loss = 0.82747038\n",
            "Iteration 21, loss = 0.79776605\n",
            "Iteration 22, loss = 0.78199087\n",
            "Iteration 23, loss = 0.76080614\n",
            "Iteration 24, loss = 0.74192773\n",
            "Iteration 25, loss = 0.71845430\n",
            "Iteration 26, loss = 0.70121494\n",
            "Iteration 27, loss = 0.69044800\n",
            "Iteration 28, loss = 0.68280627\n",
            "Iteration 29, loss = 0.66834297\n",
            "Iteration 30, loss = 0.65798192\n",
            "Iteration 31, loss = 0.64909704\n",
            "Iteration 32, loss = 0.64092008\n",
            "Iteration 33, loss = 0.63679338\n",
            "Iteration 34, loss = 0.62730248\n",
            "Iteration 35, loss = 0.62387895\n",
            "Iteration 36, loss = 0.62345837\n",
            "Iteration 37, loss = 0.62367154\n",
            "Iteration 38, loss = 0.61791655\n",
            "Iteration 39, loss = 0.61183703\n",
            "Iteration 40, loss = 0.60856632\n",
            "Iteration 41, loss = 0.59799639\n",
            "Iteration 42, loss = 0.59714366\n",
            "Iteration 43, loss = 0.59245717\n",
            "Iteration 44, loss = 0.58069844\n",
            "Iteration 45, loss = 0.57726200\n",
            "Iteration 46, loss = 0.57589407\n",
            "Iteration 47, loss = 0.57367059\n",
            "Iteration 48, loss = 0.56847776\n",
            "Iteration 49, loss = 0.56918919\n",
            "Iteration 50, loss = 0.55760933\n",
            "Iteration 51, loss = 0.55830911\n",
            "Iteration 52, loss = 0.55293842\n",
            "Iteration 53, loss = 0.54262501\n",
            "Iteration 54, loss = 0.54455975\n",
            "Iteration 55, loss = 0.54488682\n",
            "Iteration 56, loss = 0.53305931\n",
            "Iteration 57, loss = 0.52925643\n",
            "Iteration 58, loss = 0.52609462\n",
            "Iteration 59, loss = 0.52420451\n",
            "Iteration 60, loss = 0.52530734\n",
            "Iteration 61, loss = 0.53124230\n",
            "Iteration 62, loss = 0.51170952\n",
            "Iteration 63, loss = 0.51794375\n",
            "Iteration 64, loss = 0.50622544\n",
            "Iteration 65, loss = 0.51095868\n",
            "Iteration 66, loss = 0.50195894\n",
            "Iteration 67, loss = 0.49625253\n",
            "Iteration 68, loss = 0.49164507\n",
            "Iteration 69, loss = 0.48893253\n",
            "Iteration 70, loss = 0.48917671\n",
            "Iteration 71, loss = 0.49472587\n",
            "Iteration 72, loss = 0.49955414\n",
            "Iteration 73, loss = 0.48442524\n",
            "Iteration 74, loss = 0.47739949\n",
            "Iteration 75, loss = 0.47414528\n",
            "Iteration 76, loss = 0.47126053\n",
            "Iteration 77, loss = 0.46488965\n",
            "Iteration 78, loss = 0.47720880\n",
            "Iteration 79, loss = 0.46488556\n",
            "Iteration 80, loss = 0.45970827\n",
            "Iteration 81, loss = 0.45764041\n",
            "Iteration 82, loss = 0.45572748\n",
            "Iteration 83, loss = 0.45709631\n",
            "Iteration 84, loss = 0.45824257\n",
            "Iteration 85, loss = 0.44910609\n",
            "Iteration 86, loss = 0.44587947\n",
            "Iteration 87, loss = 0.44430138\n",
            "Iteration 88, loss = 0.44370679\n",
            "Iteration 89, loss = 0.44305199\n",
            "Iteration 90, loss = 0.43951192\n",
            "Iteration 91, loss = 0.43604075\n",
            "Iteration 92, loss = 0.44223370\n",
            "Iteration 93, loss = 0.42904716\n",
            "Iteration 94, loss = 0.43705589\n",
            "Iteration 95, loss = 0.43297197\n",
            "Iteration 96, loss = 0.42259061\n",
            "Iteration 97, loss = 0.42800094\n",
            "Iteration 98, loss = 0.42707750\n",
            "Iteration 99, loss = 0.41977186\n",
            "Iteration 100, loss = 0.42094475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31341627\n",
            "Iteration 2, loss = 1.26335238\n",
            "Iteration 3, loss = 1.24630785\n",
            "Iteration 4, loss = 1.22953694\n",
            "Iteration 5, loss = 1.21890351\n",
            "Iteration 6, loss = 1.20540049\n",
            "Iteration 7, loss = 1.19424635\n",
            "Iteration 8, loss = 1.17670511\n",
            "Iteration 9, loss = 1.15932641\n",
            "Iteration 10, loss = 1.13828391\n",
            "Iteration 11, loss = 1.11666027\n",
            "Iteration 12, loss = 1.09109976\n",
            "Iteration 13, loss = 1.06550759\n",
            "Iteration 14, loss = 1.03223068\n",
            "Iteration 15, loss = 0.99987983\n",
            "Iteration 16, loss = 0.96485000\n",
            "Iteration 17, loss = 0.93108649\n",
            "Iteration 18, loss = 0.89592193\n",
            "Iteration 19, loss = 0.86256233\n",
            "Iteration 20, loss = 0.83061879\n",
            "Iteration 21, loss = 0.80043558\n",
            "Iteration 22, loss = 0.78875120\n",
            "Iteration 23, loss = 0.76681516\n",
            "Iteration 24, loss = 0.74634044\n",
            "Iteration 25, loss = 0.71967714\n",
            "Iteration 26, loss = 0.70571669\n",
            "Iteration 27, loss = 0.69635923\n",
            "Iteration 28, loss = 0.68747584\n",
            "Iteration 29, loss = 0.67214091\n",
            "Iteration 30, loss = 0.66490345\n",
            "Iteration 31, loss = 0.65447882\n",
            "Iteration 32, loss = 0.64473544\n",
            "Iteration 33, loss = 0.64340805\n",
            "Iteration 34, loss = 0.63203531\n",
            "Iteration 35, loss = 0.62923216\n",
            "Iteration 36, loss = 0.62614942\n",
            "Iteration 37, loss = 0.62734966\n",
            "Iteration 38, loss = 0.61931269\n",
            "Iteration 39, loss = 0.61014343\n",
            "Iteration 40, loss = 0.61485881\n",
            "Iteration 41, loss = 0.60690416\n",
            "Iteration 42, loss = 0.59970544\n",
            "Iteration 43, loss = 0.59847364\n",
            "Iteration 44, loss = 0.59042634\n",
            "Iteration 45, loss = 0.58742551\n",
            "Iteration 46, loss = 0.58918861\n",
            "Iteration 47, loss = 0.58565415\n",
            "Iteration 48, loss = 0.58065770\n",
            "Iteration 49, loss = 0.57488802\n",
            "Iteration 50, loss = 0.56929542\n",
            "Iteration 51, loss = 0.57020765\n",
            "Iteration 52, loss = 0.56316128\n",
            "Iteration 53, loss = 0.55265789\n",
            "Iteration 54, loss = 0.55817622\n",
            "Iteration 55, loss = 0.55266557\n",
            "Iteration 56, loss = 0.54333945\n",
            "Iteration 57, loss = 0.53912824\n",
            "Iteration 58, loss = 0.53779425\n",
            "Iteration 59, loss = 0.53560696\n",
            "Iteration 60, loss = 0.53344298\n",
            "Iteration 61, loss = 0.54368747\n",
            "Iteration 62, loss = 0.52878349\n",
            "Iteration 63, loss = 0.53196480\n",
            "Iteration 64, loss = 0.51819425\n",
            "Iteration 65, loss = 0.52060237\n",
            "Iteration 66, loss = 0.51525703\n",
            "Iteration 67, loss = 0.50703564\n",
            "Iteration 68, loss = 0.50332588\n",
            "Iteration 69, loss = 0.50337715\n",
            "Iteration 70, loss = 0.50531013\n",
            "Iteration 71, loss = 0.51354718\n",
            "Iteration 72, loss = 0.51203412\n",
            "Iteration 73, loss = 0.49552663\n",
            "Iteration 74, loss = 0.48854511\n",
            "Iteration 75, loss = 0.48071633\n",
            "Iteration 76, loss = 0.48212557\n",
            "Iteration 77, loss = 0.47518887\n",
            "Iteration 78, loss = 0.47971415\n",
            "Iteration 79, loss = 0.47330767\n",
            "Iteration 80, loss = 0.47062924\n",
            "Iteration 81, loss = 0.46658452\n",
            "Iteration 82, loss = 0.46313667\n",
            "Iteration 83, loss = 0.46074300\n",
            "Iteration 84, loss = 0.46098286\n",
            "Iteration 85, loss = 0.45672138\n",
            "Iteration 86, loss = 0.45458900\n",
            "Iteration 87, loss = 0.45180358\n",
            "Iteration 88, loss = 0.45043583\n",
            "Iteration 89, loss = 0.45101227\n",
            "Iteration 90, loss = 0.44418358\n",
            "Iteration 91, loss = 0.44032914\n",
            "Iteration 92, loss = 0.44887692\n",
            "Iteration 93, loss = 0.43872456\n",
            "Iteration 94, loss = 0.44062901\n",
            "Iteration 95, loss = 0.43795470\n",
            "Iteration 96, loss = 0.42714212\n",
            "Iteration 97, loss = 0.42790342\n",
            "Iteration 98, loss = 0.42928804\n",
            "Iteration 99, loss = 0.42490931\n",
            "Iteration 100, loss = 0.42749779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.31049764\n",
            "Iteration 2, loss = 1.25613651\n",
            "Iteration 3, loss = 1.23689870\n",
            "Iteration 4, loss = 1.22113458\n",
            "Iteration 5, loss = 1.21275492\n",
            "Iteration 6, loss = 1.19947935\n",
            "Iteration 7, loss = 1.18651969\n",
            "Iteration 8, loss = 1.17391633\n",
            "Iteration 9, loss = 1.15357589\n",
            "Iteration 10, loss = 1.13485656\n",
            "Iteration 11, loss = 1.11294615\n",
            "Iteration 12, loss = 1.08830844\n",
            "Iteration 13, loss = 1.06140773\n",
            "Iteration 14, loss = 1.02957641\n",
            "Iteration 15, loss = 0.99633262\n",
            "Iteration 16, loss = 0.96260518\n",
            "Iteration 17, loss = 0.92720867\n",
            "Iteration 18, loss = 0.89005419\n",
            "Iteration 19, loss = 0.85636898\n",
            "Iteration 20, loss = 0.82239853\n",
            "Iteration 21, loss = 0.79034832\n",
            "Iteration 22, loss = 0.76500822\n",
            "Iteration 23, loss = 0.74390758\n",
            "Iteration 24, loss = 0.72258482\n",
            "Iteration 25, loss = 0.70575508\n",
            "Iteration 26, loss = 0.68706491\n",
            "Iteration 27, loss = 0.67922765\n",
            "Iteration 28, loss = 0.67452383\n",
            "Iteration 29, loss = 0.65656729\n",
            "Iteration 30, loss = 0.65283641\n",
            "Iteration 31, loss = 0.63900315\n",
            "Iteration 32, loss = 0.62801178\n",
            "Iteration 33, loss = 0.62719639\n",
            "Iteration 34, loss = 0.61537196\n",
            "Iteration 35, loss = 0.61360327\n",
            "Iteration 36, loss = 0.60615646\n",
            "Iteration 37, loss = 0.60712706\n",
            "Iteration 38, loss = 0.60207583\n",
            "Iteration 39, loss = 0.59032619\n",
            "Iteration 40, loss = 0.59353093\n",
            "Iteration 41, loss = 0.58415336\n",
            "Iteration 42, loss = 0.58757238\n",
            "Iteration 43, loss = 0.58282175\n",
            "Iteration 44, loss = 0.57187733\n",
            "Iteration 45, loss = 0.57559459\n",
            "Iteration 46, loss = 0.58684619\n",
            "Iteration 47, loss = 0.57501901\n",
            "Iteration 48, loss = 0.56882233\n",
            "Iteration 49, loss = 0.56173408\n",
            "Iteration 50, loss = 0.55569157\n",
            "Iteration 51, loss = 0.55058985\n",
            "Iteration 52, loss = 0.54632741\n",
            "Iteration 53, loss = 0.53774390\n",
            "Iteration 54, loss = 0.54273066\n",
            "Iteration 55, loss = 0.53618643\n",
            "Iteration 56, loss = 0.52882500\n",
            "Iteration 57, loss = 0.52572680\n",
            "Iteration 58, loss = 0.52504078\n",
            "Iteration 59, loss = 0.51895641\n",
            "Iteration 60, loss = 0.51440718\n",
            "Iteration 61, loss = 0.52539105\n",
            "Iteration 62, loss = 0.50852548\n",
            "Iteration 63, loss = 0.51244144\n",
            "Iteration 64, loss = 0.50363122\n",
            "Iteration 65, loss = 0.50668018\n",
            "Iteration 66, loss = 0.49985825\n",
            "Iteration 67, loss = 0.49396474\n",
            "Iteration 68, loss = 0.49047259\n",
            "Iteration 69, loss = 0.48951341\n",
            "Iteration 70, loss = 0.49290203\n",
            "Iteration 71, loss = 0.49537610\n",
            "Iteration 72, loss = 0.49549031\n",
            "Iteration 73, loss = 0.48342059\n",
            "Iteration 74, loss = 0.47624748\n",
            "Iteration 75, loss = 0.46751228\n",
            "Iteration 76, loss = 0.46894707\n",
            "Iteration 77, loss = 0.46237108\n",
            "Iteration 78, loss = 0.46682871\n",
            "Iteration 79, loss = 0.46157978\n",
            "Iteration 80, loss = 0.45501507\n",
            "Iteration 81, loss = 0.45372303\n",
            "Iteration 82, loss = 0.44945543\n",
            "Iteration 83, loss = 0.44969043\n",
            "Iteration 84, loss = 0.44604661\n",
            "Iteration 85, loss = 0.44297631\n",
            "Iteration 86, loss = 0.44066885\n",
            "Iteration 87, loss = 0.44567077\n",
            "Iteration 88, loss = 0.44430024\n",
            "Iteration 89, loss = 0.44454360\n",
            "Iteration 90, loss = 0.43281049\n",
            "Iteration 91, loss = 0.42902746\n",
            "Iteration 92, loss = 0.44386645\n",
            "Iteration 93, loss = 0.42350382\n",
            "Iteration 94, loss = 0.43092232\n",
            "Iteration 95, loss = 0.42694063\n",
            "Iteration 96, loss = 0.41536846\n",
            "Iteration 97, loss = 0.41768175\n",
            "Iteration 98, loss = 0.41774945\n",
            "Iteration 99, loss = 0.41254548\n",
            "Iteration 100, loss = 0.41189600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30641459\n",
            "Iteration 2, loss = 1.24850360\n",
            "Iteration 3, loss = 1.22974228\n",
            "Iteration 4, loss = 1.21360779\n",
            "Iteration 5, loss = 1.20360860\n",
            "Iteration 6, loss = 1.19203617\n",
            "Iteration 7, loss = 1.17858601\n",
            "Iteration 8, loss = 1.16648110\n",
            "Iteration 9, loss = 1.14844859\n",
            "Iteration 10, loss = 1.13054122\n",
            "Iteration 11, loss = 1.10791975\n",
            "Iteration 12, loss = 1.08599183\n",
            "Iteration 13, loss = 1.05667857\n",
            "Iteration 14, loss = 1.02519800\n",
            "Iteration 15, loss = 0.99199473\n",
            "Iteration 16, loss = 0.95751482\n",
            "Iteration 17, loss = 0.92172012\n",
            "Iteration 18, loss = 0.88407380\n",
            "Iteration 19, loss = 0.84606305\n",
            "Iteration 20, loss = 0.81267763\n",
            "Iteration 21, loss = 0.77989040\n",
            "Iteration 22, loss = 0.76365642\n",
            "Iteration 23, loss = 0.73781960\n",
            "Iteration 24, loss = 0.71641513\n",
            "Iteration 25, loss = 0.69813613\n",
            "Iteration 26, loss = 0.68249892\n",
            "Iteration 27, loss = 0.67447618\n",
            "Iteration 28, loss = 0.66539412\n",
            "Iteration 29, loss = 0.65229147\n",
            "Iteration 30, loss = 0.64596295\n",
            "Iteration 31, loss = 0.63679751\n",
            "Iteration 32, loss = 0.63073055\n",
            "Iteration 33, loss = 0.62687687\n",
            "Iteration 34, loss = 0.61691534\n",
            "Iteration 35, loss = 0.61436729\n",
            "Iteration 36, loss = 0.60876605\n",
            "Iteration 37, loss = 0.61270063\n",
            "Iteration 38, loss = 0.60823392\n",
            "Iteration 39, loss = 0.59166209\n",
            "Iteration 40, loss = 0.59541470\n",
            "Iteration 41, loss = 0.58580362\n",
            "Iteration 42, loss = 0.59323331\n",
            "Iteration 43, loss = 0.58114001\n",
            "Iteration 44, loss = 0.57754854\n",
            "Iteration 45, loss = 0.58055269\n",
            "Iteration 46, loss = 0.58387386\n",
            "Iteration 47, loss = 0.57116101\n",
            "Iteration 48, loss = 0.56130227\n",
            "Iteration 49, loss = 0.56004822\n",
            "Iteration 50, loss = 0.55269839\n",
            "Iteration 51, loss = 0.54819342\n",
            "Iteration 52, loss = 0.54570428\n",
            "Iteration 53, loss = 0.53859805\n",
            "Iteration 54, loss = 0.54105436\n",
            "Iteration 55, loss = 0.53519688\n",
            "Iteration 56, loss = 0.53025473\n",
            "Iteration 57, loss = 0.52405497\n",
            "Iteration 58, loss = 0.52517997\n",
            "Iteration 59, loss = 0.51839460\n",
            "Iteration 60, loss = 0.51655033\n",
            "Iteration 61, loss = 0.51712164\n",
            "Iteration 62, loss = 0.50817652\n",
            "Iteration 63, loss = 0.51057527\n",
            "Iteration 64, loss = 0.50659177\n",
            "Iteration 65, loss = 0.50801590\n",
            "Iteration 66, loss = 0.49813223\n",
            "Iteration 67, loss = 0.49696176\n",
            "Iteration 68, loss = 0.49202115\n",
            "Iteration 69, loss = 0.49391386\n",
            "Iteration 70, loss = 0.48927371\n",
            "Iteration 71, loss = 0.48703225\n",
            "Iteration 72, loss = 0.48269346\n",
            "Iteration 73, loss = 0.47386370\n",
            "Iteration 74, loss = 0.47310900\n",
            "Iteration 75, loss = 0.46733658\n",
            "Iteration 76, loss = 0.46643867\n",
            "Iteration 77, loss = 0.46314561\n",
            "Iteration 78, loss = 0.46383379\n",
            "Iteration 79, loss = 0.45975369\n",
            "Iteration 80, loss = 0.45428470\n",
            "Iteration 81, loss = 0.44953683\n",
            "Iteration 82, loss = 0.44823428\n",
            "Iteration 83, loss = 0.44538094\n",
            "Iteration 84, loss = 0.44549100\n",
            "Iteration 85, loss = 0.44397783\n",
            "Iteration 86, loss = 0.44084658\n",
            "Iteration 87, loss = 0.44173266\n",
            "Iteration 88, loss = 0.44334028\n",
            "Iteration 89, loss = 0.44003128\n",
            "Iteration 90, loss = 0.43106595\n",
            "Iteration 91, loss = 0.43230414\n",
            "Iteration 92, loss = 0.44448460\n",
            "Iteration 93, loss = 0.43087187\n",
            "Iteration 94, loss = 0.43242003\n",
            "Iteration 95, loss = 0.42644686\n",
            "Iteration 96, loss = 0.41966971\n",
            "Iteration 97, loss = 0.42195727\n",
            "Iteration 98, loss = 0.42321576\n",
            "Iteration 99, loss = 0.41946525\n",
            "Iteration 100, loss = 0.42330016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30741546\n",
            "Iteration 2, loss = 1.25409167\n",
            "Iteration 3, loss = 1.23550853\n",
            "Iteration 4, loss = 1.21909520\n",
            "Iteration 5, loss = 1.21099607\n",
            "Iteration 6, loss = 1.19836334\n",
            "Iteration 7, loss = 1.18433308\n",
            "Iteration 8, loss = 1.17169308\n",
            "Iteration 9, loss = 1.15476734\n",
            "Iteration 10, loss = 1.13474675\n",
            "Iteration 11, loss = 1.11332977\n",
            "Iteration 12, loss = 1.09198532\n",
            "Iteration 13, loss = 1.06390341\n",
            "Iteration 14, loss = 1.03297831\n",
            "Iteration 15, loss = 1.00042070\n",
            "Iteration 16, loss = 0.96756606\n",
            "Iteration 17, loss = 0.93195977\n",
            "Iteration 18, loss = 0.90031950\n",
            "Iteration 19, loss = 0.86337233\n",
            "Iteration 20, loss = 0.83466967\n",
            "Iteration 21, loss = 0.80094949\n",
            "Iteration 22, loss = 0.77768676\n",
            "Iteration 23, loss = 0.75628768\n",
            "Iteration 24, loss = 0.73161998\n",
            "Iteration 25, loss = 0.71104367\n",
            "Iteration 26, loss = 0.69568636\n",
            "Iteration 27, loss = 0.68481908\n",
            "Iteration 28, loss = 0.67512163\n",
            "Iteration 29, loss = 0.65890595\n",
            "Iteration 30, loss = 0.65351046\n",
            "Iteration 31, loss = 0.64439386\n",
            "Iteration 32, loss = 0.63914750\n",
            "Iteration 33, loss = 0.63067569\n",
            "Iteration 34, loss = 0.62099296\n",
            "Iteration 35, loss = 0.61899019\n",
            "Iteration 36, loss = 0.61367127\n",
            "Iteration 37, loss = 0.61252196\n",
            "Iteration 38, loss = 0.60559794\n",
            "Iteration 39, loss = 0.59630440\n",
            "Iteration 40, loss = 0.59014775\n",
            "Iteration 41, loss = 0.58680570\n",
            "Iteration 42, loss = 0.58222855\n",
            "Iteration 43, loss = 0.57863759\n",
            "Iteration 44, loss = 0.58724888\n",
            "Iteration 45, loss = 0.60047779\n",
            "Iteration 46, loss = 0.60274447\n",
            "Iteration 47, loss = 0.57970455\n",
            "Iteration 48, loss = 0.56107102\n",
            "Iteration 49, loss = 0.55899172\n",
            "Iteration 50, loss = 0.55177458\n",
            "Iteration 51, loss = 0.54112860\n",
            "Iteration 52, loss = 0.53920316\n",
            "Iteration 53, loss = 0.53296426\n",
            "Iteration 54, loss = 0.52940708\n",
            "Iteration 55, loss = 0.52489216\n",
            "Iteration 56, loss = 0.52124194\n",
            "Iteration 57, loss = 0.51587790\n",
            "Iteration 58, loss = 0.51449458\n",
            "Iteration 59, loss = 0.50658804\n",
            "Iteration 60, loss = 0.50843733\n",
            "Iteration 61, loss = 0.50862080\n",
            "Iteration 62, loss = 0.49622485\n",
            "Iteration 63, loss = 0.50065906\n",
            "Iteration 64, loss = 0.49494261\n",
            "Iteration 65, loss = 0.50235158\n",
            "Iteration 66, loss = 0.49038061\n",
            "Iteration 67, loss = 0.48233041\n",
            "Iteration 68, loss = 0.48012795\n",
            "Iteration 69, loss = 0.48350539\n",
            "Iteration 70, loss = 0.47569566\n",
            "Iteration 71, loss = 0.46946004\n",
            "Iteration 72, loss = 0.46456979\n",
            "Iteration 73, loss = 0.45697734\n",
            "Iteration 74, loss = 0.45688671\n",
            "Iteration 75, loss = 0.45057557\n",
            "Iteration 76, loss = 0.44876689\n",
            "Iteration 77, loss = 0.44490890\n",
            "Iteration 78, loss = 0.44021649\n",
            "Iteration 79, loss = 0.43856794\n",
            "Iteration 80, loss = 0.43738648\n",
            "Iteration 81, loss = 0.43122464\n",
            "Iteration 82, loss = 0.42892856\n",
            "Iteration 83, loss = 0.42581294\n",
            "Iteration 84, loss = 0.42541183\n",
            "Iteration 85, loss = 0.42139287\n",
            "Iteration 86, loss = 0.42070717\n",
            "Iteration 87, loss = 0.41867051\n",
            "Iteration 88, loss = 0.41885738\n",
            "Iteration 89, loss = 0.41493320\n",
            "Iteration 90, loss = 0.41270464\n",
            "Iteration 91, loss = 0.41496784\n",
            "Iteration 92, loss = 0.41998268\n",
            "Iteration 93, loss = 0.41011342\n",
            "Iteration 94, loss = 0.40589095\n",
            "Iteration 95, loss = 0.40435560\n",
            "Iteration 96, loss = 0.39938110\n",
            "Iteration 97, loss = 0.40172428\n",
            "Iteration 98, loss = 0.40292474\n",
            "Iteration 99, loss = 0.40285968\n",
            "Iteration 100, loss = 0.41470901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30588450\n",
            "Iteration 2, loss = 1.25513132\n",
            "Iteration 3, loss = 1.23561095\n",
            "Iteration 4, loss = 1.21908436\n",
            "Iteration 5, loss = 1.20719649\n",
            "Iteration 6, loss = 1.19381520\n",
            "Iteration 7, loss = 1.18453279\n",
            "Iteration 8, loss = 1.17171019\n",
            "Iteration 9, loss = 1.15359403\n",
            "Iteration 10, loss = 1.14014157\n",
            "Iteration 11, loss = 1.11942701\n",
            "Iteration 12, loss = 1.09684742\n",
            "Iteration 13, loss = 1.07116850\n",
            "Iteration 14, loss = 1.04352071\n",
            "Iteration 15, loss = 1.01123251\n",
            "Iteration 16, loss = 0.97783991\n",
            "Iteration 17, loss = 0.93838234\n",
            "Iteration 18, loss = 0.90004622\n",
            "Iteration 19, loss = 0.86230063\n",
            "Iteration 20, loss = 0.82788882\n",
            "Iteration 21, loss = 0.79756287\n",
            "Iteration 22, loss = 0.77159323\n",
            "Iteration 23, loss = 0.74681120\n",
            "Iteration 24, loss = 0.72293623\n",
            "Iteration 25, loss = 0.69214594\n",
            "Iteration 26, loss = 0.68089194\n",
            "Iteration 27, loss = 0.66504285\n",
            "Iteration 28, loss = 0.65764958\n",
            "Iteration 29, loss = 0.64939524\n",
            "Iteration 30, loss = 0.63711751\n",
            "Iteration 31, loss = 0.63541466\n",
            "Iteration 32, loss = 0.63598437\n",
            "Iteration 33, loss = 0.62142556\n",
            "Iteration 34, loss = 0.61503310\n",
            "Iteration 35, loss = 0.60838688\n",
            "Iteration 36, loss = 0.60736549\n",
            "Iteration 37, loss = 0.59685069\n",
            "Iteration 38, loss = 0.59177211\n",
            "Iteration 39, loss = 0.59595942\n",
            "Iteration 40, loss = 0.58766507\n",
            "Iteration 41, loss = 0.58739721\n",
            "Iteration 42, loss = 0.58679242\n",
            "Iteration 43, loss = 0.59555858\n",
            "Iteration 44, loss = 0.58416703\n",
            "Iteration 45, loss = 0.57818266\n",
            "Iteration 46, loss = 0.56711256\n",
            "Iteration 47, loss = 0.56592642\n",
            "Iteration 48, loss = 0.56616129\n",
            "Iteration 49, loss = 0.56004667\n",
            "Iteration 50, loss = 0.55310390\n",
            "Iteration 51, loss = 0.55463080\n",
            "Iteration 52, loss = 0.54863566\n",
            "Iteration 53, loss = 0.54840037\n",
            "Iteration 54, loss = 0.54428721\n",
            "Iteration 55, loss = 0.53764843\n",
            "Iteration 56, loss = 0.53670881\n",
            "Iteration 57, loss = 0.53300447\n",
            "Iteration 58, loss = 0.53933764\n",
            "Iteration 59, loss = 0.53980610\n",
            "Iteration 60, loss = 0.53119405\n",
            "Iteration 61, loss = 0.52165764\n",
            "Iteration 62, loss = 0.52340000\n",
            "Iteration 63, loss = 0.51789166\n",
            "Iteration 64, loss = 0.51636174\n",
            "Iteration 65, loss = 0.51167906\n",
            "Iteration 66, loss = 0.50743629\n",
            "Iteration 67, loss = 0.51058848\n",
            "Iteration 68, loss = 0.49974681\n",
            "Iteration 69, loss = 0.50044010\n",
            "Iteration 70, loss = 0.50468936\n",
            "Iteration 71, loss = 0.49398043\n",
            "Iteration 72, loss = 0.50110464\n",
            "Iteration 73, loss = 0.49170893\n",
            "Iteration 74, loss = 0.49099675\n",
            "Iteration 75, loss = 0.48310966\n",
            "Iteration 76, loss = 0.48706686\n",
            "Iteration 77, loss = 0.48440851\n",
            "Iteration 78, loss = 0.47842364\n",
            "Iteration 79, loss = 0.47597601\n",
            "Iteration 80, loss = 0.47388596\n",
            "Iteration 81, loss = 0.47949046\n",
            "Iteration 82, loss = 0.47400619\n",
            "Iteration 83, loss = 0.47209144\n",
            "Iteration 84, loss = 0.46560555\n",
            "Iteration 85, loss = 0.46870001\n",
            "Iteration 86, loss = 0.46859708\n",
            "Iteration 87, loss = 0.47105028\n",
            "Iteration 88, loss = 0.46682777\n",
            "Iteration 89, loss = 0.45709383\n",
            "Iteration 90, loss = 0.45858639\n",
            "Iteration 91, loss = 0.45306621\n",
            "Iteration 92, loss = 0.44863694\n",
            "Iteration 93, loss = 0.45516797\n",
            "Iteration 94, loss = 0.45122818\n",
            "Iteration 95, loss = 0.44552987\n",
            "Iteration 96, loss = 0.44933442\n",
            "Iteration 97, loss = 0.44107039\n",
            "Iteration 98, loss = 0.43878651\n",
            "Iteration 99, loss = 0.44403125\n",
            "Iteration 100, loss = 0.43788401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30635049\n",
            "Iteration 2, loss = 1.24850290\n",
            "Iteration 3, loss = 1.22839659\n",
            "Iteration 4, loss = 1.21225104\n",
            "Iteration 5, loss = 1.19822604\n",
            "Iteration 6, loss = 1.18543993\n",
            "Iteration 7, loss = 1.17153215\n",
            "Iteration 8, loss = 1.16106852\n",
            "Iteration 9, loss = 1.13925709\n",
            "Iteration 10, loss = 1.12528088\n",
            "Iteration 11, loss = 1.10290396\n",
            "Iteration 12, loss = 1.07854453\n",
            "Iteration 13, loss = 1.05125496\n",
            "Iteration 14, loss = 1.02264621\n",
            "Iteration 15, loss = 0.98978291\n",
            "Iteration 16, loss = 0.95567355\n",
            "Iteration 17, loss = 0.91535004\n",
            "Iteration 18, loss = 0.87765284\n",
            "Iteration 19, loss = 0.83574623\n",
            "Iteration 20, loss = 0.79775916\n",
            "Iteration 21, loss = 0.76406611\n",
            "Iteration 22, loss = 0.74535255\n",
            "Iteration 23, loss = 0.71092365\n",
            "Iteration 24, loss = 0.67753401\n",
            "Iteration 25, loss = 0.65485026\n",
            "Iteration 26, loss = 0.64146657\n",
            "Iteration 27, loss = 0.62502831\n",
            "Iteration 28, loss = 0.61293192\n",
            "Iteration 29, loss = 0.60897625\n",
            "Iteration 30, loss = 0.59464636\n",
            "Iteration 31, loss = 0.58314919\n",
            "Iteration 32, loss = 0.57770292\n",
            "Iteration 33, loss = 0.57150725\n",
            "Iteration 34, loss = 0.56431431\n",
            "Iteration 35, loss = 0.55926497\n",
            "Iteration 36, loss = 0.55430741\n",
            "Iteration 37, loss = 0.55084399\n",
            "Iteration 38, loss = 0.54208775\n",
            "Iteration 39, loss = 0.54524875\n",
            "Iteration 40, loss = 0.53879402\n",
            "Iteration 41, loss = 0.53553039\n",
            "Iteration 42, loss = 0.54153636\n",
            "Iteration 43, loss = 0.56066661\n",
            "Iteration 44, loss = 0.53131385\n",
            "Iteration 45, loss = 0.53368665\n",
            "Iteration 46, loss = 0.52424195\n",
            "Iteration 47, loss = 0.52026564\n",
            "Iteration 48, loss = 0.51804780\n",
            "Iteration 49, loss = 0.51839689\n",
            "Iteration 50, loss = 0.50994882\n",
            "Iteration 51, loss = 0.50750678\n",
            "Iteration 52, loss = 0.50825709\n",
            "Iteration 53, loss = 0.50739584\n",
            "Iteration 54, loss = 0.50207571\n",
            "Iteration 55, loss = 0.50101266\n",
            "Iteration 56, loss = 0.49783538\n",
            "Iteration 57, loss = 0.49970274\n",
            "Iteration 58, loss = 0.50394401\n",
            "Iteration 59, loss = 0.49568391\n",
            "Iteration 60, loss = 0.49076019\n",
            "Iteration 61, loss = 0.49112515\n",
            "Iteration 62, loss = 0.48333929\n",
            "Iteration 63, loss = 0.48341585\n",
            "Iteration 64, loss = 0.48613089\n",
            "Iteration 65, loss = 0.47956061\n",
            "Iteration 66, loss = 0.48032586\n",
            "Iteration 67, loss = 0.47729712\n",
            "Iteration 68, loss = 0.47135749\n",
            "Iteration 69, loss = 0.46958395\n",
            "Iteration 70, loss = 0.46729551\n",
            "Iteration 71, loss = 0.46803365\n",
            "Iteration 72, loss = 0.47197331\n",
            "Iteration 73, loss = 0.45927398\n",
            "Iteration 74, loss = 0.46415005\n",
            "Iteration 75, loss = 0.45560243\n",
            "Iteration 76, loss = 0.45530181\n",
            "Iteration 77, loss = 0.45870443\n",
            "Iteration 78, loss = 0.45092373\n",
            "Iteration 79, loss = 0.44898123\n",
            "Iteration 80, loss = 0.44604559\n",
            "Iteration 81, loss = 0.44409870\n",
            "Iteration 82, loss = 0.44516884\n",
            "Iteration 83, loss = 0.43956053\n",
            "Iteration 84, loss = 0.44163997\n",
            "Iteration 85, loss = 0.44073902\n",
            "Iteration 86, loss = 0.43405968\n",
            "Iteration 87, loss = 0.43120782\n",
            "Iteration 88, loss = 0.43299846\n",
            "Iteration 89, loss = 0.43146269\n",
            "Iteration 90, loss = 0.43224153\n",
            "Iteration 91, loss = 0.42583854\n",
            "Iteration 92, loss = 0.42426359\n",
            "Iteration 93, loss = 0.42323885\n",
            "Iteration 94, loss = 0.42704347\n",
            "Iteration 95, loss = 0.42456148\n",
            "Iteration 96, loss = 0.43063583\n",
            "Iteration 97, loss = 0.42381482\n",
            "Iteration 98, loss = 0.41714898\n",
            "Iteration 99, loss = 0.42531573\n",
            "Iteration 100, loss = 0.41472459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30459316\n",
            "Iteration 2, loss = 1.24462813\n",
            "Iteration 3, loss = 1.22349562\n",
            "Iteration 4, loss = 1.20431765\n",
            "Iteration 5, loss = 1.19084975\n",
            "Iteration 6, loss = 1.17832589\n",
            "Iteration 7, loss = 1.16391041\n",
            "Iteration 8, loss = 1.15155238\n",
            "Iteration 9, loss = 1.13245458\n",
            "Iteration 10, loss = 1.11762131\n",
            "Iteration 11, loss = 1.09676189\n",
            "Iteration 12, loss = 1.07161437\n",
            "Iteration 13, loss = 1.04548463\n",
            "Iteration 14, loss = 1.01593440\n",
            "Iteration 15, loss = 0.98493841\n",
            "Iteration 16, loss = 0.95296659\n",
            "Iteration 17, loss = 0.91415712\n",
            "Iteration 18, loss = 0.88168670\n",
            "Iteration 19, loss = 0.84691906\n",
            "Iteration 20, loss = 0.81125225\n",
            "Iteration 21, loss = 0.77739114\n",
            "Iteration 22, loss = 0.74849570\n",
            "Iteration 23, loss = 0.72601968\n",
            "Iteration 24, loss = 0.70604666\n",
            "Iteration 25, loss = 0.67762833\n",
            "Iteration 26, loss = 0.66975553\n",
            "Iteration 27, loss = 0.65981742\n",
            "Iteration 28, loss = 0.65878793\n",
            "Iteration 29, loss = 0.64155351\n",
            "Iteration 30, loss = 0.62968181\n",
            "Iteration 31, loss = 0.61768100\n",
            "Iteration 32, loss = 0.61155076\n",
            "Iteration 33, loss = 0.60684160\n",
            "Iteration 34, loss = 0.59957547\n",
            "Iteration 35, loss = 0.58815027\n",
            "Iteration 36, loss = 0.58382166\n",
            "Iteration 37, loss = 0.58031935\n",
            "Iteration 38, loss = 0.57107007\n",
            "Iteration 39, loss = 0.57620703\n",
            "Iteration 40, loss = 0.56513392\n",
            "Iteration 41, loss = 0.56313428\n",
            "Iteration 42, loss = 0.57401298\n",
            "Iteration 43, loss = 0.58052013\n",
            "Iteration 44, loss = 0.55757865\n",
            "Iteration 45, loss = 0.55782354\n",
            "Iteration 46, loss = 0.54727731\n",
            "Iteration 47, loss = 0.54200705\n",
            "Iteration 48, loss = 0.53723686\n",
            "Iteration 49, loss = 0.53588658\n",
            "Iteration 50, loss = 0.52801973\n",
            "Iteration 51, loss = 0.52442392\n",
            "Iteration 52, loss = 0.52341435\n",
            "Iteration 53, loss = 0.51916207\n",
            "Iteration 54, loss = 0.51273971\n",
            "Iteration 55, loss = 0.51176412\n",
            "Iteration 56, loss = 0.51008446\n",
            "Iteration 57, loss = 0.51285075\n",
            "Iteration 58, loss = 0.51006673\n",
            "Iteration 59, loss = 0.50223649\n",
            "Iteration 60, loss = 0.49574890\n",
            "Iteration 61, loss = 0.49506192\n",
            "Iteration 62, loss = 0.49345346\n",
            "Iteration 63, loss = 0.48788631\n",
            "Iteration 64, loss = 0.48751484\n",
            "Iteration 65, loss = 0.48044567\n",
            "Iteration 66, loss = 0.47734468\n",
            "Iteration 67, loss = 0.47713430\n",
            "Iteration 68, loss = 0.47015333\n",
            "Iteration 69, loss = 0.46756067\n",
            "Iteration 70, loss = 0.46934410\n",
            "Iteration 71, loss = 0.46321224\n",
            "Iteration 72, loss = 0.46114770\n",
            "Iteration 73, loss = 0.45658118\n",
            "Iteration 74, loss = 0.45597474\n",
            "Iteration 75, loss = 0.45141436\n",
            "Iteration 76, loss = 0.44578257\n",
            "Iteration 77, loss = 0.44734855\n",
            "Iteration 78, loss = 0.44119886\n",
            "Iteration 79, loss = 0.43684746\n",
            "Iteration 80, loss = 0.43358992\n",
            "Iteration 81, loss = 0.43296383\n",
            "Iteration 82, loss = 0.42977852\n",
            "Iteration 83, loss = 0.42822264\n",
            "Iteration 84, loss = 0.42515032\n",
            "Iteration 85, loss = 0.42384336\n",
            "Iteration 86, loss = 0.42226937\n",
            "Iteration 87, loss = 0.41661323\n",
            "Iteration 88, loss = 0.41636577\n",
            "Iteration 89, loss = 0.41778716\n",
            "Iteration 90, loss = 0.41732820\n",
            "Iteration 91, loss = 0.41150893\n",
            "Iteration 92, loss = 0.41008433\n",
            "Iteration 93, loss = 0.41477841\n",
            "Iteration 94, loss = 0.43015932\n",
            "Iteration 95, loss = 0.41855883\n",
            "Iteration 96, loss = 0.42257366\n",
            "Iteration 97, loss = 0.41707912\n",
            "Iteration 98, loss = 0.40232250\n",
            "Iteration 99, loss = 0.40473774\n",
            "Iteration 100, loss = 0.39612879\n",
            "23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.30166065\n",
            "Iteration 2, loss = 1.22947936\n",
            "Iteration 3, loss = 1.19058047\n",
            "Iteration 4, loss = 1.15283959\n",
            "Iteration 5, loss = 1.11352308\n",
            "Iteration 6, loss = 1.06764254\n",
            "Iteration 7, loss = 1.00756077\n",
            "Iteration 8, loss = 0.94667448\n",
            "Iteration 9, loss = 0.87459607\n",
            "Iteration 10, loss = 0.80075491\n",
            "Iteration 11, loss = 0.72732054\n",
            "Iteration 12, loss = 0.65494854\n",
            "Iteration 13, loss = 0.59405974\n",
            "Iteration 14, loss = 0.53587157\n",
            "Iteration 15, loss = 0.49472557\n",
            "Iteration 16, loss = 0.45198936\n",
            "Iteration 17, loss = 0.42386219\n",
            "Iteration 18, loss = 0.40004462\n",
            "Iteration 19, loss = 0.38932621\n",
            "Iteration 20, loss = 0.38168700\n",
            "Iteration 21, loss = 0.36882947\n",
            "Iteration 22, loss = 0.35880744\n",
            "Iteration 23, loss = 0.35160384\n",
            "Iteration 24, loss = 0.36001123\n",
            "Iteration 25, loss = 0.34237275\n",
            "Iteration 26, loss = 0.33979200\n",
            "Iteration 27, loss = 0.33795965\n",
            "Iteration 28, loss = 0.33443908\n",
            "Iteration 29, loss = 0.33505158\n",
            "Iteration 30, loss = 0.33417984\n",
            "Iteration 31, loss = 0.33692304\n",
            "Iteration 32, loss = 0.33255461\n",
            "Iteration 33, loss = 0.33391928\n",
            "Iteration 34, loss = 0.33500355\n",
            "Iteration 35, loss = 0.33170955\n",
            "Iteration 36, loss = 0.35262098\n",
            "Iteration 37, loss = 0.34150272\n",
            "Iteration 38, loss = 0.34314743\n",
            "Iteration 39, loss = 0.32800731\n",
            "Iteration 40, loss = 0.33172258\n",
            "Iteration 41, loss = 0.33756502\n",
            "Iteration 42, loss = 0.32519616\n",
            "Iteration 43, loss = 0.32784567\n",
            "Iteration 44, loss = 0.34852474\n",
            "Iteration 45, loss = 0.33357888\n",
            "Iteration 46, loss = 0.33017785\n",
            "Iteration 47, loss = 0.32274609\n",
            "Iteration 48, loss = 0.32610379\n",
            "Iteration 49, loss = 0.32311528\n",
            "Iteration 50, loss = 0.32917540\n",
            "Iteration 51, loss = 0.32766037\n",
            "Iteration 52, loss = 0.32519486\n",
            "Iteration 53, loss = 0.32336810\n",
            "Iteration 54, loss = 0.32359881\n",
            "Iteration 55, loss = 0.32552154\n",
            "Iteration 56, loss = 0.32442274\n",
            "Iteration 57, loss = 0.32195642\n",
            "Iteration 58, loss = 0.32331405\n",
            "Iteration 59, loss = 0.32372119\n",
            "Iteration 60, loss = 0.33437455\n",
            "Iteration 61, loss = 0.32170683\n",
            "Iteration 62, loss = 0.33744787\n",
            "Iteration 63, loss = 0.32175177\n",
            "Iteration 64, loss = 0.32313999\n",
            "Iteration 65, loss = 0.32661533\n",
            "Iteration 66, loss = 0.32691279\n",
            "Iteration 67, loss = 0.32801073\n",
            "Iteration 68, loss = 0.33006175\n",
            "Iteration 69, loss = 0.33707060\n",
            "Iteration 70, loss = 0.32294807\n",
            "Iteration 71, loss = 0.33367778\n",
            "Iteration 72, loss = 0.33349653\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30150311\n",
            "Iteration 2, loss = 1.22744966\n",
            "Iteration 3, loss = 1.18839231\n",
            "Iteration 4, loss = 1.15126848\n",
            "Iteration 5, loss = 1.11113294\n",
            "Iteration 6, loss = 1.06339473\n",
            "Iteration 7, loss = 1.00410137\n",
            "Iteration 8, loss = 0.94103558\n",
            "Iteration 9, loss = 0.87031851\n",
            "Iteration 10, loss = 0.79525508\n",
            "Iteration 11, loss = 0.72197146\n",
            "Iteration 12, loss = 0.64929110\n",
            "Iteration 13, loss = 0.59293925\n",
            "Iteration 14, loss = 0.53217425\n",
            "Iteration 15, loss = 0.49267625\n",
            "Iteration 16, loss = 0.45240672\n",
            "Iteration 17, loss = 0.42416338\n",
            "Iteration 18, loss = 0.40557310\n",
            "Iteration 19, loss = 0.38627405\n",
            "Iteration 20, loss = 0.38129501\n",
            "Iteration 21, loss = 0.36933640\n",
            "Iteration 22, loss = 0.36096967\n",
            "Iteration 23, loss = 0.35613445\n",
            "Iteration 24, loss = 0.35438163\n",
            "Iteration 25, loss = 0.35165163\n",
            "Iteration 26, loss = 0.34492553\n",
            "Iteration 27, loss = 0.34072555\n",
            "Iteration 28, loss = 0.33931263\n",
            "Iteration 29, loss = 0.34051105\n",
            "Iteration 30, loss = 0.34011097\n",
            "Iteration 31, loss = 0.34383077\n",
            "Iteration 32, loss = 0.34302318\n",
            "Iteration 33, loss = 0.34293938\n",
            "Iteration 34, loss = 0.33821515\n",
            "Iteration 35, loss = 0.34182079\n",
            "Iteration 36, loss = 0.36737320\n",
            "Iteration 37, loss = 0.35429450\n",
            "Iteration 38, loss = 0.35264585\n",
            "Iteration 39, loss = 0.33554999\n",
            "Iteration 40, loss = 0.34877778\n",
            "Iteration 41, loss = 0.34610255\n",
            "Iteration 42, loss = 0.33548706\n",
            "Iteration 43, loss = 0.33695090\n",
            "Iteration 44, loss = 0.34885338\n",
            "Iteration 45, loss = 0.33914447\n",
            "Iteration 46, loss = 0.33826001\n",
            "Iteration 47, loss = 0.33051879\n",
            "Iteration 48, loss = 0.33233774\n",
            "Iteration 49, loss = 0.33333899\n",
            "Iteration 50, loss = 0.33863656\n",
            "Iteration 51, loss = 0.33568299\n",
            "Iteration 52, loss = 0.33246964\n",
            "Iteration 53, loss = 0.33173846\n",
            "Iteration 54, loss = 0.33205236\n",
            "Iteration 55, loss = 0.33295486\n",
            "Iteration 56, loss = 0.32988799\n",
            "Iteration 57, loss = 0.33131755\n",
            "Iteration 58, loss = 0.32832061\n",
            "Iteration 59, loss = 0.33145600\n",
            "Iteration 60, loss = 0.33468247\n",
            "Iteration 61, loss = 0.32749729\n",
            "Iteration 62, loss = 0.33513165\n",
            "Iteration 63, loss = 0.32866053\n",
            "Iteration 64, loss = 0.33092045\n",
            "Iteration 65, loss = 0.33126810\n",
            "Iteration 66, loss = 0.33739148\n",
            "Iteration 67, loss = 0.34180905\n",
            "Iteration 68, loss = 0.34650088\n",
            "Iteration 69, loss = 0.34386208\n",
            "Iteration 70, loss = 0.33517401\n",
            "Iteration 71, loss = 0.33186049\n",
            "Iteration 72, loss = 0.34228551\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30414921\n",
            "Iteration 2, loss = 1.23166017\n",
            "Iteration 3, loss = 1.19351934\n",
            "Iteration 4, loss = 1.15715096\n",
            "Iteration 5, loss = 1.11501542\n",
            "Iteration 6, loss = 1.06557664\n",
            "Iteration 7, loss = 1.00438759\n",
            "Iteration 8, loss = 0.93656028\n",
            "Iteration 9, loss = 0.86430946\n",
            "Iteration 10, loss = 0.78902244\n",
            "Iteration 11, loss = 0.71433553\n",
            "Iteration 12, loss = 0.64519713\n",
            "Iteration 13, loss = 0.58592984\n",
            "Iteration 14, loss = 0.53334632\n",
            "Iteration 15, loss = 0.49699084\n",
            "Iteration 16, loss = 0.45562479\n",
            "Iteration 17, loss = 0.43201330\n",
            "Iteration 18, loss = 0.41684599\n",
            "Iteration 19, loss = 0.40110978\n",
            "Iteration 20, loss = 0.38880795\n",
            "Iteration 21, loss = 0.38551445\n",
            "Iteration 22, loss = 0.37193444\n",
            "Iteration 23, loss = 0.37000792\n",
            "Iteration 24, loss = 0.36408924\n",
            "Iteration 25, loss = 0.36282909\n",
            "Iteration 26, loss = 0.35892708\n",
            "Iteration 27, loss = 0.35740108\n",
            "Iteration 28, loss = 0.35597570\n",
            "Iteration 29, loss = 0.36049603\n",
            "Iteration 30, loss = 0.35648769\n",
            "Iteration 31, loss = 0.35469541\n",
            "Iteration 32, loss = 0.35714636\n",
            "Iteration 33, loss = 0.35339043\n",
            "Iteration 34, loss = 0.35269721\n",
            "Iteration 35, loss = 0.35558797\n",
            "Iteration 36, loss = 0.37443646\n",
            "Iteration 37, loss = 0.36135419\n",
            "Iteration 38, loss = 0.36685222\n",
            "Iteration 39, loss = 0.35106671\n",
            "Iteration 40, loss = 0.35923010\n",
            "Iteration 41, loss = 0.35439241\n",
            "Iteration 42, loss = 0.35103898\n",
            "Iteration 43, loss = 0.35115218\n",
            "Iteration 44, loss = 0.36061049\n",
            "Iteration 45, loss = 0.35124350\n",
            "Iteration 46, loss = 0.35198985\n",
            "Iteration 47, loss = 0.34963643\n",
            "Iteration 48, loss = 0.34773552\n",
            "Iteration 49, loss = 0.34661484\n",
            "Iteration 50, loss = 0.35617280\n",
            "Iteration 51, loss = 0.34884533\n",
            "Iteration 52, loss = 0.35044925\n",
            "Iteration 53, loss = 0.35182862\n",
            "Iteration 54, loss = 0.34873563\n",
            "Iteration 55, loss = 0.34697168\n",
            "Iteration 56, loss = 0.34843856\n",
            "Iteration 57, loss = 0.34789400\n",
            "Iteration 58, loss = 0.34605120\n",
            "Iteration 59, loss = 0.35199317\n",
            "Iteration 60, loss = 0.36091947\n",
            "Iteration 61, loss = 0.34187485\n",
            "Iteration 62, loss = 0.35544179\n",
            "Iteration 63, loss = 0.34332148\n",
            "Iteration 64, loss = 0.34784728\n",
            "Iteration 65, loss = 0.34625376\n",
            "Iteration 66, loss = 0.34560624\n",
            "Iteration 67, loss = 0.34707252\n",
            "Iteration 68, loss = 0.35103869\n",
            "Iteration 69, loss = 0.35230301\n",
            "Iteration 70, loss = 0.34634775\n",
            "Iteration 71, loss = 0.34739921\n",
            "Iteration 72, loss = 0.35364920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30437715\n",
            "Iteration 2, loss = 1.23600556\n",
            "Iteration 3, loss = 1.19787097\n",
            "Iteration 4, loss = 1.16031710\n",
            "Iteration 5, loss = 1.11770899\n",
            "Iteration 6, loss = 1.06688065\n",
            "Iteration 7, loss = 1.00796224\n",
            "Iteration 8, loss = 0.94101289\n",
            "Iteration 9, loss = 0.86835534\n",
            "Iteration 10, loss = 0.79252060\n",
            "Iteration 11, loss = 0.71523269\n",
            "Iteration 12, loss = 0.65116250\n",
            "Iteration 13, loss = 0.58976486\n",
            "Iteration 14, loss = 0.53700442\n",
            "Iteration 15, loss = 0.50146023\n",
            "Iteration 16, loss = 0.46374514\n",
            "Iteration 17, loss = 0.43964590\n",
            "Iteration 18, loss = 0.42121410\n",
            "Iteration 19, loss = 0.41063530\n",
            "Iteration 20, loss = 0.39343973\n",
            "Iteration 21, loss = 0.38828440\n",
            "Iteration 22, loss = 0.37905722\n",
            "Iteration 23, loss = 0.37587409\n",
            "Iteration 24, loss = 0.36934610\n",
            "Iteration 25, loss = 0.36824824\n",
            "Iteration 26, loss = 0.36539857\n",
            "Iteration 27, loss = 0.36375838\n",
            "Iteration 28, loss = 0.36507881\n",
            "Iteration 29, loss = 0.36339103\n",
            "Iteration 30, loss = 0.35900795\n",
            "Iteration 31, loss = 0.35767769\n",
            "Iteration 32, loss = 0.35895416\n",
            "Iteration 33, loss = 0.36043966\n",
            "Iteration 34, loss = 0.36895850\n",
            "Iteration 35, loss = 0.36646260\n",
            "Iteration 36, loss = 0.38702895\n",
            "Iteration 37, loss = 0.36246534\n",
            "Iteration 38, loss = 0.37784672\n",
            "Iteration 39, loss = 0.36159754\n",
            "Iteration 40, loss = 0.35978763\n",
            "Iteration 41, loss = 0.36058071\n",
            "Iteration 42, loss = 0.35569729\n",
            "Iteration 43, loss = 0.35815652\n",
            "Iteration 44, loss = 0.37314641\n",
            "Iteration 45, loss = 0.35895862\n",
            "Iteration 46, loss = 0.35970152\n",
            "Iteration 47, loss = 0.35565603\n",
            "Iteration 48, loss = 0.35494476\n",
            "Iteration 49, loss = 0.35305410\n",
            "Iteration 50, loss = 0.35482347\n",
            "Iteration 51, loss = 0.35455620\n",
            "Iteration 52, loss = 0.35453791\n",
            "Iteration 53, loss = 0.36371601\n",
            "Iteration 54, loss = 0.35607869\n",
            "Iteration 55, loss = 0.35558985\n",
            "Iteration 56, loss = 0.35505382\n",
            "Iteration 57, loss = 0.35214254\n",
            "Iteration 58, loss = 0.35010185\n",
            "Iteration 59, loss = 0.35560225\n",
            "Iteration 60, loss = 0.35946454\n",
            "Iteration 61, loss = 0.35015537\n",
            "Iteration 62, loss = 0.35518698\n",
            "Iteration 63, loss = 0.34990946\n",
            "Iteration 64, loss = 0.35916734\n",
            "Iteration 65, loss = 0.35355565\n",
            "Iteration 66, loss = 0.35489138\n",
            "Iteration 67, loss = 0.35504597\n",
            "Iteration 68, loss = 0.35439574\n",
            "Iteration 69, loss = 0.35988886\n",
            "Iteration 70, loss = 0.35152578\n",
            "Iteration 71, loss = 0.35453273\n",
            "Iteration 72, loss = 0.36142337\n",
            "Iteration 73, loss = 0.35594636\n",
            "Iteration 74, loss = 0.35645096\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30170604\n",
            "Iteration 2, loss = 1.22849724\n",
            "Iteration 3, loss = 1.18836490\n",
            "Iteration 4, loss = 1.15177521\n",
            "Iteration 5, loss = 1.11030077\n",
            "Iteration 6, loss = 1.06289019\n",
            "Iteration 7, loss = 1.00559340\n",
            "Iteration 8, loss = 0.94486687\n",
            "Iteration 9, loss = 0.87496522\n",
            "Iteration 10, loss = 0.80369976\n",
            "Iteration 11, loss = 0.72933998\n",
            "Iteration 12, loss = 0.66624400\n",
            "Iteration 13, loss = 0.60423785\n",
            "Iteration 14, loss = 0.55037177\n",
            "Iteration 15, loss = 0.50579233\n",
            "Iteration 16, loss = 0.47598208\n",
            "Iteration 17, loss = 0.44751658\n",
            "Iteration 18, loss = 0.43016847\n",
            "Iteration 19, loss = 0.41531341\n",
            "Iteration 20, loss = 0.40074613\n",
            "Iteration 21, loss = 0.39423168\n",
            "Iteration 22, loss = 0.38763965\n",
            "Iteration 23, loss = 0.38057093\n",
            "Iteration 24, loss = 0.37245335\n",
            "Iteration 25, loss = 0.37287572\n",
            "Iteration 26, loss = 0.37050633\n",
            "Iteration 27, loss = 0.36623038\n",
            "Iteration 28, loss = 0.36809392\n",
            "Iteration 29, loss = 0.36351912\n",
            "Iteration 30, loss = 0.36021672\n",
            "Iteration 31, loss = 0.35905174\n",
            "Iteration 32, loss = 0.36082371\n",
            "Iteration 33, loss = 0.36010380\n",
            "Iteration 34, loss = 0.36507906\n",
            "Iteration 35, loss = 0.36421152\n",
            "Iteration 36, loss = 0.36372652\n",
            "Iteration 37, loss = 0.35939208\n",
            "Iteration 38, loss = 0.36459339\n",
            "Iteration 39, loss = 0.35974538\n",
            "Iteration 40, loss = 0.35446751\n",
            "Iteration 41, loss = 0.35749815\n",
            "Iteration 42, loss = 0.35427493\n",
            "Iteration 43, loss = 0.35673548\n",
            "Iteration 44, loss = 0.36585390\n",
            "Iteration 45, loss = 0.36065837\n",
            "Iteration 46, loss = 0.35712083\n",
            "Iteration 47, loss = 0.35443405\n",
            "Iteration 48, loss = 0.35289632\n",
            "Iteration 49, loss = 0.35282226\n",
            "Iteration 50, loss = 0.35408144\n",
            "Iteration 51, loss = 0.35341807\n",
            "Iteration 52, loss = 0.35321676\n",
            "Iteration 53, loss = 0.36760693\n",
            "Iteration 54, loss = 0.35869701\n",
            "Iteration 55, loss = 0.36184849\n",
            "Iteration 56, loss = 0.35688614\n",
            "Iteration 57, loss = 0.35390072\n",
            "Iteration 58, loss = 0.34793551\n",
            "Iteration 59, loss = 0.35428483\n",
            "Iteration 60, loss = 0.35905576\n",
            "Iteration 61, loss = 0.34963382\n",
            "Iteration 62, loss = 0.35798668\n",
            "Iteration 63, loss = 0.34862064\n",
            "Iteration 64, loss = 0.35768668\n",
            "Iteration 65, loss = 0.35366296\n",
            "Iteration 66, loss = 0.35342708\n",
            "Iteration 67, loss = 0.35947616\n",
            "Iteration 68, loss = 0.35203784\n",
            "Iteration 69, loss = 0.36171816\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29740853\n",
            "Iteration 2, loss = 1.22121093\n",
            "Iteration 3, loss = 1.18152690\n",
            "Iteration 4, loss = 1.14563322\n",
            "Iteration 5, loss = 1.10289369\n",
            "Iteration 6, loss = 1.05707784\n",
            "Iteration 7, loss = 1.00044659\n",
            "Iteration 8, loss = 0.93963196\n",
            "Iteration 9, loss = 0.87040873\n",
            "Iteration 10, loss = 0.79872061\n",
            "Iteration 11, loss = 0.72476796\n",
            "Iteration 12, loss = 0.66249884\n",
            "Iteration 13, loss = 0.59519708\n",
            "Iteration 14, loss = 0.54317009\n",
            "Iteration 15, loss = 0.49620428\n",
            "Iteration 16, loss = 0.46512263\n",
            "Iteration 17, loss = 0.43495965\n",
            "Iteration 18, loss = 0.41815751\n",
            "Iteration 19, loss = 0.39928847\n",
            "Iteration 20, loss = 0.38664524\n",
            "Iteration 21, loss = 0.37960637\n",
            "Iteration 22, loss = 0.37432612\n",
            "Iteration 23, loss = 0.36414051\n",
            "Iteration 24, loss = 0.36178889\n",
            "Iteration 25, loss = 0.36549302\n",
            "Iteration 26, loss = 0.35879432\n",
            "Iteration 27, loss = 0.35220430\n",
            "Iteration 28, loss = 0.35671991\n",
            "Iteration 29, loss = 0.35026864\n",
            "Iteration 30, loss = 0.34717523\n",
            "Iteration 31, loss = 0.34694245\n",
            "Iteration 32, loss = 0.34762327\n",
            "Iteration 33, loss = 0.34684217\n",
            "Iteration 34, loss = 0.35888468\n",
            "Iteration 35, loss = 0.35620544\n",
            "Iteration 36, loss = 0.34986113\n",
            "Iteration 37, loss = 0.34848695\n",
            "Iteration 38, loss = 0.35488357\n",
            "Iteration 39, loss = 0.34758888\n",
            "Iteration 40, loss = 0.34240346\n",
            "Iteration 41, loss = 0.34464876\n",
            "Iteration 42, loss = 0.34359861\n",
            "Iteration 43, loss = 0.34716395\n",
            "Iteration 44, loss = 0.35652201\n",
            "Iteration 45, loss = 0.34592189\n",
            "Iteration 46, loss = 0.34564018\n",
            "Iteration 47, loss = 0.34421679\n",
            "Iteration 48, loss = 0.34180651\n",
            "Iteration 49, loss = 0.33980395\n",
            "Iteration 50, loss = 0.34167687\n",
            "Iteration 51, loss = 0.34196846\n",
            "Iteration 52, loss = 0.34179748\n",
            "Iteration 53, loss = 0.34709459\n",
            "Iteration 54, loss = 0.34050520\n",
            "Iteration 55, loss = 0.34533762\n",
            "Iteration 56, loss = 0.34267829\n",
            "Iteration 57, loss = 0.34208217\n",
            "Iteration 58, loss = 0.33615763\n",
            "Iteration 59, loss = 0.34260539\n",
            "Iteration 60, loss = 0.34549500\n",
            "Iteration 61, loss = 0.33583905\n",
            "Iteration 62, loss = 0.34658895\n",
            "Iteration 63, loss = 0.33739808\n",
            "Iteration 64, loss = 0.33848250\n",
            "Iteration 65, loss = 0.33864640\n",
            "Iteration 66, loss = 0.33978209\n",
            "Iteration 67, loss = 0.34256057\n",
            "Iteration 68, loss = 0.34103809\n",
            "Iteration 69, loss = 0.34354032\n",
            "Iteration 70, loss = 0.34022590\n",
            "Iteration 71, loss = 0.33980895\n",
            "Iteration 72, loss = 0.35221803\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29846924\n",
            "Iteration 2, loss = 1.22684004\n",
            "Iteration 3, loss = 1.18741730\n",
            "Iteration 4, loss = 1.14922376\n",
            "Iteration 5, loss = 1.10609085\n",
            "Iteration 6, loss = 1.05674643\n",
            "Iteration 7, loss = 0.99959512\n",
            "Iteration 8, loss = 0.93542225\n",
            "Iteration 9, loss = 0.86417745\n",
            "Iteration 10, loss = 0.78874431\n",
            "Iteration 11, loss = 0.71353008\n",
            "Iteration 12, loss = 0.65272879\n",
            "Iteration 13, loss = 0.58497837\n",
            "Iteration 14, loss = 0.53313367\n",
            "Iteration 15, loss = 0.48930099\n",
            "Iteration 16, loss = 0.46118002\n",
            "Iteration 17, loss = 0.43226960\n",
            "Iteration 18, loss = 0.41924155\n",
            "Iteration 19, loss = 0.40076898\n",
            "Iteration 20, loss = 0.38673730\n",
            "Iteration 21, loss = 0.38338248\n",
            "Iteration 22, loss = 0.37592128\n",
            "Iteration 23, loss = 0.36738917\n",
            "Iteration 24, loss = 0.36400736\n",
            "Iteration 25, loss = 0.36784802\n",
            "Iteration 26, loss = 0.36789241\n",
            "Iteration 27, loss = 0.36122009\n",
            "Iteration 28, loss = 0.36139963\n",
            "Iteration 29, loss = 0.35889138\n",
            "Iteration 30, loss = 0.35480925\n",
            "Iteration 31, loss = 0.35465605\n",
            "Iteration 32, loss = 0.35386613\n",
            "Iteration 33, loss = 0.35554943\n",
            "Iteration 34, loss = 0.36706900\n",
            "Iteration 35, loss = 0.35796903\n",
            "Iteration 36, loss = 0.35204981\n",
            "Iteration 37, loss = 0.35009076\n",
            "Iteration 38, loss = 0.35420709\n",
            "Iteration 39, loss = 0.34754937\n",
            "Iteration 40, loss = 0.34673248\n",
            "Iteration 41, loss = 0.34844472\n",
            "Iteration 42, loss = 0.34765558\n",
            "Iteration 43, loss = 0.34988474\n",
            "Iteration 44, loss = 0.36888097\n",
            "Iteration 45, loss = 0.35017459\n",
            "Iteration 46, loss = 0.35059570\n",
            "Iteration 47, loss = 0.35437094\n",
            "Iteration 48, loss = 0.35419415\n",
            "Iteration 49, loss = 0.34609378\n",
            "Iteration 50, loss = 0.34646375\n",
            "Iteration 51, loss = 0.34520745\n",
            "Iteration 52, loss = 0.34513230\n",
            "Iteration 53, loss = 0.34758959\n",
            "Iteration 54, loss = 0.34410291\n",
            "Iteration 55, loss = 0.34731942\n",
            "Iteration 56, loss = 0.34404859\n",
            "Iteration 57, loss = 0.34284811\n",
            "Iteration 58, loss = 0.34256527\n",
            "Iteration 59, loss = 0.34408042\n",
            "Iteration 60, loss = 0.34680885\n",
            "Iteration 61, loss = 0.34347475\n",
            "Iteration 62, loss = 0.34671752\n",
            "Iteration 63, loss = 0.34049257\n",
            "Iteration 64, loss = 0.35167008\n",
            "Iteration 65, loss = 0.34054270\n",
            "Iteration 66, loss = 0.34816924\n",
            "Iteration 67, loss = 0.35132621\n",
            "Iteration 68, loss = 0.34634850\n",
            "Iteration 69, loss = 0.34389632\n",
            "Iteration 70, loss = 0.34687335\n",
            "Iteration 71, loss = 0.34438981\n",
            "Iteration 72, loss = 0.34758439\n",
            "Iteration 73, loss = 0.35718935\n",
            "Iteration 74, loss = 0.35279440\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29842884\n",
            "Iteration 2, loss = 1.22938151\n",
            "Iteration 3, loss = 1.18990473\n",
            "Iteration 4, loss = 1.15632731\n",
            "Iteration 5, loss = 1.11565269\n",
            "Iteration 6, loss = 1.06515643\n",
            "Iteration 7, loss = 1.01155388\n",
            "Iteration 8, loss = 0.95182248\n",
            "Iteration 9, loss = 0.88242792\n",
            "Iteration 10, loss = 0.81083500\n",
            "Iteration 11, loss = 0.74452834\n",
            "Iteration 12, loss = 0.67313058\n",
            "Iteration 13, loss = 0.61303521\n",
            "Iteration 14, loss = 0.55860907\n",
            "Iteration 15, loss = 0.51267770\n",
            "Iteration 16, loss = 0.47808100\n",
            "Iteration 17, loss = 0.44643326\n",
            "Iteration 18, loss = 0.42394779\n",
            "Iteration 19, loss = 0.40620009\n",
            "Iteration 20, loss = 0.39459334\n",
            "Iteration 21, loss = 0.38958190\n",
            "Iteration 22, loss = 0.37873563\n",
            "Iteration 23, loss = 0.37274463\n",
            "Iteration 24, loss = 0.36982227\n",
            "Iteration 25, loss = 0.36428026\n",
            "Iteration 26, loss = 0.36440888\n",
            "Iteration 27, loss = 0.36096230\n",
            "Iteration 28, loss = 0.36165810\n",
            "Iteration 29, loss = 0.35876779\n",
            "Iteration 30, loss = 0.35452718\n",
            "Iteration 31, loss = 0.35501919\n",
            "Iteration 32, loss = 0.35270124\n",
            "Iteration 33, loss = 0.35582365\n",
            "Iteration 34, loss = 0.35162308\n",
            "Iteration 35, loss = 0.35344322\n",
            "Iteration 36, loss = 0.35131794\n",
            "Iteration 37, loss = 0.34971304\n",
            "Iteration 38, loss = 0.34825079\n",
            "Iteration 39, loss = 0.34689200\n",
            "Iteration 40, loss = 0.35962904\n",
            "Iteration 41, loss = 0.35036605\n",
            "Iteration 42, loss = 0.34986607\n",
            "Iteration 43, loss = 0.36247042\n",
            "Iteration 44, loss = 0.37094547\n",
            "Iteration 45, loss = 0.34884866\n",
            "Iteration 46, loss = 0.34802204\n",
            "Iteration 47, loss = 0.34508675\n",
            "Iteration 48, loss = 0.34750045\n",
            "Iteration 49, loss = 0.34648711\n",
            "Iteration 50, loss = 0.34585270\n",
            "Iteration 51, loss = 0.34496715\n",
            "Iteration 52, loss = 0.35152527\n",
            "Iteration 53, loss = 0.35379514\n",
            "Iteration 54, loss = 0.34475497\n",
            "Iteration 55, loss = 0.34960657\n",
            "Iteration 56, loss = 0.34751515\n",
            "Iteration 57, loss = 0.34700507\n",
            "Iteration 58, loss = 0.34864482\n",
            "Iteration 59, loss = 0.34709887\n",
            "Iteration 60, loss = 0.35045666\n",
            "Iteration 61, loss = 0.34516681\n",
            "Iteration 62, loss = 0.34050079\n",
            "Iteration 63, loss = 0.35180856\n",
            "Iteration 64, loss = 0.34967280\n",
            "Iteration 65, loss = 0.34631007\n",
            "Iteration 66, loss = 0.34585672\n",
            "Iteration 67, loss = 0.34736235\n",
            "Iteration 68, loss = 0.34426961\n",
            "Iteration 69, loss = 0.34413483\n",
            "Iteration 70, loss = 0.34106738\n",
            "Iteration 71, loss = 0.34048539\n",
            "Iteration 72, loss = 0.34218316\n",
            "Iteration 73, loss = 0.34349466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29958581\n",
            "Iteration 2, loss = 1.22609795\n",
            "Iteration 3, loss = 1.18802160\n",
            "Iteration 4, loss = 1.15651620\n",
            "Iteration 5, loss = 1.11797236\n",
            "Iteration 6, loss = 1.07409314\n",
            "Iteration 7, loss = 1.02270199\n",
            "Iteration 8, loss = 0.96803323\n",
            "Iteration 9, loss = 0.90213089\n",
            "Iteration 10, loss = 0.83848258\n",
            "Iteration 11, loss = 0.77558832\n",
            "Iteration 12, loss = 0.71011974\n",
            "Iteration 13, loss = 0.65163734\n",
            "Iteration 14, loss = 0.59826359\n",
            "Iteration 15, loss = 0.54827727\n",
            "Iteration 16, loss = 0.51256260\n",
            "Iteration 17, loss = 0.47566427\n",
            "Iteration 18, loss = 0.45121350\n",
            "Iteration 19, loss = 0.43060325\n",
            "Iteration 20, loss = 0.41797168\n",
            "Iteration 21, loss = 0.41003790\n",
            "Iteration 22, loss = 0.39902379\n",
            "Iteration 23, loss = 0.39320577\n",
            "Iteration 24, loss = 0.39166789\n",
            "Iteration 25, loss = 0.38383546\n",
            "Iteration 26, loss = 0.38229277\n",
            "Iteration 27, loss = 0.38176231\n",
            "Iteration 28, loss = 0.37761711\n",
            "Iteration 29, loss = 0.37760083\n",
            "Iteration 30, loss = 0.37260879\n",
            "Iteration 31, loss = 0.37474481\n",
            "Iteration 32, loss = 0.36845756\n",
            "Iteration 33, loss = 0.37251707\n",
            "Iteration 34, loss = 0.36779066\n",
            "Iteration 35, loss = 0.36742767\n",
            "Iteration 36, loss = 0.37042661\n",
            "Iteration 37, loss = 0.36940747\n",
            "Iteration 38, loss = 0.36684861\n",
            "Iteration 39, loss = 0.36768054\n",
            "Iteration 40, loss = 0.37831875\n",
            "Iteration 41, loss = 0.36941120\n",
            "Iteration 42, loss = 0.37051218\n",
            "Iteration 43, loss = 0.38153225\n",
            "Iteration 44, loss = 0.37954456\n",
            "Iteration 45, loss = 0.36524622\n",
            "Iteration 46, loss = 0.36950028\n",
            "Iteration 47, loss = 0.36390292\n",
            "Iteration 48, loss = 0.36574837\n",
            "Iteration 49, loss = 0.36738322\n",
            "Iteration 50, loss = 0.36784198\n",
            "Iteration 51, loss = 0.36248469\n",
            "Iteration 52, loss = 0.36821408\n",
            "Iteration 53, loss = 0.37766511\n",
            "Iteration 54, loss = 0.36299408\n",
            "Iteration 55, loss = 0.36905152\n",
            "Iteration 56, loss = 0.36867389\n",
            "Iteration 57, loss = 0.36378033\n",
            "Iteration 58, loss = 0.36626871\n",
            "Iteration 59, loss = 0.36435562\n",
            "Iteration 60, loss = 0.36751317\n",
            "Iteration 61, loss = 0.36434413\n",
            "Iteration 62, loss = 0.36002532\n",
            "Iteration 63, loss = 0.37164101\n",
            "Iteration 64, loss = 0.37109598\n",
            "Iteration 65, loss = 0.36902733\n",
            "Iteration 66, loss = 0.36306944\n",
            "Iteration 67, loss = 0.36391891\n",
            "Iteration 68, loss = 0.36476013\n",
            "Iteration 69, loss = 0.36475381\n",
            "Iteration 70, loss = 0.35909010\n",
            "Iteration 71, loss = 0.35986403\n",
            "Iteration 72, loss = 0.36168096\n",
            "Iteration 73, loss = 0.35938518\n",
            "Iteration 74, loss = 0.36354064\n",
            "Iteration 75, loss = 0.36029211\n",
            "Iteration 76, loss = 0.36304721\n",
            "Iteration 77, loss = 0.36231759\n",
            "Iteration 78, loss = 0.36081482\n",
            "Iteration 79, loss = 0.35980089\n",
            "Iteration 80, loss = 0.36046912\n",
            "Iteration 81, loss = 0.35936267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29671563\n",
            "Iteration 2, loss = 1.22195235\n",
            "Iteration 3, loss = 1.18091654\n",
            "Iteration 4, loss = 1.14469486\n",
            "Iteration 5, loss = 1.10363750\n",
            "Iteration 6, loss = 1.05736228\n",
            "Iteration 7, loss = 1.00292636\n",
            "Iteration 8, loss = 0.94391370\n",
            "Iteration 9, loss = 0.87698416\n",
            "Iteration 10, loss = 0.81150762\n",
            "Iteration 11, loss = 0.74754940\n",
            "Iteration 12, loss = 0.68420189\n",
            "Iteration 13, loss = 0.62615016\n",
            "Iteration 14, loss = 0.57522558\n",
            "Iteration 15, loss = 0.52900846\n",
            "Iteration 16, loss = 0.49597057\n",
            "Iteration 17, loss = 0.46463171\n",
            "Iteration 18, loss = 0.44215824\n",
            "Iteration 19, loss = 0.42618284\n",
            "Iteration 20, loss = 0.42227867\n",
            "Iteration 21, loss = 0.41541222\n",
            "Iteration 22, loss = 0.40151531\n",
            "Iteration 23, loss = 0.39201153\n",
            "Iteration 24, loss = 0.39502515\n",
            "Iteration 25, loss = 0.38549609\n",
            "Iteration 26, loss = 0.38599254\n",
            "Iteration 27, loss = 0.38290158\n",
            "Iteration 28, loss = 0.37761731\n",
            "Iteration 29, loss = 0.37929986\n",
            "Iteration 30, loss = 0.37409797\n",
            "Iteration 31, loss = 0.37516758\n",
            "Iteration 32, loss = 0.36823163\n",
            "Iteration 33, loss = 0.37375654\n",
            "Iteration 34, loss = 0.36882190\n",
            "Iteration 35, loss = 0.36785185\n",
            "Iteration 36, loss = 0.37551350\n",
            "Iteration 37, loss = 0.37136380\n",
            "Iteration 38, loss = 0.36743623\n",
            "Iteration 39, loss = 0.37153067\n",
            "Iteration 40, loss = 0.37379462\n",
            "Iteration 41, loss = 0.36416487\n",
            "Iteration 42, loss = 0.37305303\n",
            "Iteration 43, loss = 0.37790666\n",
            "Iteration 44, loss = 0.37358307\n",
            "Iteration 45, loss = 0.36650272\n",
            "Iteration 46, loss = 0.37371457\n",
            "Iteration 47, loss = 0.36580570\n",
            "Iteration 48, loss = 0.36706997\n",
            "Iteration 49, loss = 0.37073808\n",
            "Iteration 50, loss = 0.36591690\n",
            "Iteration 51, loss = 0.36265237\n",
            "Iteration 52, loss = 0.36594851\n",
            "Iteration 53, loss = 0.37440157\n",
            "Iteration 54, loss = 0.36571484\n",
            "Iteration 55, loss = 0.36745509\n",
            "Iteration 56, loss = 0.37201001\n",
            "Iteration 57, loss = 0.36234123\n",
            "Iteration 58, loss = 0.36838725\n",
            "Iteration 59, loss = 0.36275578\n",
            "Iteration 60, loss = 0.36976199\n",
            "Iteration 61, loss = 0.36410586\n",
            "Iteration 62, loss = 0.36069196\n",
            "Iteration 63, loss = 0.37563136\n",
            "Iteration 64, loss = 0.37231086\n",
            "Iteration 65, loss = 0.37180323\n",
            "Iteration 66, loss = 0.36513255\n",
            "Iteration 67, loss = 0.36590212\n",
            "Iteration 68, loss = 0.36846187\n",
            "Iteration 69, loss = 0.36630494\n",
            "Iteration 70, loss = 0.36147883\n",
            "Iteration 71, loss = 0.36082027\n",
            "Iteration 72, loss = 0.36191351\n",
            "Iteration 73, loss = 0.35973977\n",
            "Iteration 74, loss = 0.36588840\n",
            "Iteration 75, loss = 0.36174948\n",
            "Iteration 76, loss = 0.36428635\n",
            "Iteration 77, loss = 0.36395849\n",
            "Iteration 78, loss = 0.36189987\n",
            "Iteration 79, loss = 0.36057444\n",
            "Iteration 80, loss = 0.36067528\n",
            "Iteration 81, loss = 0.36028104\n",
            "Iteration 82, loss = 0.36415372\n",
            "Iteration 83, loss = 0.36101167\n",
            "Iteration 84, loss = 0.35781691\n",
            "Iteration 85, loss = 0.37755788\n",
            "Iteration 86, loss = 0.36550874\n",
            "Iteration 87, loss = 0.36767457\n",
            "Iteration 88, loss = 0.36806170\n",
            "Iteration 89, loss = 0.36133697\n",
            "Iteration 90, loss = 0.36772104\n",
            "Iteration 91, loss = 0.35898161\n",
            "Iteration 92, loss = 0.36067868\n",
            "Iteration 93, loss = 0.36251834\n",
            "Iteration 94, loss = 0.36227570\n",
            "Iteration 95, loss = 0.36069585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "24\n",
            "Iteration 1, loss = 1.31764489\n",
            "Iteration 2, loss = 1.24549511\n",
            "Iteration 3, loss = 1.20990916\n",
            "Iteration 4, loss = 1.17539452\n",
            "Iteration 5, loss = 1.13994356\n",
            "Iteration 6, loss = 1.10284808\n",
            "Iteration 7, loss = 1.05357292\n",
            "Iteration 8, loss = 1.00303030\n",
            "Iteration 9, loss = 0.93960350\n",
            "Iteration 10, loss = 0.87297409\n",
            "Iteration 11, loss = 0.80598139\n",
            "Iteration 12, loss = 0.74027494\n",
            "Iteration 13, loss = 0.68142596\n",
            "Iteration 14, loss = 0.62607743\n",
            "Iteration 15, loss = 0.58405112\n",
            "Iteration 16, loss = 0.54124465\n",
            "Iteration 17, loss = 0.51203044\n",
            "Iteration 18, loss = 0.48762512\n",
            "Iteration 19, loss = 0.47024020\n",
            "Iteration 20, loss = 0.46685219\n",
            "Iteration 21, loss = 0.44609264\n",
            "Iteration 22, loss = 0.43394846\n",
            "Iteration 23, loss = 0.42459434\n",
            "Iteration 24, loss = 0.43043516\n",
            "Iteration 25, loss = 0.42041342\n",
            "Iteration 26, loss = 0.40793091\n",
            "Iteration 27, loss = 0.40232017\n",
            "Iteration 28, loss = 0.40830133\n",
            "Iteration 29, loss = 0.40180668\n",
            "Iteration 30, loss = 0.39657937\n",
            "Iteration 31, loss = 0.40672005\n",
            "Iteration 32, loss = 0.39471575\n",
            "Iteration 33, loss = 0.39421916\n",
            "Iteration 34, loss = 0.40425567\n",
            "Iteration 35, loss = 0.38810209\n",
            "Iteration 36, loss = 0.39720300\n",
            "Iteration 37, loss = 0.38809038\n",
            "Iteration 38, loss = 0.38874380\n",
            "Iteration 39, loss = 0.38386997\n",
            "Iteration 40, loss = 0.37920682\n",
            "Iteration 41, loss = 0.38864185\n",
            "Iteration 42, loss = 0.38132246\n",
            "Iteration 43, loss = 0.37801829\n",
            "Iteration 44, loss = 0.39011579\n",
            "Iteration 45, loss = 0.38191760\n",
            "Iteration 46, loss = 0.37760235\n",
            "Iteration 47, loss = 0.37968972\n",
            "Iteration 48, loss = 0.37988935\n",
            "Iteration 49, loss = 0.37467714\n",
            "Iteration 50, loss = 0.38521982\n",
            "Iteration 51, loss = 0.38284943\n",
            "Iteration 52, loss = 0.38182864\n",
            "Iteration 53, loss = 0.37850166\n",
            "Iteration 54, loss = 0.37591431\n",
            "Iteration 55, loss = 0.38330783\n",
            "Iteration 56, loss = 0.38363237\n",
            "Iteration 57, loss = 0.38038009\n",
            "Iteration 58, loss = 0.38354550\n",
            "Iteration 59, loss = 0.37741559\n",
            "Iteration 60, loss = 0.38666048\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31765390\n",
            "Iteration 2, loss = 1.24342111\n",
            "Iteration 3, loss = 1.20791788\n",
            "Iteration 4, loss = 1.17460047\n",
            "Iteration 5, loss = 1.14052489\n",
            "Iteration 6, loss = 1.10324329\n",
            "Iteration 7, loss = 1.05544723\n",
            "Iteration 8, loss = 1.00231480\n",
            "Iteration 9, loss = 0.94179176\n",
            "Iteration 10, loss = 0.87215596\n",
            "Iteration 11, loss = 0.80784222\n",
            "Iteration 12, loss = 0.73772049\n",
            "Iteration 13, loss = 0.68280444\n",
            "Iteration 14, loss = 0.62201011\n",
            "Iteration 15, loss = 0.57983423\n",
            "Iteration 16, loss = 0.53981354\n",
            "Iteration 17, loss = 0.50919155\n",
            "Iteration 18, loss = 0.49070861\n",
            "Iteration 19, loss = 0.46726636\n",
            "Iteration 20, loss = 0.45646689\n",
            "Iteration 21, loss = 0.44457726\n",
            "Iteration 22, loss = 0.43209217\n",
            "Iteration 23, loss = 0.42622382\n",
            "Iteration 24, loss = 0.43027884\n",
            "Iteration 25, loss = 0.42449743\n",
            "Iteration 26, loss = 0.41221849\n",
            "Iteration 27, loss = 0.40501077\n",
            "Iteration 28, loss = 0.40550494\n",
            "Iteration 29, loss = 0.40402546\n",
            "Iteration 30, loss = 0.39664818\n",
            "Iteration 31, loss = 0.40764056\n",
            "Iteration 32, loss = 0.40331155\n",
            "Iteration 33, loss = 0.39537252\n",
            "Iteration 34, loss = 0.39791503\n",
            "Iteration 35, loss = 0.39171682\n",
            "Iteration 36, loss = 0.40346367\n",
            "Iteration 37, loss = 0.39883830\n",
            "Iteration 38, loss = 0.40098732\n",
            "Iteration 39, loss = 0.39219423\n",
            "Iteration 40, loss = 0.38940031\n",
            "Iteration 41, loss = 0.40413844\n",
            "Iteration 42, loss = 0.38810849\n",
            "Iteration 43, loss = 0.38757929\n",
            "Iteration 44, loss = 0.39342021\n",
            "Iteration 45, loss = 0.38658545\n",
            "Iteration 46, loss = 0.38840537\n",
            "Iteration 47, loss = 0.38819429\n",
            "Iteration 48, loss = 0.38953287\n",
            "Iteration 49, loss = 0.38590460\n",
            "Iteration 50, loss = 0.40020444\n",
            "Iteration 51, loss = 0.38926967\n",
            "Iteration 52, loss = 0.38945660\n",
            "Iteration 53, loss = 0.38421999\n",
            "Iteration 54, loss = 0.38419059\n",
            "Iteration 55, loss = 0.38803036\n",
            "Iteration 56, loss = 0.38385875\n",
            "Iteration 57, loss = 0.38350119\n",
            "Iteration 58, loss = 0.38483680\n",
            "Iteration 59, loss = 0.38444303\n",
            "Iteration 60, loss = 0.38999966\n",
            "Iteration 61, loss = 0.39088776\n",
            "Iteration 62, loss = 0.38462326\n",
            "Iteration 63, loss = 0.38438453\n",
            "Iteration 64, loss = 0.39126679\n",
            "Iteration 65, loss = 0.38952372\n",
            "Iteration 66, loss = 0.39004674\n",
            "Iteration 67, loss = 0.39888692\n",
            "Iteration 68, loss = 0.40153793\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32080166\n",
            "Iteration 2, loss = 1.24710271\n",
            "Iteration 3, loss = 1.21196644\n",
            "Iteration 4, loss = 1.17731301\n",
            "Iteration 5, loss = 1.14108200\n",
            "Iteration 6, loss = 1.10058973\n",
            "Iteration 7, loss = 1.04933388\n",
            "Iteration 8, loss = 0.99370427\n",
            "Iteration 9, loss = 0.93119271\n",
            "Iteration 10, loss = 0.86453228\n",
            "Iteration 11, loss = 0.79684224\n",
            "Iteration 12, loss = 0.72831734\n",
            "Iteration 13, loss = 0.67176327\n",
            "Iteration 14, loss = 0.61662284\n",
            "Iteration 15, loss = 0.57759844\n",
            "Iteration 16, loss = 0.53548065\n",
            "Iteration 17, loss = 0.51062309\n",
            "Iteration 18, loss = 0.49746154\n",
            "Iteration 19, loss = 0.47728856\n",
            "Iteration 20, loss = 0.45696035\n",
            "Iteration 21, loss = 0.45123989\n",
            "Iteration 22, loss = 0.43904274\n",
            "Iteration 23, loss = 0.43173036\n",
            "Iteration 24, loss = 0.41963155\n",
            "Iteration 25, loss = 0.41643247\n",
            "Iteration 26, loss = 0.41173787\n",
            "Iteration 27, loss = 0.40738426\n",
            "Iteration 28, loss = 0.40795963\n",
            "Iteration 29, loss = 0.40701852\n",
            "Iteration 30, loss = 0.39963680\n",
            "Iteration 31, loss = 0.40024124\n",
            "Iteration 32, loss = 0.40277539\n",
            "Iteration 33, loss = 0.39641501\n",
            "Iteration 34, loss = 0.39554691\n",
            "Iteration 35, loss = 0.39198349\n",
            "Iteration 36, loss = 0.40362570\n",
            "Iteration 37, loss = 0.39651943\n",
            "Iteration 38, loss = 0.40132211\n",
            "Iteration 39, loss = 0.40298184\n",
            "Iteration 40, loss = 0.38789510\n",
            "Iteration 41, loss = 0.39696697\n",
            "Iteration 42, loss = 0.39118719\n",
            "Iteration 43, loss = 0.38983356\n",
            "Iteration 44, loss = 0.39208527\n",
            "Iteration 45, loss = 0.39033279\n",
            "Iteration 46, loss = 0.38897066\n",
            "Iteration 47, loss = 0.39371675\n",
            "Iteration 48, loss = 0.39815869\n",
            "Iteration 49, loss = 0.38829385\n",
            "Iteration 50, loss = 0.39034148\n",
            "Iteration 51, loss = 0.39552782\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32211886\n",
            "Iteration 2, loss = 1.25187612\n",
            "Iteration 3, loss = 1.21631788\n",
            "Iteration 4, loss = 1.18134839\n",
            "Iteration 5, loss = 1.14500641\n",
            "Iteration 6, loss = 1.10334639\n",
            "Iteration 7, loss = 1.05313997\n",
            "Iteration 8, loss = 0.99648153\n",
            "Iteration 9, loss = 0.93289216\n",
            "Iteration 10, loss = 0.86679247\n",
            "Iteration 11, loss = 0.79445120\n",
            "Iteration 12, loss = 0.73304005\n",
            "Iteration 13, loss = 0.67417895\n",
            "Iteration 14, loss = 0.62098998\n",
            "Iteration 15, loss = 0.58267950\n",
            "Iteration 16, loss = 0.54166566\n",
            "Iteration 17, loss = 0.51775261\n",
            "Iteration 18, loss = 0.50200719\n",
            "Iteration 19, loss = 0.49117677\n",
            "Iteration 20, loss = 0.46168717\n",
            "Iteration 21, loss = 0.45421208\n",
            "Iteration 22, loss = 0.44848766\n",
            "Iteration 23, loss = 0.43681145\n",
            "Iteration 24, loss = 0.42711623\n",
            "Iteration 25, loss = 0.42201286\n",
            "Iteration 26, loss = 0.42142698\n",
            "Iteration 27, loss = 0.41410714\n",
            "Iteration 28, loss = 0.41507728\n",
            "Iteration 29, loss = 0.40777418\n",
            "Iteration 30, loss = 0.40538740\n",
            "Iteration 31, loss = 0.40513769\n",
            "Iteration 32, loss = 0.40405967\n",
            "Iteration 33, loss = 0.40327775\n",
            "Iteration 34, loss = 0.40749479\n",
            "Iteration 35, loss = 0.40449048\n",
            "Iteration 36, loss = 0.40819821\n",
            "Iteration 37, loss = 0.40469075\n",
            "Iteration 38, loss = 0.41156041\n",
            "Iteration 39, loss = 0.40227573\n",
            "Iteration 40, loss = 0.39706756\n",
            "Iteration 41, loss = 0.40280876\n",
            "Iteration 42, loss = 0.39623448\n",
            "Iteration 43, loss = 0.39523985\n",
            "Iteration 44, loss = 0.40176952\n",
            "Iteration 45, loss = 0.39657827\n",
            "Iteration 46, loss = 0.39640054\n",
            "Iteration 47, loss = 0.40249375\n",
            "Iteration 48, loss = 0.41025585\n",
            "Iteration 49, loss = 0.39849466\n",
            "Iteration 50, loss = 0.39345641\n",
            "Iteration 51, loss = 0.40917098\n",
            "Iteration 52, loss = 0.39734158\n",
            "Iteration 53, loss = 0.40896116\n",
            "Iteration 54, loss = 0.39508686\n",
            "Iteration 55, loss = 0.39540427\n",
            "Iteration 56, loss = 0.39308766\n",
            "Iteration 57, loss = 0.39059428\n",
            "Iteration 58, loss = 0.39425233\n",
            "Iteration 59, loss = 0.39075124\n",
            "Iteration 60, loss = 0.39424059\n",
            "Iteration 61, loss = 0.39480809\n",
            "Iteration 62, loss = 0.39738729\n",
            "Iteration 63, loss = 0.39235031\n",
            "Iteration 64, loss = 0.40775188\n",
            "Iteration 65, loss = 0.40526235\n",
            "Iteration 66, loss = 0.39192024\n",
            "Iteration 67, loss = 0.39726804\n",
            "Iteration 68, loss = 0.38919406\n",
            "Iteration 69, loss = 0.40067273\n",
            "Iteration 70, loss = 0.39282301\n",
            "Iteration 71, loss = 0.39382810\n",
            "Iteration 72, loss = 0.39241718\n",
            "Iteration 73, loss = 0.39190228\n",
            "Iteration 74, loss = 0.39202674\n",
            "Iteration 75, loss = 0.39163275\n",
            "Iteration 76, loss = 0.39589516\n",
            "Iteration 77, loss = 0.39627435\n",
            "Iteration 78, loss = 0.39099958\n",
            "Iteration 79, loss = 0.39404188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31831149\n",
            "Iteration 2, loss = 1.24449464\n",
            "Iteration 3, loss = 1.20766773\n",
            "Iteration 4, loss = 1.17441357\n",
            "Iteration 5, loss = 1.13949784\n",
            "Iteration 6, loss = 1.10246312\n",
            "Iteration 7, loss = 1.05474427\n",
            "Iteration 8, loss = 1.00527247\n",
            "Iteration 9, loss = 0.94388454\n",
            "Iteration 10, loss = 0.87905656\n",
            "Iteration 11, loss = 0.81062039\n",
            "Iteration 12, loss = 0.75155018\n",
            "Iteration 13, loss = 0.69458904\n",
            "Iteration 14, loss = 0.63642283\n",
            "Iteration 15, loss = 0.59027581\n",
            "Iteration 16, loss = 0.55496809\n",
            "Iteration 17, loss = 0.52491250\n",
            "Iteration 18, loss = 0.50584096\n",
            "Iteration 19, loss = 0.48812913\n",
            "Iteration 20, loss = 0.46395832\n",
            "Iteration 21, loss = 0.45735425\n",
            "Iteration 22, loss = 0.44999288\n",
            "Iteration 23, loss = 0.43748916\n",
            "Iteration 24, loss = 0.42586051\n",
            "Iteration 25, loss = 0.42281021\n",
            "Iteration 26, loss = 0.42287076\n",
            "Iteration 27, loss = 0.41679975\n",
            "Iteration 28, loss = 0.41561937\n",
            "Iteration 29, loss = 0.40915944\n",
            "Iteration 30, loss = 0.40735986\n",
            "Iteration 31, loss = 0.40633544\n",
            "Iteration 32, loss = 0.40627181\n",
            "Iteration 33, loss = 0.40852316\n",
            "Iteration 34, loss = 0.40269469\n",
            "Iteration 35, loss = 0.40713172\n",
            "Iteration 36, loss = 0.41212219\n",
            "Iteration 37, loss = 0.40371601\n",
            "Iteration 38, loss = 0.40814331\n",
            "Iteration 39, loss = 0.40079077\n",
            "Iteration 40, loss = 0.39542500\n",
            "Iteration 41, loss = 0.40111234\n",
            "Iteration 42, loss = 0.39812485\n",
            "Iteration 43, loss = 0.39703292\n",
            "Iteration 44, loss = 0.40960110\n",
            "Iteration 45, loss = 0.39977488\n",
            "Iteration 46, loss = 0.39887195\n",
            "Iteration 47, loss = 0.40294420\n",
            "Iteration 48, loss = 0.40940231\n",
            "Iteration 49, loss = 0.40078615\n",
            "Iteration 50, loss = 0.39394860\n",
            "Iteration 51, loss = 0.40670790\n",
            "Iteration 52, loss = 0.40089373\n",
            "Iteration 53, loss = 0.40785084\n",
            "Iteration 54, loss = 0.40090439\n",
            "Iteration 55, loss = 0.40096139\n",
            "Iteration 56, loss = 0.39666269\n",
            "Iteration 57, loss = 0.39339311\n",
            "Iteration 58, loss = 0.39543145\n",
            "Iteration 59, loss = 0.39176470\n",
            "Iteration 60, loss = 0.39341754\n",
            "Iteration 61, loss = 0.39566617\n",
            "Iteration 62, loss = 0.39883388\n",
            "Iteration 63, loss = 0.39397474\n",
            "Iteration 64, loss = 0.40114747\n",
            "Iteration 65, loss = 0.40593212\n",
            "Iteration 66, loss = 0.39335916\n",
            "Iteration 67, loss = 0.40215536\n",
            "Iteration 68, loss = 0.39135946\n",
            "Iteration 69, loss = 0.40604992\n",
            "Iteration 70, loss = 0.39720929\n",
            "Iteration 71, loss = 0.39761415\n",
            "Iteration 72, loss = 0.39894532\n",
            "Iteration 73, loss = 0.40208797\n",
            "Iteration 74, loss = 0.39830396\n",
            "Iteration 75, loss = 0.39884447\n",
            "Iteration 76, loss = 0.39866756\n",
            "Iteration 77, loss = 0.40093617\n",
            "Iteration 78, loss = 0.39674901\n",
            "Iteration 79, loss = 0.40113809\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31430181\n",
            "Iteration 2, loss = 1.23815786\n",
            "Iteration 3, loss = 1.20155011\n",
            "Iteration 4, loss = 1.16836371\n",
            "Iteration 5, loss = 1.13344918\n",
            "Iteration 6, loss = 1.09566407\n",
            "Iteration 7, loss = 1.04700161\n",
            "Iteration 8, loss = 0.99827491\n",
            "Iteration 9, loss = 0.93568848\n",
            "Iteration 10, loss = 0.87542073\n",
            "Iteration 11, loss = 0.80728233\n",
            "Iteration 12, loss = 0.74551816\n",
            "Iteration 13, loss = 0.67895724\n",
            "Iteration 14, loss = 0.62450734\n",
            "Iteration 15, loss = 0.57748651\n",
            "Iteration 16, loss = 0.53895963\n",
            "Iteration 17, loss = 0.50799849\n",
            "Iteration 18, loss = 0.48346908\n",
            "Iteration 19, loss = 0.46411374\n",
            "Iteration 20, loss = 0.45076054\n",
            "Iteration 21, loss = 0.44079649\n",
            "Iteration 22, loss = 0.43982852\n",
            "Iteration 23, loss = 0.42391967\n",
            "Iteration 24, loss = 0.41327853\n",
            "Iteration 25, loss = 0.41157641\n",
            "Iteration 26, loss = 0.40893377\n",
            "Iteration 27, loss = 0.40670208\n",
            "Iteration 28, loss = 0.40331088\n",
            "Iteration 29, loss = 0.39791517\n",
            "Iteration 30, loss = 0.39604596\n",
            "Iteration 31, loss = 0.39501652\n",
            "Iteration 32, loss = 0.39692168\n",
            "Iteration 33, loss = 0.39685787\n",
            "Iteration 34, loss = 0.39598797\n",
            "Iteration 35, loss = 0.39931692\n",
            "Iteration 36, loss = 0.40555993\n",
            "Iteration 37, loss = 0.41179019\n",
            "Iteration 38, loss = 0.40654268\n",
            "Iteration 39, loss = 0.39329800\n",
            "Iteration 40, loss = 0.39283348\n",
            "Iteration 41, loss = 0.38855122\n",
            "Iteration 42, loss = 0.38735259\n",
            "Iteration 43, loss = 0.39155238\n",
            "Iteration 44, loss = 0.39981699\n",
            "Iteration 45, loss = 0.38938770\n",
            "Iteration 46, loss = 0.39794026\n",
            "Iteration 47, loss = 0.39985932\n",
            "Iteration 48, loss = 0.39570590\n",
            "Iteration 49, loss = 0.38639146\n",
            "Iteration 50, loss = 0.38080152\n",
            "Iteration 51, loss = 0.38494992\n",
            "Iteration 52, loss = 0.38483863\n",
            "Iteration 53, loss = 0.39145665\n",
            "Iteration 54, loss = 0.38397303\n",
            "Iteration 55, loss = 0.38415815\n",
            "Iteration 56, loss = 0.38319769\n",
            "Iteration 57, loss = 0.38121194\n",
            "Iteration 58, loss = 0.38261474\n",
            "Iteration 59, loss = 0.38073748\n",
            "Iteration 60, loss = 0.38239654\n",
            "Iteration 61, loss = 0.38012614\n",
            "Iteration 62, loss = 0.38511784\n",
            "Iteration 63, loss = 0.37916183\n",
            "Iteration 64, loss = 0.38445121\n",
            "Iteration 65, loss = 0.38315787\n",
            "Iteration 66, loss = 0.38284925\n",
            "Iteration 67, loss = 0.38319381\n",
            "Iteration 68, loss = 0.37887052\n",
            "Iteration 69, loss = 0.38937980\n",
            "Iteration 70, loss = 0.38431376\n",
            "Iteration 71, loss = 0.38454624\n",
            "Iteration 72, loss = 0.38770498\n",
            "Iteration 73, loss = 0.38815483\n",
            "Iteration 74, loss = 0.39016201\n",
            "Iteration 75, loss = 0.38636875\n",
            "Iteration 76, loss = 0.38808135\n",
            "Iteration 77, loss = 0.38272119\n",
            "Iteration 78, loss = 0.37870817\n",
            "Iteration 79, loss = 0.38407764\n",
            "Iteration 80, loss = 0.38183499\n",
            "Iteration 81, loss = 0.38702731\n",
            "Iteration 82, loss = 0.38428069\n",
            "Iteration 83, loss = 0.38593098\n",
            "Iteration 84, loss = 0.38569307\n",
            "Iteration 85, loss = 0.38365075\n",
            "Iteration 86, loss = 0.37989468\n",
            "Iteration 87, loss = 0.40264098\n",
            "Iteration 88, loss = 0.38452065\n",
            "Iteration 89, loss = 0.38709159\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31581557\n",
            "Iteration 2, loss = 1.24462677\n",
            "Iteration 3, loss = 1.20732366\n",
            "Iteration 4, loss = 1.17382284\n",
            "Iteration 5, loss = 1.13969762\n",
            "Iteration 6, loss = 1.09805400\n",
            "Iteration 7, loss = 1.05186229\n",
            "Iteration 8, loss = 0.99891120\n",
            "Iteration 9, loss = 0.93490107\n",
            "Iteration 10, loss = 0.86782676\n",
            "Iteration 11, loss = 0.79535975\n",
            "Iteration 12, loss = 0.73137601\n",
            "Iteration 13, loss = 0.66367901\n",
            "Iteration 14, loss = 0.60961545\n",
            "Iteration 15, loss = 0.56804075\n",
            "Iteration 16, loss = 0.53365402\n",
            "Iteration 17, loss = 0.50265847\n",
            "Iteration 18, loss = 0.48290028\n",
            "Iteration 19, loss = 0.46452443\n",
            "Iteration 20, loss = 0.44909293\n",
            "Iteration 21, loss = 0.44176938\n",
            "Iteration 22, loss = 0.43947637\n",
            "Iteration 23, loss = 0.42489986\n",
            "Iteration 24, loss = 0.41601131\n",
            "Iteration 25, loss = 0.41249943\n",
            "Iteration 26, loss = 0.41084952\n",
            "Iteration 27, loss = 0.42006490\n",
            "Iteration 28, loss = 0.41236333\n",
            "Iteration 29, loss = 0.40506105\n",
            "Iteration 30, loss = 0.40556907\n",
            "Iteration 31, loss = 0.39830948\n",
            "Iteration 32, loss = 0.40197083\n",
            "Iteration 33, loss = 0.39654755\n",
            "Iteration 34, loss = 0.40355367\n",
            "Iteration 35, loss = 0.40146080\n",
            "Iteration 36, loss = 0.39497696\n",
            "Iteration 37, loss = 0.40254165\n",
            "Iteration 38, loss = 0.40200862\n",
            "Iteration 39, loss = 0.39119074\n",
            "Iteration 40, loss = 0.39259271\n",
            "Iteration 41, loss = 0.38998317\n",
            "Iteration 42, loss = 0.38917615\n",
            "Iteration 43, loss = 0.39239013\n",
            "Iteration 44, loss = 0.41044151\n",
            "Iteration 45, loss = 0.39505780\n",
            "Iteration 46, loss = 0.40028503\n",
            "Iteration 47, loss = 0.40833765\n",
            "Iteration 48, loss = 0.39618034\n",
            "Iteration 49, loss = 0.39065435\n",
            "Iteration 50, loss = 0.38498574\n",
            "Iteration 51, loss = 0.38793761\n",
            "Iteration 52, loss = 0.38657763\n",
            "Iteration 53, loss = 0.39460011\n",
            "Iteration 54, loss = 0.38560732\n",
            "Iteration 55, loss = 0.38904144\n",
            "Iteration 56, loss = 0.38641566\n",
            "Iteration 57, loss = 0.38287133\n",
            "Iteration 58, loss = 0.38527961\n",
            "Iteration 59, loss = 0.38267171\n",
            "Iteration 60, loss = 0.38417319\n",
            "Iteration 61, loss = 0.38244583\n",
            "Iteration 62, loss = 0.38510943\n",
            "Iteration 63, loss = 0.38292919\n",
            "Iteration 64, loss = 0.38689393\n",
            "Iteration 65, loss = 0.38221598\n",
            "Iteration 66, loss = 0.39086797\n",
            "Iteration 67, loss = 0.38668613\n",
            "Iteration 68, loss = 0.38649293\n",
            "Iteration 69, loss = 0.39071400\n",
            "Iteration 70, loss = 0.38946723\n",
            "Iteration 71, loss = 0.38929869\n",
            "Iteration 72, loss = 0.38984788\n",
            "Iteration 73, loss = 0.39105835\n",
            "Iteration 74, loss = 0.38919943\n",
            "Iteration 75, loss = 0.38217163\n",
            "Iteration 76, loss = 0.39836094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31301310\n",
            "Iteration 2, loss = 1.24508209\n",
            "Iteration 3, loss = 1.20845500\n",
            "Iteration 4, loss = 1.17822491\n",
            "Iteration 5, loss = 1.14571651\n",
            "Iteration 6, loss = 1.10945447\n",
            "Iteration 7, loss = 1.07054187\n",
            "Iteration 8, loss = 1.02234139\n",
            "Iteration 9, loss = 0.96625041\n",
            "Iteration 10, loss = 0.90401426\n",
            "Iteration 11, loss = 0.84210047\n",
            "Iteration 12, loss = 0.77545520\n",
            "Iteration 13, loss = 0.71676735\n",
            "Iteration 14, loss = 0.66238745\n",
            "Iteration 15, loss = 0.61145367\n",
            "Iteration 16, loss = 0.57183162\n",
            "Iteration 17, loss = 0.53452684\n",
            "Iteration 18, loss = 0.50639494\n",
            "Iteration 19, loss = 0.49010224\n",
            "Iteration 20, loss = 0.47108656\n",
            "Iteration 21, loss = 0.46078739\n",
            "Iteration 22, loss = 0.44740591\n",
            "Iteration 23, loss = 0.45602700\n",
            "Iteration 24, loss = 0.43658073\n",
            "Iteration 25, loss = 0.43024828\n",
            "Iteration 26, loss = 0.43276595\n",
            "Iteration 27, loss = 0.42466299\n",
            "Iteration 28, loss = 0.42685513\n",
            "Iteration 29, loss = 0.42150423\n",
            "Iteration 30, loss = 0.41379079\n",
            "Iteration 31, loss = 0.41283430\n",
            "Iteration 32, loss = 0.41436971\n",
            "Iteration 33, loss = 0.40733670\n",
            "Iteration 34, loss = 0.40785027\n",
            "Iteration 35, loss = 0.40696484\n",
            "Iteration 36, loss = 0.41454471\n",
            "Iteration 37, loss = 0.40118925\n",
            "Iteration 38, loss = 0.40782036\n",
            "Iteration 39, loss = 0.41338254\n",
            "Iteration 40, loss = 0.40203726\n",
            "Iteration 41, loss = 0.40310288\n",
            "Iteration 42, loss = 0.40425136\n",
            "Iteration 43, loss = 0.41959937\n",
            "Iteration 44, loss = 0.42046165\n",
            "Iteration 45, loss = 0.40383756\n",
            "Iteration 46, loss = 0.40153885\n",
            "Iteration 47, loss = 0.40160320\n",
            "Iteration 48, loss = 0.40613496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31340769\n",
            "Iteration 2, loss = 1.24052826\n",
            "Iteration 3, loss = 1.20518632\n",
            "Iteration 4, loss = 1.17636585\n",
            "Iteration 5, loss = 1.14398793\n",
            "Iteration 6, loss = 1.10878992\n",
            "Iteration 7, loss = 1.06883367\n",
            "Iteration 8, loss = 1.02338324\n",
            "Iteration 9, loss = 0.96808471\n",
            "Iteration 10, loss = 0.91043320\n",
            "Iteration 11, loss = 0.85312449\n",
            "Iteration 12, loss = 0.79304214\n",
            "Iteration 13, loss = 0.73559223\n",
            "Iteration 14, loss = 0.68500159\n",
            "Iteration 15, loss = 0.63239810\n",
            "Iteration 16, loss = 0.59505955\n",
            "Iteration 17, loss = 0.55457017\n",
            "Iteration 18, loss = 0.52528243\n",
            "Iteration 19, loss = 0.50240338\n",
            "Iteration 20, loss = 0.48548138\n",
            "Iteration 21, loss = 0.46935838\n",
            "Iteration 22, loss = 0.45717284\n",
            "Iteration 23, loss = 0.45700067\n",
            "Iteration 24, loss = 0.44453019\n",
            "Iteration 25, loss = 0.43957488\n",
            "Iteration 26, loss = 0.44291732\n",
            "Iteration 27, loss = 0.43391844\n",
            "Iteration 28, loss = 0.43144795\n",
            "Iteration 29, loss = 0.42856831\n",
            "Iteration 30, loss = 0.42063774\n",
            "Iteration 31, loss = 0.42429233\n",
            "Iteration 32, loss = 0.41262945\n",
            "Iteration 33, loss = 0.41281817\n",
            "Iteration 34, loss = 0.41446114\n",
            "Iteration 35, loss = 0.40928164\n",
            "Iteration 36, loss = 0.42013638\n",
            "Iteration 37, loss = 0.41192791\n",
            "Iteration 38, loss = 0.41101758\n",
            "Iteration 39, loss = 0.41611560\n",
            "Iteration 40, loss = 0.40757850\n",
            "Iteration 41, loss = 0.40703785\n",
            "Iteration 42, loss = 0.40829193\n",
            "Iteration 43, loss = 0.43149175\n",
            "Iteration 44, loss = 0.41842724\n",
            "Iteration 45, loss = 0.40779018\n",
            "Iteration 46, loss = 0.40540285\n",
            "Iteration 47, loss = 0.40603225\n",
            "Iteration 48, loss = 0.41426792\n",
            "Iteration 49, loss = 0.41378584\n",
            "Iteration 50, loss = 0.40498700\n",
            "Iteration 51, loss = 0.40171414\n",
            "Iteration 52, loss = 0.40570889\n",
            "Iteration 53, loss = 0.41580084\n",
            "Iteration 54, loss = 0.40001416\n",
            "Iteration 55, loss = 0.40507000\n",
            "Iteration 56, loss = 0.40850264\n",
            "Iteration 57, loss = 0.39968163\n",
            "Iteration 58, loss = 0.40720046\n",
            "Iteration 59, loss = 0.40388716\n",
            "Iteration 60, loss = 0.41036984\n",
            "Iteration 61, loss = 0.40357981\n",
            "Iteration 62, loss = 0.39931873\n",
            "Iteration 63, loss = 0.41586010\n",
            "Iteration 64, loss = 0.42241625\n",
            "Iteration 65, loss = 0.40494679\n",
            "Iteration 66, loss = 0.41140231\n",
            "Iteration 67, loss = 0.40368095\n",
            "Iteration 68, loss = 0.40162192\n",
            "Iteration 69, loss = 0.40720402\n",
            "Iteration 70, loss = 0.39819091\n",
            "Iteration 71, loss = 0.40024843\n",
            "Iteration 72, loss = 0.40975036\n",
            "Iteration 73, loss = 0.39805998\n",
            "Iteration 74, loss = 0.41406701\n",
            "Iteration 75, loss = 0.40413709\n",
            "Iteration 76, loss = 0.40233356\n",
            "Iteration 77, loss = 0.40632718\n",
            "Iteration 78, loss = 0.40448412\n",
            "Iteration 79, loss = 0.39963616\n",
            "Iteration 80, loss = 0.40052467\n",
            "Iteration 81, loss = 0.39956588\n",
            "Iteration 82, loss = 0.40172458\n",
            "Iteration 83, loss = 0.39871158\n",
            "Iteration 84, loss = 0.40006451\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31148630\n",
            "Iteration 2, loss = 1.23659961\n",
            "Iteration 3, loss = 1.19769481\n",
            "Iteration 4, loss = 1.16496358\n",
            "Iteration 5, loss = 1.13210211\n",
            "Iteration 6, loss = 1.09495283\n",
            "Iteration 7, loss = 1.05439609\n",
            "Iteration 8, loss = 1.00540342\n",
            "Iteration 9, loss = 0.95075345\n",
            "Iteration 10, loss = 0.88935491\n",
            "Iteration 11, loss = 0.82999789\n",
            "Iteration 12, loss = 0.77740900\n",
            "Iteration 13, loss = 0.71543857\n",
            "Iteration 14, loss = 0.67109146\n",
            "Iteration 15, loss = 0.61949107\n",
            "Iteration 16, loss = 0.58401015\n",
            "Iteration 17, loss = 0.54923995\n",
            "Iteration 18, loss = 0.52201767\n",
            "Iteration 19, loss = 0.50625021\n",
            "Iteration 20, loss = 0.49816892\n",
            "Iteration 21, loss = 0.47627679\n",
            "Iteration 22, loss = 0.46381406\n",
            "Iteration 23, loss = 0.46707238\n",
            "Iteration 24, loss = 0.45627780\n",
            "Iteration 25, loss = 0.44811269\n",
            "Iteration 26, loss = 0.44964831\n",
            "Iteration 27, loss = 0.43882986\n",
            "Iteration 28, loss = 0.44169191\n",
            "Iteration 29, loss = 0.43419482\n",
            "Iteration 30, loss = 0.42858710\n",
            "Iteration 31, loss = 0.42932966\n",
            "Iteration 32, loss = 0.41958234\n",
            "Iteration 33, loss = 0.42239152\n",
            "Iteration 34, loss = 0.42088383\n",
            "Iteration 35, loss = 0.41448002\n",
            "Iteration 36, loss = 0.42719273\n",
            "Iteration 37, loss = 0.42022451\n",
            "Iteration 38, loss = 0.41295158\n",
            "Iteration 39, loss = 0.41881767\n",
            "Iteration 40, loss = 0.41574928\n",
            "Iteration 41, loss = 0.41018513\n",
            "Iteration 42, loss = 0.42420362\n",
            "Iteration 43, loss = 0.43172582\n",
            "Iteration 44, loss = 0.41914677\n",
            "Iteration 45, loss = 0.41523276\n",
            "Iteration 46, loss = 0.41622413\n",
            "Iteration 47, loss = 0.41103058\n",
            "Iteration 48, loss = 0.41614506\n",
            "Iteration 49, loss = 0.42510482\n",
            "Iteration 50, loss = 0.41251913\n",
            "Iteration 51, loss = 0.40816623\n",
            "Iteration 52, loss = 0.41178685\n",
            "Iteration 53, loss = 0.42140659\n",
            "Iteration 54, loss = 0.40719115\n",
            "Iteration 55, loss = 0.41351030\n",
            "Iteration 56, loss = 0.41474115\n",
            "Iteration 57, loss = 0.40579869\n",
            "Iteration 58, loss = 0.41742095\n",
            "Iteration 59, loss = 0.40897478\n",
            "Iteration 60, loss = 0.41434323\n",
            "Iteration 61, loss = 0.41306001\n",
            "Iteration 62, loss = 0.41164827\n",
            "Iteration 63, loss = 0.41900440\n",
            "Iteration 64, loss = 0.43034604\n",
            "Iteration 65, loss = 0.41623832\n",
            "Iteration 66, loss = 0.41597774\n",
            "Iteration 67, loss = 0.41769747\n",
            "Iteration 68, loss = 0.41406512\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "25\n",
            "Iteration 1, loss = 1.30996129\n",
            "Iteration 2, loss = 1.23300613\n",
            "Iteration 3, loss = 1.19867920\n",
            "Iteration 4, loss = 1.15926520\n",
            "Iteration 5, loss = 1.12609826\n",
            "Iteration 6, loss = 1.08186970\n",
            "Iteration 7, loss = 1.03137819\n",
            "Iteration 8, loss = 0.97426122\n",
            "Iteration 9, loss = 0.90966390\n",
            "Iteration 10, loss = 0.83905555\n",
            "Iteration 11, loss = 0.77203933\n",
            "Iteration 12, loss = 0.70485083\n",
            "Iteration 13, loss = 0.64577704\n",
            "Iteration 14, loss = 0.59262449\n",
            "Iteration 15, loss = 0.54533264\n",
            "Iteration 16, loss = 0.50469077\n",
            "Iteration 17, loss = 0.47020641\n",
            "Iteration 18, loss = 0.44343043\n",
            "Iteration 19, loss = 0.42128210\n",
            "Iteration 20, loss = 0.40648737\n",
            "Iteration 21, loss = 0.39502378\n",
            "Iteration 22, loss = 0.38580333\n",
            "Iteration 23, loss = 0.38457369\n",
            "Iteration 24, loss = 0.37723043\n",
            "Iteration 25, loss = 0.37446490\n",
            "Iteration 26, loss = 0.36842832\n",
            "Iteration 27, loss = 0.36769974\n",
            "Iteration 28, loss = 0.36137214\n",
            "Iteration 29, loss = 0.36028998\n",
            "Iteration 30, loss = 0.35931473\n",
            "Iteration 31, loss = 0.35752670\n",
            "Iteration 32, loss = 0.35956023\n",
            "Iteration 33, loss = 0.35745590\n",
            "Iteration 34, loss = 0.35912900\n",
            "Iteration 35, loss = 0.35753094\n",
            "Iteration 36, loss = 0.35694821\n",
            "Iteration 37, loss = 0.35826504\n",
            "Iteration 38, loss = 0.35621708\n",
            "Iteration 39, loss = 0.35662880\n",
            "Iteration 40, loss = 0.35477943\n",
            "Iteration 41, loss = 0.36247711\n",
            "Iteration 42, loss = 0.36255340\n",
            "Iteration 43, loss = 0.35344391\n",
            "Iteration 44, loss = 0.35821714\n",
            "Iteration 45, loss = 0.35525081\n",
            "Iteration 46, loss = 0.35653229\n",
            "Iteration 47, loss = 0.34947232\n",
            "Iteration 48, loss = 0.35952001\n",
            "Iteration 49, loss = 0.35166770\n",
            "Iteration 50, loss = 0.35280948\n",
            "Iteration 51, loss = 0.35312760\n",
            "Iteration 52, loss = 0.34880181\n",
            "Iteration 53, loss = 0.34890655\n",
            "Iteration 54, loss = 0.34977222\n",
            "Iteration 55, loss = 0.35688470\n",
            "Iteration 56, loss = 0.35757026\n",
            "Iteration 57, loss = 0.34632144\n",
            "Iteration 58, loss = 0.36202990\n",
            "Iteration 59, loss = 0.34594518\n",
            "Iteration 60, loss = 0.34822356\n",
            "Iteration 61, loss = 0.35024275\n",
            "Iteration 62, loss = 0.35134708\n",
            "Iteration 63, loss = 0.34791213\n",
            "Iteration 64, loss = 0.36122696\n",
            "Iteration 65, loss = 0.35415902\n",
            "Iteration 66, loss = 0.35206076\n",
            "Iteration 67, loss = 0.34798959\n",
            "Iteration 68, loss = 0.34859234\n",
            "Iteration 69, loss = 0.34899585\n",
            "Iteration 70, loss = 0.34617873\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31022226\n",
            "Iteration 2, loss = 1.23146649\n",
            "Iteration 3, loss = 1.20084160\n",
            "Iteration 4, loss = 1.16648929\n",
            "Iteration 5, loss = 1.13298756\n",
            "Iteration 6, loss = 1.09225557\n",
            "Iteration 7, loss = 1.04480042\n",
            "Iteration 8, loss = 0.99011256\n",
            "Iteration 9, loss = 0.92968291\n",
            "Iteration 10, loss = 0.86312552\n",
            "Iteration 11, loss = 0.79863244\n",
            "Iteration 12, loss = 0.73461541\n",
            "Iteration 13, loss = 0.67829297\n",
            "Iteration 14, loss = 0.62163063\n",
            "Iteration 15, loss = 0.57237888\n",
            "Iteration 16, loss = 0.53178415\n",
            "Iteration 17, loss = 0.49423484\n",
            "Iteration 18, loss = 0.47026886\n",
            "Iteration 19, loss = 0.44525665\n",
            "Iteration 20, loss = 0.42639885\n",
            "Iteration 21, loss = 0.41653626\n",
            "Iteration 22, loss = 0.40766198\n",
            "Iteration 23, loss = 0.40555357\n",
            "Iteration 24, loss = 0.40164106\n",
            "Iteration 25, loss = 0.39818863\n",
            "Iteration 26, loss = 0.39718440\n",
            "Iteration 27, loss = 0.38874810\n",
            "Iteration 28, loss = 0.38844736\n",
            "Iteration 29, loss = 0.38769793\n",
            "Iteration 30, loss = 0.38272403\n",
            "Iteration 31, loss = 0.38147107\n",
            "Iteration 32, loss = 0.38215014\n",
            "Iteration 33, loss = 0.38060480\n",
            "Iteration 34, loss = 0.37960455\n",
            "Iteration 35, loss = 0.37976569\n",
            "Iteration 36, loss = 0.38169817\n",
            "Iteration 37, loss = 0.38924046\n",
            "Iteration 38, loss = 0.37940971\n",
            "Iteration 39, loss = 0.38461719\n",
            "Iteration 40, loss = 0.37741686\n",
            "Iteration 41, loss = 0.38898472\n",
            "Iteration 42, loss = 0.38072835\n",
            "Iteration 43, loss = 0.38637766\n",
            "Iteration 44, loss = 0.37779977\n",
            "Iteration 45, loss = 0.38552672\n",
            "Iteration 46, loss = 0.38219669\n",
            "Iteration 47, loss = 0.37746611\n",
            "Iteration 48, loss = 0.37558283\n",
            "Iteration 49, loss = 0.37979244\n",
            "Iteration 50, loss = 0.37761700\n",
            "Iteration 51, loss = 0.37538702\n",
            "Iteration 52, loss = 0.37170098\n",
            "Iteration 53, loss = 0.37173901\n",
            "Iteration 54, loss = 0.37171227\n",
            "Iteration 55, loss = 0.37787344\n",
            "Iteration 56, loss = 0.37178704\n",
            "Iteration 57, loss = 0.37164720\n",
            "Iteration 58, loss = 0.37588105\n",
            "Iteration 59, loss = 0.37148840\n",
            "Iteration 60, loss = 0.37039447\n",
            "Iteration 61, loss = 0.37396150\n",
            "Iteration 62, loss = 0.37454432\n",
            "Iteration 63, loss = 0.37269858\n",
            "Iteration 64, loss = 0.38432831\n",
            "Iteration 65, loss = 0.37488338\n",
            "Iteration 66, loss = 0.37382458\n",
            "Iteration 67, loss = 0.37295507\n",
            "Iteration 68, loss = 0.37822840\n",
            "Iteration 69, loss = 0.37101951\n",
            "Iteration 70, loss = 0.37284987\n",
            "Iteration 71, loss = 0.36957068\n",
            "Iteration 72, loss = 0.36928088\n",
            "Iteration 73, loss = 0.37356220\n",
            "Iteration 74, loss = 0.37273143\n",
            "Iteration 75, loss = 0.36600861\n",
            "Iteration 76, loss = 0.37162550\n",
            "Iteration 77, loss = 0.37889737\n",
            "Iteration 78, loss = 0.37204475\n",
            "Iteration 79, loss = 0.37607513\n",
            "Iteration 80, loss = 0.37070267\n",
            "Iteration 81, loss = 0.37752270\n",
            "Iteration 82, loss = 0.37389533\n",
            "Iteration 83, loss = 0.37494729\n",
            "Iteration 84, loss = 0.37969783\n",
            "Iteration 85, loss = 0.37705920\n",
            "Iteration 86, loss = 0.37914064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31237665\n",
            "Iteration 2, loss = 1.23501600\n",
            "Iteration 3, loss = 1.20249563\n",
            "Iteration 4, loss = 1.16447300\n",
            "Iteration 5, loss = 1.12756432\n",
            "Iteration 6, loss = 1.08134396\n",
            "Iteration 7, loss = 1.03041949\n",
            "Iteration 8, loss = 0.97007497\n",
            "Iteration 9, loss = 0.90697955\n",
            "Iteration 10, loss = 0.83871040\n",
            "Iteration 11, loss = 0.77097497\n",
            "Iteration 12, loss = 0.71131601\n",
            "Iteration 13, loss = 0.65197703\n",
            "Iteration 14, loss = 0.59924401\n",
            "Iteration 15, loss = 0.55567720\n",
            "Iteration 16, loss = 0.51961160\n",
            "Iteration 17, loss = 0.48513633\n",
            "Iteration 18, loss = 0.46417566\n",
            "Iteration 19, loss = 0.44888176\n",
            "Iteration 20, loss = 0.42320813\n",
            "Iteration 21, loss = 0.41592303\n",
            "Iteration 22, loss = 0.40019918\n",
            "Iteration 23, loss = 0.39655808\n",
            "Iteration 24, loss = 0.39655972\n",
            "Iteration 25, loss = 0.38703988\n",
            "Iteration 26, loss = 0.39573362\n",
            "Iteration 27, loss = 0.38081267\n",
            "Iteration 28, loss = 0.38668056\n",
            "Iteration 29, loss = 0.39139308\n",
            "Iteration 30, loss = 0.37863054\n",
            "Iteration 31, loss = 0.37305585\n",
            "Iteration 32, loss = 0.37562899\n",
            "Iteration 33, loss = 0.37669348\n",
            "Iteration 34, loss = 0.37193195\n",
            "Iteration 35, loss = 0.36824687\n",
            "Iteration 36, loss = 0.36785923\n",
            "Iteration 37, loss = 0.36851961\n",
            "Iteration 38, loss = 0.36909627\n",
            "Iteration 39, loss = 0.37172359\n",
            "Iteration 40, loss = 0.36617270\n",
            "Iteration 41, loss = 0.37164569\n",
            "Iteration 42, loss = 0.36623036\n",
            "Iteration 43, loss = 0.37098992\n",
            "Iteration 44, loss = 0.36608421\n",
            "Iteration 45, loss = 0.37295693\n",
            "Iteration 46, loss = 0.37100801\n",
            "Iteration 47, loss = 0.36578244\n",
            "Iteration 48, loss = 0.36781559\n",
            "Iteration 49, loss = 0.36846685\n",
            "Iteration 50, loss = 0.36434253\n",
            "Iteration 51, loss = 0.36602271\n",
            "Iteration 52, loss = 0.36380287\n",
            "Iteration 53, loss = 0.36687558\n",
            "Iteration 54, loss = 0.36239506\n",
            "Iteration 55, loss = 0.37068046\n",
            "Iteration 56, loss = 0.36102820\n",
            "Iteration 57, loss = 0.36772916\n",
            "Iteration 58, loss = 0.36780090\n",
            "Iteration 59, loss = 0.36510692\n",
            "Iteration 60, loss = 0.36609606\n",
            "Iteration 61, loss = 0.36750272\n",
            "Iteration 62, loss = 0.36876318\n",
            "Iteration 63, loss = 0.36452926\n",
            "Iteration 64, loss = 0.36841831\n",
            "Iteration 65, loss = 0.36453372\n",
            "Iteration 66, loss = 0.36698587\n",
            "Iteration 67, loss = 0.36217566\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31284725\n",
            "Iteration 2, loss = 1.24020824\n",
            "Iteration 3, loss = 1.20703610\n",
            "Iteration 4, loss = 1.16902056\n",
            "Iteration 5, loss = 1.12970934\n",
            "Iteration 6, loss = 1.08363313\n",
            "Iteration 7, loss = 1.03422756\n",
            "Iteration 8, loss = 0.97259594\n",
            "Iteration 9, loss = 0.90809599\n",
            "Iteration 10, loss = 0.83705304\n",
            "Iteration 11, loss = 0.76614290\n",
            "Iteration 12, loss = 0.71195315\n",
            "Iteration 13, loss = 0.64811264\n",
            "Iteration 14, loss = 0.59638985\n",
            "Iteration 15, loss = 0.55339783\n",
            "Iteration 16, loss = 0.51975422\n",
            "Iteration 17, loss = 0.48880205\n",
            "Iteration 18, loss = 0.46761922\n",
            "Iteration 19, loss = 0.45776384\n",
            "Iteration 20, loss = 0.42934241\n",
            "Iteration 21, loss = 0.42216949\n",
            "Iteration 22, loss = 0.40962928\n",
            "Iteration 23, loss = 0.40298464\n",
            "Iteration 24, loss = 0.40128676\n",
            "Iteration 25, loss = 0.39239317\n",
            "Iteration 26, loss = 0.39711483\n",
            "Iteration 27, loss = 0.38785361\n",
            "Iteration 28, loss = 0.38913043\n",
            "Iteration 29, loss = 0.38891479\n",
            "Iteration 30, loss = 0.38198498\n",
            "Iteration 31, loss = 0.38069690\n",
            "Iteration 32, loss = 0.38047690\n",
            "Iteration 33, loss = 0.38091877\n",
            "Iteration 34, loss = 0.38005368\n",
            "Iteration 35, loss = 0.37846171\n",
            "Iteration 36, loss = 0.37579250\n",
            "Iteration 37, loss = 0.37864430\n",
            "Iteration 38, loss = 0.37706327\n",
            "Iteration 39, loss = 0.37454925\n",
            "Iteration 40, loss = 0.37233532\n",
            "Iteration 41, loss = 0.37586584\n",
            "Iteration 42, loss = 0.37259996\n",
            "Iteration 43, loss = 0.37888914\n",
            "Iteration 44, loss = 0.37317985\n",
            "Iteration 45, loss = 0.37718153\n",
            "Iteration 46, loss = 0.38622222\n",
            "Iteration 47, loss = 0.37544131\n",
            "Iteration 48, loss = 0.37333976\n",
            "Iteration 49, loss = 0.37733740\n",
            "Iteration 50, loss = 0.37196820\n",
            "Iteration 51, loss = 0.37341146\n",
            "Iteration 52, loss = 0.37738440\n",
            "Iteration 53, loss = 0.37698772\n",
            "Iteration 54, loss = 0.37057510\n",
            "Iteration 55, loss = 0.37679773\n",
            "Iteration 56, loss = 0.36759432\n",
            "Iteration 57, loss = 0.37748721\n",
            "Iteration 58, loss = 0.37976121\n",
            "Iteration 59, loss = 0.36923448\n",
            "Iteration 60, loss = 0.37455085\n",
            "Iteration 61, loss = 0.37405306\n",
            "Iteration 62, loss = 0.37470523\n",
            "Iteration 63, loss = 0.37192258\n",
            "Iteration 64, loss = 0.37665998\n",
            "Iteration 65, loss = 0.37464493\n",
            "Iteration 66, loss = 0.37676923\n",
            "Iteration 67, loss = 0.36922216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30995013\n",
            "Iteration 2, loss = 1.23200608\n",
            "Iteration 3, loss = 1.19764822\n",
            "Iteration 4, loss = 1.16051960\n",
            "Iteration 5, loss = 1.12408922\n",
            "Iteration 6, loss = 1.07825194\n",
            "Iteration 7, loss = 1.03016764\n",
            "Iteration 8, loss = 0.97185562\n",
            "Iteration 9, loss = 0.90847320\n",
            "Iteration 10, loss = 0.83992390\n",
            "Iteration 11, loss = 0.77043837\n",
            "Iteration 12, loss = 0.71552485\n",
            "Iteration 13, loss = 0.65080009\n",
            "Iteration 14, loss = 0.59712185\n",
            "Iteration 15, loss = 0.55288346\n",
            "Iteration 16, loss = 0.51775153\n",
            "Iteration 17, loss = 0.48118058\n",
            "Iteration 18, loss = 0.45845282\n",
            "Iteration 19, loss = 0.43833369\n",
            "Iteration 20, loss = 0.41570474\n",
            "Iteration 21, loss = 0.40378511\n",
            "Iteration 22, loss = 0.39297786\n",
            "Iteration 23, loss = 0.38273915\n",
            "Iteration 24, loss = 0.37834314\n",
            "Iteration 25, loss = 0.37279780\n",
            "Iteration 26, loss = 0.37589495\n",
            "Iteration 27, loss = 0.36838401\n",
            "Iteration 28, loss = 0.37303714\n",
            "Iteration 29, loss = 0.36691491\n",
            "Iteration 30, loss = 0.36141950\n",
            "Iteration 31, loss = 0.36059521\n",
            "Iteration 32, loss = 0.36094781\n",
            "Iteration 33, loss = 0.35840997\n",
            "Iteration 34, loss = 0.35753999\n",
            "Iteration 35, loss = 0.35968820\n",
            "Iteration 36, loss = 0.35505327\n",
            "Iteration 37, loss = 0.35963453\n",
            "Iteration 38, loss = 0.35742576\n",
            "Iteration 39, loss = 0.35398635\n",
            "Iteration 40, loss = 0.35197903\n",
            "Iteration 41, loss = 0.35630950\n",
            "Iteration 42, loss = 0.35517996\n",
            "Iteration 43, loss = 0.35274424\n",
            "Iteration 44, loss = 0.35798118\n",
            "Iteration 45, loss = 0.35525283\n",
            "Iteration 46, loss = 0.36494877\n",
            "Iteration 47, loss = 0.35838436\n",
            "Iteration 48, loss = 0.35872310\n",
            "Iteration 49, loss = 0.35776822\n",
            "Iteration 50, loss = 0.35253496\n",
            "Iteration 51, loss = 0.34956058\n",
            "Iteration 52, loss = 0.35682488\n",
            "Iteration 53, loss = 0.36332790\n",
            "Iteration 54, loss = 0.35277044\n",
            "Iteration 55, loss = 0.35616902\n",
            "Iteration 56, loss = 0.34706659\n",
            "Iteration 57, loss = 0.35572813\n",
            "Iteration 58, loss = 0.35689361\n",
            "Iteration 59, loss = 0.34722834\n",
            "Iteration 60, loss = 0.35513432\n",
            "Iteration 61, loss = 0.35491291\n",
            "Iteration 62, loss = 0.35378557\n",
            "Iteration 63, loss = 0.34952833\n",
            "Iteration 64, loss = 0.35282752\n",
            "Iteration 65, loss = 0.35279319\n",
            "Iteration 66, loss = 0.35310096\n",
            "Iteration 67, loss = 0.34807919\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30591817\n",
            "Iteration 2, loss = 1.22439737\n",
            "Iteration 3, loss = 1.19092340\n",
            "Iteration 4, loss = 1.15486493\n",
            "Iteration 5, loss = 1.11630425\n",
            "Iteration 6, loss = 1.07419148\n",
            "Iteration 7, loss = 1.02556814\n",
            "Iteration 8, loss = 0.97140206\n",
            "Iteration 9, loss = 0.91021582\n",
            "Iteration 10, loss = 0.84485151\n",
            "Iteration 11, loss = 0.77677644\n",
            "Iteration 12, loss = 0.72220955\n",
            "Iteration 13, loss = 0.65287152\n",
            "Iteration 14, loss = 0.60179415\n",
            "Iteration 15, loss = 0.55697425\n",
            "Iteration 16, loss = 0.52272797\n",
            "Iteration 17, loss = 0.48406128\n",
            "Iteration 18, loss = 0.46586820\n",
            "Iteration 19, loss = 0.43986092\n",
            "Iteration 20, loss = 0.42257711\n",
            "Iteration 21, loss = 0.41081869\n",
            "Iteration 22, loss = 0.40146134\n",
            "Iteration 23, loss = 0.39367208\n",
            "Iteration 24, loss = 0.38870307\n",
            "Iteration 25, loss = 0.38676666\n",
            "Iteration 26, loss = 0.38668900\n",
            "Iteration 27, loss = 0.37856623\n",
            "Iteration 28, loss = 0.37769548\n",
            "Iteration 29, loss = 0.37706446\n",
            "Iteration 30, loss = 0.37187841\n",
            "Iteration 31, loss = 0.37053912\n",
            "Iteration 32, loss = 0.37171251\n",
            "Iteration 33, loss = 0.37179809\n",
            "Iteration 34, loss = 0.37424545\n",
            "Iteration 35, loss = 0.37221549\n",
            "Iteration 36, loss = 0.36893524\n",
            "Iteration 37, loss = 0.36671552\n",
            "Iteration 38, loss = 0.37389231\n",
            "Iteration 39, loss = 0.36338260\n",
            "Iteration 40, loss = 0.36515157\n",
            "Iteration 41, loss = 0.36856554\n",
            "Iteration 42, loss = 0.36689694\n",
            "Iteration 43, loss = 0.36385893\n",
            "Iteration 44, loss = 0.37680180\n",
            "Iteration 45, loss = 0.36692115\n",
            "Iteration 46, loss = 0.36946674\n",
            "Iteration 47, loss = 0.37056752\n",
            "Iteration 48, loss = 0.36247113\n",
            "Iteration 49, loss = 0.36225803\n",
            "Iteration 50, loss = 0.36062945\n",
            "Iteration 51, loss = 0.35968204\n",
            "Iteration 52, loss = 0.36624814\n",
            "Iteration 53, loss = 0.37501013\n",
            "Iteration 54, loss = 0.36128023\n",
            "Iteration 55, loss = 0.36498458\n",
            "Iteration 56, loss = 0.35796612\n",
            "Iteration 57, loss = 0.36320726\n",
            "Iteration 58, loss = 0.35980485\n",
            "Iteration 59, loss = 0.36131726\n",
            "Iteration 60, loss = 0.36872219\n",
            "Iteration 61, loss = 0.35763857\n",
            "Iteration 62, loss = 0.36527824\n",
            "Iteration 63, loss = 0.35800280\n",
            "Iteration 64, loss = 0.36082953\n",
            "Iteration 65, loss = 0.36016099\n",
            "Iteration 66, loss = 0.36044045\n",
            "Iteration 67, loss = 0.35893177\n",
            "Iteration 68, loss = 0.35575050\n",
            "Iteration 69, loss = 0.36118662\n",
            "Iteration 70, loss = 0.36509300\n",
            "Iteration 71, loss = 0.36472594\n",
            "Iteration 72, loss = 0.36014337\n",
            "Iteration 73, loss = 0.37394418\n",
            "Iteration 74, loss = 0.36982623\n",
            "Iteration 75, loss = 0.35621652\n",
            "Iteration 76, loss = 0.36026043\n",
            "Iteration 77, loss = 0.35755400\n",
            "Iteration 78, loss = 0.35833091\n",
            "Iteration 79, loss = 0.36804130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30676054\n",
            "Iteration 2, loss = 1.23044740\n",
            "Iteration 3, loss = 1.19618014\n",
            "Iteration 4, loss = 1.15829443\n",
            "Iteration 5, loss = 1.11982233\n",
            "Iteration 6, loss = 1.07476640\n",
            "Iteration 7, loss = 1.02574462\n",
            "Iteration 8, loss = 0.96964819\n",
            "Iteration 9, loss = 0.90549410\n",
            "Iteration 10, loss = 0.83896195\n",
            "Iteration 11, loss = 0.76948142\n",
            "Iteration 12, loss = 0.71219804\n",
            "Iteration 13, loss = 0.64384625\n",
            "Iteration 14, loss = 0.59321129\n",
            "Iteration 15, loss = 0.55693784\n",
            "Iteration 16, loss = 0.52793363\n",
            "Iteration 17, loss = 0.48829277\n",
            "Iteration 18, loss = 0.47326599\n",
            "Iteration 19, loss = 0.44838504\n",
            "Iteration 20, loss = 0.43113123\n",
            "Iteration 21, loss = 0.42133732\n",
            "Iteration 22, loss = 0.41182687\n",
            "Iteration 23, loss = 0.40125277\n",
            "Iteration 24, loss = 0.39670576\n",
            "Iteration 25, loss = 0.39921959\n",
            "Iteration 26, loss = 0.40032347\n",
            "Iteration 27, loss = 0.38848186\n",
            "Iteration 28, loss = 0.38603508\n",
            "Iteration 29, loss = 0.38630495\n",
            "Iteration 30, loss = 0.38035001\n",
            "Iteration 31, loss = 0.37856994\n",
            "Iteration 32, loss = 0.38251908\n",
            "Iteration 33, loss = 0.38158727\n",
            "Iteration 34, loss = 0.38113800\n",
            "Iteration 35, loss = 0.38092772\n",
            "Iteration 36, loss = 0.37737925\n",
            "Iteration 37, loss = 0.37759968\n",
            "Iteration 38, loss = 0.37692207\n",
            "Iteration 39, loss = 0.37164416\n",
            "Iteration 40, loss = 0.37452811\n",
            "Iteration 41, loss = 0.37902975\n",
            "Iteration 42, loss = 0.37548907\n",
            "Iteration 43, loss = 0.37334839\n",
            "Iteration 44, loss = 0.38603945\n",
            "Iteration 45, loss = 0.37752859\n",
            "Iteration 46, loss = 0.37443122\n",
            "Iteration 47, loss = 0.37912808\n",
            "Iteration 48, loss = 0.37270082\n",
            "Iteration 49, loss = 0.37021771\n",
            "Iteration 50, loss = 0.37058348\n",
            "Iteration 51, loss = 0.36972732\n",
            "Iteration 52, loss = 0.37089504\n",
            "Iteration 53, loss = 0.38538459\n",
            "Iteration 54, loss = 0.37057864\n",
            "Iteration 55, loss = 0.37356712\n",
            "Iteration 56, loss = 0.36715866\n",
            "Iteration 57, loss = 0.37057915\n",
            "Iteration 58, loss = 0.36574493\n",
            "Iteration 59, loss = 0.37145239\n",
            "Iteration 60, loss = 0.38015327\n",
            "Iteration 61, loss = 0.36580512\n",
            "Iteration 62, loss = 0.37479350\n",
            "Iteration 63, loss = 0.36786410\n",
            "Iteration 64, loss = 0.37527938\n",
            "Iteration 65, loss = 0.37270769\n",
            "Iteration 66, loss = 0.36892763\n",
            "Iteration 67, loss = 0.37043503\n",
            "Iteration 68, loss = 0.36786434\n",
            "Iteration 69, loss = 0.37161922\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30639889\n",
            "Iteration 2, loss = 1.23426315\n",
            "Iteration 3, loss = 1.20156452\n",
            "Iteration 4, loss = 1.16577717\n",
            "Iteration 5, loss = 1.12879307\n",
            "Iteration 6, loss = 1.08498703\n",
            "Iteration 7, loss = 1.04151172\n",
            "Iteration 8, loss = 0.98584063\n",
            "Iteration 9, loss = 0.92784046\n",
            "Iteration 10, loss = 0.86097864\n",
            "Iteration 11, loss = 0.80137875\n",
            "Iteration 12, loss = 0.73444923\n",
            "Iteration 13, loss = 0.67390349\n",
            "Iteration 14, loss = 0.62247742\n",
            "Iteration 15, loss = 0.57370164\n",
            "Iteration 16, loss = 0.53692665\n",
            "Iteration 17, loss = 0.50680276\n",
            "Iteration 18, loss = 0.47410338\n",
            "Iteration 19, loss = 0.45720824\n",
            "Iteration 20, loss = 0.43736937\n",
            "Iteration 21, loss = 0.42449633\n",
            "Iteration 22, loss = 0.41667088\n",
            "Iteration 23, loss = 0.41666114\n",
            "Iteration 24, loss = 0.40502561\n",
            "Iteration 25, loss = 0.40265505\n",
            "Iteration 26, loss = 0.40236813\n",
            "Iteration 27, loss = 0.39490344\n",
            "Iteration 28, loss = 0.39061897\n",
            "Iteration 29, loss = 0.39397967\n",
            "Iteration 30, loss = 0.38796824\n",
            "Iteration 31, loss = 0.38960739\n",
            "Iteration 32, loss = 0.39070713\n",
            "Iteration 33, loss = 0.38850254\n",
            "Iteration 34, loss = 0.38327672\n",
            "Iteration 35, loss = 0.39230301\n",
            "Iteration 36, loss = 0.38148791\n",
            "Iteration 37, loss = 0.37834684\n",
            "Iteration 38, loss = 0.37989817\n",
            "Iteration 39, loss = 0.38255868\n",
            "Iteration 40, loss = 0.38111937\n",
            "Iteration 41, loss = 0.38560449\n",
            "Iteration 42, loss = 0.38890029\n",
            "Iteration 43, loss = 0.39065729\n",
            "Iteration 44, loss = 0.38872552\n",
            "Iteration 45, loss = 0.37866605\n",
            "Iteration 46, loss = 0.37830157\n",
            "Iteration 47, loss = 0.38300565\n",
            "Iteration 48, loss = 0.37698975\n",
            "Iteration 49, loss = 0.37668126\n",
            "Iteration 50, loss = 0.37727117\n",
            "Iteration 51, loss = 0.37837220\n",
            "Iteration 52, loss = 0.37559926\n",
            "Iteration 53, loss = 0.37739123\n",
            "Iteration 54, loss = 0.37547830\n",
            "Iteration 55, loss = 0.38719293\n",
            "Iteration 56, loss = 0.39093833\n",
            "Iteration 57, loss = 0.37984364\n",
            "Iteration 58, loss = 0.38309530\n",
            "Iteration 59, loss = 0.38672071\n",
            "Iteration 60, loss = 0.38877976\n",
            "Iteration 61, loss = 0.38293982\n",
            "Iteration 62, loss = 0.39210696\n",
            "Iteration 63, loss = 0.38126743\n",
            "Iteration 64, loss = 0.37971281\n",
            "Iteration 65, loss = 0.37918075\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30755976\n",
            "Iteration 2, loss = 1.22970011\n",
            "Iteration 3, loss = 1.19746486\n",
            "Iteration 4, loss = 1.16631811\n",
            "Iteration 5, loss = 1.12817986\n",
            "Iteration 6, loss = 1.08908706\n",
            "Iteration 7, loss = 1.04378223\n",
            "Iteration 8, loss = 0.99603055\n",
            "Iteration 9, loss = 0.93648766\n",
            "Iteration 10, loss = 0.87813620\n",
            "Iteration 11, loss = 0.82147508\n",
            "Iteration 12, loss = 0.75701536\n",
            "Iteration 13, loss = 0.69865398\n",
            "Iteration 14, loss = 0.64637223\n",
            "Iteration 15, loss = 0.59743587\n",
            "Iteration 16, loss = 0.56182634\n",
            "Iteration 17, loss = 0.53036426\n",
            "Iteration 18, loss = 0.49853562\n",
            "Iteration 19, loss = 0.47842128\n",
            "Iteration 20, loss = 0.45801173\n",
            "Iteration 21, loss = 0.44381733\n",
            "Iteration 22, loss = 0.43365849\n",
            "Iteration 23, loss = 0.43840471\n",
            "Iteration 24, loss = 0.42550867\n",
            "Iteration 25, loss = 0.41997272\n",
            "Iteration 26, loss = 0.41680464\n",
            "Iteration 27, loss = 0.41123454\n",
            "Iteration 28, loss = 0.40483374\n",
            "Iteration 29, loss = 0.40424381\n",
            "Iteration 30, loss = 0.39939750\n",
            "Iteration 31, loss = 0.40083084\n",
            "Iteration 32, loss = 0.39603295\n",
            "Iteration 33, loss = 0.39779493\n",
            "Iteration 34, loss = 0.39685019\n",
            "Iteration 35, loss = 0.40241814\n",
            "Iteration 36, loss = 0.39223813\n",
            "Iteration 37, loss = 0.39124561\n",
            "Iteration 38, loss = 0.39208933\n",
            "Iteration 39, loss = 0.39729728\n",
            "Iteration 40, loss = 0.39201649\n",
            "Iteration 41, loss = 0.39249029\n",
            "Iteration 42, loss = 0.39457196\n",
            "Iteration 43, loss = 0.39871425\n",
            "Iteration 44, loss = 0.39281146\n",
            "Iteration 45, loss = 0.38560575\n",
            "Iteration 46, loss = 0.38719183\n",
            "Iteration 47, loss = 0.38945946\n",
            "Iteration 48, loss = 0.38536564\n",
            "Iteration 49, loss = 0.38748605\n",
            "Iteration 50, loss = 0.38528957\n",
            "Iteration 51, loss = 0.38844025\n",
            "Iteration 52, loss = 0.38417839\n",
            "Iteration 53, loss = 0.38287732\n",
            "Iteration 54, loss = 0.38405490\n",
            "Iteration 55, loss = 0.39032963\n",
            "Iteration 56, loss = 0.40096989\n",
            "Iteration 57, loss = 0.38669860\n",
            "Iteration 58, loss = 0.38862784\n",
            "Iteration 59, loss = 0.39028149\n",
            "Iteration 60, loss = 0.39073478\n",
            "Iteration 61, loss = 0.38973708\n",
            "Iteration 62, loss = 0.39638094\n",
            "Iteration 63, loss = 0.38665367\n",
            "Iteration 64, loss = 0.38623253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30498013\n",
            "Iteration 2, loss = 1.22448184\n",
            "Iteration 3, loss = 1.19005656\n",
            "Iteration 4, loss = 1.15307671\n",
            "Iteration 5, loss = 1.11419897\n",
            "Iteration 6, loss = 1.07315104\n",
            "Iteration 7, loss = 1.02644476\n",
            "Iteration 8, loss = 0.97476224\n",
            "Iteration 9, loss = 0.91490281\n",
            "Iteration 10, loss = 0.85391436\n",
            "Iteration 11, loss = 0.79670939\n",
            "Iteration 12, loss = 0.73639387\n",
            "Iteration 13, loss = 0.68011091\n",
            "Iteration 14, loss = 0.63572019\n",
            "Iteration 15, loss = 0.59258694\n",
            "Iteration 16, loss = 0.56370623\n",
            "Iteration 17, loss = 0.53847905\n",
            "Iteration 18, loss = 0.50983987\n",
            "Iteration 19, loss = 0.49251249\n",
            "Iteration 20, loss = 0.47520229\n",
            "Iteration 21, loss = 0.45976004\n",
            "Iteration 22, loss = 0.44853800\n",
            "Iteration 23, loss = 0.44671080\n",
            "Iteration 24, loss = 0.43947413\n",
            "Iteration 25, loss = 0.43277898\n",
            "Iteration 26, loss = 0.42408351\n",
            "Iteration 27, loss = 0.42269870\n",
            "Iteration 28, loss = 0.41723361\n",
            "Iteration 29, loss = 0.41132987\n",
            "Iteration 30, loss = 0.40764059\n",
            "Iteration 31, loss = 0.40865623\n",
            "Iteration 32, loss = 0.40212168\n",
            "Iteration 33, loss = 0.40277563\n",
            "Iteration 34, loss = 0.40214004\n",
            "Iteration 35, loss = 0.40213876\n",
            "Iteration 36, loss = 0.39710953\n",
            "Iteration 37, loss = 0.39856968\n",
            "Iteration 38, loss = 0.39410730\n",
            "Iteration 39, loss = 0.40686192\n",
            "Iteration 40, loss = 0.39695316\n",
            "Iteration 41, loss = 0.39787021\n",
            "Iteration 42, loss = 0.40421231\n",
            "Iteration 43, loss = 0.40337208\n",
            "Iteration 44, loss = 0.39808977\n",
            "Iteration 45, loss = 0.39262696\n",
            "Iteration 46, loss = 0.39767067\n",
            "Iteration 47, loss = 0.40171842\n",
            "Iteration 48, loss = 0.39343150\n",
            "Iteration 49, loss = 0.39844890\n",
            "Iteration 50, loss = 0.39256662\n",
            "Iteration 51, loss = 0.39810906\n",
            "Iteration 52, loss = 0.39289300\n",
            "Iteration 53, loss = 0.38841319\n",
            "Iteration 54, loss = 0.39083589\n",
            "Iteration 55, loss = 0.39136523\n",
            "Iteration 56, loss = 0.40403708\n",
            "Iteration 57, loss = 0.39392720\n",
            "Iteration 58, loss = 0.39284306\n",
            "Iteration 59, loss = 0.39253482\n",
            "Iteration 60, loss = 0.39732543\n",
            "Iteration 61, loss = 0.39233565\n",
            "Iteration 62, loss = 0.40559421\n",
            "Iteration 63, loss = 0.39829286\n",
            "Iteration 64, loss = 0.39550363\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "26\n",
            "Iteration 1, loss = 1.31844471\n",
            "Iteration 2, loss = 1.26092391\n",
            "Iteration 3, loss = 1.24061514\n",
            "Iteration 4, loss = 1.22029122\n",
            "Iteration 5, loss = 1.20565237\n",
            "Iteration 6, loss = 1.19048817\n",
            "Iteration 7, loss = 1.16702042\n",
            "Iteration 8, loss = 1.14791356\n",
            "Iteration 9, loss = 1.12245656\n",
            "Iteration 10, loss = 1.09372677\n",
            "Iteration 11, loss = 1.06676561\n",
            "Iteration 12, loss = 1.03419839\n",
            "Iteration 13, loss = 1.00750917\n",
            "Iteration 14, loss = 0.97791971\n",
            "Iteration 15, loss = 0.95365297\n",
            "Iteration 16, loss = 0.93387060\n",
            "Iteration 17, loss = 0.91668913\n",
            "Iteration 18, loss = 0.90169989\n",
            "Iteration 19, loss = 0.88888118\n",
            "Iteration 20, loss = 0.88251950\n",
            "Iteration 21, loss = 0.87562990\n",
            "Iteration 22, loss = 0.87482989\n",
            "Iteration 23, loss = 0.88499770\n",
            "Iteration 24, loss = 0.86772295\n",
            "Iteration 25, loss = 0.86531535\n",
            "Iteration 26, loss = 0.86898601\n",
            "Iteration 27, loss = 0.86941261\n",
            "Iteration 28, loss = 0.86406079\n",
            "Iteration 29, loss = 0.86712397\n",
            "Iteration 30, loss = 0.86428136\n",
            "Iteration 31, loss = 0.86392699\n",
            "Iteration 32, loss = 0.85784932\n",
            "Iteration 33, loss = 0.85709838\n",
            "Iteration 34, loss = 0.86519759\n",
            "Iteration 35, loss = 0.85886438\n",
            "Iteration 36, loss = 0.86518341\n",
            "Iteration 37, loss = 0.86562984\n",
            "Iteration 38, loss = 0.86235748\n",
            "Iteration 39, loss = 0.85607850\n",
            "Iteration 40, loss = 0.85521687\n",
            "Iteration 41, loss = 0.86090802\n",
            "Iteration 42, loss = 0.85381334\n",
            "Iteration 43, loss = 0.85848003\n",
            "Iteration 44, loss = 0.86821444\n",
            "Iteration 45, loss = 0.85627798\n",
            "Iteration 46, loss = 0.85322142\n",
            "Iteration 47, loss = 0.85391804\n",
            "Iteration 48, loss = 0.85466467\n",
            "Iteration 49, loss = 0.84928614\n",
            "Iteration 50, loss = 0.85629801\n",
            "Iteration 51, loss = 0.85667026\n",
            "Iteration 52, loss = 0.85465306\n",
            "Iteration 53, loss = 0.85228774\n",
            "Iteration 54, loss = 0.85387678\n",
            "Iteration 55, loss = 0.85485828\n",
            "Iteration 56, loss = 0.86396061\n",
            "Iteration 57, loss = 0.84960046\n",
            "Iteration 58, loss = 0.86421901\n",
            "Iteration 59, loss = 0.85050725\n",
            "Iteration 60, loss = 0.85284248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31873042\n",
            "Iteration 2, loss = 1.25960060\n",
            "Iteration 3, loss = 1.23955520\n",
            "Iteration 4, loss = 1.21975732\n",
            "Iteration 5, loss = 1.20550697\n",
            "Iteration 6, loss = 1.18994955\n",
            "Iteration 7, loss = 1.16876935\n",
            "Iteration 8, loss = 1.14924740\n",
            "Iteration 9, loss = 1.12839036\n",
            "Iteration 10, loss = 1.09913116\n",
            "Iteration 11, loss = 1.07644474\n",
            "Iteration 12, loss = 1.04415084\n",
            "Iteration 13, loss = 1.02312837\n",
            "Iteration 14, loss = 0.99082186\n",
            "Iteration 15, loss = 0.97093701\n",
            "Iteration 16, loss = 0.94896195\n",
            "Iteration 17, loss = 0.92940535\n",
            "Iteration 18, loss = 0.92033292\n",
            "Iteration 19, loss = 0.90490217\n",
            "Iteration 20, loss = 0.89507391\n",
            "Iteration 21, loss = 0.88916058\n",
            "Iteration 22, loss = 0.88667112\n",
            "Iteration 23, loss = 0.89186605\n",
            "Iteration 24, loss = 0.87417467\n",
            "Iteration 25, loss = 0.87861468\n",
            "Iteration 26, loss = 0.87682819\n",
            "Iteration 27, loss = 0.87191006\n",
            "Iteration 28, loss = 0.86772844\n",
            "Iteration 29, loss = 0.86804292\n",
            "Iteration 30, loss = 0.86508955\n",
            "Iteration 31, loss = 0.86489113\n",
            "Iteration 32, loss = 0.86632952\n",
            "Iteration 33, loss = 0.86286777\n",
            "Iteration 34, loss = 0.86589448\n",
            "Iteration 35, loss = 0.86179404\n",
            "Iteration 36, loss = 0.86772010\n",
            "Iteration 37, loss = 0.87194521\n",
            "Iteration 38, loss = 0.87083819\n",
            "Iteration 39, loss = 0.86068969\n",
            "Iteration 40, loss = 0.86071520\n",
            "Iteration 41, loss = 0.86472259\n",
            "Iteration 42, loss = 0.85575099\n",
            "Iteration 43, loss = 0.86501199\n",
            "Iteration 44, loss = 0.86704937\n",
            "Iteration 45, loss = 0.86223262\n",
            "Iteration 46, loss = 0.85733746\n",
            "Iteration 47, loss = 0.86085128\n",
            "Iteration 48, loss = 0.85712984\n",
            "Iteration 49, loss = 0.85575590\n",
            "Iteration 50, loss = 0.85988360\n",
            "Iteration 51, loss = 0.85869042\n",
            "Iteration 52, loss = 0.85652908\n",
            "Iteration 53, loss = 0.85500699\n",
            "Iteration 54, loss = 0.85494633\n",
            "Iteration 55, loss = 0.85699931\n",
            "Iteration 56, loss = 0.85731537\n",
            "Iteration 57, loss = 0.85174091\n",
            "Iteration 58, loss = 0.85571034\n",
            "Iteration 59, loss = 0.85495308\n",
            "Iteration 60, loss = 0.85207788\n",
            "Iteration 61, loss = 0.85313809\n",
            "Iteration 62, loss = 0.85588998\n",
            "Iteration 63, loss = 0.85862645\n",
            "Iteration 64, loss = 0.85142311\n",
            "Iteration 65, loss = 0.85479735\n",
            "Iteration 66, loss = 0.85258533\n",
            "Iteration 67, loss = 0.85209840\n",
            "Iteration 68, loss = 0.85999182\n",
            "Iteration 69, loss = 0.85170864\n",
            "Iteration 70, loss = 0.85509228\n",
            "Iteration 71, loss = 0.85367535\n",
            "Iteration 72, loss = 0.86063501\n",
            "Iteration 73, loss = 0.85080788\n",
            "Iteration 74, loss = 0.85211229\n",
            "Iteration 75, loss = 0.85212236\n",
            "Iteration 76, loss = 0.85252717\n",
            "Iteration 77, loss = 0.85674971\n",
            "Iteration 78, loss = 0.85223601\n",
            "Iteration 79, loss = 0.85562646\n",
            "Iteration 80, loss = 0.84999268\n",
            "Iteration 81, loss = 0.86625254\n",
            "Iteration 82, loss = 0.84945350\n",
            "Iteration 83, loss = 0.85705922\n",
            "Iteration 84, loss = 0.85329053\n",
            "Iteration 85, loss = 0.86270106\n",
            "Iteration 86, loss = 0.85672419\n",
            "Iteration 87, loss = 0.85747859\n",
            "Iteration 88, loss = 0.85972045\n",
            "Iteration 89, loss = 0.85090613\n",
            "Iteration 90, loss = 0.85130715\n",
            "Iteration 91, loss = 0.85227708\n",
            "Iteration 92, loss = 0.85564186\n",
            "Iteration 93, loss = 0.85134437\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32205520\n",
            "Iteration 2, loss = 1.26269825\n",
            "Iteration 3, loss = 1.24159116\n",
            "Iteration 4, loss = 1.22236738\n",
            "Iteration 5, loss = 1.20468077\n",
            "Iteration 6, loss = 1.18696907\n",
            "Iteration 7, loss = 1.16406246\n",
            "Iteration 8, loss = 1.13939807\n",
            "Iteration 9, loss = 1.11597793\n",
            "Iteration 10, loss = 1.08471883\n",
            "Iteration 11, loss = 1.05682900\n",
            "Iteration 12, loss = 1.02198702\n",
            "Iteration 13, loss = 0.99687258\n",
            "Iteration 14, loss = 0.96438479\n",
            "Iteration 15, loss = 0.94350626\n",
            "Iteration 16, loss = 0.91799045\n",
            "Iteration 17, loss = 0.90101453\n",
            "Iteration 18, loss = 0.90027752\n",
            "Iteration 19, loss = 0.88291631\n",
            "Iteration 20, loss = 0.87166656\n",
            "Iteration 21, loss = 0.86971925\n",
            "Iteration 22, loss = 0.86271395\n",
            "Iteration 23, loss = 0.86252537\n",
            "Iteration 24, loss = 0.84897208\n",
            "Iteration 25, loss = 0.85975069\n",
            "Iteration 26, loss = 0.85017746\n",
            "Iteration 27, loss = 0.84580763\n",
            "Iteration 28, loss = 0.84523847\n",
            "Iteration 29, loss = 0.84522071\n",
            "Iteration 30, loss = 0.84194124\n",
            "Iteration 31, loss = 0.84397539\n",
            "Iteration 32, loss = 0.84751212\n",
            "Iteration 33, loss = 0.84596712\n",
            "Iteration 34, loss = 0.84621534\n",
            "Iteration 35, loss = 0.84232018\n",
            "Iteration 36, loss = 0.84022454\n",
            "Iteration 37, loss = 0.84454922\n",
            "Iteration 38, loss = 0.84488060\n",
            "Iteration 39, loss = 0.83600918\n",
            "Iteration 40, loss = 0.83956767\n",
            "Iteration 41, loss = 0.84386914\n",
            "Iteration 42, loss = 0.83266429\n",
            "Iteration 43, loss = 0.83863775\n",
            "Iteration 44, loss = 0.83586951\n",
            "Iteration 45, loss = 0.83691876\n",
            "Iteration 46, loss = 0.83423751\n",
            "Iteration 47, loss = 0.83763686\n",
            "Iteration 48, loss = 0.83695522\n",
            "Iteration 49, loss = 0.83622210\n",
            "Iteration 50, loss = 0.83755075\n",
            "Iteration 51, loss = 0.84147829\n",
            "Iteration 52, loss = 0.83299622\n",
            "Iteration 53, loss = 0.83338089\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32280459\n",
            "Iteration 2, loss = 1.26817890\n",
            "Iteration 3, loss = 1.24717095\n",
            "Iteration 4, loss = 1.22594232\n",
            "Iteration 5, loss = 1.20894422\n",
            "Iteration 6, loss = 1.19021282\n",
            "Iteration 7, loss = 1.16942521\n",
            "Iteration 8, loss = 1.14468291\n",
            "Iteration 9, loss = 1.12038817\n",
            "Iteration 10, loss = 1.09041437\n",
            "Iteration 11, loss = 1.05929596\n",
            "Iteration 12, loss = 1.02872004\n",
            "Iteration 13, loss = 1.00470442\n",
            "Iteration 14, loss = 0.97157559\n",
            "Iteration 15, loss = 0.95009567\n",
            "Iteration 16, loss = 0.92954076\n",
            "Iteration 17, loss = 0.91372735\n",
            "Iteration 18, loss = 0.91142299\n",
            "Iteration 19, loss = 0.89633966\n",
            "Iteration 20, loss = 0.88459385\n",
            "Iteration 21, loss = 0.88189778\n",
            "Iteration 22, loss = 0.87785108\n",
            "Iteration 23, loss = 0.88200656\n",
            "Iteration 24, loss = 0.86652072\n",
            "Iteration 25, loss = 0.87857085\n",
            "Iteration 26, loss = 0.87418080\n",
            "Iteration 27, loss = 0.86495396\n",
            "Iteration 28, loss = 0.86446243\n",
            "Iteration 29, loss = 0.87448879\n",
            "Iteration 30, loss = 0.86343349\n",
            "Iteration 31, loss = 0.86600113\n",
            "Iteration 32, loss = 0.86739300\n",
            "Iteration 33, loss = 0.86439781\n",
            "Iteration 34, loss = 0.86474009\n",
            "Iteration 35, loss = 0.86302205\n",
            "Iteration 36, loss = 0.86175891\n",
            "Iteration 37, loss = 0.86542318\n",
            "Iteration 38, loss = 0.86569922\n",
            "Iteration 39, loss = 0.85655403\n",
            "Iteration 40, loss = 0.86315808\n",
            "Iteration 41, loss = 0.85918141\n",
            "Iteration 42, loss = 0.85227017\n",
            "Iteration 43, loss = 0.85605922\n",
            "Iteration 44, loss = 0.85691342\n",
            "Iteration 45, loss = 0.85416989\n",
            "Iteration 46, loss = 0.85711023\n",
            "Iteration 47, loss = 0.85579898\n",
            "Iteration 48, loss = 0.85579374\n",
            "Iteration 49, loss = 0.85544575\n",
            "Iteration 50, loss = 0.85420725\n",
            "Iteration 51, loss = 0.86724409\n",
            "Iteration 52, loss = 0.85714797\n",
            "Iteration 53, loss = 0.85473697\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31924212\n",
            "Iteration 2, loss = 1.25979754\n",
            "Iteration 3, loss = 1.23629515\n",
            "Iteration 4, loss = 1.21636381\n",
            "Iteration 5, loss = 1.19894782\n",
            "Iteration 6, loss = 1.18208256\n",
            "Iteration 7, loss = 1.15917835\n",
            "Iteration 8, loss = 1.14004371\n",
            "Iteration 9, loss = 1.11329790\n",
            "Iteration 10, loss = 1.08443894\n",
            "Iteration 11, loss = 1.05421772\n",
            "Iteration 12, loss = 1.02382655\n",
            "Iteration 13, loss = 0.99771954\n",
            "Iteration 14, loss = 0.96696004\n",
            "Iteration 15, loss = 0.94113792\n",
            "Iteration 16, loss = 0.92268563\n",
            "Iteration 17, loss = 0.90117967\n",
            "Iteration 18, loss = 0.89719206\n",
            "Iteration 19, loss = 0.88071467\n",
            "Iteration 20, loss = 0.87164011\n",
            "Iteration 21, loss = 0.86782536\n",
            "Iteration 22, loss = 0.85902735\n",
            "Iteration 23, loss = 0.86353758\n",
            "Iteration 24, loss = 0.84958124\n",
            "Iteration 25, loss = 0.85794173\n",
            "Iteration 26, loss = 0.85883872\n",
            "Iteration 27, loss = 0.85318769\n",
            "Iteration 28, loss = 0.85070016\n",
            "Iteration 29, loss = 0.86229333\n",
            "Iteration 30, loss = 0.85107241\n",
            "Iteration 31, loss = 0.85692395\n",
            "Iteration 32, loss = 0.85179533\n",
            "Iteration 33, loss = 0.85254970\n",
            "Iteration 34, loss = 0.85502485\n",
            "Iteration 35, loss = 0.84659503\n",
            "Iteration 36, loss = 0.84680438\n",
            "Iteration 37, loss = 0.84605163\n",
            "Iteration 38, loss = 0.84544881\n",
            "Iteration 39, loss = 0.84653167\n",
            "Iteration 40, loss = 0.84265944\n",
            "Iteration 41, loss = 0.85083949\n",
            "Iteration 42, loss = 0.83562542\n",
            "Iteration 43, loss = 0.84275271\n",
            "Iteration 44, loss = 0.84088782\n",
            "Iteration 45, loss = 0.83837286\n",
            "Iteration 46, loss = 0.83911130\n",
            "Iteration 47, loss = 0.83717883\n",
            "Iteration 48, loss = 0.83636867\n",
            "Iteration 49, loss = 0.84122347\n",
            "Iteration 50, loss = 0.83471634\n",
            "Iteration 51, loss = 0.84695436\n",
            "Iteration 52, loss = 0.83744764\n",
            "Iteration 53, loss = 0.83825475\n",
            "Iteration 54, loss = 0.83658927\n",
            "Iteration 55, loss = 0.83929815\n",
            "Iteration 56, loss = 0.83859215\n",
            "Iteration 57, loss = 0.83822854\n",
            "Iteration 58, loss = 0.83871079\n",
            "Iteration 59, loss = 0.84263972\n",
            "Iteration 60, loss = 0.84237987\n",
            "Iteration 61, loss = 0.83487828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31472801\n",
            "Iteration 2, loss = 1.25302300\n",
            "Iteration 3, loss = 1.23068594\n",
            "Iteration 4, loss = 1.21136851\n",
            "Iteration 5, loss = 1.19476881\n",
            "Iteration 6, loss = 1.18014348\n",
            "Iteration 7, loss = 1.15830795\n",
            "Iteration 8, loss = 1.13910622\n",
            "Iteration 9, loss = 1.11475250\n",
            "Iteration 10, loss = 1.08823269\n",
            "Iteration 11, loss = 1.05848512\n",
            "Iteration 12, loss = 1.03113085\n",
            "Iteration 13, loss = 1.00246625\n",
            "Iteration 14, loss = 0.97396521\n",
            "Iteration 15, loss = 0.95082727\n",
            "Iteration 16, loss = 0.93439474\n",
            "Iteration 17, loss = 0.91550206\n",
            "Iteration 18, loss = 0.90593225\n",
            "Iteration 19, loss = 0.89098913\n",
            "Iteration 20, loss = 0.88653124\n",
            "Iteration 21, loss = 0.88020032\n",
            "Iteration 22, loss = 0.87416130\n",
            "Iteration 23, loss = 0.88100890\n",
            "Iteration 24, loss = 0.86792260\n",
            "Iteration 25, loss = 0.87518602\n",
            "Iteration 26, loss = 0.87762143\n",
            "Iteration 27, loss = 0.87025650\n",
            "Iteration 28, loss = 0.86636251\n",
            "Iteration 29, loss = 0.87145850\n",
            "Iteration 30, loss = 0.86175661\n",
            "Iteration 31, loss = 0.87102912\n",
            "Iteration 32, loss = 0.86702739\n",
            "Iteration 33, loss = 0.86565095\n",
            "Iteration 34, loss = 0.87529241\n",
            "Iteration 35, loss = 0.86272656\n",
            "Iteration 36, loss = 0.86376780\n",
            "Iteration 37, loss = 0.86967624\n",
            "Iteration 38, loss = 0.86559107\n",
            "Iteration 39, loss = 0.86796012\n",
            "Iteration 40, loss = 0.86062403\n",
            "Iteration 41, loss = 0.86957038\n",
            "Iteration 42, loss = 0.85585191\n",
            "Iteration 43, loss = 0.85798087\n",
            "Iteration 44, loss = 0.85628746\n",
            "Iteration 45, loss = 0.85386404\n",
            "Iteration 46, loss = 0.85573865\n",
            "Iteration 47, loss = 0.85364268\n",
            "Iteration 48, loss = 0.85302074\n",
            "Iteration 49, loss = 0.85620011\n",
            "Iteration 50, loss = 0.85057595\n",
            "Iteration 51, loss = 0.86121166\n",
            "Iteration 52, loss = 0.85346819\n",
            "Iteration 53, loss = 0.85387133\n",
            "Iteration 54, loss = 0.85181789\n",
            "Iteration 55, loss = 0.85675750\n",
            "Iteration 56, loss = 0.85268561\n",
            "Iteration 57, loss = 0.85652992\n",
            "Iteration 58, loss = 0.85426782\n",
            "Iteration 59, loss = 0.86323014\n",
            "Iteration 60, loss = 0.86635869\n",
            "Iteration 61, loss = 0.85555153\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31617209\n",
            "Iteration 2, loss = 1.25830365\n",
            "Iteration 3, loss = 1.23561967\n",
            "Iteration 4, loss = 1.21521717\n",
            "Iteration 5, loss = 1.19909456\n",
            "Iteration 6, loss = 1.17937845\n",
            "Iteration 7, loss = 1.15777297\n",
            "Iteration 8, loss = 1.13774667\n",
            "Iteration 9, loss = 1.10852145\n",
            "Iteration 10, loss = 1.08005444\n",
            "Iteration 11, loss = 1.04862594\n",
            "Iteration 12, loss = 1.01892325\n",
            "Iteration 13, loss = 0.98777759\n",
            "Iteration 14, loss = 0.95569277\n",
            "Iteration 15, loss = 0.93088542\n",
            "Iteration 16, loss = 0.91711453\n",
            "Iteration 17, loss = 0.89321419\n",
            "Iteration 18, loss = 0.88509254\n",
            "Iteration 19, loss = 0.86690039\n",
            "Iteration 20, loss = 0.86391198\n",
            "Iteration 21, loss = 0.86092538\n",
            "Iteration 22, loss = 0.84715389\n",
            "Iteration 23, loss = 0.84826052\n",
            "Iteration 24, loss = 0.84345296\n",
            "Iteration 25, loss = 0.84214450\n",
            "Iteration 26, loss = 0.83982655\n",
            "Iteration 27, loss = 0.84628187\n",
            "Iteration 28, loss = 0.83730190\n",
            "Iteration 29, loss = 0.84426846\n",
            "Iteration 30, loss = 0.83373539\n",
            "Iteration 31, loss = 0.84118159\n",
            "Iteration 32, loss = 0.84955544\n",
            "Iteration 33, loss = 0.84020517\n",
            "Iteration 34, loss = 0.84274930\n",
            "Iteration 35, loss = 0.83945754\n",
            "Iteration 36, loss = 0.83143173\n",
            "Iteration 37, loss = 0.84190427\n",
            "Iteration 38, loss = 0.83787928\n",
            "Iteration 39, loss = 0.83445075\n",
            "Iteration 40, loss = 0.83895170\n",
            "Iteration 41, loss = 0.83504604\n",
            "Iteration 42, loss = 0.83107964\n",
            "Iteration 43, loss = 0.83123003\n",
            "Iteration 44, loss = 0.82942257\n",
            "Iteration 45, loss = 0.82616099\n",
            "Iteration 46, loss = 0.82752580\n",
            "Iteration 47, loss = 0.83116950\n",
            "Iteration 48, loss = 0.82396657\n",
            "Iteration 49, loss = 0.82672695\n",
            "Iteration 50, loss = 0.82589129\n",
            "Iteration 51, loss = 0.83125924\n",
            "Iteration 52, loss = 0.82490207\n",
            "Iteration 53, loss = 0.83056592\n",
            "Iteration 54, loss = 0.82413162\n",
            "Iteration 55, loss = 0.82841092\n",
            "Iteration 56, loss = 0.82672285\n",
            "Iteration 57, loss = 0.82817394\n",
            "Iteration 58, loss = 0.82392322\n",
            "Iteration 59, loss = 0.82961149\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31448418\n",
            "Iteration 2, loss = 1.25973646\n",
            "Iteration 3, loss = 1.23593851\n",
            "Iteration 4, loss = 1.21864156\n",
            "Iteration 5, loss = 1.20135345\n",
            "Iteration 6, loss = 1.18203490\n",
            "Iteration 7, loss = 1.16192523\n",
            "Iteration 8, loss = 1.14003074\n",
            "Iteration 9, loss = 1.11171735\n",
            "Iteration 10, loss = 1.08649819\n",
            "Iteration 11, loss = 1.05644923\n",
            "Iteration 12, loss = 1.02282291\n",
            "Iteration 13, loss = 0.98891141\n",
            "Iteration 14, loss = 0.95701383\n",
            "Iteration 15, loss = 0.93011004\n",
            "Iteration 16, loss = 0.90946082\n",
            "Iteration 17, loss = 0.88153698\n",
            "Iteration 18, loss = 0.87187362\n",
            "Iteration 19, loss = 0.85816570\n",
            "Iteration 20, loss = 0.85394466\n",
            "Iteration 21, loss = 0.84579091\n",
            "Iteration 22, loss = 0.83604278\n",
            "Iteration 23, loss = 0.84159094\n",
            "Iteration 24, loss = 0.83054900\n",
            "Iteration 25, loss = 0.83435718\n",
            "Iteration 26, loss = 0.82231686\n",
            "Iteration 27, loss = 0.83190051\n",
            "Iteration 28, loss = 0.82326260\n",
            "Iteration 29, loss = 0.83053605\n",
            "Iteration 30, loss = 0.81867947\n",
            "Iteration 31, loss = 0.83528547\n",
            "Iteration 32, loss = 0.83919241\n",
            "Iteration 33, loss = 0.82572318\n",
            "Iteration 34, loss = 0.82126452\n",
            "Iteration 35, loss = 0.82305546\n",
            "Iteration 36, loss = 0.82310005\n",
            "Iteration 37, loss = 0.82548685\n",
            "Iteration 38, loss = 0.81543730\n",
            "Iteration 39, loss = 0.82120862\n",
            "Iteration 40, loss = 0.81640834\n",
            "Iteration 41, loss = 0.81437248\n",
            "Iteration 42, loss = 0.82674959\n",
            "Iteration 43, loss = 0.82965235\n",
            "Iteration 44, loss = 0.81696472\n",
            "Iteration 45, loss = 0.81865021\n",
            "Iteration 46, loss = 0.82105382\n",
            "Iteration 47, loss = 0.81717713\n",
            "Iteration 48, loss = 0.81467648\n",
            "Iteration 49, loss = 0.82211553\n",
            "Iteration 50, loss = 0.81310905\n",
            "Iteration 51, loss = 0.81425909\n",
            "Iteration 52, loss = 0.81550767\n",
            "Iteration 53, loss = 0.81392252\n",
            "Iteration 54, loss = 0.81382404\n",
            "Iteration 55, loss = 0.80979012\n",
            "Iteration 56, loss = 0.82706125\n",
            "Iteration 57, loss = 0.80588154\n",
            "Iteration 58, loss = 0.82582155\n",
            "Iteration 59, loss = 0.81051039\n",
            "Iteration 60, loss = 0.81871332\n",
            "Iteration 61, loss = 0.81735260\n",
            "Iteration 62, loss = 0.81513872\n",
            "Iteration 63, loss = 0.81454929\n",
            "Iteration 64, loss = 0.81223822\n",
            "Iteration 65, loss = 0.81388941\n",
            "Iteration 66, loss = 0.80866534\n",
            "Iteration 67, loss = 0.81382000\n",
            "Iteration 68, loss = 0.80874842\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31515091\n",
            "Iteration 2, loss = 1.25586293\n",
            "Iteration 3, loss = 1.23299615\n",
            "Iteration 4, loss = 1.21739172\n",
            "Iteration 5, loss = 1.19992520\n",
            "Iteration 6, loss = 1.18434399\n",
            "Iteration 7, loss = 1.16672890\n",
            "Iteration 8, loss = 1.14803345\n",
            "Iteration 9, loss = 1.12368437\n",
            "Iteration 10, loss = 1.10136724\n",
            "Iteration 11, loss = 1.07623475\n",
            "Iteration 12, loss = 1.05106467\n",
            "Iteration 13, loss = 1.02194247\n",
            "Iteration 14, loss = 0.99348684\n",
            "Iteration 15, loss = 0.96877180\n",
            "Iteration 16, loss = 0.95056107\n",
            "Iteration 17, loss = 0.92634323\n",
            "Iteration 18, loss = 0.91081715\n",
            "Iteration 19, loss = 0.89568828\n",
            "Iteration 20, loss = 0.89217673\n",
            "Iteration 21, loss = 0.87688836\n",
            "Iteration 22, loss = 0.87400142\n",
            "Iteration 23, loss = 0.87948112\n",
            "Iteration 24, loss = 0.86305449\n",
            "Iteration 25, loss = 0.86718618\n",
            "Iteration 26, loss = 0.86128297\n",
            "Iteration 27, loss = 0.86923312\n",
            "Iteration 28, loss = 0.85586202\n",
            "Iteration 29, loss = 0.86483571\n",
            "Iteration 30, loss = 0.85681460\n",
            "Iteration 31, loss = 0.86056128\n",
            "Iteration 32, loss = 0.86406433\n",
            "Iteration 33, loss = 0.85828323\n",
            "Iteration 34, loss = 0.85161875\n",
            "Iteration 35, loss = 0.85229036\n",
            "Iteration 36, loss = 0.85914204\n",
            "Iteration 37, loss = 0.86429334\n",
            "Iteration 38, loss = 0.85027158\n",
            "Iteration 39, loss = 0.85799654\n",
            "Iteration 40, loss = 0.84937038\n",
            "Iteration 41, loss = 0.85086601\n",
            "Iteration 42, loss = 0.85758425\n",
            "Iteration 43, loss = 0.86261248\n",
            "Iteration 44, loss = 0.85133818\n",
            "Iteration 45, loss = 0.85197811\n",
            "Iteration 46, loss = 0.86271918\n",
            "Iteration 47, loss = 0.85223465\n",
            "Iteration 48, loss = 0.85315786\n",
            "Iteration 49, loss = 0.84985871\n",
            "Iteration 50, loss = 0.84701531\n",
            "Iteration 51, loss = 0.84734133\n",
            "Iteration 52, loss = 0.84997000\n",
            "Iteration 53, loss = 0.84992122\n",
            "Iteration 54, loss = 0.84643732\n",
            "Iteration 55, loss = 0.84469875\n",
            "Iteration 56, loss = 0.86209007\n",
            "Iteration 57, loss = 0.84176024\n",
            "Iteration 58, loss = 0.85638879\n",
            "Iteration 59, loss = 0.84538258\n",
            "Iteration 60, loss = 0.85412969\n",
            "Iteration 61, loss = 0.84845230\n",
            "Iteration 62, loss = 0.84950442\n",
            "Iteration 63, loss = 0.84983296\n",
            "Iteration 64, loss = 0.84560238\n",
            "Iteration 65, loss = 0.84639643\n",
            "Iteration 66, loss = 0.84330880\n",
            "Iteration 67, loss = 0.84936228\n",
            "Iteration 68, loss = 0.84427272\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31290284\n",
            "Iteration 2, loss = 1.25069412\n",
            "Iteration 3, loss = 1.22470710\n",
            "Iteration 4, loss = 1.20611285\n",
            "Iteration 5, loss = 1.18887149\n",
            "Iteration 6, loss = 1.17208943\n",
            "Iteration 7, loss = 1.15484833\n",
            "Iteration 8, loss = 1.13410253\n",
            "Iteration 9, loss = 1.10842831\n",
            "Iteration 10, loss = 1.08345096\n",
            "Iteration 11, loss = 1.05886005\n",
            "Iteration 12, loss = 1.03403264\n",
            "Iteration 13, loss = 1.00499354\n",
            "Iteration 14, loss = 0.97825855\n",
            "Iteration 15, loss = 0.95512271\n",
            "Iteration 16, loss = 0.93761481\n",
            "Iteration 17, loss = 0.91600771\n",
            "Iteration 18, loss = 0.90328806\n",
            "Iteration 19, loss = 0.89296574\n",
            "Iteration 20, loss = 0.88773165\n",
            "Iteration 21, loss = 0.87754392\n",
            "Iteration 22, loss = 0.87115937\n",
            "Iteration 23, loss = 0.87153250\n",
            "Iteration 24, loss = 0.86594588\n",
            "Iteration 25, loss = 0.86503637\n",
            "Iteration 26, loss = 0.86330823\n",
            "Iteration 27, loss = 0.86563301\n",
            "Iteration 28, loss = 0.85903769\n",
            "Iteration 29, loss = 0.86231653\n",
            "Iteration 30, loss = 0.86046847\n",
            "Iteration 31, loss = 0.85417653\n",
            "Iteration 32, loss = 0.86172708\n",
            "Iteration 33, loss = 0.86123851\n",
            "Iteration 34, loss = 0.85230406\n",
            "Iteration 35, loss = 0.85355842\n",
            "Iteration 36, loss = 0.85663564\n",
            "Iteration 37, loss = 0.86493535\n",
            "Iteration 38, loss = 0.85108631\n",
            "Iteration 39, loss = 0.86058560\n",
            "Iteration 40, loss = 0.84675837\n",
            "Iteration 41, loss = 0.85378019\n",
            "Iteration 42, loss = 0.86251219\n",
            "Iteration 43, loss = 0.86284540\n",
            "Iteration 44, loss = 0.85536315\n",
            "Iteration 45, loss = 0.85215940\n",
            "Iteration 46, loss = 0.86619339\n",
            "Iteration 47, loss = 0.84986500\n",
            "Iteration 48, loss = 0.85380995\n",
            "Iteration 49, loss = 0.84917161\n",
            "Iteration 50, loss = 0.84731150\n",
            "Iteration 51, loss = 0.84790721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "27\n",
            "Iteration 1, loss = 1.30742139\n",
            "Iteration 2, loss = 1.23069988\n",
            "Iteration 3, loss = 1.19498078\n",
            "Iteration 4, loss = 1.15261711\n",
            "Iteration 5, loss = 1.11589452\n",
            "Iteration 6, loss = 1.06957439\n",
            "Iteration 7, loss = 1.01450790\n",
            "Iteration 8, loss = 0.95602430\n",
            "Iteration 9, loss = 0.88896860\n",
            "Iteration 10, loss = 0.81671214\n",
            "Iteration 11, loss = 0.74517179\n",
            "Iteration 12, loss = 0.67399601\n",
            "Iteration 13, loss = 0.61209612\n",
            "Iteration 14, loss = 0.55383539\n",
            "Iteration 15, loss = 0.50768715\n",
            "Iteration 16, loss = 0.47004326\n",
            "Iteration 17, loss = 0.43845934\n",
            "Iteration 18, loss = 0.41701306\n",
            "Iteration 19, loss = 0.40165768\n",
            "Iteration 20, loss = 0.39227452\n",
            "Iteration 21, loss = 0.38160177\n",
            "Iteration 22, loss = 0.37330434\n",
            "Iteration 23, loss = 0.37544482\n",
            "Iteration 24, loss = 0.37233431\n",
            "Iteration 25, loss = 0.36887017\n",
            "Iteration 26, loss = 0.36234859\n",
            "Iteration 27, loss = 0.35394407\n",
            "Iteration 28, loss = 0.35305271\n",
            "Iteration 29, loss = 0.35226252\n",
            "Iteration 30, loss = 0.35698567\n",
            "Iteration 31, loss = 0.35134321\n",
            "Iteration 32, loss = 0.35203533\n",
            "Iteration 33, loss = 0.34895969\n",
            "Iteration 34, loss = 0.35112053\n",
            "Iteration 35, loss = 0.34878444\n",
            "Iteration 36, loss = 0.34943926\n",
            "Iteration 37, loss = 0.34827708\n",
            "Iteration 38, loss = 0.34480829\n",
            "Iteration 39, loss = 0.34730414\n",
            "Iteration 40, loss = 0.34599104\n",
            "Iteration 41, loss = 0.35104372\n",
            "Iteration 42, loss = 0.34638078\n",
            "Iteration 43, loss = 0.34561233\n",
            "Iteration 44, loss = 0.34892695\n",
            "Iteration 45, loss = 0.34373314\n",
            "Iteration 46, loss = 0.34153236\n",
            "Iteration 47, loss = 0.34909557\n",
            "Iteration 48, loss = 0.35623308\n",
            "Iteration 49, loss = 0.34627987\n",
            "Iteration 50, loss = 0.35430706\n",
            "Iteration 51, loss = 0.35069010\n",
            "Iteration 52, loss = 0.34611020\n",
            "Iteration 53, loss = 0.34399066\n",
            "Iteration 54, loss = 0.34157457\n",
            "Iteration 55, loss = 0.35296309\n",
            "Iteration 56, loss = 0.34870887\n",
            "Iteration 57, loss = 0.34072560\n",
            "Iteration 58, loss = 0.35195180\n",
            "Iteration 59, loss = 0.33868808\n",
            "Iteration 60, loss = 0.34581105\n",
            "Iteration 61, loss = 0.34546268\n",
            "Iteration 62, loss = 0.34128414\n",
            "Iteration 63, loss = 0.34510043\n",
            "Iteration 64, loss = 0.35064787\n",
            "Iteration 65, loss = 0.34635326\n",
            "Iteration 66, loss = 0.34534117\n",
            "Iteration 67, loss = 0.34738965\n",
            "Iteration 68, loss = 0.34531618\n",
            "Iteration 69, loss = 0.34569478\n",
            "Iteration 70, loss = 0.34024001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30776841\n",
            "Iteration 2, loss = 1.22851578\n",
            "Iteration 3, loss = 1.19396073\n",
            "Iteration 4, loss = 1.15467737\n",
            "Iteration 5, loss = 1.11572213\n",
            "Iteration 6, loss = 1.06959874\n",
            "Iteration 7, loss = 1.01512452\n",
            "Iteration 8, loss = 0.95498251\n",
            "Iteration 9, loss = 0.88794894\n",
            "Iteration 10, loss = 0.81579230\n",
            "Iteration 11, loss = 0.74479914\n",
            "Iteration 12, loss = 0.67340045\n",
            "Iteration 13, loss = 0.61387992\n",
            "Iteration 14, loss = 0.55228348\n",
            "Iteration 15, loss = 0.51180131\n",
            "Iteration 16, loss = 0.47693120\n",
            "Iteration 17, loss = 0.44479278\n",
            "Iteration 18, loss = 0.42842133\n",
            "Iteration 19, loss = 0.40853957\n",
            "Iteration 20, loss = 0.39787383\n",
            "Iteration 21, loss = 0.38678067\n",
            "Iteration 22, loss = 0.38022072\n",
            "Iteration 23, loss = 0.38127744\n",
            "Iteration 24, loss = 0.37279934\n",
            "Iteration 25, loss = 0.37500399\n",
            "Iteration 26, loss = 0.36522766\n",
            "Iteration 27, loss = 0.36000113\n",
            "Iteration 28, loss = 0.35798955\n",
            "Iteration 29, loss = 0.35701836\n",
            "Iteration 30, loss = 0.35407222\n",
            "Iteration 31, loss = 0.35476269\n",
            "Iteration 32, loss = 0.35955211\n",
            "Iteration 33, loss = 0.35181942\n",
            "Iteration 34, loss = 0.35041212\n",
            "Iteration 35, loss = 0.35045980\n",
            "Iteration 36, loss = 0.37121601\n",
            "Iteration 37, loss = 0.36966824\n",
            "Iteration 38, loss = 0.35401703\n",
            "Iteration 39, loss = 0.34860120\n",
            "Iteration 40, loss = 0.34925961\n",
            "Iteration 41, loss = 0.36416942\n",
            "Iteration 42, loss = 0.34836740\n",
            "Iteration 43, loss = 0.34838096\n",
            "Iteration 44, loss = 0.35377582\n",
            "Iteration 45, loss = 0.34694141\n",
            "Iteration 46, loss = 0.34394357\n",
            "Iteration 47, loss = 0.34970541\n",
            "Iteration 48, loss = 0.34737693\n",
            "Iteration 49, loss = 0.34855015\n",
            "Iteration 50, loss = 0.35330327\n",
            "Iteration 51, loss = 0.34766015\n",
            "Iteration 52, loss = 0.34801745\n",
            "Iteration 53, loss = 0.34433812\n",
            "Iteration 54, loss = 0.34448279\n",
            "Iteration 55, loss = 0.35621142\n",
            "Iteration 56, loss = 0.34694210\n",
            "Iteration 57, loss = 0.34361057\n",
            "Iteration 58, loss = 0.34832436\n",
            "Iteration 59, loss = 0.34313599\n",
            "Iteration 60, loss = 0.35102743\n",
            "Iteration 61, loss = 0.34846747\n",
            "Iteration 62, loss = 0.34374516\n",
            "Iteration 63, loss = 0.34572288\n",
            "Iteration 64, loss = 0.35191731\n",
            "Iteration 65, loss = 0.34785220\n",
            "Iteration 66, loss = 0.34855912\n",
            "Iteration 67, loss = 0.35225939\n",
            "Iteration 68, loss = 0.35091682\n",
            "Iteration 69, loss = 0.34860041\n",
            "Iteration 70, loss = 0.34273747\n",
            "Iteration 71, loss = 0.34856723\n",
            "Iteration 72, loss = 0.35107386\n",
            "Iteration 73, loss = 0.34402140\n",
            "Iteration 74, loss = 0.34617300\n",
            "Iteration 75, loss = 0.34356917\n",
            "Iteration 76, loss = 0.34819851\n",
            "Iteration 77, loss = 0.34908392\n",
            "Iteration 78, loss = 0.34715488\n",
            "Iteration 79, loss = 0.34689769\n",
            "Iteration 80, loss = 0.35206694\n",
            "Iteration 81, loss = 0.36181879\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31018977\n",
            "Iteration 2, loss = 1.23319288\n",
            "Iteration 3, loss = 1.19773828\n",
            "Iteration 4, loss = 1.15607646\n",
            "Iteration 5, loss = 1.11356423\n",
            "Iteration 6, loss = 1.06355493\n",
            "Iteration 7, loss = 1.00713245\n",
            "Iteration 8, loss = 0.94124891\n",
            "Iteration 9, loss = 0.87412254\n",
            "Iteration 10, loss = 0.80097252\n",
            "Iteration 11, loss = 0.73044687\n",
            "Iteration 12, loss = 0.66394911\n",
            "Iteration 13, loss = 0.60207644\n",
            "Iteration 14, loss = 0.54505717\n",
            "Iteration 15, loss = 0.50360642\n",
            "Iteration 16, loss = 0.47395417\n",
            "Iteration 17, loss = 0.44461922\n",
            "Iteration 18, loss = 0.43088856\n",
            "Iteration 19, loss = 0.41904588\n",
            "Iteration 20, loss = 0.39846188\n",
            "Iteration 21, loss = 0.39225239\n",
            "Iteration 22, loss = 0.38338828\n",
            "Iteration 23, loss = 0.38191395\n",
            "Iteration 24, loss = 0.37549426\n",
            "Iteration 25, loss = 0.37513965\n",
            "Iteration 26, loss = 0.37367581\n",
            "Iteration 27, loss = 0.37101157\n",
            "Iteration 28, loss = 0.36667571\n",
            "Iteration 29, loss = 0.36519905\n",
            "Iteration 30, loss = 0.36404683\n",
            "Iteration 31, loss = 0.36406178\n",
            "Iteration 32, loss = 0.36434893\n",
            "Iteration 33, loss = 0.36487698\n",
            "Iteration 34, loss = 0.36128980\n",
            "Iteration 35, loss = 0.36168973\n",
            "Iteration 36, loss = 0.37102927\n",
            "Iteration 37, loss = 0.36219241\n",
            "Iteration 38, loss = 0.35950719\n",
            "Iteration 39, loss = 0.35733094\n",
            "Iteration 40, loss = 0.35400817\n",
            "Iteration 41, loss = 0.36227707\n",
            "Iteration 42, loss = 0.35426294\n",
            "Iteration 43, loss = 0.35672033\n",
            "Iteration 44, loss = 0.35605518\n",
            "Iteration 45, loss = 0.35547638\n",
            "Iteration 46, loss = 0.35327856\n",
            "Iteration 47, loss = 0.35654625\n",
            "Iteration 48, loss = 0.36421892\n",
            "Iteration 49, loss = 0.35697105\n",
            "Iteration 50, loss = 0.36434182\n",
            "Iteration 51, loss = 0.36248674\n",
            "Iteration 52, loss = 0.35534500\n",
            "Iteration 53, loss = 0.35336540\n",
            "Iteration 54, loss = 0.35218652\n",
            "Iteration 55, loss = 0.37287185\n",
            "Iteration 56, loss = 0.35406315\n",
            "Iteration 57, loss = 0.35067952\n",
            "Iteration 58, loss = 0.35851935\n",
            "Iteration 59, loss = 0.35231388\n",
            "Iteration 60, loss = 0.36715795\n",
            "Iteration 61, loss = 0.35392643\n",
            "Iteration 62, loss = 0.35400478\n",
            "Iteration 63, loss = 0.35284178\n",
            "Iteration 64, loss = 0.36112141\n",
            "Iteration 65, loss = 0.35504918\n",
            "Iteration 66, loss = 0.35711206\n",
            "Iteration 67, loss = 0.35670360\n",
            "Iteration 68, loss = 0.35639240\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31028342\n",
            "Iteration 2, loss = 1.23789378\n",
            "Iteration 3, loss = 1.20178631\n",
            "Iteration 4, loss = 1.16102742\n",
            "Iteration 5, loss = 1.11811522\n",
            "Iteration 6, loss = 1.06778084\n",
            "Iteration 7, loss = 1.01431962\n",
            "Iteration 8, loss = 0.94954820\n",
            "Iteration 9, loss = 0.88286178\n",
            "Iteration 10, loss = 0.80960723\n",
            "Iteration 11, loss = 0.73721023\n",
            "Iteration 12, loss = 0.67505539\n",
            "Iteration 13, loss = 0.61486615\n",
            "Iteration 14, loss = 0.55941803\n",
            "Iteration 15, loss = 0.51487655\n",
            "Iteration 16, loss = 0.49227327\n",
            "Iteration 17, loss = 0.46246252\n",
            "Iteration 18, loss = 0.44696361\n",
            "Iteration 19, loss = 0.43714768\n",
            "Iteration 20, loss = 0.41447120\n",
            "Iteration 21, loss = 0.41120712\n",
            "Iteration 22, loss = 0.40637203\n",
            "Iteration 23, loss = 0.39787051\n",
            "Iteration 24, loss = 0.38828437\n",
            "Iteration 25, loss = 0.39177514\n",
            "Iteration 26, loss = 0.38693212\n",
            "Iteration 27, loss = 0.38441564\n",
            "Iteration 28, loss = 0.38150076\n",
            "Iteration 29, loss = 0.38435723\n",
            "Iteration 30, loss = 0.37520777\n",
            "Iteration 31, loss = 0.37641131\n",
            "Iteration 32, loss = 0.37941365\n",
            "Iteration 33, loss = 0.38096828\n",
            "Iteration 34, loss = 0.37746067\n",
            "Iteration 35, loss = 0.37829011\n",
            "Iteration 36, loss = 0.38278509\n",
            "Iteration 37, loss = 0.37426212\n",
            "Iteration 38, loss = 0.37603497\n",
            "Iteration 39, loss = 0.36848688\n",
            "Iteration 40, loss = 0.36679370\n",
            "Iteration 41, loss = 0.37212259\n",
            "Iteration 42, loss = 0.36726455\n",
            "Iteration 43, loss = 0.36789505\n",
            "Iteration 44, loss = 0.36886972\n",
            "Iteration 45, loss = 0.36864548\n",
            "Iteration 46, loss = 0.36635525\n",
            "Iteration 47, loss = 0.37085480\n",
            "Iteration 48, loss = 0.38107137\n",
            "Iteration 49, loss = 0.37659685\n",
            "Iteration 50, loss = 0.37317431\n",
            "Iteration 51, loss = 0.38282466\n",
            "Iteration 52, loss = 0.37108252\n",
            "Iteration 53, loss = 0.36843577\n",
            "Iteration 54, loss = 0.36924039\n",
            "Iteration 55, loss = 0.38920310\n",
            "Iteration 56, loss = 0.36493277\n",
            "Iteration 57, loss = 0.36539319\n",
            "Iteration 58, loss = 0.37517357\n",
            "Iteration 59, loss = 0.36324920\n",
            "Iteration 60, loss = 0.37971836\n",
            "Iteration 61, loss = 0.36633460\n",
            "Iteration 62, loss = 0.36612373\n",
            "Iteration 63, loss = 0.36787724\n",
            "Iteration 64, loss = 0.37992515\n",
            "Iteration 65, loss = 0.37507456\n",
            "Iteration 66, loss = 0.37841894\n",
            "Iteration 67, loss = 0.37026207\n",
            "Iteration 68, loss = 0.37631861\n",
            "Iteration 69, loss = 0.37147633\n",
            "Iteration 70, loss = 0.36532710\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30749363\n",
            "Iteration 2, loss = 1.22889099\n",
            "Iteration 3, loss = 1.19112561\n",
            "Iteration 4, loss = 1.15113800\n",
            "Iteration 5, loss = 1.10981105\n",
            "Iteration 6, loss = 1.06020249\n",
            "Iteration 7, loss = 1.00635747\n",
            "Iteration 8, loss = 0.94516717\n",
            "Iteration 9, loss = 0.87865689\n",
            "Iteration 10, loss = 0.80790858\n",
            "Iteration 11, loss = 0.73693316\n",
            "Iteration 12, loss = 0.67567465\n",
            "Iteration 13, loss = 0.61636431\n",
            "Iteration 14, loss = 0.56056717\n",
            "Iteration 15, loss = 0.51986117\n",
            "Iteration 16, loss = 0.49000021\n",
            "Iteration 17, loss = 0.45722607\n",
            "Iteration 18, loss = 0.44111359\n",
            "Iteration 19, loss = 0.42106237\n",
            "Iteration 20, loss = 0.40618079\n",
            "Iteration 21, loss = 0.39653378\n",
            "Iteration 22, loss = 0.39032141\n",
            "Iteration 23, loss = 0.38477302\n",
            "Iteration 24, loss = 0.37932787\n",
            "Iteration 25, loss = 0.38073656\n",
            "Iteration 26, loss = 0.37798659\n",
            "Iteration 27, loss = 0.37262184\n",
            "Iteration 28, loss = 0.36881681\n",
            "Iteration 29, loss = 0.37437306\n",
            "Iteration 30, loss = 0.36673692\n",
            "Iteration 31, loss = 0.36376563\n",
            "Iteration 32, loss = 0.36639831\n",
            "Iteration 33, loss = 0.37496711\n",
            "Iteration 34, loss = 0.36389235\n",
            "Iteration 35, loss = 0.37031363\n",
            "Iteration 36, loss = 0.36641651\n",
            "Iteration 37, loss = 0.36261221\n",
            "Iteration 38, loss = 0.36670808\n",
            "Iteration 39, loss = 0.35911730\n",
            "Iteration 40, loss = 0.35877948\n",
            "Iteration 41, loss = 0.36039098\n",
            "Iteration 42, loss = 0.35919549\n",
            "Iteration 43, loss = 0.35691008\n",
            "Iteration 44, loss = 0.36303098\n",
            "Iteration 45, loss = 0.35749156\n",
            "Iteration 46, loss = 0.35648864\n",
            "Iteration 47, loss = 0.36185899\n",
            "Iteration 48, loss = 0.37384621\n",
            "Iteration 49, loss = 0.36558490\n",
            "Iteration 50, loss = 0.36062666\n",
            "Iteration 51, loss = 0.36983734\n",
            "Iteration 52, loss = 0.36033005\n",
            "Iteration 53, loss = 0.35932951\n",
            "Iteration 54, loss = 0.36058545\n",
            "Iteration 55, loss = 0.37767164\n",
            "Iteration 56, loss = 0.35898433\n",
            "Iteration 57, loss = 0.35631400\n",
            "Iteration 58, loss = 0.36463332\n",
            "Iteration 59, loss = 0.35566780\n",
            "Iteration 60, loss = 0.36990848\n",
            "Iteration 61, loss = 0.35747859\n",
            "Iteration 62, loss = 0.35773317\n",
            "Iteration 63, loss = 0.35684524\n",
            "Iteration 64, loss = 0.36445692\n",
            "Iteration 65, loss = 0.36316569\n",
            "Iteration 66, loss = 0.36678212\n",
            "Iteration 67, loss = 0.36024733\n",
            "Iteration 68, loss = 0.36630420\n",
            "Iteration 69, loss = 0.36400265\n",
            "Iteration 70, loss = 0.35835275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30356225\n",
            "Iteration 2, loss = 1.22209851\n",
            "Iteration 3, loss = 1.18575234\n",
            "Iteration 4, loss = 1.14949780\n",
            "Iteration 5, loss = 1.10678022\n",
            "Iteration 6, loss = 1.06187228\n",
            "Iteration 7, loss = 1.00733416\n",
            "Iteration 8, loss = 0.95022561\n",
            "Iteration 9, loss = 0.88415798\n",
            "Iteration 10, loss = 0.81530543\n",
            "Iteration 11, loss = 0.74462507\n",
            "Iteration 12, loss = 0.68479657\n",
            "Iteration 13, loss = 0.61775090\n",
            "Iteration 14, loss = 0.56064959\n",
            "Iteration 15, loss = 0.52050315\n",
            "Iteration 16, loss = 0.49066486\n",
            "Iteration 17, loss = 0.45665472\n",
            "Iteration 18, loss = 0.43783373\n",
            "Iteration 19, loss = 0.41698342\n",
            "Iteration 20, loss = 0.40265847\n",
            "Iteration 21, loss = 0.39218645\n",
            "Iteration 22, loss = 0.38496891\n",
            "Iteration 23, loss = 0.38229204\n",
            "Iteration 24, loss = 0.37470462\n",
            "Iteration 25, loss = 0.37558088\n",
            "Iteration 26, loss = 0.37431529\n",
            "Iteration 27, loss = 0.36514466\n",
            "Iteration 28, loss = 0.36218876\n",
            "Iteration 29, loss = 0.36689529\n",
            "Iteration 30, loss = 0.36099264\n",
            "Iteration 31, loss = 0.35780575\n",
            "Iteration 32, loss = 0.36299712\n",
            "Iteration 33, loss = 0.36867650\n",
            "Iteration 34, loss = 0.35756752\n",
            "Iteration 35, loss = 0.36436887\n",
            "Iteration 36, loss = 0.36032139\n",
            "Iteration 37, loss = 0.35597142\n",
            "Iteration 38, loss = 0.36391207\n",
            "Iteration 39, loss = 0.35222642\n",
            "Iteration 40, loss = 0.35311266\n",
            "Iteration 41, loss = 0.35267647\n",
            "Iteration 42, loss = 0.35221883\n",
            "Iteration 43, loss = 0.34913424\n",
            "Iteration 44, loss = 0.35499999\n",
            "Iteration 45, loss = 0.35092494\n",
            "Iteration 46, loss = 0.35360847\n",
            "Iteration 47, loss = 0.35999076\n",
            "Iteration 48, loss = 0.35778434\n",
            "Iteration 49, loss = 0.35667110\n",
            "Iteration 50, loss = 0.35154665\n",
            "Iteration 51, loss = 0.35770684\n",
            "Iteration 52, loss = 0.35146735\n",
            "Iteration 53, loss = 0.35278608\n",
            "Iteration 54, loss = 0.35121135\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30460203\n",
            "Iteration 2, loss = 1.22852929\n",
            "Iteration 3, loss = 1.19126984\n",
            "Iteration 4, loss = 1.15162330\n",
            "Iteration 5, loss = 1.11044784\n",
            "Iteration 6, loss = 1.06203055\n",
            "Iteration 7, loss = 1.00889532\n",
            "Iteration 8, loss = 0.94847232\n",
            "Iteration 9, loss = 0.88206997\n",
            "Iteration 10, loss = 0.81165631\n",
            "Iteration 11, loss = 0.74136789\n",
            "Iteration 12, loss = 0.67795505\n",
            "Iteration 13, loss = 0.61215465\n",
            "Iteration 14, loss = 0.55943124\n",
            "Iteration 15, loss = 0.52612298\n",
            "Iteration 16, loss = 0.50435053\n",
            "Iteration 17, loss = 0.46661285\n",
            "Iteration 18, loss = 0.44980368\n",
            "Iteration 19, loss = 0.42693676\n",
            "Iteration 20, loss = 0.41529446\n",
            "Iteration 21, loss = 0.40602857\n",
            "Iteration 22, loss = 0.39832638\n",
            "Iteration 23, loss = 0.39198038\n",
            "Iteration 24, loss = 0.38489383\n",
            "Iteration 25, loss = 0.39358773\n",
            "Iteration 26, loss = 0.38925137\n",
            "Iteration 27, loss = 0.37734960\n",
            "Iteration 28, loss = 0.37953810\n",
            "Iteration 29, loss = 0.38173459\n",
            "Iteration 30, loss = 0.37080983\n",
            "Iteration 31, loss = 0.37183752\n",
            "Iteration 32, loss = 0.38079543\n",
            "Iteration 33, loss = 0.37967455\n",
            "Iteration 34, loss = 0.37097175\n",
            "Iteration 35, loss = 0.37555194\n",
            "Iteration 36, loss = 0.36949236\n",
            "Iteration 37, loss = 0.37424184\n",
            "Iteration 38, loss = 0.37665648\n",
            "Iteration 39, loss = 0.36428504\n",
            "Iteration 40, loss = 0.36983133\n",
            "Iteration 41, loss = 0.37066394\n",
            "Iteration 42, loss = 0.36926168\n",
            "Iteration 43, loss = 0.36291397\n",
            "Iteration 44, loss = 0.37832179\n",
            "Iteration 45, loss = 0.36730064\n",
            "Iteration 46, loss = 0.36565306\n",
            "Iteration 47, loss = 0.37986325\n",
            "Iteration 48, loss = 0.37318597\n",
            "Iteration 49, loss = 0.36888536\n",
            "Iteration 50, loss = 0.37010744\n",
            "Iteration 51, loss = 0.36801822\n",
            "Iteration 52, loss = 0.36312568\n",
            "Iteration 53, loss = 0.36595547\n",
            "Iteration 54, loss = 0.36242094\n",
            "Iteration 55, loss = 0.36323598\n",
            "Iteration 56, loss = 0.36016061\n",
            "Iteration 57, loss = 0.36066244\n",
            "Iteration 58, loss = 0.36279202\n",
            "Iteration 59, loss = 0.36196133\n",
            "Iteration 60, loss = 0.37469649\n",
            "Iteration 61, loss = 0.36269328\n",
            "Iteration 62, loss = 0.36120257\n",
            "Iteration 63, loss = 0.36122433\n",
            "Iteration 64, loss = 0.37070429\n",
            "Iteration 65, loss = 0.36725376\n",
            "Iteration 66, loss = 0.36837872\n",
            "Iteration 67, loss = 0.36475039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30426880\n",
            "Iteration 2, loss = 1.23092699\n",
            "Iteration 3, loss = 1.19643693\n",
            "Iteration 4, loss = 1.16136496\n",
            "Iteration 5, loss = 1.12152272\n",
            "Iteration 6, loss = 1.07619896\n",
            "Iteration 7, loss = 1.03127422\n",
            "Iteration 8, loss = 0.97456707\n",
            "Iteration 9, loss = 0.91535032\n",
            "Iteration 10, loss = 0.84919343\n",
            "Iteration 11, loss = 0.78701020\n",
            "Iteration 12, loss = 0.72040589\n",
            "Iteration 13, loss = 0.65833523\n",
            "Iteration 14, loss = 0.60663231\n",
            "Iteration 15, loss = 0.55727417\n",
            "Iteration 16, loss = 0.51766969\n",
            "Iteration 17, loss = 0.48431404\n",
            "Iteration 18, loss = 0.45683575\n",
            "Iteration 19, loss = 0.43713854\n",
            "Iteration 20, loss = 0.42179649\n",
            "Iteration 21, loss = 0.40979587\n",
            "Iteration 22, loss = 0.40137316\n",
            "Iteration 23, loss = 0.40368023\n",
            "Iteration 24, loss = 0.40084718\n",
            "Iteration 25, loss = 0.39730730\n",
            "Iteration 26, loss = 0.39690009\n",
            "Iteration 27, loss = 0.38878246\n",
            "Iteration 28, loss = 0.38562453\n",
            "Iteration 29, loss = 0.39081676\n",
            "Iteration 30, loss = 0.38231659\n",
            "Iteration 31, loss = 0.38234985\n",
            "Iteration 32, loss = 0.38464567\n",
            "Iteration 33, loss = 0.37689185\n",
            "Iteration 34, loss = 0.37258762\n",
            "Iteration 35, loss = 0.37529666\n",
            "Iteration 36, loss = 0.37372435\n",
            "Iteration 37, loss = 0.36884715\n",
            "Iteration 38, loss = 0.36982401\n",
            "Iteration 39, loss = 0.37685572\n",
            "Iteration 40, loss = 0.37016161\n",
            "Iteration 41, loss = 0.37226134\n",
            "Iteration 42, loss = 0.37073318\n",
            "Iteration 43, loss = 0.37004689\n",
            "Iteration 44, loss = 0.37443333\n",
            "Iteration 45, loss = 0.36669310\n",
            "Iteration 46, loss = 0.36928634\n",
            "Iteration 47, loss = 0.37113156\n",
            "Iteration 48, loss = 0.36885676\n",
            "Iteration 49, loss = 0.37056139\n",
            "Iteration 50, loss = 0.36554905\n",
            "Iteration 51, loss = 0.36905233\n",
            "Iteration 52, loss = 0.36678818\n",
            "Iteration 53, loss = 0.37377925\n",
            "Iteration 54, loss = 0.36903415\n",
            "Iteration 55, loss = 0.39220023\n",
            "Iteration 56, loss = 0.40046572\n",
            "Iteration 57, loss = 0.37742410\n",
            "Iteration 58, loss = 0.39038522\n",
            "Iteration 59, loss = 0.37799021\n",
            "Iteration 60, loss = 0.38095919\n",
            "Iteration 61, loss = 0.36732673\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30510618\n",
            "Iteration 2, loss = 1.22538156\n",
            "Iteration 3, loss = 1.19135203\n",
            "Iteration 4, loss = 1.15826216\n",
            "Iteration 5, loss = 1.11789756\n",
            "Iteration 6, loss = 1.07531494\n",
            "Iteration 7, loss = 1.02701590\n",
            "Iteration 8, loss = 0.97515449\n",
            "Iteration 9, loss = 0.91308404\n",
            "Iteration 10, loss = 0.85133077\n",
            "Iteration 11, loss = 0.78988197\n",
            "Iteration 12, loss = 0.72594061\n",
            "Iteration 13, loss = 0.66587206\n",
            "Iteration 14, loss = 0.61363787\n",
            "Iteration 15, loss = 0.56379260\n",
            "Iteration 16, loss = 0.52404632\n",
            "Iteration 17, loss = 0.48922252\n",
            "Iteration 18, loss = 0.46140903\n",
            "Iteration 19, loss = 0.44033151\n",
            "Iteration 20, loss = 0.42477354\n",
            "Iteration 21, loss = 0.41108167\n",
            "Iteration 22, loss = 0.40022298\n",
            "Iteration 23, loss = 0.40358994\n",
            "Iteration 24, loss = 0.39665501\n",
            "Iteration 25, loss = 0.39179706\n",
            "Iteration 26, loss = 0.39130912\n",
            "Iteration 27, loss = 0.38629254\n",
            "Iteration 28, loss = 0.38196221\n",
            "Iteration 29, loss = 0.38288059\n",
            "Iteration 30, loss = 0.37249009\n",
            "Iteration 31, loss = 0.37555750\n",
            "Iteration 32, loss = 0.37127715\n",
            "Iteration 33, loss = 0.36987174\n",
            "Iteration 34, loss = 0.36547584\n",
            "Iteration 35, loss = 0.36663305\n",
            "Iteration 36, loss = 0.36636488\n",
            "Iteration 37, loss = 0.36223547\n",
            "Iteration 38, loss = 0.36150297\n",
            "Iteration 39, loss = 0.36895760\n",
            "Iteration 40, loss = 0.36308076\n",
            "Iteration 41, loss = 0.36370422\n",
            "Iteration 42, loss = 0.36194073\n",
            "Iteration 43, loss = 0.36056141\n",
            "Iteration 44, loss = 0.36521949\n",
            "Iteration 45, loss = 0.35956310\n",
            "Iteration 46, loss = 0.36195534\n",
            "Iteration 47, loss = 0.36565796\n",
            "Iteration 48, loss = 0.35996790\n",
            "Iteration 49, loss = 0.36266456\n",
            "Iteration 50, loss = 0.35658033\n",
            "Iteration 51, loss = 0.36551733\n",
            "Iteration 52, loss = 0.35730944\n",
            "Iteration 53, loss = 0.36415602\n",
            "Iteration 54, loss = 0.36031220\n",
            "Iteration 55, loss = 0.38200275\n",
            "Iteration 56, loss = 0.39090145\n",
            "Iteration 57, loss = 0.36508949\n",
            "Iteration 58, loss = 0.37733169\n",
            "Iteration 59, loss = 0.36577124\n",
            "Iteration 60, loss = 0.36807855\n",
            "Iteration 61, loss = 0.35897082\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30273372\n",
            "Iteration 2, loss = 1.22183677\n",
            "Iteration 3, loss = 1.18570486\n",
            "Iteration 4, loss = 1.14781099\n",
            "Iteration 5, loss = 1.10746964\n",
            "Iteration 6, loss = 1.06469748\n",
            "Iteration 7, loss = 1.01591365\n",
            "Iteration 8, loss = 0.96274806\n",
            "Iteration 9, loss = 0.90145607\n",
            "Iteration 10, loss = 0.83961630\n",
            "Iteration 11, loss = 0.77941583\n",
            "Iteration 12, loss = 0.71967958\n",
            "Iteration 13, loss = 0.65771178\n",
            "Iteration 14, loss = 0.60977286\n",
            "Iteration 15, loss = 0.56356796\n",
            "Iteration 16, loss = 0.52586811\n",
            "Iteration 17, loss = 0.49469057\n",
            "Iteration 18, loss = 0.46784765\n",
            "Iteration 19, loss = 0.44872443\n",
            "Iteration 20, loss = 0.43601894\n",
            "Iteration 21, loss = 0.42380204\n",
            "Iteration 22, loss = 0.41280942\n",
            "Iteration 23, loss = 0.41139449\n",
            "Iteration 24, loss = 0.40605596\n",
            "Iteration 25, loss = 0.40022809\n",
            "Iteration 26, loss = 0.39780316\n",
            "Iteration 27, loss = 0.39636722\n",
            "Iteration 28, loss = 0.39498208\n",
            "Iteration 29, loss = 0.39088904\n",
            "Iteration 30, loss = 0.38718718\n",
            "Iteration 31, loss = 0.38819814\n",
            "Iteration 32, loss = 0.38577161\n",
            "Iteration 33, loss = 0.38352971\n",
            "Iteration 34, loss = 0.37645148\n",
            "Iteration 35, loss = 0.38150329\n",
            "Iteration 36, loss = 0.38083102\n",
            "Iteration 37, loss = 0.37601014\n",
            "Iteration 38, loss = 0.37462436\n",
            "Iteration 39, loss = 0.38546512\n",
            "Iteration 40, loss = 0.37750079\n",
            "Iteration 41, loss = 0.37587738\n",
            "Iteration 42, loss = 0.37455388\n",
            "Iteration 43, loss = 0.37204508\n",
            "Iteration 44, loss = 0.37588645\n",
            "Iteration 45, loss = 0.37157233\n",
            "Iteration 46, loss = 0.37861664\n",
            "Iteration 47, loss = 0.38292808\n",
            "Iteration 48, loss = 0.37318098\n",
            "Iteration 49, loss = 0.37647214\n",
            "Iteration 50, loss = 0.37103740\n",
            "Iteration 51, loss = 0.37754273\n",
            "Iteration 52, loss = 0.37044330\n",
            "Iteration 53, loss = 0.37610661\n",
            "Iteration 54, loss = 0.37122612\n",
            "Iteration 55, loss = 0.38653925\n",
            "Iteration 56, loss = 0.39909179\n",
            "Iteration 57, loss = 0.37692332\n",
            "Iteration 58, loss = 0.38227057\n",
            "Iteration 59, loss = 0.37633390\n",
            "Iteration 60, loss = 0.37631930\n",
            "Iteration 61, loss = 0.37364978\n",
            "Iteration 62, loss = 0.37157977\n",
            "Iteration 63, loss = 0.39136941\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "28\n",
            "Iteration 1, loss = 1.29703102\n",
            "Iteration 2, loss = 1.20064484\n",
            "Iteration 3, loss = 1.14018727\n",
            "Iteration 4, loss = 1.07810046\n",
            "Iteration 5, loss = 1.00682041\n",
            "Iteration 6, loss = 0.92597255\n",
            "Iteration 7, loss = 0.83040004\n",
            "Iteration 8, loss = 0.72927688\n",
            "Iteration 9, loss = 0.62055653\n",
            "Iteration 10, loss = 0.51176303\n",
            "Iteration 11, loss = 0.40902845\n",
            "Iteration 12, loss = 0.32021273\n",
            "Iteration 13, loss = 0.25016038\n",
            "Iteration 14, loss = 0.19438805\n",
            "Iteration 15, loss = 0.15477064\n",
            "Iteration 16, loss = 0.12404183\n",
            "Iteration 17, loss = 0.10440630\n",
            "Iteration 18, loss = 0.08714644\n",
            "Iteration 19, loss = 0.07653740\n",
            "Iteration 20, loss = 0.07189874\n",
            "Iteration 21, loss = 0.06368224\n",
            "Iteration 22, loss = 0.05851005\n",
            "Iteration 23, loss = 0.05375550\n",
            "Iteration 24, loss = 0.05194100\n",
            "Iteration 25, loss = 0.04802883\n",
            "Iteration 26, loss = 0.04666513\n",
            "Iteration 27, loss = 0.04380518\n",
            "Iteration 28, loss = 0.04226958\n",
            "Iteration 29, loss = 0.04239289\n",
            "Iteration 30, loss = 0.04113195\n",
            "Iteration 31, loss = 0.03955583\n",
            "Iteration 32, loss = 0.03763972\n",
            "Iteration 33, loss = 0.03763002\n",
            "Iteration 34, loss = 0.03724013\n",
            "Iteration 35, loss = 0.03531055\n",
            "Iteration 36, loss = 0.03496771\n",
            "Iteration 37, loss = 0.03445191\n",
            "Iteration 38, loss = 0.03281138\n",
            "Iteration 39, loss = 0.03443465\n",
            "Iteration 40, loss = 0.03352283\n",
            "Iteration 41, loss = 0.03361423\n",
            "Iteration 42, loss = 0.03353949\n",
            "Iteration 43, loss = 0.03114938\n",
            "Iteration 44, loss = 0.03155174\n",
            "Iteration 45, loss = 0.02993018\n",
            "Iteration 46, loss = 0.02970076\n",
            "Iteration 47, loss = 0.02977106\n",
            "Iteration 48, loss = 0.02958661\n",
            "Iteration 49, loss = 0.03113995\n",
            "Iteration 50, loss = 0.02740363\n",
            "Iteration 51, loss = 0.03237021\n",
            "Iteration 52, loss = 0.02929695\n",
            "Iteration 53, loss = 0.02784917\n",
            "Iteration 54, loss = 0.02870565\n",
            "Iteration 55, loss = 0.02747804\n",
            "Iteration 56, loss = 0.02883515\n",
            "Iteration 57, loss = 0.02866870\n",
            "Iteration 58, loss = 0.03040964\n",
            "Iteration 59, loss = 0.02896743\n",
            "Iteration 60, loss = 0.02658970\n",
            "Iteration 61, loss = 0.02731012\n",
            "Iteration 62, loss = 0.02666048\n",
            "Iteration 63, loss = 0.02624278\n",
            "Iteration 64, loss = 0.02580744\n",
            "Iteration 65, loss = 0.02779328\n",
            "Iteration 66, loss = 0.02544711\n",
            "Iteration 67, loss = 0.02841579\n",
            "Iteration 68, loss = 0.02469698\n",
            "Iteration 69, loss = 0.02502984\n",
            "Iteration 70, loss = 0.02465543\n",
            "Iteration 71, loss = 0.02440505\n",
            "Iteration 72, loss = 0.02531521\n",
            "Iteration 73, loss = 0.02404996\n",
            "Iteration 74, loss = 0.02578620\n",
            "Iteration 75, loss = 0.02464756\n",
            "Iteration 76, loss = 0.02634845\n",
            "Iteration 77, loss = 0.02837892\n",
            "Iteration 78, loss = 0.02745648\n",
            "Iteration 79, loss = 0.02449440\n",
            "Iteration 80, loss = 0.02645069\n",
            "Iteration 81, loss = 0.02586303\n",
            "Iteration 82, loss = 0.02562121\n",
            "Iteration 83, loss = 0.02188516\n",
            "Iteration 84, loss = 0.02877913\n",
            "Iteration 85, loss = 0.02354919\n",
            "Iteration 86, loss = 0.03203628\n",
            "Iteration 87, loss = 0.02249421\n",
            "Iteration 88, loss = 0.02466028\n",
            "Iteration 89, loss = 0.02426879\n",
            "Iteration 90, loss = 0.02428451\n",
            "Iteration 91, loss = 0.02405911\n",
            "Iteration 92, loss = 0.02450161\n",
            "Iteration 93, loss = 0.02213134\n",
            "Iteration 94, loss = 0.02284746\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.29688470\n",
            "Iteration 2, loss = 1.19932475\n",
            "Iteration 3, loss = 1.14018538\n",
            "Iteration 4, loss = 1.08283891\n",
            "Iteration 5, loss = 1.01598358\n",
            "Iteration 6, loss = 0.93362522\n",
            "Iteration 7, loss = 0.84093571\n",
            "Iteration 8, loss = 0.74147054\n",
            "Iteration 9, loss = 0.63356857\n",
            "Iteration 10, loss = 0.52414378\n",
            "Iteration 11, loss = 0.42252494\n",
            "Iteration 12, loss = 0.33508769\n",
            "Iteration 13, loss = 0.26586751\n",
            "Iteration 14, loss = 0.20938610\n",
            "Iteration 15, loss = 0.16920873\n",
            "Iteration 16, loss = 0.13652798\n",
            "Iteration 17, loss = 0.11552739\n",
            "Iteration 18, loss = 0.09794187\n",
            "Iteration 19, loss = 0.08670304\n",
            "Iteration 20, loss = 0.08260913\n",
            "Iteration 21, loss = 0.07318837\n",
            "Iteration 22, loss = 0.06777347\n",
            "Iteration 23, loss = 0.06274015\n",
            "Iteration 24, loss = 0.06088612\n",
            "Iteration 25, loss = 0.05649038\n",
            "Iteration 26, loss = 0.05360549\n",
            "Iteration 27, loss = 0.05088511\n",
            "Iteration 28, loss = 0.04949355\n",
            "Iteration 29, loss = 0.04880576\n",
            "Iteration 30, loss = 0.04755197\n",
            "Iteration 31, loss = 0.04550037\n",
            "Iteration 32, loss = 0.04443579\n",
            "Iteration 33, loss = 0.04392718\n",
            "Iteration 34, loss = 0.04603238\n",
            "Iteration 35, loss = 0.04315236\n",
            "Iteration 36, loss = 0.04196382\n",
            "Iteration 37, loss = 0.04042790\n",
            "Iteration 38, loss = 0.03858943\n",
            "Iteration 39, loss = 0.04089149\n",
            "Iteration 40, loss = 0.03956508\n",
            "Iteration 41, loss = 0.03987532\n",
            "Iteration 42, loss = 0.04137006\n",
            "Iteration 43, loss = 0.03715246\n",
            "Iteration 44, loss = 0.03973345\n",
            "Iteration 45, loss = 0.03455625\n",
            "Iteration 46, loss = 0.03605716\n",
            "Iteration 47, loss = 0.03567151\n",
            "Iteration 48, loss = 0.03373182\n",
            "Iteration 49, loss = 0.03732004\n",
            "Iteration 50, loss = 0.03280438\n",
            "Iteration 51, loss = 0.03814994\n",
            "Iteration 52, loss = 0.03448164\n",
            "Iteration 53, loss = 0.03691027\n",
            "Iteration 54, loss = 0.03520148\n",
            "Iteration 55, loss = 0.03256451\n",
            "Iteration 56, loss = 0.03409506\n",
            "Iteration 57, loss = 0.03130057\n",
            "Iteration 58, loss = 0.03144266\n",
            "Iteration 59, loss = 0.03068515\n",
            "Iteration 60, loss = 0.03086774\n",
            "Iteration 61, loss = 0.02990813\n",
            "Iteration 62, loss = 0.03045328\n",
            "Iteration 63, loss = 0.02912923\n",
            "Iteration 64, loss = 0.03043949\n",
            "Iteration 65, loss = 0.03144951\n",
            "Iteration 66, loss = 0.02972860\n",
            "Iteration 67, loss = 0.03172597\n",
            "Iteration 68, loss = 0.02832022\n",
            "Iteration 69, loss = 0.02885503\n",
            "Iteration 70, loss = 0.02806935\n",
            "Iteration 71, loss = 0.02891454\n",
            "Iteration 72, loss = 0.03085231\n",
            "Iteration 73, loss = 0.02735878\n",
            "Iteration 74, loss = 0.02943851\n",
            "Iteration 75, loss = 0.02891107\n",
            "Iteration 76, loss = 0.02856899\n",
            "Iteration 77, loss = 0.03219104\n",
            "Iteration 78, loss = 0.02928448\n",
            "Iteration 79, loss = 0.02874497\n",
            "Iteration 80, loss = 0.02815454\n",
            "Iteration 81, loss = 0.02879227\n",
            "Iteration 82, loss = 0.02692435\n",
            "Iteration 83, loss = 0.02539768\n",
            "Iteration 84, loss = 0.02926989\n",
            "Iteration 85, loss = 0.02574554\n",
            "Iteration 86, loss = 0.03232686\n",
            "Iteration 87, loss = 0.02723289\n",
            "Iteration 88, loss = 0.02649288\n",
            "Iteration 89, loss = 0.02761719\n",
            "Iteration 90, loss = 0.02694370\n",
            "Iteration 91, loss = 0.02737659\n",
            "Iteration 92, loss = 0.02660560\n",
            "Iteration 93, loss = 0.02555128\n",
            "Iteration 94, loss = 0.02528372\n",
            "Iteration 95, loss = 0.02511266\n",
            "Iteration 96, loss = 0.02614876\n",
            "Iteration 97, loss = 0.02604024\n",
            "Iteration 98, loss = 0.02807556\n",
            "Iteration 99, loss = 0.02567881\n",
            "Iteration 100, loss = 0.02521352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29937832\n",
            "Iteration 2, loss = 1.20341647\n",
            "Iteration 3, loss = 1.14449370\n",
            "Iteration 4, loss = 1.08511550\n",
            "Iteration 5, loss = 1.00880959\n",
            "Iteration 6, loss = 0.92447727\n",
            "Iteration 7, loss = 0.82742613\n",
            "Iteration 8, loss = 0.72200688\n",
            "Iteration 9, loss = 0.61305843\n",
            "Iteration 10, loss = 0.50362912\n",
            "Iteration 11, loss = 0.40478519\n",
            "Iteration 12, loss = 0.32036249\n",
            "Iteration 13, loss = 0.25348584\n",
            "Iteration 14, loss = 0.19993265\n",
            "Iteration 15, loss = 0.16099824\n",
            "Iteration 16, loss = 0.13159901\n",
            "Iteration 17, loss = 0.11119183\n",
            "Iteration 18, loss = 0.09542217\n",
            "Iteration 19, loss = 0.08485709\n",
            "Iteration 20, loss = 0.08288831\n",
            "Iteration 21, loss = 0.07263226\n",
            "Iteration 22, loss = 0.06876182\n",
            "Iteration 23, loss = 0.06282072\n",
            "Iteration 24, loss = 0.06178299\n",
            "Iteration 25, loss = 0.05684137\n",
            "Iteration 26, loss = 0.05377335\n",
            "Iteration 27, loss = 0.05133228\n",
            "Iteration 28, loss = 0.04990778\n",
            "Iteration 29, loss = 0.04945529\n",
            "Iteration 30, loss = 0.04796168\n",
            "Iteration 31, loss = 0.04627939\n",
            "Iteration 32, loss = 0.04552088\n",
            "Iteration 33, loss = 0.04531004\n",
            "Iteration 34, loss = 0.04787739\n",
            "Iteration 35, loss = 0.04474277\n",
            "Iteration 36, loss = 0.04270775\n",
            "Iteration 37, loss = 0.04120806\n",
            "Iteration 38, loss = 0.03927575\n",
            "Iteration 39, loss = 0.04118653\n",
            "Iteration 40, loss = 0.04039859\n",
            "Iteration 41, loss = 0.04088375\n",
            "Iteration 42, loss = 0.04141662\n",
            "Iteration 43, loss = 0.03777536\n",
            "Iteration 44, loss = 0.03907955\n",
            "Iteration 45, loss = 0.03554325\n",
            "Iteration 46, loss = 0.03671319\n",
            "Iteration 47, loss = 0.03641022\n",
            "Iteration 48, loss = 0.03493932\n",
            "Iteration 49, loss = 0.03870371\n",
            "Iteration 50, loss = 0.03490418\n",
            "Iteration 51, loss = 0.03958283\n",
            "Iteration 52, loss = 0.03610278\n",
            "Iteration 53, loss = 0.03834116\n",
            "Iteration 54, loss = 0.03696185\n",
            "Iteration 55, loss = 0.03488594\n",
            "Iteration 56, loss = 0.03677643\n",
            "Iteration 57, loss = 0.03274291\n",
            "Iteration 58, loss = 0.03257130\n",
            "Iteration 59, loss = 0.03181924\n",
            "Iteration 60, loss = 0.03246405\n",
            "Iteration 61, loss = 0.03117745\n",
            "Iteration 62, loss = 0.03189121\n",
            "Iteration 63, loss = 0.03039077\n",
            "Iteration 64, loss = 0.03210400\n",
            "Iteration 65, loss = 0.03307962\n",
            "Iteration 66, loss = 0.03136356\n",
            "Iteration 67, loss = 0.03394604\n",
            "Iteration 68, loss = 0.02936670\n",
            "Iteration 69, loss = 0.03077536\n",
            "Iteration 70, loss = 0.02946603\n",
            "Iteration 71, loss = 0.03049238\n",
            "Iteration 72, loss = 0.03260225\n",
            "Iteration 73, loss = 0.02874570\n",
            "Iteration 74, loss = 0.03093836\n",
            "Iteration 75, loss = 0.03010540\n",
            "Iteration 76, loss = 0.03023901\n",
            "Iteration 77, loss = 0.03241281\n",
            "Iteration 78, loss = 0.03153477\n",
            "Iteration 79, loss = 0.02985704\n",
            "Iteration 80, loss = 0.03017229\n",
            "Iteration 81, loss = 0.03060797\n",
            "Iteration 82, loss = 0.02837985\n",
            "Iteration 83, loss = 0.02718575\n",
            "Iteration 84, loss = 0.03077561\n",
            "Iteration 85, loss = 0.02694531\n",
            "Iteration 86, loss = 0.03364831\n",
            "Iteration 87, loss = 0.02835171\n",
            "Iteration 88, loss = 0.02840671\n",
            "Iteration 89, loss = 0.02918871\n",
            "Iteration 90, loss = 0.02817919\n",
            "Iteration 91, loss = 0.02914930\n",
            "Iteration 92, loss = 0.02819162\n",
            "Iteration 93, loss = 0.02682977\n",
            "Iteration 94, loss = 0.02708046\n",
            "Iteration 95, loss = 0.02692078\n",
            "Iteration 96, loss = 0.02794998\n",
            "Iteration 97, loss = 0.02756093\n",
            "Iteration 98, loss = 0.02910505\n",
            "Iteration 99, loss = 0.02760600\n",
            "Iteration 100, loss = 0.02690227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29914034\n",
            "Iteration 2, loss = 1.20699661\n",
            "Iteration 3, loss = 1.14727102\n",
            "Iteration 4, loss = 1.08505905\n",
            "Iteration 5, loss = 1.00726378\n",
            "Iteration 6, loss = 0.92111897\n",
            "Iteration 7, loss = 0.82498982\n",
            "Iteration 8, loss = 0.71778736\n",
            "Iteration 9, loss = 0.61000625\n",
            "Iteration 10, loss = 0.50001324\n",
            "Iteration 11, loss = 0.39969437\n",
            "Iteration 12, loss = 0.31588702\n",
            "Iteration 13, loss = 0.24743657\n",
            "Iteration 14, loss = 0.19520983\n",
            "Iteration 15, loss = 0.15625870\n",
            "Iteration 16, loss = 0.12813931\n",
            "Iteration 17, loss = 0.10771309\n",
            "Iteration 18, loss = 0.09237395\n",
            "Iteration 19, loss = 0.08146988\n",
            "Iteration 20, loss = 0.08053724\n",
            "Iteration 21, loss = 0.06940253\n",
            "Iteration 22, loss = 0.06674894\n",
            "Iteration 23, loss = 0.06031948\n",
            "Iteration 24, loss = 0.05863982\n",
            "Iteration 25, loss = 0.05481062\n",
            "Iteration 26, loss = 0.05107871\n",
            "Iteration 27, loss = 0.04932737\n",
            "Iteration 28, loss = 0.04765385\n",
            "Iteration 29, loss = 0.04744113\n",
            "Iteration 30, loss = 0.04575291\n",
            "Iteration 31, loss = 0.04423467\n",
            "Iteration 32, loss = 0.04405717\n",
            "Iteration 33, loss = 0.04331801\n",
            "Iteration 34, loss = 0.04569537\n",
            "Iteration 35, loss = 0.04269662\n",
            "Iteration 36, loss = 0.04031801\n",
            "Iteration 37, loss = 0.03965548\n",
            "Iteration 38, loss = 0.03790287\n",
            "Iteration 39, loss = 0.03979133\n",
            "Iteration 40, loss = 0.03888521\n",
            "Iteration 41, loss = 0.03969192\n",
            "Iteration 42, loss = 0.04065796\n",
            "Iteration 43, loss = 0.03679333\n",
            "Iteration 44, loss = 0.03802702\n",
            "Iteration 45, loss = 0.03406422\n",
            "Iteration 46, loss = 0.03568404\n",
            "Iteration 47, loss = 0.03507645\n",
            "Iteration 48, loss = 0.03391707\n",
            "Iteration 49, loss = 0.03833934\n",
            "Iteration 50, loss = 0.03287034\n",
            "Iteration 51, loss = 0.03923852\n",
            "Iteration 52, loss = 0.03463287\n",
            "Iteration 53, loss = 0.03827506\n",
            "Iteration 54, loss = 0.03687618\n",
            "Iteration 55, loss = 0.03432436\n",
            "Iteration 56, loss = 0.03633818\n",
            "Iteration 57, loss = 0.03176711\n",
            "Iteration 58, loss = 0.03134748\n",
            "Iteration 59, loss = 0.03072928\n",
            "Iteration 60, loss = 0.03161998\n",
            "Iteration 61, loss = 0.03021225\n",
            "Iteration 62, loss = 0.03079839\n",
            "Iteration 63, loss = 0.02929187\n",
            "Iteration 64, loss = 0.03091905\n",
            "Iteration 65, loss = 0.03247366\n",
            "Iteration 66, loss = 0.03027243\n",
            "Iteration 67, loss = 0.03297346\n",
            "Iteration 68, loss = 0.02835937\n",
            "Iteration 69, loss = 0.02964977\n",
            "Iteration 70, loss = 0.02868545\n",
            "Iteration 71, loss = 0.02965413\n",
            "Iteration 72, loss = 0.03174488\n",
            "Iteration 73, loss = 0.02768172\n",
            "Iteration 74, loss = 0.03020629\n",
            "Iteration 75, loss = 0.02931795\n",
            "Iteration 76, loss = 0.02929371\n",
            "Iteration 77, loss = 0.03162293\n",
            "Iteration 78, loss = 0.03079089\n",
            "Iteration 79, loss = 0.02885653\n",
            "Iteration 80, loss = 0.02968199\n",
            "Iteration 81, loss = 0.02976191\n",
            "Iteration 82, loss = 0.02724808\n",
            "Iteration 83, loss = 0.02656312\n",
            "Iteration 84, loss = 0.02957054\n",
            "Iteration 85, loss = 0.02608313\n",
            "Iteration 86, loss = 0.03390280\n",
            "Iteration 87, loss = 0.02733581\n",
            "Iteration 88, loss = 0.02725762\n",
            "Iteration 89, loss = 0.02785321\n",
            "Iteration 90, loss = 0.02744818\n",
            "Iteration 91, loss = 0.02734275\n",
            "Iteration 92, loss = 0.02652991\n",
            "Iteration 93, loss = 0.02588138\n",
            "Iteration 94, loss = 0.02642690\n",
            "Iteration 95, loss = 0.02670535\n",
            "Iteration 96, loss = 0.02700443\n",
            "Iteration 97, loss = 0.02602896\n",
            "Iteration 98, loss = 0.02761816\n",
            "Iteration 99, loss = 0.02649360\n",
            "Iteration 100, loss = 0.02584686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29712545\n",
            "Iteration 2, loss = 1.20051403\n",
            "Iteration 3, loss = 1.13996391\n",
            "Iteration 4, loss = 1.08277194\n",
            "Iteration 5, loss = 1.01065893\n",
            "Iteration 6, loss = 0.92894667\n",
            "Iteration 7, loss = 0.83588535\n",
            "Iteration 8, loss = 0.73717330\n",
            "Iteration 9, loss = 0.62937616\n",
            "Iteration 10, loss = 0.51941589\n",
            "Iteration 11, loss = 0.41621134\n",
            "Iteration 12, loss = 0.32764175\n",
            "Iteration 13, loss = 0.25573918\n",
            "Iteration 14, loss = 0.20112254\n",
            "Iteration 15, loss = 0.15985846\n",
            "Iteration 16, loss = 0.13066564\n",
            "Iteration 17, loss = 0.10971733\n",
            "Iteration 18, loss = 0.09400292\n",
            "Iteration 19, loss = 0.08315205\n",
            "Iteration 20, loss = 0.08375592\n",
            "Iteration 21, loss = 0.07162168\n",
            "Iteration 22, loss = 0.06940146\n",
            "Iteration 23, loss = 0.06243738\n",
            "Iteration 24, loss = 0.06055203\n",
            "Iteration 25, loss = 0.05698528\n",
            "Iteration 26, loss = 0.05284045\n",
            "Iteration 27, loss = 0.05103044\n",
            "Iteration 28, loss = 0.04934052\n",
            "Iteration 29, loss = 0.04909344\n",
            "Iteration 30, loss = 0.04716698\n",
            "Iteration 31, loss = 0.04556031\n",
            "Iteration 32, loss = 0.04564071\n",
            "Iteration 33, loss = 0.04509464\n",
            "Iteration 34, loss = 0.04847561\n",
            "Iteration 35, loss = 0.04443614\n",
            "Iteration 36, loss = 0.04270486\n",
            "Iteration 37, loss = 0.04130931\n",
            "Iteration 38, loss = 0.03922526\n",
            "Iteration 39, loss = 0.04151532\n",
            "Iteration 40, loss = 0.04017349\n",
            "Iteration 41, loss = 0.04167799\n",
            "Iteration 42, loss = 0.04199515\n",
            "Iteration 43, loss = 0.03878799\n",
            "Iteration 44, loss = 0.03942305\n",
            "Iteration 45, loss = 0.03560471\n",
            "Iteration 46, loss = 0.03717097\n",
            "Iteration 47, loss = 0.03654905\n",
            "Iteration 48, loss = 0.03574512\n",
            "Iteration 49, loss = 0.03805717\n",
            "Iteration 50, loss = 0.03425467\n",
            "Iteration 51, loss = 0.04051881\n",
            "Iteration 52, loss = 0.03558137\n",
            "Iteration 53, loss = 0.03917901\n",
            "Iteration 54, loss = 0.03632031\n",
            "Iteration 55, loss = 0.03599583\n",
            "Iteration 56, loss = 0.03656287\n",
            "Iteration 57, loss = 0.03275110\n",
            "Iteration 58, loss = 0.03218299\n",
            "Iteration 59, loss = 0.03187983\n",
            "Iteration 60, loss = 0.03284951\n",
            "Iteration 61, loss = 0.03081089\n",
            "Iteration 62, loss = 0.03249623\n",
            "Iteration 63, loss = 0.03046256\n",
            "Iteration 64, loss = 0.03190593\n",
            "Iteration 65, loss = 0.03377247\n",
            "Iteration 66, loss = 0.03131838\n",
            "Iteration 67, loss = 0.03383621\n",
            "Iteration 68, loss = 0.02994374\n",
            "Iteration 69, loss = 0.03061949\n",
            "Iteration 70, loss = 0.03010840\n",
            "Iteration 71, loss = 0.03064274\n",
            "Iteration 72, loss = 0.03314560\n",
            "Iteration 73, loss = 0.02904268\n",
            "Iteration 74, loss = 0.03096316\n",
            "Iteration 75, loss = 0.03005488\n",
            "Iteration 76, loss = 0.03015607\n",
            "Iteration 77, loss = 0.03223984\n",
            "Iteration 78, loss = 0.03235593\n",
            "Iteration 79, loss = 0.03003947\n",
            "Iteration 80, loss = 0.03049786\n",
            "Iteration 81, loss = 0.03054322\n",
            "Iteration 82, loss = 0.02821692\n",
            "Iteration 83, loss = 0.02784553\n",
            "Iteration 84, loss = 0.03060659\n",
            "Iteration 85, loss = 0.02753007\n",
            "Iteration 86, loss = 0.03533245\n",
            "Iteration 87, loss = 0.02832453\n",
            "Iteration 88, loss = 0.02871494\n",
            "Iteration 89, loss = 0.02864580\n",
            "Iteration 90, loss = 0.02871639\n",
            "Iteration 91, loss = 0.02860613\n",
            "Iteration 92, loss = 0.02764819\n",
            "Iteration 93, loss = 0.02694618\n",
            "Iteration 94, loss = 0.02730511\n",
            "Iteration 95, loss = 0.02775351\n",
            "Iteration 96, loss = 0.02864667\n",
            "Iteration 97, loss = 0.02724564\n",
            "Iteration 98, loss = 0.02952504\n",
            "Iteration 99, loss = 0.02810555\n",
            "Iteration 100, loss = 0.02722166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29348126\n",
            "Iteration 2, loss = 1.19397158\n",
            "Iteration 3, loss = 1.13445473\n",
            "Iteration 4, loss = 1.07759678\n",
            "Iteration 5, loss = 1.00439335\n",
            "Iteration 6, loss = 0.92380040\n",
            "Iteration 7, loss = 0.83107401\n",
            "Iteration 8, loss = 0.73253367\n",
            "Iteration 9, loss = 0.62561855\n",
            "Iteration 10, loss = 0.51882507\n",
            "Iteration 11, loss = 0.41941839\n",
            "Iteration 12, loss = 0.33283679\n",
            "Iteration 13, loss = 0.26134938\n",
            "Iteration 14, loss = 0.20634656\n",
            "Iteration 15, loss = 0.16356830\n",
            "Iteration 16, loss = 0.13302342\n",
            "Iteration 17, loss = 0.11164918\n",
            "Iteration 18, loss = 0.09468132\n",
            "Iteration 19, loss = 0.08289963\n",
            "Iteration 20, loss = 0.08429557\n",
            "Iteration 21, loss = 0.07107077\n",
            "Iteration 22, loss = 0.06800547\n",
            "Iteration 23, loss = 0.06166402\n",
            "Iteration 24, loss = 0.06021601\n",
            "Iteration 25, loss = 0.05603408\n",
            "Iteration 26, loss = 0.05237727\n",
            "Iteration 27, loss = 0.05031970\n",
            "Iteration 28, loss = 0.04877258\n",
            "Iteration 29, loss = 0.04838219\n",
            "Iteration 30, loss = 0.04682135\n",
            "Iteration 31, loss = 0.04541972\n",
            "Iteration 32, loss = 0.04511329\n",
            "Iteration 33, loss = 0.04518659\n",
            "Iteration 34, loss = 0.04859192\n",
            "Iteration 35, loss = 0.04407108\n",
            "Iteration 36, loss = 0.04258145\n",
            "Iteration 37, loss = 0.04076915\n",
            "Iteration 38, loss = 0.03932751\n",
            "Iteration 39, loss = 0.04094339\n",
            "Iteration 40, loss = 0.03931599\n",
            "Iteration 41, loss = 0.03963929\n",
            "Iteration 42, loss = 0.04112691\n",
            "Iteration 43, loss = 0.03845199\n",
            "Iteration 44, loss = 0.03900133\n",
            "Iteration 45, loss = 0.03541061\n",
            "Iteration 46, loss = 0.03654810\n",
            "Iteration 47, loss = 0.03598334\n",
            "Iteration 48, loss = 0.03589846\n",
            "Iteration 49, loss = 0.03826015\n",
            "Iteration 50, loss = 0.03417797\n",
            "Iteration 51, loss = 0.04081248\n",
            "Iteration 52, loss = 0.03548077\n",
            "Iteration 53, loss = 0.03960025\n",
            "Iteration 54, loss = 0.03640306\n",
            "Iteration 55, loss = 0.03633508\n",
            "Iteration 56, loss = 0.03640851\n",
            "Iteration 57, loss = 0.03256169\n",
            "Iteration 58, loss = 0.03194339\n",
            "Iteration 59, loss = 0.03157478\n",
            "Iteration 60, loss = 0.03277723\n",
            "Iteration 61, loss = 0.03083586\n",
            "Iteration 62, loss = 0.03229207\n",
            "Iteration 63, loss = 0.03023705\n",
            "Iteration 64, loss = 0.03180925\n",
            "Iteration 65, loss = 0.03312325\n",
            "Iteration 66, loss = 0.03127599\n",
            "Iteration 67, loss = 0.03306550\n",
            "Iteration 68, loss = 0.02944496\n",
            "Iteration 69, loss = 0.03082293\n",
            "Iteration 70, loss = 0.02978868\n",
            "Iteration 71, loss = 0.03069505\n",
            "Iteration 72, loss = 0.03335855\n",
            "Iteration 73, loss = 0.02862931\n",
            "Iteration 74, loss = 0.03061012\n",
            "Iteration 75, loss = 0.02947517\n",
            "Iteration 76, loss = 0.02961980\n",
            "Iteration 77, loss = 0.03140679\n",
            "Iteration 78, loss = 0.03121148\n",
            "Iteration 79, loss = 0.02974639\n",
            "Iteration 80, loss = 0.02949394\n",
            "Iteration 81, loss = 0.03014200\n",
            "Iteration 82, loss = 0.02782898\n",
            "Iteration 83, loss = 0.02745445\n",
            "Iteration 84, loss = 0.03061805\n",
            "Iteration 85, loss = 0.02692749\n",
            "Iteration 86, loss = 0.03537860\n",
            "Iteration 87, loss = 0.02787458\n",
            "Iteration 88, loss = 0.02805862\n",
            "Iteration 89, loss = 0.02916966\n",
            "Iteration 90, loss = 0.02802520\n",
            "Iteration 91, loss = 0.02839133\n",
            "Iteration 92, loss = 0.02766835\n",
            "Iteration 93, loss = 0.02656784\n",
            "Iteration 94, loss = 0.02701784\n",
            "Iteration 95, loss = 0.02740485\n",
            "Iteration 96, loss = 0.02826055\n",
            "Iteration 97, loss = 0.02672391\n",
            "Iteration 98, loss = 0.02887144\n",
            "Iteration 99, loss = 0.02755319\n",
            "Iteration 100, loss = 0.02678659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29387748\n",
            "Iteration 2, loss = 1.19872263\n",
            "Iteration 3, loss = 1.13899573\n",
            "Iteration 4, loss = 1.07738663\n",
            "Iteration 5, loss = 1.00077923\n",
            "Iteration 6, loss = 0.91637082\n",
            "Iteration 7, loss = 0.82197850\n",
            "Iteration 8, loss = 0.71885684\n",
            "Iteration 9, loss = 0.61394107\n",
            "Iteration 10, loss = 0.50728621\n",
            "Iteration 11, loss = 0.40903269\n",
            "Iteration 12, loss = 0.32343129\n",
            "Iteration 13, loss = 0.25388970\n",
            "Iteration 14, loss = 0.20138717\n",
            "Iteration 15, loss = 0.15937866\n",
            "Iteration 16, loss = 0.12980973\n",
            "Iteration 17, loss = 0.10933902\n",
            "Iteration 18, loss = 0.09352338\n",
            "Iteration 19, loss = 0.08221153\n",
            "Iteration 20, loss = 0.08442501\n",
            "Iteration 21, loss = 0.07084948\n",
            "Iteration 22, loss = 0.06790315\n",
            "Iteration 23, loss = 0.06160823\n",
            "Iteration 24, loss = 0.06001552\n",
            "Iteration 25, loss = 0.05615696\n",
            "Iteration 26, loss = 0.05196059\n",
            "Iteration 27, loss = 0.05008902\n",
            "Iteration 28, loss = 0.04835774\n",
            "Iteration 29, loss = 0.04794809\n",
            "Iteration 30, loss = 0.04614154\n",
            "Iteration 31, loss = 0.04487401\n",
            "Iteration 32, loss = 0.04501204\n",
            "Iteration 33, loss = 0.04466212\n",
            "Iteration 34, loss = 0.04844975\n",
            "Iteration 35, loss = 0.04313247\n",
            "Iteration 36, loss = 0.04232852\n",
            "Iteration 37, loss = 0.04008196\n",
            "Iteration 38, loss = 0.03859428\n",
            "Iteration 39, loss = 0.04042795\n",
            "Iteration 40, loss = 0.03881337\n",
            "Iteration 41, loss = 0.03877046\n",
            "Iteration 42, loss = 0.04073275\n",
            "Iteration 43, loss = 0.03788034\n",
            "Iteration 44, loss = 0.03838557\n",
            "Iteration 45, loss = 0.03447171\n",
            "Iteration 46, loss = 0.03547460\n",
            "Iteration 47, loss = 0.03509979\n",
            "Iteration 48, loss = 0.03465944\n",
            "Iteration 49, loss = 0.03792275\n",
            "Iteration 50, loss = 0.03284227\n",
            "Iteration 51, loss = 0.03938828\n",
            "Iteration 52, loss = 0.03449164\n",
            "Iteration 53, loss = 0.03777249\n",
            "Iteration 54, loss = 0.03554237\n",
            "Iteration 55, loss = 0.03460209\n",
            "Iteration 56, loss = 0.03551036\n",
            "Iteration 57, loss = 0.03188285\n",
            "Iteration 58, loss = 0.03122685\n",
            "Iteration 59, loss = 0.03113458\n",
            "Iteration 60, loss = 0.03087592\n",
            "Iteration 61, loss = 0.03010522\n",
            "Iteration 62, loss = 0.03093564\n",
            "Iteration 63, loss = 0.02955346\n",
            "Iteration 64, loss = 0.03173438\n",
            "Iteration 65, loss = 0.03308446\n",
            "Iteration 66, loss = 0.03049667\n",
            "Iteration 67, loss = 0.03339370\n",
            "Iteration 68, loss = 0.02844320\n",
            "Iteration 69, loss = 0.03031196\n",
            "Iteration 70, loss = 0.02891005\n",
            "Iteration 71, loss = 0.03010503\n",
            "Iteration 72, loss = 0.03215709\n",
            "Iteration 73, loss = 0.02820287\n",
            "Iteration 74, loss = 0.02889231\n",
            "Iteration 75, loss = 0.02878253\n",
            "Iteration 76, loss = 0.02941566\n",
            "Iteration 77, loss = 0.03140699\n",
            "Iteration 78, loss = 0.03001074\n",
            "Iteration 79, loss = 0.02930713\n",
            "Iteration 80, loss = 0.02964826\n",
            "Iteration 81, loss = 0.03085566\n",
            "Iteration 82, loss = 0.02759945\n",
            "Iteration 83, loss = 0.02677843\n",
            "Iteration 84, loss = 0.02916470\n",
            "Iteration 85, loss = 0.02738594\n",
            "Iteration 86, loss = 0.03443384\n",
            "Iteration 87, loss = 0.02772133\n",
            "Iteration 88, loss = 0.02747367\n",
            "Iteration 89, loss = 0.02833458\n",
            "Iteration 90, loss = 0.02788032\n",
            "Iteration 91, loss = 0.02803382\n",
            "Iteration 92, loss = 0.02685899\n",
            "Iteration 93, loss = 0.02636221\n",
            "Iteration 94, loss = 0.02722243\n",
            "Iteration 95, loss = 0.02657116\n",
            "Iteration 96, loss = 0.02778721\n",
            "Iteration 97, loss = 0.02657631\n",
            "Iteration 98, loss = 0.02751130\n",
            "Iteration 99, loss = 0.02761307\n",
            "Iteration 100, loss = 0.02602018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29319066\n",
            "Iteration 2, loss = 1.19996674\n",
            "Iteration 3, loss = 1.13969511\n",
            "Iteration 4, loss = 1.08289863\n",
            "Iteration 5, loss = 1.00984317\n",
            "Iteration 6, loss = 0.92625386\n",
            "Iteration 7, loss = 0.83671960\n",
            "Iteration 8, loss = 0.73596300\n",
            "Iteration 9, loss = 0.63179860\n",
            "Iteration 10, loss = 0.52509078\n",
            "Iteration 11, loss = 0.42519095\n",
            "Iteration 12, loss = 0.33861904\n",
            "Iteration 13, loss = 0.26331444\n",
            "Iteration 14, loss = 0.20651891\n",
            "Iteration 15, loss = 0.16244262\n",
            "Iteration 16, loss = 0.13043455\n",
            "Iteration 17, loss = 0.10705299\n",
            "Iteration 18, loss = 0.08947715\n",
            "Iteration 19, loss = 0.07727196\n",
            "Iteration 20, loss = 0.06980567\n",
            "Iteration 21, loss = 0.06314016\n",
            "Iteration 22, loss = 0.05789984\n",
            "Iteration 23, loss = 0.05263430\n",
            "Iteration 24, loss = 0.04947683\n",
            "Iteration 25, loss = 0.04679453\n",
            "Iteration 26, loss = 0.04707015\n",
            "Iteration 27, loss = 0.04324742\n",
            "Iteration 28, loss = 0.04194017\n",
            "Iteration 29, loss = 0.03996331\n",
            "Iteration 30, loss = 0.03769463\n",
            "Iteration 31, loss = 0.03709025\n",
            "Iteration 32, loss = 0.03566330\n",
            "Iteration 33, loss = 0.03509885\n",
            "Iteration 34, loss = 0.03414799\n",
            "Iteration 35, loss = 0.03420827\n",
            "Iteration 36, loss = 0.03365693\n",
            "Iteration 37, loss = 0.03278679\n",
            "Iteration 38, loss = 0.03312261\n",
            "Iteration 39, loss = 0.03318980\n",
            "Iteration 40, loss = 0.03356231\n",
            "Iteration 41, loss = 0.02944405\n",
            "Iteration 42, loss = 0.03332435\n",
            "Iteration 43, loss = 0.03204649\n",
            "Iteration 44, loss = 0.03056353\n",
            "Iteration 45, loss = 0.03048207\n",
            "Iteration 46, loss = 0.02785633\n",
            "Iteration 47, loss = 0.03316883\n",
            "Iteration 48, loss = 0.03288175\n",
            "Iteration 49, loss = 0.03234003\n",
            "Iteration 50, loss = 0.02824883\n",
            "Iteration 51, loss = 0.02937356\n",
            "Iteration 52, loss = 0.02973278\n",
            "Iteration 53, loss = 0.02738098\n",
            "Iteration 54, loss = 0.02788908\n",
            "Iteration 55, loss = 0.02765939\n",
            "Iteration 56, loss = 0.02589531\n",
            "Iteration 57, loss = 0.02657517\n",
            "Iteration 58, loss = 0.02846812\n",
            "Iteration 59, loss = 0.02817898\n",
            "Iteration 60, loss = 0.03040581\n",
            "Iteration 61, loss = 0.02556185\n",
            "Iteration 62, loss = 0.02666602\n",
            "Iteration 63, loss = 0.02563473\n",
            "Iteration 64, loss = 0.02525814\n",
            "Iteration 65, loss = 0.02526975\n",
            "Iteration 66, loss = 0.02512078\n",
            "Iteration 67, loss = 0.02535652\n",
            "Iteration 68, loss = 0.02895915\n",
            "Iteration 69, loss = 0.02852895\n",
            "Iteration 70, loss = 0.02652865\n",
            "Iteration 71, loss = 0.02545916\n",
            "Iteration 72, loss = 0.02401419\n",
            "Iteration 73, loss = 0.02711283\n",
            "Iteration 74, loss = 0.02403313\n",
            "Iteration 75, loss = 0.02551370\n",
            "Iteration 76, loss = 0.02355962\n",
            "Iteration 77, loss = 0.02393006\n",
            "Iteration 78, loss = 0.02397725\n",
            "Iteration 79, loss = 0.02450942\n",
            "Iteration 80, loss = 0.02442956\n",
            "Iteration 81, loss = 0.02325172\n",
            "Iteration 82, loss = 0.02475564\n",
            "Iteration 83, loss = 0.02390865\n",
            "Iteration 84, loss = 0.02328696\n",
            "Iteration 85, loss = 0.02373710\n",
            "Iteration 86, loss = 0.02184497\n",
            "Iteration 87, loss = 0.02539751\n",
            "Iteration 88, loss = 0.02347138\n",
            "Iteration 89, loss = 0.02295633\n",
            "Iteration 90, loss = 0.02256326\n",
            "Iteration 91, loss = 0.02378280\n",
            "Iteration 92, loss = 0.02159638\n",
            "Iteration 93, loss = 0.02613884\n",
            "Iteration 94, loss = 0.02236221\n",
            "Iteration 95, loss = 0.02386493\n",
            "Iteration 96, loss = 0.02340531\n",
            "Iteration 97, loss = 0.02260687\n",
            "Iteration 98, loss = 0.02262975\n",
            "Iteration 99, loss = 0.02595475\n",
            "Iteration 100, loss = 0.02584875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29449933\n",
            "Iteration 2, loss = 1.19606186\n",
            "Iteration 3, loss = 1.13701412\n",
            "Iteration 4, loss = 1.08270951\n",
            "Iteration 5, loss = 1.01120265\n",
            "Iteration 6, loss = 0.93102816\n",
            "Iteration 7, loss = 0.84242965\n",
            "Iteration 8, loss = 0.74734673\n",
            "Iteration 9, loss = 0.64262364\n",
            "Iteration 10, loss = 0.54172865\n",
            "Iteration 11, loss = 0.44026602\n",
            "Iteration 12, loss = 0.35114925\n",
            "Iteration 13, loss = 0.27456405\n",
            "Iteration 14, loss = 0.21504282\n",
            "Iteration 15, loss = 0.16861735\n",
            "Iteration 16, loss = 0.13575985\n",
            "Iteration 17, loss = 0.11052430\n",
            "Iteration 18, loss = 0.09210888\n",
            "Iteration 19, loss = 0.08088413\n",
            "Iteration 20, loss = 0.07046764\n",
            "Iteration 21, loss = 0.06285853\n",
            "Iteration 22, loss = 0.05690994\n",
            "Iteration 23, loss = 0.05205865\n",
            "Iteration 24, loss = 0.04834149\n",
            "Iteration 25, loss = 0.04534881\n",
            "Iteration 26, loss = 0.04409811\n",
            "Iteration 27, loss = 0.04079926\n",
            "Iteration 28, loss = 0.03969060\n",
            "Iteration 29, loss = 0.03679871\n",
            "Iteration 30, loss = 0.03555997\n",
            "Iteration 31, loss = 0.03428513\n",
            "Iteration 32, loss = 0.03280142\n",
            "Iteration 33, loss = 0.03190855\n",
            "Iteration 34, loss = 0.03080133\n",
            "Iteration 35, loss = 0.03014386\n",
            "Iteration 36, loss = 0.03040249\n",
            "Iteration 37, loss = 0.02857277\n",
            "Iteration 38, loss = 0.02739731\n",
            "Iteration 39, loss = 0.02920206\n",
            "Iteration 40, loss = 0.02608504\n",
            "Iteration 41, loss = 0.02587353\n",
            "Iteration 42, loss = 0.02685765\n",
            "Iteration 43, loss = 0.02637471\n",
            "Iteration 44, loss = 0.02412512\n",
            "Iteration 45, loss = 0.02457797\n",
            "Iteration 46, loss = 0.02336228\n",
            "Iteration 47, loss = 0.02441155\n",
            "Iteration 48, loss = 0.02235204\n",
            "Iteration 49, loss = 0.02146490\n",
            "Iteration 50, loss = 0.02128006\n",
            "Iteration 51, loss = 0.02079300\n",
            "Iteration 52, loss = 0.02095216\n",
            "Iteration 53, loss = 0.02017801\n",
            "Iteration 54, loss = 0.02344850\n",
            "Iteration 55, loss = 0.02113711\n",
            "Iteration 56, loss = 0.02153546\n",
            "Iteration 57, loss = 0.02152539\n",
            "Iteration 58, loss = 0.02137990\n",
            "Iteration 59, loss = 0.01898434\n",
            "Iteration 60, loss = 0.02111126\n",
            "Iteration 61, loss = 0.01847989\n",
            "Iteration 62, loss = 0.01843371\n",
            "Iteration 63, loss = 0.01768439\n",
            "Iteration 64, loss = 0.01767500\n",
            "Iteration 65, loss = 0.01735765\n",
            "Iteration 66, loss = 0.01777501\n",
            "Iteration 67, loss = 0.01686846\n",
            "Iteration 68, loss = 0.01800096\n",
            "Iteration 69, loss = 0.01793261\n",
            "Iteration 70, loss = 0.01830041\n",
            "Iteration 71, loss = 0.01692084\n",
            "Iteration 72, loss = 0.01633004\n",
            "Iteration 73, loss = 0.01679979\n",
            "Iteration 74, loss = 0.01571121\n",
            "Iteration 75, loss = 0.01565487\n",
            "Iteration 76, loss = 0.01587578\n",
            "Iteration 77, loss = 0.01579382\n",
            "Iteration 78, loss = 0.01553081\n",
            "Iteration 79, loss = 0.01537934\n",
            "Iteration 80, loss = 0.01648706\n",
            "Iteration 81, loss = 0.01537107\n",
            "Iteration 82, loss = 0.01559620\n",
            "Iteration 83, loss = 0.01510661\n",
            "Iteration 84, loss = 0.01500550\n",
            "Iteration 85, loss = 0.01519666\n",
            "Iteration 86, loss = 0.01586513\n",
            "Iteration 87, loss = 0.01643761\n",
            "Iteration 88, loss = 0.01499108\n",
            "Iteration 89, loss = 0.01481501\n",
            "Iteration 90, loss = 0.01459869\n",
            "Iteration 91, loss = 0.01427008\n",
            "Iteration 92, loss = 0.01450472\n",
            "Iteration 93, loss = 0.01640575\n",
            "Iteration 94, loss = 0.01560451\n",
            "Iteration 95, loss = 0.01469384\n",
            "Iteration 96, loss = 0.01432569\n",
            "Iteration 97, loss = 0.01412066\n",
            "Iteration 98, loss = 0.01373688\n",
            "Iteration 99, loss = 0.01843138\n",
            "Iteration 100, loss = 0.01697293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.29140233\n",
            "Iteration 2, loss = 1.19248550\n",
            "Iteration 3, loss = 1.13139610\n",
            "Iteration 4, loss = 1.07366202\n",
            "Iteration 5, loss = 0.99987104\n",
            "Iteration 6, loss = 0.91666253\n",
            "Iteration 7, loss = 0.82449759\n",
            "Iteration 8, loss = 0.72449154\n",
            "Iteration 9, loss = 0.61611182\n",
            "Iteration 10, loss = 0.51115003\n",
            "Iteration 11, loss = 0.41142759\n",
            "Iteration 12, loss = 0.32738126\n",
            "Iteration 13, loss = 0.25417804\n",
            "Iteration 14, loss = 0.20076718\n",
            "Iteration 15, loss = 0.15801878\n",
            "Iteration 16, loss = 0.12783354\n",
            "Iteration 17, loss = 0.10678875\n",
            "Iteration 18, loss = 0.09053536\n",
            "Iteration 19, loss = 0.07840230\n",
            "Iteration 20, loss = 0.07028272\n",
            "Iteration 21, loss = 0.06363024\n",
            "Iteration 22, loss = 0.05826071\n",
            "Iteration 23, loss = 0.05446769\n",
            "Iteration 24, loss = 0.05106028\n",
            "Iteration 25, loss = 0.04863281\n",
            "Iteration 26, loss = 0.05167757\n",
            "Iteration 27, loss = 0.04474938\n",
            "Iteration 28, loss = 0.04665454\n",
            "Iteration 29, loss = 0.04287442\n",
            "Iteration 30, loss = 0.04146780\n",
            "Iteration 31, loss = 0.03883519\n",
            "Iteration 32, loss = 0.03774266\n",
            "Iteration 33, loss = 0.03804794\n",
            "Iteration 34, loss = 0.03874128\n",
            "Iteration 35, loss = 0.03686709\n",
            "Iteration 36, loss = 0.03693865\n",
            "Iteration 37, loss = 0.03620646\n",
            "Iteration 38, loss = 0.03384707\n",
            "Iteration 39, loss = 0.03869791\n",
            "Iteration 40, loss = 0.03546519\n",
            "Iteration 41, loss = 0.03434180\n",
            "Iteration 42, loss = 0.03192978\n",
            "Iteration 43, loss = 0.03187504\n",
            "Iteration 44, loss = 0.03170224\n",
            "Iteration 45, loss = 0.03120409\n",
            "Iteration 46, loss = 0.03262100\n",
            "Iteration 47, loss = 0.02996416\n",
            "Iteration 48, loss = 0.02992069\n",
            "Iteration 49, loss = 0.03013141\n",
            "Iteration 50, loss = 0.03089906\n",
            "Iteration 51, loss = 0.02758633\n",
            "Iteration 52, loss = 0.03176228\n",
            "Iteration 53, loss = 0.02971115\n",
            "Iteration 54, loss = 0.02913905\n",
            "Iteration 55, loss = 0.03247082\n",
            "Iteration 56, loss = 0.03103542\n",
            "Iteration 57, loss = 0.03346051\n",
            "Iteration 58, loss = 0.03228219\n",
            "Iteration 59, loss = 0.02704859\n",
            "Iteration 60, loss = 0.02735357\n",
            "Iteration 61, loss = 0.02735542\n",
            "Iteration 62, loss = 0.02756981\n",
            "Iteration 63, loss = 0.02599863\n",
            "Iteration 64, loss = 0.02638616\n",
            "Iteration 65, loss = 0.02822070\n",
            "Iteration 66, loss = 0.02633105\n",
            "Iteration 67, loss = 0.02661858\n",
            "Iteration 68, loss = 0.02708104\n",
            "Iteration 69, loss = 0.02512785\n",
            "Iteration 70, loss = 0.02926247\n",
            "Iteration 71, loss = 0.02517403\n",
            "Iteration 72, loss = 0.02524215\n",
            "Iteration 73, loss = 0.02576882\n",
            "Iteration 74, loss = 0.02520739\n",
            "Iteration 75, loss = 0.02436520\n",
            "Iteration 76, loss = 0.02441008\n",
            "Iteration 77, loss = 0.02416753\n",
            "Iteration 78, loss = 0.02432771\n",
            "Iteration 79, loss = 0.02365821\n",
            "Iteration 80, loss = 0.02478343\n",
            "Iteration 81, loss = 0.02458873\n",
            "Iteration 82, loss = 0.02428694\n",
            "Iteration 83, loss = 0.02559982\n",
            "Iteration 84, loss = 0.02596566\n",
            "Iteration 85, loss = 0.02730579\n",
            "Iteration 86, loss = 0.02487249\n",
            "Iteration 87, loss = 0.02539889\n",
            "Iteration 88, loss = 0.02386927\n",
            "Iteration 89, loss = 0.02462593\n",
            "Iteration 90, loss = 0.02300945\n",
            "Iteration 91, loss = 0.02357704\n",
            "Iteration 92, loss = 0.02288601\n",
            "Iteration 93, loss = 0.02325254\n",
            "Iteration 94, loss = 0.02264317\n",
            "Iteration 95, loss = 0.02228575\n",
            "Iteration 96, loss = 0.02257782\n",
            "Iteration 97, loss = 0.02293571\n",
            "Iteration 98, loss = 0.02322654\n",
            "Iteration 99, loss = 0.02614866\n",
            "Iteration 100, loss = 0.02475345\n",
            "29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.32001066\n",
            "Iteration 2, loss = 0.90977362\n",
            "Iteration 3, loss = 0.40812223\n",
            "Iteration 4, loss = 0.10462511\n",
            "Iteration 5, loss = 0.01875287\n",
            "Iteration 6, loss = 0.00416603\n",
            "Iteration 7, loss = 0.00166658\n",
            "Iteration 8, loss = 0.00096832\n",
            "Iteration 9, loss = 0.00073343\n",
            "Iteration 10, loss = 0.00062665\n",
            "Iteration 11, loss = 0.00056158\n",
            "Iteration 12, loss = 0.00052064\n",
            "Iteration 13, loss = 0.00048603\n",
            "Iteration 14, loss = 0.00045829\n",
            "Iteration 15, loss = 0.00043526\n",
            "Iteration 16, loss = 0.00041434\n",
            "Iteration 17, loss = 0.00039540\n",
            "Iteration 18, loss = 0.00037748\n",
            "Iteration 19, loss = 0.00036205\n",
            "Iteration 20, loss = 0.00034722\n",
            "Iteration 21, loss = 0.00033381\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31516473\n",
            "Iteration 2, loss = 0.89933194\n",
            "Iteration 3, loss = 0.40122404\n",
            "Iteration 4, loss = 0.10304666\n",
            "Iteration 5, loss = 0.01891147\n",
            "Iteration 6, loss = 0.00411662\n",
            "Iteration 7, loss = 0.00162491\n",
            "Iteration 8, loss = 0.00094359\n",
            "Iteration 9, loss = 0.00071660\n",
            "Iteration 10, loss = 0.00060838\n",
            "Iteration 11, loss = 0.00054543\n",
            "Iteration 12, loss = 0.00050402\n",
            "Iteration 13, loss = 0.00047028\n",
            "Iteration 14, loss = 0.00044328\n",
            "Iteration 15, loss = 0.00042105\n",
            "Iteration 16, loss = 0.00040082\n",
            "Iteration 17, loss = 0.00038285\n",
            "Iteration 18, loss = 0.00036650\n",
            "Iteration 19, loss = 0.00035271\n",
            "Iteration 20, loss = 0.00033969\n",
            "Iteration 21, loss = 0.00032781\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31769745\n",
            "Iteration 2, loss = 0.90760699\n",
            "Iteration 3, loss = 0.40523926\n",
            "Iteration 4, loss = 0.10413489\n",
            "Iteration 5, loss = 0.01955679\n",
            "Iteration 6, loss = 0.00426205\n",
            "Iteration 7, loss = 0.00170668\n",
            "Iteration 8, loss = 0.00099942\n",
            "Iteration 9, loss = 0.00075783\n",
            "Iteration 10, loss = 0.00064478\n",
            "Iteration 11, loss = 0.00057680\n",
            "Iteration 12, loss = 0.00053247\n",
            "Iteration 13, loss = 0.00049538\n",
            "Iteration 14, loss = 0.00046510\n",
            "Iteration 15, loss = 0.00043970\n",
            "Iteration 16, loss = 0.00041593\n",
            "Iteration 17, loss = 0.00039281\n",
            "Iteration 18, loss = 0.00037179\n",
            "Iteration 19, loss = 0.00035311\n",
            "Iteration 20, loss = 0.00033622\n",
            "Iteration 21, loss = 0.00032056\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31835915\n",
            "Iteration 2, loss = 0.91251346\n",
            "Iteration 3, loss = 0.41120478\n",
            "Iteration 4, loss = 0.10602128\n",
            "Iteration 5, loss = 0.01871106\n",
            "Iteration 6, loss = 0.00387898\n",
            "Iteration 7, loss = 0.00154245\n",
            "Iteration 8, loss = 0.00090386\n",
            "Iteration 9, loss = 0.00069433\n",
            "Iteration 10, loss = 0.00059844\n",
            "Iteration 11, loss = 0.00054452\n",
            "Iteration 12, loss = 0.00050972\n",
            "Iteration 13, loss = 0.00048071\n",
            "Iteration 14, loss = 0.00045840\n",
            "Iteration 15, loss = 0.00043951\n",
            "Iteration 16, loss = 0.00042187\n",
            "Iteration 17, loss = 0.00040605\n",
            "Iteration 18, loss = 0.00039206\n",
            "Iteration 19, loss = 0.00037923\n",
            "Iteration 20, loss = 0.00036754\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31524461\n",
            "Iteration 2, loss = 0.89656262\n",
            "Iteration 3, loss = 0.37982484\n",
            "Iteration 4, loss = 0.08622174\n",
            "Iteration 5, loss = 0.01396343\n",
            "Iteration 6, loss = 0.00297454\n",
            "Iteration 7, loss = 0.00123311\n",
            "Iteration 8, loss = 0.00075795\n",
            "Iteration 9, loss = 0.00060084\n",
            "Iteration 10, loss = 0.00051945\n",
            "Iteration 11, loss = 0.00047536\n",
            "Iteration 12, loss = 0.00044492\n",
            "Iteration 13, loss = 0.00042101\n",
            "Iteration 14, loss = 0.00040271\n",
            "Iteration 15, loss = 0.00038455\n",
            "Iteration 16, loss = 0.00036955\n",
            "Iteration 17, loss = 0.00035527\n",
            "Iteration 18, loss = 0.00034282\n",
            "Iteration 19, loss = 0.00033100\n",
            "Iteration 20, loss = 0.00032116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30713297\n",
            "Iteration 2, loss = 0.92190005\n",
            "Iteration 3, loss = 0.39935470\n",
            "Iteration 4, loss = 0.09158256\n",
            "Iteration 5, loss = 0.01530569\n",
            "Iteration 6, loss = 0.00354048\n",
            "Iteration 7, loss = 0.00141396\n",
            "Iteration 8, loss = 0.00084542\n",
            "Iteration 9, loss = 0.00066082\n",
            "Iteration 10, loss = 0.00056891\n",
            "Iteration 11, loss = 0.00051727\n",
            "Iteration 12, loss = 0.00048245\n",
            "Iteration 13, loss = 0.00045529\n",
            "Iteration 14, loss = 0.00043428\n",
            "Iteration 15, loss = 0.00041522\n",
            "Iteration 16, loss = 0.00039869\n",
            "Iteration 17, loss = 0.00038324\n",
            "Iteration 18, loss = 0.00036964\n",
            "Iteration 19, loss = 0.00035744\n",
            "Iteration 20, loss = 0.00034622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30207134\n",
            "Iteration 2, loss = 0.94863260\n",
            "Iteration 3, loss = 0.42789279\n",
            "Iteration 4, loss = 0.09640273\n",
            "Iteration 5, loss = 0.01590771\n",
            "Iteration 6, loss = 0.00371129\n",
            "Iteration 7, loss = 0.00149075\n",
            "Iteration 8, loss = 0.00088873\n",
            "Iteration 9, loss = 0.00068816\n",
            "Iteration 10, loss = 0.00058936\n",
            "Iteration 11, loss = 0.00053067\n",
            "Iteration 12, loss = 0.00049066\n",
            "Iteration 13, loss = 0.00046083\n",
            "Iteration 14, loss = 0.00043620\n",
            "Iteration 15, loss = 0.00041514\n",
            "Iteration 16, loss = 0.00039614\n",
            "Iteration 17, loss = 0.00037910\n",
            "Iteration 18, loss = 0.00036383\n",
            "Iteration 19, loss = 0.00035084\n",
            "Iteration 20, loss = 0.00033842\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31017078\n",
            "Iteration 2, loss = 0.92683958\n",
            "Iteration 3, loss = 0.42186951\n",
            "Iteration 4, loss = 0.11546670\n",
            "Iteration 5, loss = 0.02238735\n",
            "Iteration 6, loss = 0.00497307\n",
            "Iteration 7, loss = 0.00183507\n",
            "Iteration 8, loss = 0.00101385\n",
            "Iteration 9, loss = 0.00076513\n",
            "Iteration 10, loss = 0.00065257\n",
            "Iteration 11, loss = 0.00058535\n",
            "Iteration 12, loss = 0.00054314\n",
            "Iteration 13, loss = 0.00051107\n",
            "Iteration 14, loss = 0.00048552\n",
            "Iteration 15, loss = 0.00046383\n",
            "Iteration 16, loss = 0.00044426\n",
            "Iteration 17, loss = 0.00042648\n",
            "Iteration 18, loss = 0.00041050\n",
            "Iteration 19, loss = 0.00039668\n",
            "Iteration 20, loss = 0.00038334\n",
            "Iteration 21, loss = 0.00037166\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32389633\n",
            "Iteration 2, loss = 0.94007732\n",
            "Iteration 3, loss = 0.45762472\n",
            "Iteration 4, loss = 0.14025965\n",
            "Iteration 5, loss = 0.02690450\n",
            "Iteration 6, loss = 0.00562741\n",
            "Iteration 7, loss = 0.00191579\n",
            "Iteration 8, loss = 0.00105895\n",
            "Iteration 9, loss = 0.00078419\n",
            "Iteration 10, loss = 0.00066335\n",
            "Iteration 11, loss = 0.00059331\n",
            "Iteration 12, loss = 0.00054909\n",
            "Iteration 13, loss = 0.00051697\n",
            "Iteration 14, loss = 0.00049075\n",
            "Iteration 15, loss = 0.00046830\n",
            "Iteration 16, loss = 0.00044838\n",
            "Iteration 17, loss = 0.00042990\n",
            "Iteration 18, loss = 0.00041311\n",
            "Iteration 19, loss = 0.00039804\n",
            "Iteration 20, loss = 0.00038361\n",
            "Iteration 21, loss = 0.00037098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32913457\n",
            "Iteration 2, loss = 0.96843239\n",
            "Iteration 3, loss = 0.49104062\n",
            "Iteration 4, loss = 0.14985769\n",
            "Iteration 5, loss = 0.02849058\n",
            "Iteration 6, loss = 0.00629099\n",
            "Iteration 7, loss = 0.00217870\n",
            "Iteration 8, loss = 0.00119309\n",
            "Iteration 9, loss = 0.00087718\n",
            "Iteration 10, loss = 0.00072752\n",
            "Iteration 11, loss = 0.00064679\n",
            "Iteration 12, loss = 0.00059329\n",
            "Iteration 13, loss = 0.00055358\n",
            "Iteration 14, loss = 0.00052220\n",
            "Iteration 15, loss = 0.00049556\n",
            "Iteration 16, loss = 0.00047160\n",
            "Iteration 17, loss = 0.00045011\n",
            "Iteration 18, loss = 0.00043070\n",
            "Iteration 19, loss = 0.00041385\n",
            "Iteration 20, loss = 0.00039789\n",
            "Iteration 21, loss = 0.00038368\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "30\n",
            "Iteration 1, loss = 1.34856638\n",
            "Iteration 2, loss = 1.26153003\n",
            "Iteration 3, loss = 1.23838577\n",
            "Iteration 4, loss = 1.19759602\n",
            "Iteration 5, loss = 1.19203067\n",
            "Iteration 6, loss = 1.17109070\n",
            "Iteration 7, loss = 1.16863865\n",
            "Iteration 8, loss = 1.16017638\n",
            "Iteration 9, loss = 1.14996030\n",
            "Iteration 10, loss = 1.13580310\n",
            "Iteration 11, loss = 1.12898296\n",
            "Iteration 12, loss = 1.12361447\n",
            "Iteration 13, loss = 1.11866555\n",
            "Iteration 14, loss = 1.10370166\n",
            "Iteration 15, loss = 1.09294505\n",
            "Iteration 16, loss = 1.08671011\n",
            "Iteration 17, loss = 1.09507064\n",
            "Iteration 18, loss = 1.09453529\n",
            "Iteration 19, loss = 1.09032461\n",
            "Iteration 20, loss = 1.07465462\n",
            "Iteration 21, loss = 1.06287333\n",
            "Iteration 22, loss = 1.05478915\n",
            "Iteration 23, loss = 1.04125199\n",
            "Iteration 24, loss = 1.03694392\n",
            "Iteration 25, loss = 1.03147317\n",
            "Iteration 26, loss = 1.02212298\n",
            "Iteration 27, loss = 1.01344108\n",
            "Iteration 28, loss = 1.01476924\n",
            "Iteration 29, loss = 1.01951827\n",
            "Iteration 30, loss = 1.03232541\n",
            "Iteration 31, loss = 1.03650772\n",
            "Iteration 32, loss = 1.00768989\n",
            "Iteration 33, loss = 0.99207185\n",
            "Iteration 34, loss = 0.99756185\n",
            "Iteration 35, loss = 0.97922624\n",
            "Iteration 36, loss = 0.98836998\n",
            "Iteration 37, loss = 1.01493188\n",
            "Iteration 38, loss = 0.97744907\n",
            "Iteration 39, loss = 0.97092777\n",
            "Iteration 40, loss = 0.96388368\n",
            "Iteration 41, loss = 0.95686542\n",
            "Iteration 42, loss = 0.95695271\n",
            "Iteration 43, loss = 0.97880415\n",
            "Iteration 44, loss = 1.00491822\n",
            "Iteration 45, loss = 0.95762783\n",
            "Iteration 46, loss = 0.94171333\n",
            "Iteration 47, loss = 0.94803596\n",
            "Iteration 48, loss = 0.95367362\n",
            "Iteration 49, loss = 0.94106068\n",
            "Iteration 50, loss = 0.95797339\n",
            "Iteration 51, loss = 0.92977294\n",
            "Iteration 52, loss = 0.91267961\n",
            "Iteration 53, loss = 0.90276250\n",
            "Iteration 54, loss = 0.90829058\n",
            "Iteration 55, loss = 0.92440086\n",
            "Iteration 56, loss = 0.91841629\n",
            "Iteration 57, loss = 0.87478733\n",
            "Iteration 58, loss = 0.87617019\n",
            "Iteration 59, loss = 0.86743651\n",
            "Iteration 60, loss = 0.86230012\n",
            "Iteration 61, loss = 0.86049450\n",
            "Iteration 62, loss = 0.85146055\n",
            "Iteration 63, loss = 0.85853723\n",
            "Iteration 64, loss = 0.84083273\n",
            "Iteration 65, loss = 0.88011810\n",
            "Iteration 66, loss = 0.86188715\n",
            "Iteration 67, loss = 0.82716695\n",
            "Iteration 68, loss = 0.82760059\n",
            "Iteration 69, loss = 0.83575441\n",
            "Iteration 70, loss = 0.80913167\n",
            "Iteration 71, loss = 0.80428823\n",
            "Iteration 72, loss = 0.80026007\n",
            "Iteration 73, loss = 0.80468301\n",
            "Iteration 74, loss = 0.78975678\n",
            "Iteration 75, loss = 0.78185732\n",
            "Iteration 76, loss = 0.79550531\n",
            "Iteration 77, loss = 0.79057424\n",
            "Iteration 78, loss = 0.80885717\n",
            "Iteration 79, loss = 0.82833937\n",
            "Iteration 80, loss = 0.78712828\n",
            "Iteration 81, loss = 0.79161929\n",
            "Iteration 82, loss = 0.75880513\n",
            "Iteration 83, loss = 0.75834789\n",
            "Iteration 84, loss = 0.80559991\n",
            "Iteration 85, loss = 0.78546421\n",
            "Iteration 86, loss = 0.76865666\n",
            "Iteration 87, loss = 0.74970406\n",
            "Iteration 88, loss = 0.73762026\n",
            "Iteration 89, loss = 0.74138713\n",
            "Iteration 90, loss = 0.72635327\n",
            "Iteration 91, loss = 0.75579895\n",
            "Iteration 92, loss = 0.71480664\n",
            "Iteration 93, loss = 0.72178084\n",
            "Iteration 94, loss = 0.70905525\n",
            "Iteration 95, loss = 0.69309356\n",
            "Iteration 96, loss = 0.72210203\n",
            "Iteration 97, loss = 0.69723946\n",
            "Iteration 98, loss = 0.69684504\n",
            "Iteration 99, loss = 0.72446706\n",
            "Iteration 100, loss = 0.73169805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34961219\n",
            "Iteration 2, loss = 1.27200336\n",
            "Iteration 3, loss = 1.24059966\n",
            "Iteration 4, loss = 1.20222428\n",
            "Iteration 5, loss = 1.18718669\n",
            "Iteration 6, loss = 1.17105375\n",
            "Iteration 7, loss = 1.17185029\n",
            "Iteration 8, loss = 1.16109875\n",
            "Iteration 9, loss = 1.16747256\n",
            "Iteration 10, loss = 1.14316200\n",
            "Iteration 11, loss = 1.13706477\n",
            "Iteration 12, loss = 1.12911925\n",
            "Iteration 13, loss = 1.12824083\n",
            "Iteration 14, loss = 1.11427960\n",
            "Iteration 15, loss = 1.10093917\n",
            "Iteration 16, loss = 1.09129708\n",
            "Iteration 17, loss = 1.09591935\n",
            "Iteration 18, loss = 1.10153373\n",
            "Iteration 19, loss = 1.09844347\n",
            "Iteration 20, loss = 1.09283068\n",
            "Iteration 21, loss = 1.07497970\n",
            "Iteration 22, loss = 1.06560005\n",
            "Iteration 23, loss = 1.04605744\n",
            "Iteration 24, loss = 1.03459767\n",
            "Iteration 25, loss = 1.02435643\n",
            "Iteration 26, loss = 1.01182954\n",
            "Iteration 27, loss = 1.00700125\n",
            "Iteration 28, loss = 1.00597257\n",
            "Iteration 29, loss = 0.99687954\n",
            "Iteration 30, loss = 0.99850688\n",
            "Iteration 31, loss = 1.02358877\n",
            "Iteration 32, loss = 0.98932075\n",
            "Iteration 33, loss = 0.97865392\n",
            "Iteration 34, loss = 0.98064089\n",
            "Iteration 35, loss = 0.96041847\n",
            "Iteration 36, loss = 0.97032701\n",
            "Iteration 37, loss = 0.98445116\n",
            "Iteration 38, loss = 0.94768496\n",
            "Iteration 39, loss = 0.93993434\n",
            "Iteration 40, loss = 0.95794614\n",
            "Iteration 41, loss = 0.93511909\n",
            "Iteration 42, loss = 0.94922071\n",
            "Iteration 43, loss = 0.98256975\n",
            "Iteration 44, loss = 1.00480891\n",
            "Iteration 45, loss = 0.96515792\n",
            "Iteration 46, loss = 0.95175065\n",
            "Iteration 47, loss = 0.93775355\n",
            "Iteration 48, loss = 0.93464141\n",
            "Iteration 49, loss = 0.89807209\n",
            "Iteration 50, loss = 0.91073005\n",
            "Iteration 51, loss = 0.89699969\n",
            "Iteration 52, loss = 0.88938510\n",
            "Iteration 53, loss = 0.87606146\n",
            "Iteration 54, loss = 0.89535395\n",
            "Iteration 55, loss = 0.89326577\n",
            "Iteration 56, loss = 0.89518419\n",
            "Iteration 57, loss = 0.86256035\n",
            "Iteration 58, loss = 0.86152963\n",
            "Iteration 59, loss = 0.84782096\n",
            "Iteration 60, loss = 0.84416339\n",
            "Iteration 61, loss = 0.83669011\n",
            "Iteration 62, loss = 0.85789928\n",
            "Iteration 63, loss = 0.85299529\n",
            "Iteration 64, loss = 0.82237980\n",
            "Iteration 65, loss = 0.83789643\n",
            "Iteration 66, loss = 0.81855075\n",
            "Iteration 67, loss = 0.80489649\n",
            "Iteration 68, loss = 0.81943933\n",
            "Iteration 69, loss = 0.82590433\n",
            "Iteration 70, loss = 0.80572039\n",
            "Iteration 71, loss = 0.80796287\n",
            "Iteration 72, loss = 0.79173461\n",
            "Iteration 73, loss = 0.78320593\n",
            "Iteration 74, loss = 0.81779909\n",
            "Iteration 75, loss = 0.79099859\n",
            "Iteration 76, loss = 0.78455353\n",
            "Iteration 77, loss = 0.76555579\n",
            "Iteration 78, loss = 0.76312504\n",
            "Iteration 79, loss = 0.79270910\n",
            "Iteration 80, loss = 0.79283205\n",
            "Iteration 81, loss = 0.77574770\n",
            "Iteration 82, loss = 0.75709013\n",
            "Iteration 83, loss = 0.78656612\n",
            "Iteration 84, loss = 0.80927581\n",
            "Iteration 85, loss = 0.79931133\n",
            "Iteration 86, loss = 0.78594183\n",
            "Iteration 87, loss = 0.75455069\n",
            "Iteration 88, loss = 0.76777487\n",
            "Iteration 89, loss = 0.72756616\n",
            "Iteration 90, loss = 0.71495703\n",
            "Iteration 91, loss = 0.71994742\n",
            "Iteration 92, loss = 0.71206652\n",
            "Iteration 93, loss = 0.74120279\n",
            "Iteration 94, loss = 0.72391086\n",
            "Iteration 95, loss = 0.71099095\n",
            "Iteration 96, loss = 0.71553731\n",
            "Iteration 97, loss = 0.68794583\n",
            "Iteration 98, loss = 0.69670524\n",
            "Iteration 99, loss = 0.70499358\n",
            "Iteration 100, loss = 0.71114812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35816002\n",
            "Iteration 2, loss = 1.27877383\n",
            "Iteration 3, loss = 1.23422752\n",
            "Iteration 4, loss = 1.21306725\n",
            "Iteration 5, loss = 1.19163627\n",
            "Iteration 6, loss = 1.17381719\n",
            "Iteration 7, loss = 1.17144354\n",
            "Iteration 8, loss = 1.16290693\n",
            "Iteration 9, loss = 1.17262394\n",
            "Iteration 10, loss = 1.15685419\n",
            "Iteration 11, loss = 1.12928356\n",
            "Iteration 12, loss = 1.12557050\n",
            "Iteration 13, loss = 1.10661315\n",
            "Iteration 14, loss = 1.09946280\n",
            "Iteration 15, loss = 1.08797463\n",
            "Iteration 16, loss = 1.08268702\n",
            "Iteration 17, loss = 1.07554889\n",
            "Iteration 18, loss = 1.07186398\n",
            "Iteration 19, loss = 1.07459680\n",
            "Iteration 20, loss = 1.07472608\n",
            "Iteration 21, loss = 1.08174293\n",
            "Iteration 22, loss = 1.05241950\n",
            "Iteration 23, loss = 1.03593890\n",
            "Iteration 24, loss = 1.03285332\n",
            "Iteration 25, loss = 1.03523570\n",
            "Iteration 26, loss = 1.02147709\n",
            "Iteration 27, loss = 1.00850258\n",
            "Iteration 28, loss = 1.00546289\n",
            "Iteration 29, loss = 1.00127871\n",
            "Iteration 30, loss = 0.99782352\n",
            "Iteration 31, loss = 1.03412728\n",
            "Iteration 32, loss = 0.99474625\n",
            "Iteration 33, loss = 0.98406247\n",
            "Iteration 34, loss = 0.97427953\n",
            "Iteration 35, loss = 0.97239445\n",
            "Iteration 36, loss = 0.97329534\n",
            "Iteration 37, loss = 0.99155840\n",
            "Iteration 38, loss = 0.96713736\n",
            "Iteration 39, loss = 0.94537292\n",
            "Iteration 40, loss = 0.93108968\n",
            "Iteration 41, loss = 0.92070272\n",
            "Iteration 42, loss = 0.93674875\n",
            "Iteration 43, loss = 0.94535225\n",
            "Iteration 44, loss = 0.99715098\n",
            "Iteration 45, loss = 0.94541684\n",
            "Iteration 46, loss = 0.91375887\n",
            "Iteration 47, loss = 0.90154294\n",
            "Iteration 48, loss = 0.89968670\n",
            "Iteration 49, loss = 0.88127336\n",
            "Iteration 50, loss = 0.89340481\n",
            "Iteration 51, loss = 0.87580909\n",
            "Iteration 52, loss = 0.88749540\n",
            "Iteration 53, loss = 0.91133495\n",
            "Iteration 54, loss = 0.91532510\n",
            "Iteration 55, loss = 0.86726709\n",
            "Iteration 56, loss = 0.84793826\n",
            "Iteration 57, loss = 0.84452836\n",
            "Iteration 58, loss = 0.84360577\n",
            "Iteration 59, loss = 0.82721809\n",
            "Iteration 60, loss = 0.83803763\n",
            "Iteration 61, loss = 0.82887434\n",
            "Iteration 62, loss = 0.85030411\n",
            "Iteration 63, loss = 0.82964031\n",
            "Iteration 64, loss = 0.81521320\n",
            "Iteration 65, loss = 0.81158909\n",
            "Iteration 66, loss = 0.80210659\n",
            "Iteration 67, loss = 0.79578016\n",
            "Iteration 68, loss = 0.82236149\n",
            "Iteration 69, loss = 0.83707556\n",
            "Iteration 70, loss = 0.80882705\n",
            "Iteration 71, loss = 0.81088364\n",
            "Iteration 72, loss = 0.79051583\n",
            "Iteration 73, loss = 0.76831640\n",
            "Iteration 74, loss = 0.80718911\n",
            "Iteration 75, loss = 0.78771232\n",
            "Iteration 76, loss = 0.77751309\n",
            "Iteration 77, loss = 0.76702110\n",
            "Iteration 78, loss = 0.75321948\n",
            "Iteration 79, loss = 0.77482376\n",
            "Iteration 80, loss = 0.74185401\n",
            "Iteration 81, loss = 0.75158124\n",
            "Iteration 82, loss = 0.74827792\n",
            "Iteration 83, loss = 0.76734081\n",
            "Iteration 84, loss = 0.75556578\n",
            "Iteration 85, loss = 0.75050652\n",
            "Iteration 86, loss = 0.73731923\n",
            "Iteration 87, loss = 0.73458735\n",
            "Iteration 88, loss = 0.70681572\n",
            "Iteration 89, loss = 0.69503921\n",
            "Iteration 90, loss = 0.70176183\n",
            "Iteration 91, loss = 0.69770260\n",
            "Iteration 92, loss = 0.72003187\n",
            "Iteration 93, loss = 0.70369471\n",
            "Iteration 94, loss = 0.70530488\n",
            "Iteration 95, loss = 0.69260656\n",
            "Iteration 96, loss = 0.67377885\n",
            "Iteration 97, loss = 0.67002160\n",
            "Iteration 98, loss = 0.68490551\n",
            "Iteration 99, loss = 0.68839858\n",
            "Iteration 100, loss = 0.66428145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36353904\n",
            "Iteration 2, loss = 1.28543291\n",
            "Iteration 3, loss = 1.23156085\n",
            "Iteration 4, loss = 1.21249061\n",
            "Iteration 5, loss = 1.19820773\n",
            "Iteration 6, loss = 1.18099776\n",
            "Iteration 7, loss = 1.17959487\n",
            "Iteration 8, loss = 1.17629097\n",
            "Iteration 9, loss = 1.18954850\n",
            "Iteration 10, loss = 1.16826724\n",
            "Iteration 11, loss = 1.15081208\n",
            "Iteration 12, loss = 1.13007223\n",
            "Iteration 13, loss = 1.13032527\n",
            "Iteration 14, loss = 1.11336491\n",
            "Iteration 15, loss = 1.10924520\n",
            "Iteration 16, loss = 1.10527793\n",
            "Iteration 17, loss = 1.09412257\n",
            "Iteration 18, loss = 1.10534420\n",
            "Iteration 19, loss = 1.09603605\n",
            "Iteration 20, loss = 1.10804652\n",
            "Iteration 21, loss = 1.10726480\n",
            "Iteration 22, loss = 1.08948347\n",
            "Iteration 23, loss = 1.07189490\n",
            "Iteration 24, loss = 1.04438990\n",
            "Iteration 25, loss = 1.04773395\n",
            "Iteration 26, loss = 1.02791303\n",
            "Iteration 27, loss = 1.01624208\n",
            "Iteration 28, loss = 1.01037636\n",
            "Iteration 29, loss = 0.99898641\n",
            "Iteration 30, loss = 1.00579986\n",
            "Iteration 31, loss = 1.02275431\n",
            "Iteration 32, loss = 1.00346022\n",
            "Iteration 33, loss = 0.99155043\n",
            "Iteration 34, loss = 0.99880282\n",
            "Iteration 35, loss = 0.97250049\n",
            "Iteration 36, loss = 0.97602184\n",
            "Iteration 37, loss = 0.98415516\n",
            "Iteration 38, loss = 0.95669584\n",
            "Iteration 39, loss = 0.94873835\n",
            "Iteration 40, loss = 0.93103259\n",
            "Iteration 41, loss = 0.93306821\n",
            "Iteration 42, loss = 0.92740053\n",
            "Iteration 43, loss = 0.93034431\n",
            "Iteration 44, loss = 0.99083606\n",
            "Iteration 45, loss = 0.98243451\n",
            "Iteration 46, loss = 0.92871452\n",
            "Iteration 47, loss = 0.90468155\n",
            "Iteration 48, loss = 0.91900461\n",
            "Iteration 49, loss = 0.89696795\n",
            "Iteration 50, loss = 0.89033868\n",
            "Iteration 51, loss = 0.86812583\n",
            "Iteration 52, loss = 0.86838157\n",
            "Iteration 53, loss = 0.89371672\n",
            "Iteration 54, loss = 0.88288591\n",
            "Iteration 55, loss = 0.89481355\n",
            "Iteration 56, loss = 0.94979288\n",
            "Iteration 57, loss = 0.90205031\n",
            "Iteration 58, loss = 0.88204785\n",
            "Iteration 59, loss = 0.86083949\n",
            "Iteration 60, loss = 0.83629007\n",
            "Iteration 61, loss = 0.84374131\n",
            "Iteration 62, loss = 0.89542909\n",
            "Iteration 63, loss = 0.88357356\n",
            "Iteration 64, loss = 0.84743216\n",
            "Iteration 65, loss = 0.81262968\n",
            "Iteration 66, loss = 0.81051472\n",
            "Iteration 67, loss = 0.80580621\n",
            "Iteration 68, loss = 0.79512958\n",
            "Iteration 69, loss = 0.82236483\n",
            "Iteration 70, loss = 0.79769024\n",
            "Iteration 71, loss = 0.77989952\n",
            "Iteration 72, loss = 0.77947298\n",
            "Iteration 73, loss = 0.77404910\n",
            "Iteration 74, loss = 0.79872500\n",
            "Iteration 75, loss = 0.80266619\n",
            "Iteration 76, loss = 0.79206357\n",
            "Iteration 77, loss = 0.76167809\n",
            "Iteration 78, loss = 0.75320913\n",
            "Iteration 79, loss = 0.75725523\n",
            "Iteration 80, loss = 0.75297573\n",
            "Iteration 81, loss = 0.77744893\n",
            "Iteration 82, loss = 0.73254628\n",
            "Iteration 83, loss = 0.74490538\n",
            "Iteration 84, loss = 0.77416117\n",
            "Iteration 85, loss = 0.83128992\n",
            "Iteration 86, loss = 0.76887006\n",
            "Iteration 87, loss = 0.73102232\n",
            "Iteration 88, loss = 0.71644116\n",
            "Iteration 89, loss = 0.71272427\n",
            "Iteration 90, loss = 0.70103506\n",
            "Iteration 91, loss = 0.67712918\n",
            "Iteration 92, loss = 0.69732080\n",
            "Iteration 93, loss = 0.69260295\n",
            "Iteration 94, loss = 0.70067912\n",
            "Iteration 95, loss = 0.66753513\n",
            "Iteration 96, loss = 0.68422300\n",
            "Iteration 97, loss = 0.68546961\n",
            "Iteration 98, loss = 0.67023403\n",
            "Iteration 99, loss = 0.67657796\n",
            "Iteration 100, loss = 0.71250844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36392789\n",
            "Iteration 2, loss = 1.28633237\n",
            "Iteration 3, loss = 1.23390276\n",
            "Iteration 4, loss = 1.20216911\n",
            "Iteration 5, loss = 1.17527365\n",
            "Iteration 6, loss = 1.16269724\n",
            "Iteration 7, loss = 1.15976535\n",
            "Iteration 8, loss = 1.14604014\n",
            "Iteration 9, loss = 1.15603008\n",
            "Iteration 10, loss = 1.13426808\n",
            "Iteration 11, loss = 1.11568992\n",
            "Iteration 12, loss = 1.10842717\n",
            "Iteration 13, loss = 1.09899976\n",
            "Iteration 14, loss = 1.08766207\n",
            "Iteration 15, loss = 1.08193014\n",
            "Iteration 16, loss = 1.07744006\n",
            "Iteration 17, loss = 1.06553506\n",
            "Iteration 18, loss = 1.06608373\n",
            "Iteration 19, loss = 1.08455029\n",
            "Iteration 20, loss = 1.09187536\n",
            "Iteration 21, loss = 1.07214962\n",
            "Iteration 22, loss = 1.03413000\n",
            "Iteration 23, loss = 1.03045300\n",
            "Iteration 24, loss = 1.01998783\n",
            "Iteration 25, loss = 1.03421667\n",
            "Iteration 26, loss = 1.01550042\n",
            "Iteration 27, loss = 1.00505526\n",
            "Iteration 28, loss = 1.00588181\n",
            "Iteration 29, loss = 0.98996274\n",
            "Iteration 30, loss = 0.99205581\n",
            "Iteration 31, loss = 1.00915509\n",
            "Iteration 32, loss = 0.98385502\n",
            "Iteration 33, loss = 0.96809411\n",
            "Iteration 34, loss = 1.00212203\n",
            "Iteration 35, loss = 0.97146336\n",
            "Iteration 36, loss = 0.98067373\n",
            "Iteration 37, loss = 1.00442039\n",
            "Iteration 38, loss = 0.98292755\n",
            "Iteration 39, loss = 0.96678123\n",
            "Iteration 40, loss = 0.95907265\n",
            "Iteration 41, loss = 0.93446093\n",
            "Iteration 42, loss = 0.94701445\n",
            "Iteration 43, loss = 0.93427842\n",
            "Iteration 44, loss = 0.97905826\n",
            "Iteration 45, loss = 0.95351420\n",
            "Iteration 46, loss = 0.93486176\n",
            "Iteration 47, loss = 0.92221679\n",
            "Iteration 48, loss = 0.92731978\n",
            "Iteration 49, loss = 0.90124667\n",
            "Iteration 50, loss = 0.89544866\n",
            "Iteration 51, loss = 0.89094481\n",
            "Iteration 52, loss = 0.87364815\n",
            "Iteration 53, loss = 0.88363858\n",
            "Iteration 54, loss = 0.87793663\n",
            "Iteration 55, loss = 0.88201190\n",
            "Iteration 56, loss = 0.90698936\n",
            "Iteration 57, loss = 0.86827649\n",
            "Iteration 58, loss = 0.85258994\n",
            "Iteration 59, loss = 0.84056022\n",
            "Iteration 60, loss = 0.83940408\n",
            "Iteration 61, loss = 0.82286676\n",
            "Iteration 62, loss = 0.86201690\n",
            "Iteration 63, loss = 0.87367308\n",
            "Iteration 64, loss = 0.84374747\n",
            "Iteration 65, loss = 0.80719945\n",
            "Iteration 66, loss = 0.83230698\n",
            "Iteration 67, loss = 0.83060328\n",
            "Iteration 68, loss = 0.80878363\n",
            "Iteration 69, loss = 0.82372846\n",
            "Iteration 70, loss = 0.78564199\n",
            "Iteration 71, loss = 0.78466774\n",
            "Iteration 72, loss = 0.77776087\n",
            "Iteration 73, loss = 0.77591934\n",
            "Iteration 74, loss = 0.80758614\n",
            "Iteration 75, loss = 0.78469966\n",
            "Iteration 76, loss = 0.76990331\n",
            "Iteration 77, loss = 0.76904274\n",
            "Iteration 78, loss = 0.75073374\n",
            "Iteration 79, loss = 0.74997372\n",
            "Iteration 80, loss = 0.74936480\n",
            "Iteration 81, loss = 0.78779318\n",
            "Iteration 82, loss = 0.75969725\n",
            "Iteration 83, loss = 0.75477172\n",
            "Iteration 84, loss = 0.80963547\n",
            "Iteration 85, loss = 0.79158834\n",
            "Iteration 86, loss = 0.73916553\n",
            "Iteration 87, loss = 0.70760906\n",
            "Iteration 88, loss = 0.70639444\n",
            "Iteration 89, loss = 0.69718192\n",
            "Iteration 90, loss = 0.70498080\n",
            "Iteration 91, loss = 0.69356888\n",
            "Iteration 92, loss = 0.70752853\n",
            "Iteration 93, loss = 0.68343537\n",
            "Iteration 94, loss = 0.72191484\n",
            "Iteration 95, loss = 0.71593535\n",
            "Iteration 96, loss = 0.70506992\n",
            "Iteration 97, loss = 0.70210577\n",
            "Iteration 98, loss = 0.68665254\n",
            "Iteration 99, loss = 0.70287456\n",
            "Iteration 100, loss = 0.71934552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36038056\n",
            "Iteration 2, loss = 1.28920043\n",
            "Iteration 3, loss = 1.24446981\n",
            "Iteration 4, loss = 1.21572877\n",
            "Iteration 5, loss = 1.20412827\n",
            "Iteration 6, loss = 1.18822412\n",
            "Iteration 7, loss = 1.18139691\n",
            "Iteration 8, loss = 1.17030722\n",
            "Iteration 9, loss = 1.17588930\n",
            "Iteration 10, loss = 1.15714698\n",
            "Iteration 11, loss = 1.14240543\n",
            "Iteration 12, loss = 1.12788729\n",
            "Iteration 13, loss = 1.11830278\n",
            "Iteration 14, loss = 1.11987607\n",
            "Iteration 15, loss = 1.11968423\n",
            "Iteration 16, loss = 1.11161105\n",
            "Iteration 17, loss = 1.09817376\n",
            "Iteration 18, loss = 1.10546784\n",
            "Iteration 19, loss = 1.08294168\n",
            "Iteration 20, loss = 1.09257482\n",
            "Iteration 21, loss = 1.08184799\n",
            "Iteration 22, loss = 1.05985838\n",
            "Iteration 23, loss = 1.04598395\n",
            "Iteration 24, loss = 1.04570912\n",
            "Iteration 25, loss = 1.04005173\n",
            "Iteration 26, loss = 1.05627581\n",
            "Iteration 27, loss = 1.05580481\n",
            "Iteration 28, loss = 1.05835258\n",
            "Iteration 29, loss = 1.04632397\n",
            "Iteration 30, loss = 1.03353997\n",
            "Iteration 31, loss = 1.05327014\n",
            "Iteration 32, loss = 1.02522231\n",
            "Iteration 33, loss = 1.01361897\n",
            "Iteration 34, loss = 1.01646992\n",
            "Iteration 35, loss = 0.99343814\n",
            "Iteration 36, loss = 1.01693505\n",
            "Iteration 37, loss = 1.03176799\n",
            "Iteration 38, loss = 1.00430403\n",
            "Iteration 39, loss = 1.00127626\n",
            "Iteration 40, loss = 0.97863532\n",
            "Iteration 41, loss = 0.96491002\n",
            "Iteration 42, loss = 0.97747860\n",
            "Iteration 43, loss = 0.96490281\n",
            "Iteration 44, loss = 0.99785956\n",
            "Iteration 45, loss = 0.98178656\n",
            "Iteration 46, loss = 0.94842935\n",
            "Iteration 47, loss = 0.94264691\n",
            "Iteration 48, loss = 0.96322310\n",
            "Iteration 49, loss = 0.94458353\n",
            "Iteration 50, loss = 0.95124520\n",
            "Iteration 51, loss = 0.94179430\n",
            "Iteration 52, loss = 0.93177351\n",
            "Iteration 53, loss = 0.92115062\n",
            "Iteration 54, loss = 0.91750836\n",
            "Iteration 55, loss = 0.91983719\n",
            "Iteration 56, loss = 0.90715594\n",
            "Iteration 57, loss = 0.89051660\n",
            "Iteration 58, loss = 0.89276987\n",
            "Iteration 59, loss = 0.88374427\n",
            "Iteration 60, loss = 0.88682885\n",
            "Iteration 61, loss = 0.87537011\n",
            "Iteration 62, loss = 0.90244868\n",
            "Iteration 63, loss = 0.88757841\n",
            "Iteration 64, loss = 0.86500021\n",
            "Iteration 65, loss = 0.87920026\n",
            "Iteration 66, loss = 0.85539788\n",
            "Iteration 67, loss = 0.86870014\n",
            "Iteration 68, loss = 0.88055304\n",
            "Iteration 69, loss = 0.86220128\n",
            "Iteration 70, loss = 0.83875502\n",
            "Iteration 71, loss = 0.83809752\n",
            "Iteration 72, loss = 0.83294521\n",
            "Iteration 73, loss = 0.82229475\n",
            "Iteration 74, loss = 0.85024359\n",
            "Iteration 75, loss = 0.84233927\n",
            "Iteration 76, loss = 0.81340645\n",
            "Iteration 77, loss = 0.79646369\n",
            "Iteration 78, loss = 0.80387955\n",
            "Iteration 79, loss = 0.81032948\n",
            "Iteration 80, loss = 0.81165579\n",
            "Iteration 81, loss = 0.81868071\n",
            "Iteration 82, loss = 0.80852403\n",
            "Iteration 83, loss = 0.77601840\n",
            "Iteration 84, loss = 0.76737002\n",
            "Iteration 85, loss = 0.79667692\n",
            "Iteration 86, loss = 0.77913143\n",
            "Iteration 87, loss = 0.74729478\n",
            "Iteration 88, loss = 0.73423032\n",
            "Iteration 89, loss = 0.72950130\n",
            "Iteration 90, loss = 0.74155506\n",
            "Iteration 91, loss = 0.72728083\n",
            "Iteration 92, loss = 0.73040018\n",
            "Iteration 93, loss = 0.72601635\n",
            "Iteration 94, loss = 0.74444743\n",
            "Iteration 95, loss = 0.74407844\n",
            "Iteration 96, loss = 0.74030387\n",
            "Iteration 97, loss = 0.76514428\n",
            "Iteration 98, loss = 0.74537843\n",
            "Iteration 99, loss = 0.76900546\n",
            "Iteration 100, loss = 0.77921268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35032066\n",
            "Iteration 2, loss = 1.28836187\n",
            "Iteration 3, loss = 1.25512898\n",
            "Iteration 4, loss = 1.22688594\n",
            "Iteration 5, loss = 1.21681924\n",
            "Iteration 6, loss = 1.19733431\n",
            "Iteration 7, loss = 1.19252774\n",
            "Iteration 8, loss = 1.18589812\n",
            "Iteration 9, loss = 1.18827175\n",
            "Iteration 10, loss = 1.16757978\n",
            "Iteration 11, loss = 1.16503482\n",
            "Iteration 12, loss = 1.15343348\n",
            "Iteration 13, loss = 1.14722076\n",
            "Iteration 14, loss = 1.13361124\n",
            "Iteration 15, loss = 1.13492715\n",
            "Iteration 16, loss = 1.12874538\n",
            "Iteration 17, loss = 1.11829437\n",
            "Iteration 18, loss = 1.12456263\n",
            "Iteration 19, loss = 1.11279647\n",
            "Iteration 20, loss = 1.11939162\n",
            "Iteration 21, loss = 1.10392339\n",
            "Iteration 22, loss = 1.08704338\n",
            "Iteration 23, loss = 1.06977801\n",
            "Iteration 24, loss = 1.07226813\n",
            "Iteration 25, loss = 1.05914054\n",
            "Iteration 26, loss = 1.07368526\n",
            "Iteration 27, loss = 1.05700142\n",
            "Iteration 28, loss = 1.05311089\n",
            "Iteration 29, loss = 1.04934095\n",
            "Iteration 30, loss = 1.04519292\n",
            "Iteration 31, loss = 1.05726458\n",
            "Iteration 32, loss = 1.03898556\n",
            "Iteration 33, loss = 1.02817683\n",
            "Iteration 34, loss = 1.01580468\n",
            "Iteration 35, loss = 1.01092815\n",
            "Iteration 36, loss = 1.03717400\n",
            "Iteration 37, loss = 1.07358926\n",
            "Iteration 38, loss = 1.04222223\n",
            "Iteration 39, loss = 1.02134008\n",
            "Iteration 40, loss = 0.99514862\n",
            "Iteration 41, loss = 0.97491380\n",
            "Iteration 42, loss = 0.99845443\n",
            "Iteration 43, loss = 0.97963378\n",
            "Iteration 44, loss = 1.00956916\n",
            "Iteration 45, loss = 0.99485137\n",
            "Iteration 46, loss = 0.96302337\n",
            "Iteration 47, loss = 0.95911618\n",
            "Iteration 48, loss = 0.97203580\n",
            "Iteration 49, loss = 0.94226277\n",
            "Iteration 50, loss = 0.94207417\n",
            "Iteration 51, loss = 0.93379797\n",
            "Iteration 52, loss = 0.93066223\n",
            "Iteration 53, loss = 0.92803290\n",
            "Iteration 54, loss = 0.90330648\n",
            "Iteration 55, loss = 0.91413752\n",
            "Iteration 56, loss = 0.91862237\n",
            "Iteration 57, loss = 0.90814839\n",
            "Iteration 58, loss = 0.89436471\n",
            "Iteration 59, loss = 0.88159888\n",
            "Iteration 60, loss = 0.87962317\n",
            "Iteration 61, loss = 0.87797332\n",
            "Iteration 62, loss = 0.89604726\n",
            "Iteration 63, loss = 0.90394361\n",
            "Iteration 64, loss = 0.92108902\n",
            "Iteration 65, loss = 0.86745101\n",
            "Iteration 66, loss = 0.86433025\n",
            "Iteration 67, loss = 0.85509839\n",
            "Iteration 68, loss = 0.91512461\n",
            "Iteration 69, loss = 0.91943329\n",
            "Iteration 70, loss = 0.87482875\n",
            "Iteration 71, loss = 0.87790355\n",
            "Iteration 72, loss = 0.83624174\n",
            "Iteration 73, loss = 0.82372711\n",
            "Iteration 74, loss = 0.84907723\n",
            "Iteration 75, loss = 0.83201102\n",
            "Iteration 76, loss = 0.80209248\n",
            "Iteration 77, loss = 0.79829032\n",
            "Iteration 78, loss = 0.79473071\n",
            "Iteration 79, loss = 0.80951843\n",
            "Iteration 80, loss = 0.81250679\n",
            "Iteration 81, loss = 0.80166153\n",
            "Iteration 82, loss = 0.80904058\n",
            "Iteration 83, loss = 0.78465118\n",
            "Iteration 84, loss = 0.78837415\n",
            "Iteration 85, loss = 0.80335366\n",
            "Iteration 86, loss = 0.78637721\n",
            "Iteration 87, loss = 0.75435127\n",
            "Iteration 88, loss = 0.75705777\n",
            "Iteration 89, loss = 0.75186162\n",
            "Iteration 90, loss = 0.74675548\n",
            "Iteration 91, loss = 0.73631668\n",
            "Iteration 92, loss = 0.74022461\n",
            "Iteration 93, loss = 0.72961898\n",
            "Iteration 94, loss = 0.74869698\n",
            "Iteration 95, loss = 0.71890254\n",
            "Iteration 96, loss = 0.71953178\n",
            "Iteration 97, loss = 0.75510911\n",
            "Iteration 98, loss = 0.74230807\n",
            "Iteration 99, loss = 0.71075081\n",
            "Iteration 100, loss = 0.74050738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35640834\n",
            "Iteration 2, loss = 1.28549757\n",
            "Iteration 3, loss = 1.25528135\n",
            "Iteration 4, loss = 1.20858725\n",
            "Iteration 5, loss = 1.20384861\n",
            "Iteration 6, loss = 1.18688506\n",
            "Iteration 7, loss = 1.17769097\n",
            "Iteration 8, loss = 1.17243810\n",
            "Iteration 9, loss = 1.17435909\n",
            "Iteration 10, loss = 1.16130169\n",
            "Iteration 11, loss = 1.15494240\n",
            "Iteration 12, loss = 1.14469358\n",
            "Iteration 13, loss = 1.12664388\n",
            "Iteration 14, loss = 1.11549615\n",
            "Iteration 15, loss = 1.12099823\n",
            "Iteration 16, loss = 1.10648886\n",
            "Iteration 17, loss = 1.09943677\n",
            "Iteration 18, loss = 1.09128517\n",
            "Iteration 19, loss = 1.08144967\n",
            "Iteration 20, loss = 1.08018733\n",
            "Iteration 21, loss = 1.07542686\n",
            "Iteration 22, loss = 1.06017410\n",
            "Iteration 23, loss = 1.04392779\n",
            "Iteration 24, loss = 1.04336469\n",
            "Iteration 25, loss = 1.04322686\n",
            "Iteration 26, loss = 1.03327829\n",
            "Iteration 27, loss = 1.02012554\n",
            "Iteration 28, loss = 1.01122783\n",
            "Iteration 29, loss = 1.00874998\n",
            "Iteration 30, loss = 1.02424748\n",
            "Iteration 31, loss = 1.04182673\n",
            "Iteration 32, loss = 1.01255891\n",
            "Iteration 33, loss = 0.98977925\n",
            "Iteration 34, loss = 0.99296185\n",
            "Iteration 35, loss = 0.98705117\n",
            "Iteration 36, loss = 1.00243758\n",
            "Iteration 37, loss = 1.04541550\n",
            "Iteration 38, loss = 1.00177852\n",
            "Iteration 39, loss = 1.00410686\n",
            "Iteration 40, loss = 0.97273812\n",
            "Iteration 41, loss = 0.94097721\n",
            "Iteration 42, loss = 0.94965210\n",
            "Iteration 43, loss = 0.97672450\n",
            "Iteration 44, loss = 1.03438102\n",
            "Iteration 45, loss = 1.00828218\n",
            "Iteration 46, loss = 0.96864569\n",
            "Iteration 47, loss = 0.96247048\n",
            "Iteration 48, loss = 0.93394713\n",
            "Iteration 49, loss = 0.93574208\n",
            "Iteration 50, loss = 0.93575611\n",
            "Iteration 51, loss = 0.94524817\n",
            "Iteration 52, loss = 0.93513731\n",
            "Iteration 53, loss = 0.91111101\n",
            "Iteration 54, loss = 0.90224748\n",
            "Iteration 55, loss = 0.89232893\n",
            "Iteration 56, loss = 0.88738681\n",
            "Iteration 57, loss = 0.87269834\n",
            "Iteration 58, loss = 0.86716130\n",
            "Iteration 59, loss = 0.86862276\n",
            "Iteration 60, loss = 0.86563722\n",
            "Iteration 61, loss = 0.86167214\n",
            "Iteration 62, loss = 0.86413188\n",
            "Iteration 63, loss = 0.85999427\n",
            "Iteration 64, loss = 0.87765786\n",
            "Iteration 65, loss = 0.84235734\n",
            "Iteration 66, loss = 0.84699835\n",
            "Iteration 67, loss = 0.83527776\n",
            "Iteration 68, loss = 0.84282359\n",
            "Iteration 69, loss = 0.83028756\n",
            "Iteration 70, loss = 0.80858626\n",
            "Iteration 71, loss = 0.81930521\n",
            "Iteration 72, loss = 0.82177405\n",
            "Iteration 73, loss = 0.81282921\n",
            "Iteration 74, loss = 0.84078825\n",
            "Iteration 75, loss = 0.82593307\n",
            "Iteration 76, loss = 0.81446211\n",
            "Iteration 77, loss = 0.79261965\n",
            "Iteration 78, loss = 0.79160088\n",
            "Iteration 79, loss = 0.76810322\n",
            "Iteration 80, loss = 0.78376685\n",
            "Iteration 81, loss = 0.82024628\n",
            "Iteration 82, loss = 0.79631557\n",
            "Iteration 83, loss = 0.77479976\n",
            "Iteration 84, loss = 0.79049696\n",
            "Iteration 85, loss = 0.79057836\n",
            "Iteration 86, loss = 0.77322209\n",
            "Iteration 87, loss = 0.75252039\n",
            "Iteration 88, loss = 0.74340130\n",
            "Iteration 89, loss = 0.74348545\n",
            "Iteration 90, loss = 0.72743093\n",
            "Iteration 91, loss = 0.72396824\n",
            "Iteration 92, loss = 0.73105286\n",
            "Iteration 93, loss = 0.74016025\n",
            "Iteration 94, loss = 0.78625772\n",
            "Iteration 95, loss = 0.77627808\n",
            "Iteration 96, loss = 0.75362213\n",
            "Iteration 97, loss = 0.74223237\n",
            "Iteration 98, loss = 0.72716434\n",
            "Iteration 99, loss = 0.72427061\n",
            "Iteration 100, loss = 0.72187072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36102415\n",
            "Iteration 2, loss = 1.27807465\n",
            "Iteration 3, loss = 1.22060501\n",
            "Iteration 4, loss = 1.19270476\n",
            "Iteration 5, loss = 1.17483545\n",
            "Iteration 6, loss = 1.16674670\n",
            "Iteration 7, loss = 1.15516733\n",
            "Iteration 8, loss = 1.15365604\n",
            "Iteration 9, loss = 1.15017487\n",
            "Iteration 10, loss = 1.13636763\n",
            "Iteration 11, loss = 1.12263675\n",
            "Iteration 12, loss = 1.11154119\n",
            "Iteration 13, loss = 1.09566588\n",
            "Iteration 14, loss = 1.08181683\n",
            "Iteration 15, loss = 1.08283784\n",
            "Iteration 16, loss = 1.07739362\n",
            "Iteration 17, loss = 1.07050089\n",
            "Iteration 18, loss = 1.05428261\n",
            "Iteration 19, loss = 1.04485261\n",
            "Iteration 20, loss = 1.05730300\n",
            "Iteration 21, loss = 1.06456386\n",
            "Iteration 22, loss = 1.04060391\n",
            "Iteration 23, loss = 1.01098657\n",
            "Iteration 24, loss = 1.00997113\n",
            "Iteration 25, loss = 1.00624361\n",
            "Iteration 26, loss = 0.99856978\n",
            "Iteration 27, loss = 0.99835118\n",
            "Iteration 28, loss = 1.00845004\n",
            "Iteration 29, loss = 0.97765854\n",
            "Iteration 30, loss = 0.98661550\n",
            "Iteration 31, loss = 0.99136849\n",
            "Iteration 32, loss = 0.97563330\n",
            "Iteration 33, loss = 0.98983964\n",
            "Iteration 34, loss = 0.98644868\n",
            "Iteration 35, loss = 0.96606890\n",
            "Iteration 36, loss = 0.96600545\n",
            "Iteration 37, loss = 0.99255819\n",
            "Iteration 38, loss = 0.95628941\n",
            "Iteration 39, loss = 0.94038257\n",
            "Iteration 40, loss = 0.92301034\n",
            "Iteration 41, loss = 0.91340910\n",
            "Iteration 42, loss = 0.91004079\n",
            "Iteration 43, loss = 0.90872954\n",
            "Iteration 44, loss = 0.94622359\n",
            "Iteration 45, loss = 0.93386818\n",
            "Iteration 46, loss = 0.94135413\n",
            "Iteration 47, loss = 0.91204982\n",
            "Iteration 48, loss = 0.91966264\n",
            "Iteration 49, loss = 0.90408011\n",
            "Iteration 50, loss = 0.89436725\n",
            "Iteration 51, loss = 0.89665323\n",
            "Iteration 52, loss = 0.90904724\n",
            "Iteration 53, loss = 0.88525964\n",
            "Iteration 54, loss = 0.86682857\n",
            "Iteration 55, loss = 0.87750029\n",
            "Iteration 56, loss = 0.84900797\n",
            "Iteration 57, loss = 0.84978165\n",
            "Iteration 58, loss = 0.83975389\n",
            "Iteration 59, loss = 0.82635815\n",
            "Iteration 60, loss = 0.81903267\n",
            "Iteration 61, loss = 0.82394847\n",
            "Iteration 62, loss = 0.84497595\n",
            "Iteration 63, loss = 0.82381010\n",
            "Iteration 64, loss = 0.80511668\n",
            "Iteration 65, loss = 0.81950375\n",
            "Iteration 66, loss = 0.83668235\n",
            "Iteration 67, loss = 0.82099502\n",
            "Iteration 68, loss = 0.81632044\n",
            "Iteration 69, loss = 0.78220365\n",
            "Iteration 70, loss = 0.77245584\n",
            "Iteration 71, loss = 0.77046224\n",
            "Iteration 72, loss = 0.80088017\n",
            "Iteration 73, loss = 0.81874727\n",
            "Iteration 74, loss = 0.79988607\n",
            "Iteration 75, loss = 0.78283709\n",
            "Iteration 76, loss = 0.79554231\n",
            "Iteration 77, loss = 0.79189065\n",
            "Iteration 78, loss = 0.75375776\n",
            "Iteration 79, loss = 0.75977617\n",
            "Iteration 80, loss = 0.77628703\n",
            "Iteration 81, loss = 0.79347151\n",
            "Iteration 82, loss = 0.80071664\n",
            "Iteration 83, loss = 0.76532245\n",
            "Iteration 84, loss = 0.79943169\n",
            "Iteration 85, loss = 0.75673119\n",
            "Iteration 86, loss = 0.74110297\n",
            "Iteration 87, loss = 0.72374582\n",
            "Iteration 88, loss = 0.71464330\n",
            "Iteration 89, loss = 0.70804906\n",
            "Iteration 90, loss = 0.70467240\n",
            "Iteration 91, loss = 0.69221461\n",
            "Iteration 92, loss = 0.71023783\n",
            "Iteration 93, loss = 0.70409589\n",
            "Iteration 94, loss = 0.73069913\n",
            "Iteration 95, loss = 0.73039469\n",
            "Iteration 96, loss = 0.70446588\n",
            "Iteration 97, loss = 0.69872656\n",
            "Iteration 98, loss = 0.68870673\n",
            "Iteration 99, loss = 0.67861193\n",
            "Iteration 100, loss = 0.67743517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37001329\n",
            "Iteration 2, loss = 1.29740484\n",
            "Iteration 3, loss = 1.24797748\n",
            "Iteration 4, loss = 1.21753565\n",
            "Iteration 5, loss = 1.20213320\n",
            "Iteration 6, loss = 1.19875561\n",
            "Iteration 7, loss = 1.18514833\n",
            "Iteration 8, loss = 1.18380699\n",
            "Iteration 9, loss = 1.17016184\n",
            "Iteration 10, loss = 1.16051899\n",
            "Iteration 11, loss = 1.15338982\n",
            "Iteration 12, loss = 1.15281236\n",
            "Iteration 13, loss = 1.15150708\n",
            "Iteration 14, loss = 1.13432243\n",
            "Iteration 15, loss = 1.12450361\n",
            "Iteration 16, loss = 1.12974007\n",
            "Iteration 17, loss = 1.11993956\n",
            "Iteration 18, loss = 1.11430260\n",
            "Iteration 19, loss = 1.11077075\n",
            "Iteration 20, loss = 1.10533981\n",
            "Iteration 21, loss = 1.08882341\n",
            "Iteration 22, loss = 1.07378580\n",
            "Iteration 23, loss = 1.05755090\n",
            "Iteration 24, loss = 1.05652298\n",
            "Iteration 25, loss = 1.05413896\n",
            "Iteration 26, loss = 1.06188664\n",
            "Iteration 27, loss = 1.06895901\n",
            "Iteration 28, loss = 1.05424320\n",
            "Iteration 29, loss = 1.05519575\n",
            "Iteration 30, loss = 1.06279813\n",
            "Iteration 31, loss = 1.04080614\n",
            "Iteration 32, loss = 1.02290012\n",
            "Iteration 33, loss = 1.02439573\n",
            "Iteration 34, loss = 1.00273094\n",
            "Iteration 35, loss = 1.00835899\n",
            "Iteration 36, loss = 1.01321691\n",
            "Iteration 37, loss = 0.99059869\n",
            "Iteration 38, loss = 0.98278016\n",
            "Iteration 39, loss = 0.97052501\n",
            "Iteration 40, loss = 0.97512361\n",
            "Iteration 41, loss = 0.96391210\n",
            "Iteration 42, loss = 0.97174706\n",
            "Iteration 43, loss = 0.99742380\n",
            "Iteration 44, loss = 0.99177711\n",
            "Iteration 45, loss = 0.97877949\n",
            "Iteration 46, loss = 0.97158061\n",
            "Iteration 47, loss = 0.96919305\n",
            "Iteration 48, loss = 1.00032753\n",
            "Iteration 49, loss = 0.96520439\n",
            "Iteration 50, loss = 0.94328530\n",
            "Iteration 51, loss = 0.91476104\n",
            "Iteration 52, loss = 0.92578948\n",
            "Iteration 53, loss = 0.90774803\n",
            "Iteration 54, loss = 0.90789351\n",
            "Iteration 55, loss = 0.89461367\n",
            "Iteration 56, loss = 0.90336679\n",
            "Iteration 57, loss = 0.88399224\n",
            "Iteration 58, loss = 0.87891778\n",
            "Iteration 59, loss = 0.88365748\n",
            "Iteration 60, loss = 0.85517432\n",
            "Iteration 61, loss = 0.86628262\n",
            "Iteration 62, loss = 0.89138441\n",
            "Iteration 63, loss = 0.86538034\n",
            "Iteration 64, loss = 0.89069432\n",
            "Iteration 65, loss = 0.91526229\n",
            "Iteration 66, loss = 0.90798634\n",
            "Iteration 67, loss = 0.87485121\n",
            "Iteration 68, loss = 0.86716000\n",
            "Iteration 69, loss = 0.85772201\n",
            "Iteration 70, loss = 0.83283455\n",
            "Iteration 71, loss = 0.82109938\n",
            "Iteration 72, loss = 0.81197547\n",
            "Iteration 73, loss = 0.82143479\n",
            "Iteration 74, loss = 0.83944263\n",
            "Iteration 75, loss = 0.82728275\n",
            "Iteration 76, loss = 0.85474805\n",
            "Iteration 77, loss = 0.83257073\n",
            "Iteration 78, loss = 0.80652644\n",
            "Iteration 79, loss = 0.79917117\n",
            "Iteration 80, loss = 0.81073966\n",
            "Iteration 81, loss = 0.80157468\n",
            "Iteration 82, loss = 0.80109798\n",
            "Iteration 83, loss = 0.78039900\n",
            "Iteration 84, loss = 0.82223837\n",
            "Iteration 85, loss = 0.78549104\n",
            "Iteration 86, loss = 0.78958603\n",
            "Iteration 87, loss = 0.78277374\n",
            "Iteration 88, loss = 0.75983908\n",
            "Iteration 89, loss = 0.73855371\n",
            "Iteration 90, loss = 0.74551356\n",
            "Iteration 91, loss = 0.73138705\n",
            "Iteration 92, loss = 0.74093572\n",
            "Iteration 93, loss = 0.74196986\n",
            "Iteration 94, loss = 0.74757458\n",
            "Iteration 95, loss = 0.74608192\n",
            "Iteration 96, loss = 0.75077685\n",
            "Iteration 97, loss = 0.71641522\n",
            "Iteration 98, loss = 0.71364938\n",
            "Iteration 99, loss = 0.71772390\n",
            "Iteration 100, loss = 0.71108105\n",
            "31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38326538\n",
            "Iteration 2, loss = 1.35888314\n",
            "Iteration 3, loss = 1.35878537\n",
            "Iteration 4, loss = 1.34102571\n",
            "Iteration 5, loss = 1.33516219\n",
            "Iteration 6, loss = 1.32099067\n",
            "Iteration 7, loss = 1.32146822\n",
            "Iteration 8, loss = 1.30918480\n",
            "Iteration 9, loss = 1.30381661\n",
            "Iteration 10, loss = 1.28664872\n",
            "Iteration 11, loss = 1.28243488\n",
            "Iteration 12, loss = 1.28474763\n",
            "Iteration 13, loss = 1.26065689\n",
            "Iteration 14, loss = 1.25268063\n",
            "Iteration 15, loss = 1.25249497\n",
            "Iteration 16, loss = 1.22671925\n",
            "Iteration 17, loss = 1.21710821\n",
            "Iteration 18, loss = 1.20881662\n",
            "Iteration 19, loss = 1.23138169\n",
            "Iteration 20, loss = 1.20475156\n",
            "Iteration 21, loss = 1.19782217\n",
            "Iteration 22, loss = 1.19004037\n",
            "Iteration 23, loss = 1.17497945\n",
            "Iteration 24, loss = 1.17237221\n",
            "Iteration 25, loss = 1.18206473\n",
            "Iteration 26, loss = 1.14520465\n",
            "Iteration 27, loss = 1.15101394\n",
            "Iteration 28, loss = 1.15637939\n",
            "Iteration 29, loss = 1.16458607\n",
            "Iteration 30, loss = 1.16207300\n",
            "Iteration 31, loss = 1.17477233\n",
            "Iteration 32, loss = 1.12617096\n",
            "Iteration 33, loss = 1.11968794\n",
            "Iteration 34, loss = 1.15856768\n",
            "Iteration 35, loss = 1.13735770\n",
            "Iteration 36, loss = 1.11418431\n",
            "Iteration 37, loss = 1.10739771\n",
            "Iteration 38, loss = 1.09576521\n",
            "Iteration 39, loss = 1.10992980\n",
            "Iteration 40, loss = 1.11003751\n",
            "Iteration 41, loss = 1.08737791\n",
            "Iteration 42, loss = 1.11508331\n",
            "Iteration 43, loss = 1.12769408\n",
            "Iteration 44, loss = 1.14102691\n",
            "Iteration 45, loss = 1.12939048\n",
            "Iteration 46, loss = 1.09902024\n",
            "Iteration 47, loss = 1.08658987\n",
            "Iteration 48, loss = 1.10403817\n",
            "Iteration 49, loss = 1.10262585\n",
            "Iteration 50, loss = 1.06937671\n",
            "Iteration 51, loss = 1.07076684\n",
            "Iteration 52, loss = 1.06016506\n",
            "Iteration 53, loss = 1.04464445\n",
            "Iteration 54, loss = 1.05838670\n",
            "Iteration 55, loss = 1.04385894\n",
            "Iteration 56, loss = 1.04836045\n",
            "Iteration 57, loss = 1.03543336\n",
            "Iteration 58, loss = 1.03954553\n",
            "Iteration 59, loss = 1.03125536\n",
            "Iteration 60, loss = 1.03277722\n",
            "Iteration 61, loss = 1.03381038\n",
            "Iteration 62, loss = 1.06622462\n",
            "Iteration 63, loss = 1.06634868\n",
            "Iteration 64, loss = 1.02552807\n",
            "Iteration 65, loss = 1.03011740\n",
            "Iteration 66, loss = 1.01379472\n",
            "Iteration 67, loss = 1.01111165\n",
            "Iteration 68, loss = 1.01434725\n",
            "Iteration 69, loss = 1.00763563\n",
            "Iteration 70, loss = 1.01279479\n",
            "Iteration 71, loss = 1.02281890\n",
            "Iteration 72, loss = 1.00280466\n",
            "Iteration 73, loss = 1.02474294\n",
            "Iteration 74, loss = 0.99422381\n",
            "Iteration 75, loss = 0.98708658\n",
            "Iteration 76, loss = 1.02413940\n",
            "Iteration 77, loss = 1.00065884\n",
            "Iteration 78, loss = 0.99352699\n",
            "Iteration 79, loss = 0.98177335\n",
            "Iteration 80, loss = 0.96052254\n",
            "Iteration 81, loss = 0.95766426\n",
            "Iteration 82, loss = 0.96533454\n",
            "Iteration 83, loss = 0.96853334\n",
            "Iteration 84, loss = 1.00080444\n",
            "Iteration 85, loss = 0.96607826\n",
            "Iteration 86, loss = 0.96696038\n",
            "Iteration 87, loss = 0.98018165\n",
            "Iteration 88, loss = 0.98725789\n",
            "Iteration 89, loss = 0.97721544\n",
            "Iteration 90, loss = 0.97300408\n",
            "Iteration 91, loss = 0.95775306\n",
            "Iteration 92, loss = 0.96120138\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38373773\n",
            "Iteration 2, loss = 1.36679375\n",
            "Iteration 3, loss = 1.35623203\n",
            "Iteration 4, loss = 1.34432851\n",
            "Iteration 5, loss = 1.34062495\n",
            "Iteration 6, loss = 1.32907281\n",
            "Iteration 7, loss = 1.33017427\n",
            "Iteration 8, loss = 1.32340284\n",
            "Iteration 9, loss = 1.30917979\n",
            "Iteration 10, loss = 1.30470322\n",
            "Iteration 11, loss = 1.29643350\n",
            "Iteration 12, loss = 1.28259216\n",
            "Iteration 13, loss = 1.27424071\n",
            "Iteration 14, loss = 1.26673816\n",
            "Iteration 15, loss = 1.28624096\n",
            "Iteration 16, loss = 1.25810162\n",
            "Iteration 17, loss = 1.25175768\n",
            "Iteration 18, loss = 1.22164009\n",
            "Iteration 19, loss = 1.23550462\n",
            "Iteration 20, loss = 1.22296366\n",
            "Iteration 21, loss = 1.20442702\n",
            "Iteration 22, loss = 1.19780351\n",
            "Iteration 23, loss = 1.18202942\n",
            "Iteration 24, loss = 1.16854739\n",
            "Iteration 25, loss = 1.16207381\n",
            "Iteration 26, loss = 1.14223393\n",
            "Iteration 27, loss = 1.13953306\n",
            "Iteration 28, loss = 1.13500676\n",
            "Iteration 29, loss = 1.12253810\n",
            "Iteration 30, loss = 1.12554224\n",
            "Iteration 31, loss = 1.16727740\n",
            "Iteration 32, loss = 1.14099560\n",
            "Iteration 33, loss = 1.14914888\n",
            "Iteration 34, loss = 1.17012579\n",
            "Iteration 35, loss = 1.12867563\n",
            "Iteration 36, loss = 1.13079807\n",
            "Iteration 37, loss = 1.10544986\n",
            "Iteration 38, loss = 1.11342430\n",
            "Iteration 39, loss = 1.11599940\n",
            "Iteration 40, loss = 1.08731387\n",
            "Iteration 41, loss = 1.07408092\n",
            "Iteration 42, loss = 1.08698042\n",
            "Iteration 43, loss = 1.10457192\n",
            "Iteration 44, loss = 1.11832231\n",
            "Iteration 45, loss = 1.10264111\n",
            "Iteration 46, loss = 1.07789732\n",
            "Iteration 47, loss = 1.06130734\n",
            "Iteration 48, loss = 1.06032828\n",
            "Iteration 49, loss = 1.06222623\n",
            "Iteration 50, loss = 1.07692895\n",
            "Iteration 51, loss = 1.05996153\n",
            "Iteration 52, loss = 1.05709853\n",
            "Iteration 53, loss = 1.05354299\n",
            "Iteration 54, loss = 1.04079001\n",
            "Iteration 55, loss = 1.03561903\n",
            "Iteration 56, loss = 1.05147389\n",
            "Iteration 57, loss = 1.04688242\n",
            "Iteration 58, loss = 1.05877394\n",
            "Iteration 59, loss = 1.04623877\n",
            "Iteration 60, loss = 1.02790565\n",
            "Iteration 61, loss = 1.03109707\n",
            "Iteration 62, loss = 1.05305127\n",
            "Iteration 63, loss = 1.04663456\n",
            "Iteration 64, loss = 1.00687888\n",
            "Iteration 65, loss = 1.00111772\n",
            "Iteration 66, loss = 0.99539925\n",
            "Iteration 67, loss = 1.01468531\n",
            "Iteration 68, loss = 1.04889837\n",
            "Iteration 69, loss = 1.01966810\n",
            "Iteration 70, loss = 0.99375770\n",
            "Iteration 71, loss = 1.00041106\n",
            "Iteration 72, loss = 0.99683400\n",
            "Iteration 73, loss = 0.99073583\n",
            "Iteration 74, loss = 0.97633359\n",
            "Iteration 75, loss = 0.97692418\n",
            "Iteration 76, loss = 0.99623827\n",
            "Iteration 77, loss = 0.97357507\n",
            "Iteration 78, loss = 0.96453647\n",
            "Iteration 79, loss = 0.98134752\n",
            "Iteration 80, loss = 0.98341051\n",
            "Iteration 81, loss = 0.99058580\n",
            "Iteration 82, loss = 0.97516602\n",
            "Iteration 83, loss = 0.94759632\n",
            "Iteration 84, loss = 1.00647850\n",
            "Iteration 85, loss = 0.94885551\n",
            "Iteration 86, loss = 0.95910641\n",
            "Iteration 87, loss = 0.94378099\n",
            "Iteration 88, loss = 0.93619460\n",
            "Iteration 89, loss = 0.94576393\n",
            "Iteration 90, loss = 0.92749274\n",
            "Iteration 91, loss = 0.91099137\n",
            "Iteration 92, loss = 0.92930885\n",
            "Iteration 93, loss = 0.92689750\n",
            "Iteration 94, loss = 0.91568886\n",
            "Iteration 95, loss = 0.92934972\n",
            "Iteration 96, loss = 0.92490670\n",
            "Iteration 97, loss = 0.91998795\n",
            "Iteration 98, loss = 0.91438696\n",
            "Iteration 99, loss = 0.91660498\n",
            "Iteration 100, loss = 0.91758985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39014843\n",
            "Iteration 2, loss = 1.37691880\n",
            "Iteration 3, loss = 1.36802295\n",
            "Iteration 4, loss = 1.35561295\n",
            "Iteration 5, loss = 1.34816691\n",
            "Iteration 6, loss = 1.33583054\n",
            "Iteration 7, loss = 1.33678369\n",
            "Iteration 8, loss = 1.33351921\n",
            "Iteration 9, loss = 1.32281907\n",
            "Iteration 10, loss = 1.30829712\n",
            "Iteration 11, loss = 1.30498070\n",
            "Iteration 12, loss = 1.28366507\n",
            "Iteration 13, loss = 1.28254262\n",
            "Iteration 14, loss = 1.26215338\n",
            "Iteration 15, loss = 1.27804830\n",
            "Iteration 16, loss = 1.25740267\n",
            "Iteration 17, loss = 1.24143872\n",
            "Iteration 18, loss = 1.22005074\n",
            "Iteration 19, loss = 1.23934627\n",
            "Iteration 20, loss = 1.22876223\n",
            "Iteration 21, loss = 1.20907767\n",
            "Iteration 22, loss = 1.19665791\n",
            "Iteration 23, loss = 1.17984842\n",
            "Iteration 24, loss = 1.16863686\n",
            "Iteration 25, loss = 1.16710662\n",
            "Iteration 26, loss = 1.15458540\n",
            "Iteration 27, loss = 1.15294763\n",
            "Iteration 28, loss = 1.15955708\n",
            "Iteration 29, loss = 1.15643488\n",
            "Iteration 30, loss = 1.13869158\n",
            "Iteration 31, loss = 1.13762465\n",
            "Iteration 32, loss = 1.12758262\n",
            "Iteration 33, loss = 1.12568885\n",
            "Iteration 34, loss = 1.13172702\n",
            "Iteration 35, loss = 1.13171466\n",
            "Iteration 36, loss = 1.12328867\n",
            "Iteration 37, loss = 1.10115157\n",
            "Iteration 38, loss = 1.10835776\n",
            "Iteration 39, loss = 1.12399276\n",
            "Iteration 40, loss = 1.14592453\n",
            "Iteration 41, loss = 1.10672411\n",
            "Iteration 42, loss = 1.10588732\n",
            "Iteration 43, loss = 1.10700441\n",
            "Iteration 44, loss = 1.12985401\n",
            "Iteration 45, loss = 1.10699984\n",
            "Iteration 46, loss = 1.07284242\n",
            "Iteration 47, loss = 1.07054106\n",
            "Iteration 48, loss = 1.05660618\n",
            "Iteration 49, loss = 1.07066563\n",
            "Iteration 50, loss = 1.08102518\n",
            "Iteration 51, loss = 1.07883539\n",
            "Iteration 52, loss = 1.06468435\n",
            "Iteration 53, loss = 1.05873105\n",
            "Iteration 54, loss = 1.05686681\n",
            "Iteration 55, loss = 1.03728620\n",
            "Iteration 56, loss = 1.03428324\n",
            "Iteration 57, loss = 1.04275172\n",
            "Iteration 58, loss = 1.06936698\n",
            "Iteration 59, loss = 1.04176895\n",
            "Iteration 60, loss = 1.02206199\n",
            "Iteration 61, loss = 1.03707959\n",
            "Iteration 62, loss = 1.05633548\n",
            "Iteration 63, loss = 1.01579095\n",
            "Iteration 64, loss = 1.00820262\n",
            "Iteration 65, loss = 1.01113668\n",
            "Iteration 66, loss = 0.99524075\n",
            "Iteration 67, loss = 0.99630718\n",
            "Iteration 68, loss = 1.01151332\n",
            "Iteration 69, loss = 1.02856501\n",
            "Iteration 70, loss = 1.02330900\n",
            "Iteration 71, loss = 1.00570649\n",
            "Iteration 72, loss = 0.98279129\n",
            "Iteration 73, loss = 0.98093819\n",
            "Iteration 74, loss = 0.98324845\n",
            "Iteration 75, loss = 0.99761193\n",
            "Iteration 76, loss = 1.03772948\n",
            "Iteration 77, loss = 0.98902560\n",
            "Iteration 78, loss = 0.99059127\n",
            "Iteration 79, loss = 0.99593770\n",
            "Iteration 80, loss = 0.97863587\n",
            "Iteration 81, loss = 0.96680602\n",
            "Iteration 82, loss = 0.97140320\n",
            "Iteration 83, loss = 0.95010548\n",
            "Iteration 84, loss = 1.00036337\n",
            "Iteration 85, loss = 0.96180275\n",
            "Iteration 86, loss = 0.96105665\n",
            "Iteration 87, loss = 0.94699213\n",
            "Iteration 88, loss = 0.95434321\n",
            "Iteration 89, loss = 0.98045963\n",
            "Iteration 90, loss = 0.97098088\n",
            "Iteration 91, loss = 0.95865699\n",
            "Iteration 92, loss = 0.94161030\n",
            "Iteration 93, loss = 0.94507367\n",
            "Iteration 94, loss = 0.92683924\n",
            "Iteration 95, loss = 0.96126591\n",
            "Iteration 96, loss = 0.91352319\n",
            "Iteration 97, loss = 0.90300388\n",
            "Iteration 98, loss = 0.91429522\n",
            "Iteration 99, loss = 0.92160894\n",
            "Iteration 100, loss = 0.95011762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38692212\n",
            "Iteration 2, loss = 1.36909938\n",
            "Iteration 3, loss = 1.35780675\n",
            "Iteration 4, loss = 1.34584889\n",
            "Iteration 5, loss = 1.33928999\n",
            "Iteration 6, loss = 1.33477348\n",
            "Iteration 7, loss = 1.33113101\n",
            "Iteration 8, loss = 1.32676932\n",
            "Iteration 9, loss = 1.32691442\n",
            "Iteration 10, loss = 1.31183927\n",
            "Iteration 11, loss = 1.30590873\n",
            "Iteration 12, loss = 1.28052110\n",
            "Iteration 13, loss = 1.28503929\n",
            "Iteration 14, loss = 1.26502017\n",
            "Iteration 15, loss = 1.26506478\n",
            "Iteration 16, loss = 1.24902174\n",
            "Iteration 17, loss = 1.23910261\n",
            "Iteration 18, loss = 1.22597425\n",
            "Iteration 19, loss = 1.23527420\n",
            "Iteration 20, loss = 1.23495337\n",
            "Iteration 21, loss = 1.20473282\n",
            "Iteration 22, loss = 1.18661777\n",
            "Iteration 23, loss = 1.18031354\n",
            "Iteration 24, loss = 1.18329960\n",
            "Iteration 25, loss = 1.17909737\n",
            "Iteration 26, loss = 1.15721905\n",
            "Iteration 27, loss = 1.15295275\n",
            "Iteration 28, loss = 1.17059724\n",
            "Iteration 29, loss = 1.16320810\n",
            "Iteration 30, loss = 1.14551662\n",
            "Iteration 31, loss = 1.13291324\n",
            "Iteration 32, loss = 1.13749597\n",
            "Iteration 33, loss = 1.13274283\n",
            "Iteration 34, loss = 1.13396814\n",
            "Iteration 35, loss = 1.12567698\n",
            "Iteration 36, loss = 1.12106938\n",
            "Iteration 37, loss = 1.10921658\n",
            "Iteration 38, loss = 1.12361136\n",
            "Iteration 39, loss = 1.13087083\n",
            "Iteration 40, loss = 1.14457605\n",
            "Iteration 41, loss = 1.12125938\n",
            "Iteration 42, loss = 1.10800540\n",
            "Iteration 43, loss = 1.10024192\n",
            "Iteration 44, loss = 1.12092072\n",
            "Iteration 45, loss = 1.10437215\n",
            "Iteration 46, loss = 1.08079222\n",
            "Iteration 47, loss = 1.08883999\n",
            "Iteration 48, loss = 1.08773253\n",
            "Iteration 49, loss = 1.09183148\n",
            "Iteration 50, loss = 1.10055602\n",
            "Iteration 51, loss = 1.07905098\n",
            "Iteration 52, loss = 1.06581313\n",
            "Iteration 53, loss = 1.05387751\n",
            "Iteration 54, loss = 1.05307961\n",
            "Iteration 55, loss = 1.04539753\n",
            "Iteration 56, loss = 1.03680250\n",
            "Iteration 57, loss = 1.04697136\n",
            "Iteration 58, loss = 1.06837444\n",
            "Iteration 59, loss = 1.05941968\n",
            "Iteration 60, loss = 1.04548914\n",
            "Iteration 61, loss = 1.05871798\n",
            "Iteration 62, loss = 1.08313761\n",
            "Iteration 63, loss = 1.04449706\n",
            "Iteration 64, loss = 1.02766142\n",
            "Iteration 65, loss = 1.01011623\n",
            "Iteration 66, loss = 1.01154863\n",
            "Iteration 67, loss = 1.01812817\n",
            "Iteration 68, loss = 1.02848668\n",
            "Iteration 69, loss = 1.05254093\n",
            "Iteration 70, loss = 1.02718771\n",
            "Iteration 71, loss = 1.02862088\n",
            "Iteration 72, loss = 1.01998898\n",
            "Iteration 73, loss = 0.99653372\n",
            "Iteration 74, loss = 0.99654625\n",
            "Iteration 75, loss = 1.00646530\n",
            "Iteration 76, loss = 1.05708569\n",
            "Iteration 77, loss = 1.00375062\n",
            "Iteration 78, loss = 0.99090043\n",
            "Iteration 79, loss = 0.98762488\n",
            "Iteration 80, loss = 0.98520400\n",
            "Iteration 81, loss = 0.98908425\n",
            "Iteration 82, loss = 0.98574790\n",
            "Iteration 83, loss = 0.98101414\n",
            "Iteration 84, loss = 1.03333119\n",
            "Iteration 85, loss = 0.97325503\n",
            "Iteration 86, loss = 0.97602848\n",
            "Iteration 87, loss = 0.95427988\n",
            "Iteration 88, loss = 0.95223746\n",
            "Iteration 89, loss = 0.96527298\n",
            "Iteration 90, loss = 0.96210971\n",
            "Iteration 91, loss = 0.93386870\n",
            "Iteration 92, loss = 0.95576022\n",
            "Iteration 93, loss = 0.94567607\n",
            "Iteration 94, loss = 0.94038502\n",
            "Iteration 95, loss = 0.96359169\n",
            "Iteration 96, loss = 0.95892054\n",
            "Iteration 97, loss = 0.95807019\n",
            "Iteration 98, loss = 0.92533917\n",
            "Iteration 99, loss = 0.91317663\n",
            "Iteration 100, loss = 0.93403744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39131145\n",
            "Iteration 2, loss = 1.37386921\n",
            "Iteration 3, loss = 1.36508959\n",
            "Iteration 4, loss = 1.35230078\n",
            "Iteration 5, loss = 1.34077418\n",
            "Iteration 6, loss = 1.33330074\n",
            "Iteration 7, loss = 1.33131702\n",
            "Iteration 8, loss = 1.32477202\n",
            "Iteration 9, loss = 1.34352488\n",
            "Iteration 10, loss = 1.31481683\n",
            "Iteration 11, loss = 1.30749215\n",
            "Iteration 12, loss = 1.28826477\n",
            "Iteration 13, loss = 1.28411145\n",
            "Iteration 14, loss = 1.27519996\n",
            "Iteration 15, loss = 1.27471880\n",
            "Iteration 16, loss = 1.25495649\n",
            "Iteration 17, loss = 1.25011655\n",
            "Iteration 18, loss = 1.22759502\n",
            "Iteration 19, loss = 1.25581980\n",
            "Iteration 20, loss = 1.24179990\n",
            "Iteration 21, loss = 1.21154347\n",
            "Iteration 22, loss = 1.19738246\n",
            "Iteration 23, loss = 1.18852218\n",
            "Iteration 24, loss = 1.17772601\n",
            "Iteration 25, loss = 1.17255404\n",
            "Iteration 26, loss = 1.15845834\n",
            "Iteration 27, loss = 1.16152611\n",
            "Iteration 28, loss = 1.17084157\n",
            "Iteration 29, loss = 1.17311006\n",
            "Iteration 30, loss = 1.16705960\n",
            "Iteration 31, loss = 1.16439734\n",
            "Iteration 32, loss = 1.15172550\n",
            "Iteration 33, loss = 1.12583773\n",
            "Iteration 34, loss = 1.16094800\n",
            "Iteration 35, loss = 1.14040982\n",
            "Iteration 36, loss = 1.12711370\n",
            "Iteration 37, loss = 1.11669884\n",
            "Iteration 38, loss = 1.11087581\n",
            "Iteration 39, loss = 1.10322868\n",
            "Iteration 40, loss = 1.12098758\n",
            "Iteration 41, loss = 1.11279452\n",
            "Iteration 42, loss = 1.09798677\n",
            "Iteration 43, loss = 1.10591842\n",
            "Iteration 44, loss = 1.10585816\n",
            "Iteration 45, loss = 1.10290895\n",
            "Iteration 46, loss = 1.08984749\n",
            "Iteration 47, loss = 1.08922808\n",
            "Iteration 48, loss = 1.08866533\n",
            "Iteration 49, loss = 1.06644830\n",
            "Iteration 50, loss = 1.05526951\n",
            "Iteration 51, loss = 1.06896761\n",
            "Iteration 52, loss = 1.04886478\n",
            "Iteration 53, loss = 1.04488968\n",
            "Iteration 54, loss = 1.05499866\n",
            "Iteration 55, loss = 1.04340949\n",
            "Iteration 56, loss = 1.04419956\n",
            "Iteration 57, loss = 1.04322526\n",
            "Iteration 58, loss = 1.04754576\n",
            "Iteration 59, loss = 1.03786444\n",
            "Iteration 60, loss = 1.04607542\n",
            "Iteration 61, loss = 1.01603368\n",
            "Iteration 62, loss = 1.04945643\n",
            "Iteration 63, loss = 1.11941136\n",
            "Iteration 64, loss = 1.06182724\n",
            "Iteration 65, loss = 1.00733329\n",
            "Iteration 66, loss = 1.00089804\n",
            "Iteration 67, loss = 1.00143643\n",
            "Iteration 68, loss = 1.02048196\n",
            "Iteration 69, loss = 1.05149864\n",
            "Iteration 70, loss = 1.00832115\n",
            "Iteration 71, loss = 0.99691921\n",
            "Iteration 72, loss = 0.98914098\n",
            "Iteration 73, loss = 0.97542337\n",
            "Iteration 74, loss = 0.98320801\n",
            "Iteration 75, loss = 0.98658598\n",
            "Iteration 76, loss = 1.04330804\n",
            "Iteration 77, loss = 1.00019363\n",
            "Iteration 78, loss = 0.98031697\n",
            "Iteration 79, loss = 0.95859043\n",
            "Iteration 80, loss = 0.95463386\n",
            "Iteration 81, loss = 0.96609980\n",
            "Iteration 82, loss = 0.94558997\n",
            "Iteration 83, loss = 0.95048411\n",
            "Iteration 84, loss = 0.98438009\n",
            "Iteration 85, loss = 1.00899875\n",
            "Iteration 86, loss = 0.98585397\n",
            "Iteration 87, loss = 0.96089675\n",
            "Iteration 88, loss = 0.95381360\n",
            "Iteration 89, loss = 0.95168613\n",
            "Iteration 90, loss = 0.93044946\n",
            "Iteration 91, loss = 0.91287869\n",
            "Iteration 92, loss = 0.93486397\n",
            "Iteration 93, loss = 0.94405929\n",
            "Iteration 94, loss = 0.94310208\n",
            "Iteration 95, loss = 0.96574841\n",
            "Iteration 96, loss = 0.95590458\n",
            "Iteration 97, loss = 0.91903052\n",
            "Iteration 98, loss = 0.90315622\n",
            "Iteration 99, loss = 0.89274796\n",
            "Iteration 100, loss = 0.92319051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38380805\n",
            "Iteration 2, loss = 1.37094717\n",
            "Iteration 3, loss = 1.36509478\n",
            "Iteration 4, loss = 1.34917403\n",
            "Iteration 5, loss = 1.34621497\n",
            "Iteration 6, loss = 1.33817076\n",
            "Iteration 7, loss = 1.33651189\n",
            "Iteration 8, loss = 1.33256645\n",
            "Iteration 9, loss = 1.34029409\n",
            "Iteration 10, loss = 1.32374798\n",
            "Iteration 11, loss = 1.31855065\n",
            "Iteration 12, loss = 1.30984606\n",
            "Iteration 13, loss = 1.30692326\n",
            "Iteration 14, loss = 1.30398904\n",
            "Iteration 15, loss = 1.29306154\n",
            "Iteration 16, loss = 1.28381206\n",
            "Iteration 17, loss = 1.27866141\n",
            "Iteration 18, loss = 1.26083253\n",
            "Iteration 19, loss = 1.25699923\n",
            "Iteration 20, loss = 1.25944453\n",
            "Iteration 21, loss = 1.25353271\n",
            "Iteration 22, loss = 1.24090509\n",
            "Iteration 23, loss = 1.22509828\n",
            "Iteration 24, loss = 1.21790241\n",
            "Iteration 25, loss = 1.20304483\n",
            "Iteration 26, loss = 1.18741035\n",
            "Iteration 27, loss = 1.19708039\n",
            "Iteration 28, loss = 1.20801000\n",
            "Iteration 29, loss = 1.19914305\n",
            "Iteration 30, loss = 1.19941026\n",
            "Iteration 31, loss = 1.18939350\n",
            "Iteration 32, loss = 1.15751941\n",
            "Iteration 33, loss = 1.15979779\n",
            "Iteration 34, loss = 1.20130415\n",
            "Iteration 35, loss = 1.19050297\n",
            "Iteration 36, loss = 1.15751245\n",
            "Iteration 37, loss = 1.13834390\n",
            "Iteration 38, loss = 1.14437467\n",
            "Iteration 39, loss = 1.13701607\n",
            "Iteration 40, loss = 1.12403967\n",
            "Iteration 41, loss = 1.12307658\n",
            "Iteration 42, loss = 1.11458540\n",
            "Iteration 43, loss = 1.13762143\n",
            "Iteration 44, loss = 1.17644115\n",
            "Iteration 45, loss = 1.15174682\n",
            "Iteration 46, loss = 1.11511947\n",
            "Iteration 47, loss = 1.11016580\n",
            "Iteration 48, loss = 1.10220358\n",
            "Iteration 49, loss = 1.10730782\n",
            "Iteration 50, loss = 1.11383318\n",
            "Iteration 51, loss = 1.09507685\n",
            "Iteration 52, loss = 1.08784890\n",
            "Iteration 53, loss = 1.07542773\n",
            "Iteration 54, loss = 1.08107108\n",
            "Iteration 55, loss = 1.07814400\n",
            "Iteration 56, loss = 1.06363870\n",
            "Iteration 57, loss = 1.07769434\n",
            "Iteration 58, loss = 1.10892551\n",
            "Iteration 59, loss = 1.07309980\n",
            "Iteration 60, loss = 1.05889239\n",
            "Iteration 61, loss = 1.05044722\n",
            "Iteration 62, loss = 1.05803000\n",
            "Iteration 63, loss = 1.05814041\n",
            "Iteration 64, loss = 1.05139931\n",
            "Iteration 65, loss = 1.04929023\n",
            "Iteration 66, loss = 1.05386734\n",
            "Iteration 67, loss = 1.05464709\n",
            "Iteration 68, loss = 1.09518738\n",
            "Iteration 69, loss = 1.06422311\n",
            "Iteration 70, loss = 1.02571466\n",
            "Iteration 71, loss = 1.02687321\n",
            "Iteration 72, loss = 1.02596686\n",
            "Iteration 73, loss = 1.01766794\n",
            "Iteration 74, loss = 1.01321046\n",
            "Iteration 75, loss = 1.05417822\n",
            "Iteration 76, loss = 1.08319007\n",
            "Iteration 77, loss = 1.02108114\n",
            "Iteration 78, loss = 1.00869609\n",
            "Iteration 79, loss = 1.00900285\n",
            "Iteration 80, loss = 0.99559870\n",
            "Iteration 81, loss = 0.99773320\n",
            "Iteration 82, loss = 1.00497891\n",
            "Iteration 83, loss = 1.00114421\n",
            "Iteration 84, loss = 1.00112468\n",
            "Iteration 85, loss = 1.00344749\n",
            "Iteration 86, loss = 0.99013941\n",
            "Iteration 87, loss = 0.98607625\n",
            "Iteration 88, loss = 0.97584553\n",
            "Iteration 89, loss = 0.98331305\n",
            "Iteration 90, loss = 0.97704244\n",
            "Iteration 91, loss = 0.95545001\n",
            "Iteration 92, loss = 0.96848151\n",
            "Iteration 93, loss = 0.97526684\n",
            "Iteration 94, loss = 0.98693131\n",
            "Iteration 95, loss = 0.98530932\n",
            "Iteration 96, loss = 0.99985990\n",
            "Iteration 97, loss = 0.97343333\n",
            "Iteration 98, loss = 0.97696166\n",
            "Iteration 99, loss = 0.95878863\n",
            "Iteration 100, loss = 0.98639393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36673319\n",
            "Iteration 2, loss = 1.35206541\n",
            "Iteration 3, loss = 1.34276442\n",
            "Iteration 4, loss = 1.33092616\n",
            "Iteration 5, loss = 1.32573737\n",
            "Iteration 6, loss = 1.32033812\n",
            "Iteration 7, loss = 1.31352117\n",
            "Iteration 8, loss = 1.30236268\n",
            "Iteration 9, loss = 1.30794858\n",
            "Iteration 10, loss = 1.29493965\n",
            "Iteration 11, loss = 1.29730797\n",
            "Iteration 12, loss = 1.28649836\n",
            "Iteration 13, loss = 1.27704870\n",
            "Iteration 14, loss = 1.26969555\n",
            "Iteration 15, loss = 1.28084161\n",
            "Iteration 16, loss = 1.26075301\n",
            "Iteration 17, loss = 1.26577186\n",
            "Iteration 18, loss = 1.24272616\n",
            "Iteration 19, loss = 1.25032279\n",
            "Iteration 20, loss = 1.24647762\n",
            "Iteration 21, loss = 1.24611942\n",
            "Iteration 22, loss = 1.21541939\n",
            "Iteration 23, loss = 1.20205145\n",
            "Iteration 24, loss = 1.19379294\n",
            "Iteration 25, loss = 1.19277614\n",
            "Iteration 26, loss = 1.18401106\n",
            "Iteration 27, loss = 1.19223468\n",
            "Iteration 28, loss = 1.19259167\n",
            "Iteration 29, loss = 1.18529504\n",
            "Iteration 30, loss = 1.18080020\n",
            "Iteration 31, loss = 1.19891099\n",
            "Iteration 32, loss = 1.14985062\n",
            "Iteration 33, loss = 1.14475222\n",
            "Iteration 34, loss = 1.17689029\n",
            "Iteration 35, loss = 1.18443660\n",
            "Iteration 36, loss = 1.15223968\n",
            "Iteration 37, loss = 1.14745827\n",
            "Iteration 38, loss = 1.14673241\n",
            "Iteration 39, loss = 1.12749083\n",
            "Iteration 40, loss = 1.11815746\n",
            "Iteration 41, loss = 1.11049175\n",
            "Iteration 42, loss = 1.11365178\n",
            "Iteration 43, loss = 1.16868859\n",
            "Iteration 44, loss = 1.21919671\n",
            "Iteration 45, loss = 1.17470124\n",
            "Iteration 46, loss = 1.14470996\n",
            "Iteration 47, loss = 1.13360851\n",
            "Iteration 48, loss = 1.11589806\n",
            "Iteration 49, loss = 1.09985903\n",
            "Iteration 50, loss = 1.10751813\n",
            "Iteration 51, loss = 1.08917657\n",
            "Iteration 52, loss = 1.09423185\n",
            "Iteration 53, loss = 1.07807286\n",
            "Iteration 54, loss = 1.08043785\n",
            "Iteration 55, loss = 1.07337551\n",
            "Iteration 56, loss = 1.07107542\n",
            "Iteration 57, loss = 1.08102108\n",
            "Iteration 58, loss = 1.10465100\n",
            "Iteration 59, loss = 1.08978527\n",
            "Iteration 60, loss = 1.06923774\n",
            "Iteration 61, loss = 1.06151253\n",
            "Iteration 62, loss = 1.06458332\n",
            "Iteration 63, loss = 1.07683973\n",
            "Iteration 64, loss = 1.05108550\n",
            "Iteration 65, loss = 1.04131732\n",
            "Iteration 66, loss = 1.04681272\n",
            "Iteration 67, loss = 1.04195365\n",
            "Iteration 68, loss = 1.06869930\n",
            "Iteration 69, loss = 1.07664682\n",
            "Iteration 70, loss = 1.03294550\n",
            "Iteration 71, loss = 1.02838381\n",
            "Iteration 72, loss = 1.04123641\n",
            "Iteration 73, loss = 1.04211729\n",
            "Iteration 74, loss = 1.04457354\n",
            "Iteration 75, loss = 1.06165563\n",
            "Iteration 76, loss = 1.06001689\n",
            "Iteration 77, loss = 1.05069953\n",
            "Iteration 78, loss = 1.03575142\n",
            "Iteration 79, loss = 1.01986073\n",
            "Iteration 80, loss = 1.00521739\n",
            "Iteration 81, loss = 1.01104531\n",
            "Iteration 82, loss = 1.01028627\n",
            "Iteration 83, loss = 1.00822347\n",
            "Iteration 84, loss = 1.01038691\n",
            "Iteration 85, loss = 1.01692299\n",
            "Iteration 86, loss = 1.00589180\n",
            "Iteration 87, loss = 0.99979737\n",
            "Iteration 88, loss = 1.00916345\n",
            "Iteration 89, loss = 1.00546984\n",
            "Iteration 90, loss = 0.98261155\n",
            "Iteration 91, loss = 0.97256156\n",
            "Iteration 92, loss = 0.98761057\n",
            "Iteration 93, loss = 0.99507301\n",
            "Iteration 94, loss = 0.99453742\n",
            "Iteration 95, loss = 0.97042081\n",
            "Iteration 96, loss = 0.97698112\n",
            "Iteration 97, loss = 0.98075241\n",
            "Iteration 98, loss = 0.96703096\n",
            "Iteration 99, loss = 0.96083682\n",
            "Iteration 100, loss = 0.96570184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38362631\n",
            "Iteration 2, loss = 1.37229425\n",
            "Iteration 3, loss = 1.36965889\n",
            "Iteration 4, loss = 1.34638118\n",
            "Iteration 5, loss = 1.34818287\n",
            "Iteration 6, loss = 1.34009146\n",
            "Iteration 7, loss = 1.33119810\n",
            "Iteration 8, loss = 1.32140070\n",
            "Iteration 9, loss = 1.32859582\n",
            "Iteration 10, loss = 1.31293032\n",
            "Iteration 11, loss = 1.30378721\n",
            "Iteration 12, loss = 1.28932407\n",
            "Iteration 13, loss = 1.27470388\n",
            "Iteration 14, loss = 1.27417250\n",
            "Iteration 15, loss = 1.27561933\n",
            "Iteration 16, loss = 1.24895492\n",
            "Iteration 17, loss = 1.24389541\n",
            "Iteration 18, loss = 1.22574713\n",
            "Iteration 19, loss = 1.24476482\n",
            "Iteration 20, loss = 1.24050633\n",
            "Iteration 21, loss = 1.21831354\n",
            "Iteration 22, loss = 1.20681390\n",
            "Iteration 23, loss = 1.19348118\n",
            "Iteration 24, loss = 1.18276715\n",
            "Iteration 25, loss = 1.17068396\n",
            "Iteration 26, loss = 1.16999122\n",
            "Iteration 27, loss = 1.17712228\n",
            "Iteration 28, loss = 1.17864473\n",
            "Iteration 29, loss = 1.17479436\n",
            "Iteration 30, loss = 1.15989224\n",
            "Iteration 31, loss = 1.17610629\n",
            "Iteration 32, loss = 1.14549981\n",
            "Iteration 33, loss = 1.13139730\n",
            "Iteration 34, loss = 1.14931748\n",
            "Iteration 35, loss = 1.13409907\n",
            "Iteration 36, loss = 1.12820664\n",
            "Iteration 37, loss = 1.14200456\n",
            "Iteration 38, loss = 1.11397830\n",
            "Iteration 39, loss = 1.13070493\n",
            "Iteration 40, loss = 1.12324469\n",
            "Iteration 41, loss = 1.10700964\n",
            "Iteration 42, loss = 1.11064646\n",
            "Iteration 43, loss = 1.17139210\n",
            "Iteration 44, loss = 1.16755582\n",
            "Iteration 45, loss = 1.16365623\n",
            "Iteration 46, loss = 1.12492502\n",
            "Iteration 47, loss = 1.09842903\n",
            "Iteration 48, loss = 1.09551631\n",
            "Iteration 49, loss = 1.09504757\n",
            "Iteration 50, loss = 1.09630548\n",
            "Iteration 51, loss = 1.09769506\n",
            "Iteration 52, loss = 1.07329332\n",
            "Iteration 53, loss = 1.07057670\n",
            "Iteration 54, loss = 1.06518279\n",
            "Iteration 55, loss = 1.06294215\n",
            "Iteration 56, loss = 1.06045347\n",
            "Iteration 57, loss = 1.06475332\n",
            "Iteration 58, loss = 1.08242668\n",
            "Iteration 59, loss = 1.07296067\n",
            "Iteration 60, loss = 1.07130028\n",
            "Iteration 61, loss = 1.08626765\n",
            "Iteration 62, loss = 1.07812474\n",
            "Iteration 63, loss = 1.08877210\n",
            "Iteration 64, loss = 1.05305448\n",
            "Iteration 65, loss = 1.03140029\n",
            "Iteration 66, loss = 1.03787643\n",
            "Iteration 67, loss = 1.04861653\n",
            "Iteration 68, loss = 1.12100681\n",
            "Iteration 69, loss = 1.10156513\n",
            "Iteration 70, loss = 1.06268927\n",
            "Iteration 71, loss = 1.03667099\n",
            "Iteration 72, loss = 1.02526856\n",
            "Iteration 73, loss = 1.02336281\n",
            "Iteration 74, loss = 1.01534945\n",
            "Iteration 75, loss = 1.03582178\n",
            "Iteration 76, loss = 1.02629191\n",
            "Iteration 77, loss = 1.02435303\n",
            "Iteration 78, loss = 1.02115011\n",
            "Iteration 79, loss = 0.99954776\n",
            "Iteration 80, loss = 0.99354502\n",
            "Iteration 81, loss = 1.00466975\n",
            "Iteration 82, loss = 1.02760311\n",
            "Iteration 83, loss = 1.02737412\n",
            "Iteration 84, loss = 1.01829558\n",
            "Iteration 85, loss = 0.99660958\n",
            "Iteration 86, loss = 0.97400661\n",
            "Iteration 87, loss = 0.97672560\n",
            "Iteration 88, loss = 0.96762675\n",
            "Iteration 89, loss = 0.97740513\n",
            "Iteration 90, loss = 0.96443842\n",
            "Iteration 91, loss = 0.95451115\n",
            "Iteration 92, loss = 0.98980834\n",
            "Iteration 93, loss = 0.99803915\n",
            "Iteration 94, loss = 1.01971224\n",
            "Iteration 95, loss = 0.98122113\n",
            "Iteration 96, loss = 0.96411859\n",
            "Iteration 97, loss = 0.96099911\n",
            "Iteration 98, loss = 0.96260411\n",
            "Iteration 99, loss = 0.97176877\n",
            "Iteration 100, loss = 0.98294766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38753099\n",
            "Iteration 2, loss = 1.36698725\n",
            "Iteration 3, loss = 1.34952811\n",
            "Iteration 4, loss = 1.34204093\n",
            "Iteration 5, loss = 1.33224303\n",
            "Iteration 6, loss = 1.33223733\n",
            "Iteration 7, loss = 1.32439895\n",
            "Iteration 8, loss = 1.31440761\n",
            "Iteration 9, loss = 1.31449543\n",
            "Iteration 10, loss = 1.29567101\n",
            "Iteration 11, loss = 1.29518659\n",
            "Iteration 12, loss = 1.28688412\n",
            "Iteration 13, loss = 1.26917314\n",
            "Iteration 14, loss = 1.25974198\n",
            "Iteration 15, loss = 1.25614594\n",
            "Iteration 16, loss = 1.24039435\n",
            "Iteration 17, loss = 1.24347549\n",
            "Iteration 18, loss = 1.22779331\n",
            "Iteration 19, loss = 1.24829874\n",
            "Iteration 20, loss = 1.22387997\n",
            "Iteration 21, loss = 1.20870625\n",
            "Iteration 22, loss = 1.18520947\n",
            "Iteration 23, loss = 1.17633036\n",
            "Iteration 24, loss = 1.16700899\n",
            "Iteration 25, loss = 1.17700697\n",
            "Iteration 26, loss = 1.15133838\n",
            "Iteration 27, loss = 1.13905639\n",
            "Iteration 28, loss = 1.14461032\n",
            "Iteration 29, loss = 1.17419606\n",
            "Iteration 30, loss = 1.14935656\n",
            "Iteration 31, loss = 1.11970458\n",
            "Iteration 32, loss = 1.12025395\n",
            "Iteration 33, loss = 1.12618504\n",
            "Iteration 34, loss = 1.14220086\n",
            "Iteration 35, loss = 1.11757542\n",
            "Iteration 36, loss = 1.11388192\n",
            "Iteration 37, loss = 1.13178121\n",
            "Iteration 38, loss = 1.10209794\n",
            "Iteration 39, loss = 1.11591232\n",
            "Iteration 40, loss = 1.09819870\n",
            "Iteration 41, loss = 1.08151920\n",
            "Iteration 42, loss = 1.07976132\n",
            "Iteration 43, loss = 1.13348193\n",
            "Iteration 44, loss = 1.08081820\n",
            "Iteration 45, loss = 1.08095338\n",
            "Iteration 46, loss = 1.09615466\n",
            "Iteration 47, loss = 1.06932607\n",
            "Iteration 48, loss = 1.06376385\n",
            "Iteration 49, loss = 1.05263471\n",
            "Iteration 50, loss = 1.05523997\n",
            "Iteration 51, loss = 1.04068591\n",
            "Iteration 52, loss = 1.06726439\n",
            "Iteration 53, loss = 1.07605751\n",
            "Iteration 54, loss = 1.04462803\n",
            "Iteration 55, loss = 1.03938820\n",
            "Iteration 56, loss = 1.02282074\n",
            "Iteration 57, loss = 1.04659018\n",
            "Iteration 58, loss = 1.05938336\n",
            "Iteration 59, loss = 1.02902675\n",
            "Iteration 60, loss = 1.02962725\n",
            "Iteration 61, loss = 1.03161799\n",
            "Iteration 62, loss = 1.01877420\n",
            "Iteration 63, loss = 1.04061631\n",
            "Iteration 64, loss = 1.02280252\n",
            "Iteration 65, loss = 1.01393145\n",
            "Iteration 66, loss = 1.01807589\n",
            "Iteration 67, loss = 1.01195643\n",
            "Iteration 68, loss = 1.03646240\n",
            "Iteration 69, loss = 1.02996917\n",
            "Iteration 70, loss = 0.99178224\n",
            "Iteration 71, loss = 0.99553400\n",
            "Iteration 72, loss = 1.02148951\n",
            "Iteration 73, loss = 1.06609699\n",
            "Iteration 74, loss = 1.04653560\n",
            "Iteration 75, loss = 0.99917410\n",
            "Iteration 76, loss = 0.97810335\n",
            "Iteration 77, loss = 0.98438282\n",
            "Iteration 78, loss = 0.99673443\n",
            "Iteration 79, loss = 0.96329877\n",
            "Iteration 80, loss = 0.97461219\n",
            "Iteration 81, loss = 0.96795798\n",
            "Iteration 82, loss = 0.95849722\n",
            "Iteration 83, loss = 0.95576406\n",
            "Iteration 84, loss = 0.97964810\n",
            "Iteration 85, loss = 0.97688302\n",
            "Iteration 86, loss = 0.95251584\n",
            "Iteration 87, loss = 0.95025363\n",
            "Iteration 88, loss = 0.95403377\n",
            "Iteration 89, loss = 0.95961697\n",
            "Iteration 90, loss = 0.94585607\n",
            "Iteration 91, loss = 0.92210319\n",
            "Iteration 92, loss = 0.93480278\n",
            "Iteration 93, loss = 0.95933200\n",
            "Iteration 94, loss = 0.95540359\n",
            "Iteration 95, loss = 0.95374530\n",
            "Iteration 96, loss = 0.93769036\n",
            "Iteration 97, loss = 0.92961001\n",
            "Iteration 98, loss = 0.91578690\n",
            "Iteration 99, loss = 0.93262755\n",
            "Iteration 100, loss = 0.92189869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39394717\n",
            "Iteration 2, loss = 1.36260146\n",
            "Iteration 3, loss = 1.34687944\n",
            "Iteration 4, loss = 1.34348419\n",
            "Iteration 5, loss = 1.33558275\n",
            "Iteration 6, loss = 1.33282860\n",
            "Iteration 7, loss = 1.32422092\n",
            "Iteration 8, loss = 1.31100051\n",
            "Iteration 9, loss = 1.30615115\n",
            "Iteration 10, loss = 1.29419860\n",
            "Iteration 11, loss = 1.30073133\n",
            "Iteration 12, loss = 1.28891874\n",
            "Iteration 13, loss = 1.28703415\n",
            "Iteration 14, loss = 1.27646809\n",
            "Iteration 15, loss = 1.25716797\n",
            "Iteration 16, loss = 1.24664024\n",
            "Iteration 17, loss = 1.23924496\n",
            "Iteration 18, loss = 1.22679982\n",
            "Iteration 19, loss = 1.25111178\n",
            "Iteration 20, loss = 1.23218207\n",
            "Iteration 21, loss = 1.21618248\n",
            "Iteration 22, loss = 1.19434223\n",
            "Iteration 23, loss = 1.17742896\n",
            "Iteration 24, loss = 1.18153249\n",
            "Iteration 25, loss = 1.17814785\n",
            "Iteration 26, loss = 1.16510510\n",
            "Iteration 27, loss = 1.16091245\n",
            "Iteration 28, loss = 1.16270546\n",
            "Iteration 29, loss = 1.20554425\n",
            "Iteration 30, loss = 1.17363573\n",
            "Iteration 31, loss = 1.14147318\n",
            "Iteration 32, loss = 1.14141553\n",
            "Iteration 33, loss = 1.16280512\n",
            "Iteration 34, loss = 1.17532742\n",
            "Iteration 35, loss = 1.14257924\n",
            "Iteration 36, loss = 1.13545257\n",
            "Iteration 37, loss = 1.16147907\n",
            "Iteration 38, loss = 1.12478646\n",
            "Iteration 39, loss = 1.13049910\n",
            "Iteration 40, loss = 1.11689203\n",
            "Iteration 41, loss = 1.11109968\n",
            "Iteration 42, loss = 1.10767359\n",
            "Iteration 43, loss = 1.12914230\n",
            "Iteration 44, loss = 1.10871608\n",
            "Iteration 45, loss = 1.11131622\n",
            "Iteration 46, loss = 1.12564379\n",
            "Iteration 47, loss = 1.10040971\n",
            "Iteration 48, loss = 1.09197027\n",
            "Iteration 49, loss = 1.09231536\n",
            "Iteration 50, loss = 1.09515039\n",
            "Iteration 51, loss = 1.07326031\n",
            "Iteration 52, loss = 1.10351350\n",
            "Iteration 53, loss = 1.09125688\n",
            "Iteration 54, loss = 1.08727552\n",
            "Iteration 55, loss = 1.06912227\n",
            "Iteration 56, loss = 1.05877197\n",
            "Iteration 57, loss = 1.07226380\n",
            "Iteration 58, loss = 1.11543444\n",
            "Iteration 59, loss = 1.09897425\n",
            "Iteration 60, loss = 1.07098982\n",
            "Iteration 61, loss = 1.05665492\n",
            "Iteration 62, loss = 1.05970109\n",
            "Iteration 63, loss = 1.05654221\n",
            "Iteration 64, loss = 1.04381713\n",
            "Iteration 65, loss = 1.03195359\n",
            "Iteration 66, loss = 1.04082087\n",
            "Iteration 67, loss = 1.03375475\n",
            "Iteration 68, loss = 1.05773972\n",
            "Iteration 69, loss = 1.05991935\n",
            "Iteration 70, loss = 1.02861131\n",
            "Iteration 71, loss = 1.01630630\n",
            "Iteration 72, loss = 1.03964937\n",
            "Iteration 73, loss = 1.06883141\n",
            "Iteration 74, loss = 1.04991392\n",
            "Iteration 75, loss = 1.05999021\n",
            "Iteration 76, loss = 1.08091280\n",
            "Iteration 77, loss = 1.04293575\n",
            "Iteration 78, loss = 1.03825175\n",
            "Iteration 79, loss = 1.01399969\n",
            "Iteration 80, loss = 1.00116311\n",
            "Iteration 81, loss = 1.00705852\n",
            "Iteration 82, loss = 0.99336282\n",
            "Iteration 83, loss = 1.00561263\n",
            "Iteration 84, loss = 1.00239128\n",
            "Iteration 85, loss = 1.02400904\n",
            "Iteration 86, loss = 1.01602391\n",
            "Iteration 87, loss = 1.00194386\n",
            "Iteration 88, loss = 0.98534509\n",
            "Iteration 89, loss = 0.97351977\n",
            "Iteration 90, loss = 0.97877841\n",
            "Iteration 91, loss = 0.98167739\n",
            "Iteration 92, loss = 0.97644487\n",
            "Iteration 93, loss = 0.98838660\n",
            "Iteration 94, loss = 1.01484652\n",
            "Iteration 95, loss = 0.98842803\n",
            "Iteration 96, loss = 0.97800890\n",
            "Iteration 97, loss = 0.97083989\n",
            "Iteration 98, loss = 0.96747020\n",
            "Iteration 99, loss = 0.99431906\n",
            "Iteration 100, loss = 0.99300802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "Iteration 1, loss = 1.35074426\n",
            "Iteration 2, loss = 1.25099795\n",
            "Iteration 3, loss = 1.16816263\n",
            "Iteration 4, loss = 1.06286811\n",
            "Iteration 5, loss = 0.96748837\n",
            "Iteration 6, loss = 0.89612557\n",
            "Iteration 7, loss = 0.82603496\n",
            "Iteration 8, loss = 0.77896562\n",
            "Iteration 9, loss = 0.74616219\n",
            "Iteration 10, loss = 0.74385057\n",
            "Iteration 11, loss = 0.70526306\n",
            "Iteration 12, loss = 0.67832845\n",
            "Iteration 13, loss = 0.64670342\n",
            "Iteration 14, loss = 0.62787576\n",
            "Iteration 15, loss = 0.62681928\n",
            "Iteration 16, loss = 0.60399276\n",
            "Iteration 17, loss = 0.59499029\n",
            "Iteration 18, loss = 0.58953916\n",
            "Iteration 19, loss = 0.59250122\n",
            "Iteration 20, loss = 0.56323313\n",
            "Iteration 21, loss = 0.53483737\n",
            "Iteration 22, loss = 0.52928354\n",
            "Iteration 23, loss = 0.54233323\n",
            "Iteration 24, loss = 0.52508575\n",
            "Iteration 25, loss = 0.52604571\n",
            "Iteration 26, loss = 0.52981582\n",
            "Iteration 27, loss = 0.49337281\n",
            "Iteration 28, loss = 0.48689113\n",
            "Iteration 29, loss = 0.47941796\n",
            "Iteration 30, loss = 0.50398313\n",
            "Iteration 31, loss = 0.47206036\n",
            "Iteration 32, loss = 0.46364391\n",
            "Iteration 33, loss = 0.44844368\n",
            "Iteration 34, loss = 0.43924063\n",
            "Iteration 35, loss = 0.46437350\n",
            "Iteration 36, loss = 0.43268612\n",
            "Iteration 37, loss = 0.43532145\n",
            "Iteration 38, loss = 0.41751633\n",
            "Iteration 39, loss = 0.40047909\n",
            "Iteration 40, loss = 0.41533845\n",
            "Iteration 41, loss = 0.39782650\n",
            "Iteration 42, loss = 0.40008888\n",
            "Iteration 43, loss = 0.39546722\n",
            "Iteration 44, loss = 0.40916490\n",
            "Iteration 45, loss = 0.41254405\n",
            "Iteration 46, loss = 0.38536958\n",
            "Iteration 47, loss = 0.37386464\n",
            "Iteration 48, loss = 0.36171826\n",
            "Iteration 49, loss = 0.37794595\n",
            "Iteration 50, loss = 0.35975527\n",
            "Iteration 51, loss = 0.36505308\n",
            "Iteration 52, loss = 0.35903215\n",
            "Iteration 53, loss = 0.32149448\n",
            "Iteration 54, loss = 0.32037247\n",
            "Iteration 55, loss = 0.31159207\n",
            "Iteration 56, loss = 0.30694135\n",
            "Iteration 57, loss = 0.32743611\n",
            "Iteration 58, loss = 0.30556640\n",
            "Iteration 59, loss = 0.31860578\n",
            "Iteration 60, loss = 0.31622976\n",
            "Iteration 61, loss = 0.29045564\n",
            "Iteration 62, loss = 0.32448242\n",
            "Iteration 63, loss = 0.32967982\n",
            "Iteration 64, loss = 0.31141610\n",
            "Iteration 65, loss = 0.27787731\n",
            "Iteration 66, loss = 0.24654401\n",
            "Iteration 67, loss = 0.24970441\n",
            "Iteration 68, loss = 0.23918996\n",
            "Iteration 69, loss = 0.26294330\n",
            "Iteration 70, loss = 0.26601493\n",
            "Iteration 71, loss = 0.24317019\n",
            "Iteration 72, loss = 0.24180286\n",
            "Iteration 73, loss = 0.28935617\n",
            "Iteration 74, loss = 0.26438138\n",
            "Iteration 75, loss = 0.26945140\n",
            "Iteration 76, loss = 0.22187936\n",
            "Iteration 77, loss = 0.21881314\n",
            "Iteration 78, loss = 0.24609363\n",
            "Iteration 79, loss = 0.23781461\n",
            "Iteration 80, loss = 0.22693607\n",
            "Iteration 81, loss = 0.19900173\n",
            "Iteration 82, loss = 0.21317604\n",
            "Iteration 83, loss = 0.17558995\n",
            "Iteration 84, loss = 0.17545626\n",
            "Iteration 85, loss = 0.18557843\n",
            "Iteration 86, loss = 0.17007901\n",
            "Iteration 87, loss = 0.15995850\n",
            "Iteration 88, loss = 0.16574263\n",
            "Iteration 89, loss = 0.16213991\n",
            "Iteration 90, loss = 0.15490639\n",
            "Iteration 91, loss = 0.16069761\n",
            "Iteration 92, loss = 0.14091005\n",
            "Iteration 93, loss = 0.13581701\n",
            "Iteration 94, loss = 0.13294683\n",
            "Iteration 95, loss = 0.12743984\n",
            "Iteration 96, loss = 0.12310975\n",
            "Iteration 97, loss = 0.13680666\n",
            "Iteration 98, loss = 0.16601901\n",
            "Iteration 99, loss = 0.13527814\n",
            "Iteration 100, loss = 0.13162862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35122593\n",
            "Iteration 2, loss = 1.26399647\n",
            "Iteration 3, loss = 1.17971046\n",
            "Iteration 4, loss = 1.06936149\n",
            "Iteration 5, loss = 0.96608475\n",
            "Iteration 6, loss = 0.88906294\n",
            "Iteration 7, loss = 0.81497562\n",
            "Iteration 8, loss = 0.77218075\n",
            "Iteration 9, loss = 0.74671343\n",
            "Iteration 10, loss = 0.74360932\n",
            "Iteration 11, loss = 0.71603767\n",
            "Iteration 12, loss = 0.66645820\n",
            "Iteration 13, loss = 0.64692503\n",
            "Iteration 14, loss = 0.63334605\n",
            "Iteration 15, loss = 0.63085888\n",
            "Iteration 16, loss = 0.60607252\n",
            "Iteration 17, loss = 0.60492964\n",
            "Iteration 18, loss = 0.59670513\n",
            "Iteration 19, loss = 0.59593185\n",
            "Iteration 20, loss = 0.57947061\n",
            "Iteration 21, loss = 0.56831307\n",
            "Iteration 22, loss = 0.56092315\n",
            "Iteration 23, loss = 0.54620442\n",
            "Iteration 24, loss = 0.52952771\n",
            "Iteration 25, loss = 0.53711501\n",
            "Iteration 26, loss = 0.55054033\n",
            "Iteration 27, loss = 0.50758469\n",
            "Iteration 28, loss = 0.50168197\n",
            "Iteration 29, loss = 0.51878969\n",
            "Iteration 30, loss = 0.52459013\n",
            "Iteration 31, loss = 0.48605115\n",
            "Iteration 32, loss = 0.49533748\n",
            "Iteration 33, loss = 0.47766760\n",
            "Iteration 34, loss = 0.46579573\n",
            "Iteration 35, loss = 0.47083967\n",
            "Iteration 36, loss = 0.45731751\n",
            "Iteration 37, loss = 0.44405020\n",
            "Iteration 38, loss = 0.42694513\n",
            "Iteration 39, loss = 0.42938123\n",
            "Iteration 40, loss = 0.44067276\n",
            "Iteration 41, loss = 0.41369751\n",
            "Iteration 42, loss = 0.40800211\n",
            "Iteration 43, loss = 0.42326744\n",
            "Iteration 44, loss = 0.43420392\n",
            "Iteration 45, loss = 0.40973799\n",
            "Iteration 46, loss = 0.39672745\n",
            "Iteration 47, loss = 0.37728173\n",
            "Iteration 48, loss = 0.36410825\n",
            "Iteration 49, loss = 0.37361246\n",
            "Iteration 50, loss = 0.36149304\n",
            "Iteration 51, loss = 0.37158138\n",
            "Iteration 52, loss = 0.39736285\n",
            "Iteration 53, loss = 0.34706331\n",
            "Iteration 54, loss = 0.33745862\n",
            "Iteration 55, loss = 0.33607634\n",
            "Iteration 56, loss = 0.32121985\n",
            "Iteration 57, loss = 0.32369167\n",
            "Iteration 58, loss = 0.31665463\n",
            "Iteration 59, loss = 0.30130342\n",
            "Iteration 60, loss = 0.29677041\n",
            "Iteration 61, loss = 0.31097380\n",
            "Iteration 62, loss = 0.35619081\n",
            "Iteration 63, loss = 0.32446710\n",
            "Iteration 64, loss = 0.35501794\n",
            "Iteration 65, loss = 0.33596522\n",
            "Iteration 66, loss = 0.31432135\n",
            "Iteration 67, loss = 0.29846803\n",
            "Iteration 68, loss = 0.26608874\n",
            "Iteration 69, loss = 0.26123930\n",
            "Iteration 70, loss = 0.26606533\n",
            "Iteration 71, loss = 0.24837161\n",
            "Iteration 72, loss = 0.23884972\n",
            "Iteration 73, loss = 0.24839769\n",
            "Iteration 74, loss = 0.25335261\n",
            "Iteration 75, loss = 0.26157498\n",
            "Iteration 76, loss = 0.22619107\n",
            "Iteration 77, loss = 0.21754379\n",
            "Iteration 78, loss = 0.21574298\n",
            "Iteration 79, loss = 0.20797683\n",
            "Iteration 80, loss = 0.21451387\n",
            "Iteration 81, loss = 0.20006885\n",
            "Iteration 82, loss = 0.22991009\n",
            "Iteration 83, loss = 0.20766526\n",
            "Iteration 84, loss = 0.19856311\n",
            "Iteration 85, loss = 0.19245123\n",
            "Iteration 86, loss = 0.20171059\n",
            "Iteration 87, loss = 0.20130780\n",
            "Iteration 88, loss = 0.18549294\n",
            "Iteration 89, loss = 0.20608096\n",
            "Iteration 90, loss = 0.20926203\n",
            "Iteration 91, loss = 0.22830330\n",
            "Iteration 92, loss = 0.19266338\n",
            "Iteration 93, loss = 0.18768247\n",
            "Iteration 94, loss = 0.16015356\n",
            "Iteration 95, loss = 0.15396467\n",
            "Iteration 96, loss = 0.16604916\n",
            "Iteration 97, loss = 0.16485953\n",
            "Iteration 98, loss = 0.16965795\n",
            "Iteration 99, loss = 0.14598663\n",
            "Iteration 100, loss = 0.13939631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35334691\n",
            "Iteration 2, loss = 1.25277124\n",
            "Iteration 3, loss = 1.16319546\n",
            "Iteration 4, loss = 1.05653999\n",
            "Iteration 5, loss = 0.96225858\n",
            "Iteration 6, loss = 0.89532889\n",
            "Iteration 7, loss = 0.83036317\n",
            "Iteration 8, loss = 0.79457990\n",
            "Iteration 9, loss = 0.75206551\n",
            "Iteration 10, loss = 0.75091966\n",
            "Iteration 11, loss = 0.70907601\n",
            "Iteration 12, loss = 0.67648549\n",
            "Iteration 13, loss = 0.66680680\n",
            "Iteration 14, loss = 0.64896743\n",
            "Iteration 15, loss = 0.63882748\n",
            "Iteration 16, loss = 0.62049464\n",
            "Iteration 17, loss = 0.61209160\n",
            "Iteration 18, loss = 0.60763135\n",
            "Iteration 19, loss = 0.61706031\n",
            "Iteration 20, loss = 0.60219238\n",
            "Iteration 21, loss = 0.58573763\n",
            "Iteration 22, loss = 0.57548893\n",
            "Iteration 23, loss = 0.56672517\n",
            "Iteration 24, loss = 0.56049619\n",
            "Iteration 25, loss = 0.57960057\n",
            "Iteration 26, loss = 0.56498954\n",
            "Iteration 27, loss = 0.51705792\n",
            "Iteration 28, loss = 0.50907977\n",
            "Iteration 29, loss = 0.52266075\n",
            "Iteration 30, loss = 0.51269123\n",
            "Iteration 31, loss = 0.49848081\n",
            "Iteration 32, loss = 0.49745474\n",
            "Iteration 33, loss = 0.47894081\n",
            "Iteration 34, loss = 0.47719782\n",
            "Iteration 35, loss = 0.48790822\n",
            "Iteration 36, loss = 0.49049906\n",
            "Iteration 37, loss = 0.48584661\n",
            "Iteration 38, loss = 0.46346151\n",
            "Iteration 39, loss = 0.45418203\n",
            "Iteration 40, loss = 0.45441650\n",
            "Iteration 41, loss = 0.42302927\n",
            "Iteration 42, loss = 0.42014331\n",
            "Iteration 43, loss = 0.41778562\n",
            "Iteration 44, loss = 0.42788969\n",
            "Iteration 45, loss = 0.42282711\n",
            "Iteration 46, loss = 0.46224978\n",
            "Iteration 47, loss = 0.41799227\n",
            "Iteration 48, loss = 0.40383681\n",
            "Iteration 49, loss = 0.42963452\n",
            "Iteration 50, loss = 0.44823183\n",
            "Iteration 51, loss = 0.39979818\n",
            "Iteration 52, loss = 0.37000151\n",
            "Iteration 53, loss = 0.37241481\n",
            "Iteration 54, loss = 0.37935240\n",
            "Iteration 55, loss = 0.38683837\n",
            "Iteration 56, loss = 0.36459701\n",
            "Iteration 57, loss = 0.34929985\n",
            "Iteration 58, loss = 0.34435690\n",
            "Iteration 59, loss = 0.34444715\n",
            "Iteration 60, loss = 0.33319897\n",
            "Iteration 61, loss = 0.34404792\n",
            "Iteration 62, loss = 0.36670658\n",
            "Iteration 63, loss = 0.35268803\n",
            "Iteration 64, loss = 0.36862043\n",
            "Iteration 65, loss = 0.35973974\n",
            "Iteration 66, loss = 0.34357639\n",
            "Iteration 67, loss = 0.31339658\n",
            "Iteration 68, loss = 0.29427544\n",
            "Iteration 69, loss = 0.29440676\n",
            "Iteration 70, loss = 0.29412560\n",
            "Iteration 71, loss = 0.27728405\n",
            "Iteration 72, loss = 0.26703606\n",
            "Iteration 73, loss = 0.28371068\n",
            "Iteration 74, loss = 0.30904156\n",
            "Iteration 75, loss = 0.30505585\n",
            "Iteration 76, loss = 0.28067080\n",
            "Iteration 77, loss = 0.28719608\n",
            "Iteration 78, loss = 0.28453804\n",
            "Iteration 79, loss = 0.27335480\n",
            "Iteration 80, loss = 0.28752981\n",
            "Iteration 81, loss = 0.30108387\n",
            "Iteration 82, loss = 0.26168418\n",
            "Iteration 83, loss = 0.24337844\n",
            "Iteration 84, loss = 0.25898735\n",
            "Iteration 85, loss = 0.26514034\n",
            "Iteration 86, loss = 0.24207880\n",
            "Iteration 87, loss = 0.25276452\n",
            "Iteration 88, loss = 0.23257156\n",
            "Iteration 89, loss = 0.21754067\n",
            "Iteration 90, loss = 0.22819090\n",
            "Iteration 91, loss = 0.21476342\n",
            "Iteration 92, loss = 0.20540892\n",
            "Iteration 93, loss = 0.22840814\n",
            "Iteration 94, loss = 0.22719851\n",
            "Iteration 95, loss = 0.22022009\n",
            "Iteration 96, loss = 0.19939885\n",
            "Iteration 97, loss = 0.19189502\n",
            "Iteration 98, loss = 0.19786950\n",
            "Iteration 99, loss = 0.18545189\n",
            "Iteration 100, loss = 0.18870506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35034964\n",
            "Iteration 2, loss = 1.25403411\n",
            "Iteration 3, loss = 1.15292816\n",
            "Iteration 4, loss = 1.04805658\n",
            "Iteration 5, loss = 0.94498076\n",
            "Iteration 6, loss = 0.86571788\n",
            "Iteration 7, loss = 0.82639426\n",
            "Iteration 8, loss = 0.78744647\n",
            "Iteration 9, loss = 0.72747184\n",
            "Iteration 10, loss = 0.70779561\n",
            "Iteration 11, loss = 0.68790456\n",
            "Iteration 12, loss = 0.67114016\n",
            "Iteration 13, loss = 0.66030343\n",
            "Iteration 14, loss = 0.63948847\n",
            "Iteration 15, loss = 0.62874658\n",
            "Iteration 16, loss = 0.61116291\n",
            "Iteration 17, loss = 0.60424196\n",
            "Iteration 18, loss = 0.61321347\n",
            "Iteration 19, loss = 0.61385603\n",
            "Iteration 20, loss = 0.58888961\n",
            "Iteration 21, loss = 0.57273127\n",
            "Iteration 22, loss = 0.56827256\n",
            "Iteration 23, loss = 0.55674903\n",
            "Iteration 24, loss = 0.53734703\n",
            "Iteration 25, loss = 0.55725699\n",
            "Iteration 26, loss = 0.56935626\n",
            "Iteration 27, loss = 0.52720501\n",
            "Iteration 28, loss = 0.49677139\n",
            "Iteration 29, loss = 0.49820079\n",
            "Iteration 30, loss = 0.50373330\n",
            "Iteration 31, loss = 0.49015493\n",
            "Iteration 32, loss = 0.47835349\n",
            "Iteration 33, loss = 0.46420546\n",
            "Iteration 34, loss = 0.45762792\n",
            "Iteration 35, loss = 0.46728266\n",
            "Iteration 36, loss = 0.45899459\n",
            "Iteration 37, loss = 0.44899667\n",
            "Iteration 38, loss = 0.46446690\n",
            "Iteration 39, loss = 0.44828473\n",
            "Iteration 40, loss = 0.42676606\n",
            "Iteration 41, loss = 0.41740920\n",
            "Iteration 42, loss = 0.39945853\n",
            "Iteration 43, loss = 0.41814398\n",
            "Iteration 44, loss = 0.43370012\n",
            "Iteration 45, loss = 0.42695565\n",
            "Iteration 46, loss = 0.43875813\n",
            "Iteration 47, loss = 0.42035917\n",
            "Iteration 48, loss = 0.39619810\n",
            "Iteration 49, loss = 0.39529739\n",
            "Iteration 50, loss = 0.38646611\n",
            "Iteration 51, loss = 0.36418956\n",
            "Iteration 52, loss = 0.34752315\n",
            "Iteration 53, loss = 0.34478916\n",
            "Iteration 54, loss = 0.33787636\n",
            "Iteration 55, loss = 0.34833947\n",
            "Iteration 56, loss = 0.33666668\n",
            "Iteration 57, loss = 0.34287904\n",
            "Iteration 58, loss = 0.32454688\n",
            "Iteration 59, loss = 0.32329929\n",
            "Iteration 60, loss = 0.30984671\n",
            "Iteration 61, loss = 0.32206013\n",
            "Iteration 62, loss = 0.32646861\n",
            "Iteration 63, loss = 0.31326215\n",
            "Iteration 64, loss = 0.32563523\n",
            "Iteration 65, loss = 0.30092537\n",
            "Iteration 66, loss = 0.30182935\n",
            "Iteration 67, loss = 0.30453847\n",
            "Iteration 68, loss = 0.27556429\n",
            "Iteration 69, loss = 0.27296221\n",
            "Iteration 70, loss = 0.27094303\n",
            "Iteration 71, loss = 0.24366144\n",
            "Iteration 72, loss = 0.24512623\n",
            "Iteration 73, loss = 0.24895036\n",
            "Iteration 74, loss = 0.24438891\n",
            "Iteration 75, loss = 0.22953330\n",
            "Iteration 76, loss = 0.23491598\n",
            "Iteration 77, loss = 0.22826017\n",
            "Iteration 78, loss = 0.22872868\n",
            "Iteration 79, loss = 0.21138295\n",
            "Iteration 80, loss = 0.20536804\n",
            "Iteration 81, loss = 0.21066374\n",
            "Iteration 82, loss = 0.20255621\n",
            "Iteration 83, loss = 0.20057497\n",
            "Iteration 84, loss = 0.19468671\n",
            "Iteration 85, loss = 0.22630928\n",
            "Iteration 86, loss = 0.19258378\n",
            "Iteration 87, loss = 0.19067810\n",
            "Iteration 88, loss = 0.19577182\n",
            "Iteration 89, loss = 0.20839900\n",
            "Iteration 90, loss = 0.18526157\n",
            "Iteration 91, loss = 0.16382118\n",
            "Iteration 92, loss = 0.16401465\n",
            "Iteration 93, loss = 0.18109877\n",
            "Iteration 94, loss = 0.20680672\n",
            "Iteration 95, loss = 0.16898147\n",
            "Iteration 96, loss = 0.15949662\n",
            "Iteration 97, loss = 0.14365258\n",
            "Iteration 98, loss = 0.18905343\n",
            "Iteration 99, loss = 0.17865293\n",
            "Iteration 100, loss = 0.18515040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35208297\n",
            "Iteration 2, loss = 1.25118592\n",
            "Iteration 3, loss = 1.14939257\n",
            "Iteration 4, loss = 1.03472281\n",
            "Iteration 5, loss = 0.93496675\n",
            "Iteration 6, loss = 0.85159115\n",
            "Iteration 7, loss = 0.80187201\n",
            "Iteration 8, loss = 0.76803679\n",
            "Iteration 9, loss = 0.71939121\n",
            "Iteration 10, loss = 0.69821202\n",
            "Iteration 11, loss = 0.67281578\n",
            "Iteration 12, loss = 0.65105633\n",
            "Iteration 13, loss = 0.63369318\n",
            "Iteration 14, loss = 0.62035650\n",
            "Iteration 15, loss = 0.60586821\n",
            "Iteration 16, loss = 0.60661069\n",
            "Iteration 17, loss = 0.58611821\n",
            "Iteration 18, loss = 0.57549485\n",
            "Iteration 19, loss = 0.60633695\n",
            "Iteration 20, loss = 0.58878948\n",
            "Iteration 21, loss = 0.57248111\n",
            "Iteration 22, loss = 0.55228395\n",
            "Iteration 23, loss = 0.56235500\n",
            "Iteration 24, loss = 0.53346412\n",
            "Iteration 25, loss = 0.56013021\n",
            "Iteration 26, loss = 0.57784579\n",
            "Iteration 27, loss = 0.52108952\n",
            "Iteration 28, loss = 0.49162333\n",
            "Iteration 29, loss = 0.48689009\n",
            "Iteration 30, loss = 0.50005408\n",
            "Iteration 31, loss = 0.48896029\n",
            "Iteration 32, loss = 0.49514946\n",
            "Iteration 33, loss = 0.47444156\n",
            "Iteration 34, loss = 0.47541098\n",
            "Iteration 35, loss = 0.47766624\n",
            "Iteration 36, loss = 0.47438883\n",
            "Iteration 37, loss = 0.45161819\n",
            "Iteration 38, loss = 0.44549275\n",
            "Iteration 39, loss = 0.44015870\n",
            "Iteration 40, loss = 0.42555208\n",
            "Iteration 41, loss = 0.44274131\n",
            "Iteration 42, loss = 0.42939213\n",
            "Iteration 43, loss = 0.43825045\n",
            "Iteration 44, loss = 0.45068819\n",
            "Iteration 45, loss = 0.46063578\n",
            "Iteration 46, loss = 0.46601316\n",
            "Iteration 47, loss = 0.44943833\n",
            "Iteration 48, loss = 0.43619252\n",
            "Iteration 49, loss = 0.47371498\n",
            "Iteration 50, loss = 0.46986752\n",
            "Iteration 51, loss = 0.40507758\n",
            "Iteration 52, loss = 0.37208198\n",
            "Iteration 53, loss = 0.37652991\n",
            "Iteration 54, loss = 0.38070844\n",
            "Iteration 55, loss = 0.39444438\n",
            "Iteration 56, loss = 0.38457396\n",
            "Iteration 57, loss = 0.38241959\n",
            "Iteration 58, loss = 0.37635322\n",
            "Iteration 59, loss = 0.36251905\n",
            "Iteration 60, loss = 0.34657627\n",
            "Iteration 61, loss = 0.38163716\n",
            "Iteration 62, loss = 0.37009524\n",
            "Iteration 63, loss = 0.36399680\n",
            "Iteration 64, loss = 0.35338808\n",
            "Iteration 65, loss = 0.32888756\n",
            "Iteration 66, loss = 0.34275036\n",
            "Iteration 67, loss = 0.33536887\n",
            "Iteration 68, loss = 0.33138557\n",
            "Iteration 69, loss = 0.31583474\n",
            "Iteration 70, loss = 0.31064801\n",
            "Iteration 71, loss = 0.30076784\n",
            "Iteration 72, loss = 0.30805372\n",
            "Iteration 73, loss = 0.29825881\n",
            "Iteration 74, loss = 0.29191799\n",
            "Iteration 75, loss = 0.27951635\n",
            "Iteration 76, loss = 0.29022458\n",
            "Iteration 77, loss = 0.30502681\n",
            "Iteration 78, loss = 0.27651395\n",
            "Iteration 79, loss = 0.28344867\n",
            "Iteration 80, loss = 0.27108151\n",
            "Iteration 81, loss = 0.28925008\n",
            "Iteration 82, loss = 0.31421559\n",
            "Iteration 83, loss = 0.28310897\n",
            "Iteration 84, loss = 0.29702713\n",
            "Iteration 85, loss = 0.29668547\n",
            "Iteration 86, loss = 0.26170325\n",
            "Iteration 87, loss = 0.25554614\n",
            "Iteration 88, loss = 0.27499744\n",
            "Iteration 89, loss = 0.24676058\n",
            "Iteration 90, loss = 0.22814597\n",
            "Iteration 91, loss = 0.24925142\n",
            "Iteration 92, loss = 0.23416741\n",
            "Iteration 93, loss = 0.22550557\n",
            "Iteration 94, loss = 0.21626135\n",
            "Iteration 95, loss = 0.22059571\n",
            "Iteration 96, loss = 0.24748829\n",
            "Iteration 97, loss = 0.28708966\n",
            "Iteration 98, loss = 0.27055910\n",
            "Iteration 99, loss = 0.24062271\n",
            "Iteration 100, loss = 0.22405330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34952921\n",
            "Iteration 2, loss = 1.25877868\n",
            "Iteration 3, loss = 1.17497282\n",
            "Iteration 4, loss = 1.08957621\n",
            "Iteration 5, loss = 1.00276322\n",
            "Iteration 6, loss = 0.94196861\n",
            "Iteration 7, loss = 0.88737663\n",
            "Iteration 8, loss = 0.83263001\n",
            "Iteration 9, loss = 0.78009185\n",
            "Iteration 10, loss = 0.75765464\n",
            "Iteration 11, loss = 0.74396803\n",
            "Iteration 12, loss = 0.72100010\n",
            "Iteration 13, loss = 0.69029427\n",
            "Iteration 14, loss = 0.66423628\n",
            "Iteration 15, loss = 0.65403239\n",
            "Iteration 16, loss = 0.64760569\n",
            "Iteration 17, loss = 0.64187002\n",
            "Iteration 18, loss = 0.63868368\n",
            "Iteration 19, loss = 0.68149711\n",
            "Iteration 20, loss = 0.66562822\n",
            "Iteration 21, loss = 0.64209313\n",
            "Iteration 22, loss = 0.62205834\n",
            "Iteration 23, loss = 0.59902665\n",
            "Iteration 24, loss = 0.60537547\n",
            "Iteration 25, loss = 0.61227156\n",
            "Iteration 26, loss = 0.62117970\n",
            "Iteration 27, loss = 0.59001279\n",
            "Iteration 28, loss = 0.55858109\n",
            "Iteration 29, loss = 0.54549553\n",
            "Iteration 30, loss = 0.54081886\n",
            "Iteration 31, loss = 0.52842565\n",
            "Iteration 32, loss = 0.53268284\n",
            "Iteration 33, loss = 0.52043074\n",
            "Iteration 34, loss = 0.51720222\n",
            "Iteration 35, loss = 0.50820402\n",
            "Iteration 36, loss = 0.51944107\n",
            "Iteration 37, loss = 0.49181950\n",
            "Iteration 38, loss = 0.48590131\n",
            "Iteration 39, loss = 0.48461055\n",
            "Iteration 40, loss = 0.47020017\n",
            "Iteration 41, loss = 0.49896764\n",
            "Iteration 42, loss = 0.50136455\n",
            "Iteration 43, loss = 0.52487493\n",
            "Iteration 44, loss = 0.51591870\n",
            "Iteration 45, loss = 0.51383639\n",
            "Iteration 46, loss = 0.46428164\n",
            "Iteration 47, loss = 0.45067299\n",
            "Iteration 48, loss = 0.44093700\n",
            "Iteration 49, loss = 0.44382155\n",
            "Iteration 50, loss = 0.45325747\n",
            "Iteration 51, loss = 0.42262575\n",
            "Iteration 52, loss = 0.42836161\n",
            "Iteration 53, loss = 0.41965139\n",
            "Iteration 54, loss = 0.41078299\n",
            "Iteration 55, loss = 0.42622355\n",
            "Iteration 56, loss = 0.39872801\n",
            "Iteration 57, loss = 0.38952239\n",
            "Iteration 58, loss = 0.37201829\n",
            "Iteration 59, loss = 0.37321812\n",
            "Iteration 60, loss = 0.38872141\n",
            "Iteration 61, loss = 0.40187415\n",
            "Iteration 62, loss = 0.40803506\n",
            "Iteration 63, loss = 0.39880819\n",
            "Iteration 64, loss = 0.38969999\n",
            "Iteration 65, loss = 0.35762739\n",
            "Iteration 66, loss = 0.34523179\n",
            "Iteration 67, loss = 0.33507715\n",
            "Iteration 68, loss = 0.35279318\n",
            "Iteration 69, loss = 0.35461871\n",
            "Iteration 70, loss = 0.35134587\n",
            "Iteration 71, loss = 0.31004897\n",
            "Iteration 72, loss = 0.31176068\n",
            "Iteration 73, loss = 0.31071486\n",
            "Iteration 74, loss = 0.31078989\n",
            "Iteration 75, loss = 0.29452692\n",
            "Iteration 76, loss = 0.30484918\n",
            "Iteration 77, loss = 0.32680852\n",
            "Iteration 78, loss = 0.31562829\n",
            "Iteration 79, loss = 0.30437178\n",
            "Iteration 80, loss = 0.28296223\n",
            "Iteration 81, loss = 0.28350764\n",
            "Iteration 82, loss = 0.33254520\n",
            "Iteration 83, loss = 0.32793603\n",
            "Iteration 84, loss = 0.27749191\n",
            "Iteration 85, loss = 0.25581565\n",
            "Iteration 86, loss = 0.26101353\n",
            "Iteration 87, loss = 0.26439942\n",
            "Iteration 88, loss = 0.23984571\n",
            "Iteration 89, loss = 0.23126292\n",
            "Iteration 90, loss = 0.23731096\n",
            "Iteration 91, loss = 0.23308260\n",
            "Iteration 92, loss = 0.23783988\n",
            "Iteration 93, loss = 0.22296070\n",
            "Iteration 94, loss = 0.23862219\n",
            "Iteration 95, loss = 0.24961884\n",
            "Iteration 96, loss = 0.27067685\n",
            "Iteration 97, loss = 0.29393941\n",
            "Iteration 98, loss = 0.28161619\n",
            "Iteration 99, loss = 0.26133652\n",
            "Iteration 100, loss = 0.24556663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33406546\n",
            "Iteration 2, loss = 1.24767851\n",
            "Iteration 3, loss = 1.16433925\n",
            "Iteration 4, loss = 1.07766958\n",
            "Iteration 5, loss = 1.00539419\n",
            "Iteration 6, loss = 0.95001845\n",
            "Iteration 7, loss = 0.89021636\n",
            "Iteration 8, loss = 0.84323745\n",
            "Iteration 9, loss = 0.80620029\n",
            "Iteration 10, loss = 0.78844873\n",
            "Iteration 11, loss = 0.77774014\n",
            "Iteration 12, loss = 0.75470223\n",
            "Iteration 13, loss = 0.72886952\n",
            "Iteration 14, loss = 0.70607362\n",
            "Iteration 15, loss = 0.69715707\n",
            "Iteration 16, loss = 0.68546694\n",
            "Iteration 17, loss = 0.68229945\n",
            "Iteration 18, loss = 0.71042001\n",
            "Iteration 19, loss = 0.68255115\n",
            "Iteration 20, loss = 0.67566209\n",
            "Iteration 21, loss = 0.67534713\n",
            "Iteration 22, loss = 0.65254053\n",
            "Iteration 23, loss = 0.63019409\n",
            "Iteration 24, loss = 0.63036594\n",
            "Iteration 25, loss = 0.64010745\n",
            "Iteration 26, loss = 0.67560263\n",
            "Iteration 27, loss = 0.63931997\n",
            "Iteration 28, loss = 0.60277598\n",
            "Iteration 29, loss = 0.58906239\n",
            "Iteration 30, loss = 0.58785983\n",
            "Iteration 31, loss = 0.56864649\n",
            "Iteration 32, loss = 0.56526170\n",
            "Iteration 33, loss = 0.54697111\n",
            "Iteration 34, loss = 0.54297944\n",
            "Iteration 35, loss = 0.53689900\n",
            "Iteration 36, loss = 0.53996807\n",
            "Iteration 37, loss = 0.54400052\n",
            "Iteration 38, loss = 0.54801190\n",
            "Iteration 39, loss = 0.52920767\n",
            "Iteration 40, loss = 0.51225734\n",
            "Iteration 41, loss = 0.53878970\n",
            "Iteration 42, loss = 0.51343312\n",
            "Iteration 43, loss = 0.52347176\n",
            "Iteration 44, loss = 0.52253570\n",
            "Iteration 45, loss = 0.52004283\n",
            "Iteration 46, loss = 0.51557475\n",
            "Iteration 47, loss = 0.49798582\n",
            "Iteration 48, loss = 0.49968881\n",
            "Iteration 49, loss = 0.50783544\n",
            "Iteration 50, loss = 0.50577601\n",
            "Iteration 51, loss = 0.45977481\n",
            "Iteration 52, loss = 0.45911425\n",
            "Iteration 53, loss = 0.45772371\n",
            "Iteration 54, loss = 0.44909308\n",
            "Iteration 55, loss = 0.46492426\n",
            "Iteration 56, loss = 0.43603811\n",
            "Iteration 57, loss = 0.42125066\n",
            "Iteration 58, loss = 0.40203796\n",
            "Iteration 59, loss = 0.39951688\n",
            "Iteration 60, loss = 0.41017683\n",
            "Iteration 61, loss = 0.41221818\n",
            "Iteration 62, loss = 0.43446520\n",
            "Iteration 63, loss = 0.41215782\n",
            "Iteration 64, loss = 0.41559977\n",
            "Iteration 65, loss = 0.40756795\n",
            "Iteration 66, loss = 0.40424716\n",
            "Iteration 67, loss = 0.38084673\n",
            "Iteration 68, loss = 0.35882357\n",
            "Iteration 69, loss = 0.36477279\n",
            "Iteration 70, loss = 0.37142393\n",
            "Iteration 71, loss = 0.33543292\n",
            "Iteration 72, loss = 0.33565297\n",
            "Iteration 73, loss = 0.34124378\n",
            "Iteration 74, loss = 0.33406172\n",
            "Iteration 75, loss = 0.32618046\n",
            "Iteration 76, loss = 0.33507751\n",
            "Iteration 77, loss = 0.36176588\n",
            "Iteration 78, loss = 0.33153606\n",
            "Iteration 79, loss = 0.31763721\n",
            "Iteration 80, loss = 0.30138027\n",
            "Iteration 81, loss = 0.30052435\n",
            "Iteration 82, loss = 0.30413452\n",
            "Iteration 83, loss = 0.30485302\n",
            "Iteration 84, loss = 0.30968146\n",
            "Iteration 85, loss = 0.31567732\n",
            "Iteration 86, loss = 0.27929825\n",
            "Iteration 87, loss = 0.25386494\n",
            "Iteration 88, loss = 0.26278659\n",
            "Iteration 89, loss = 0.24500127\n",
            "Iteration 90, loss = 0.24318517\n",
            "Iteration 91, loss = 0.23756493\n",
            "Iteration 92, loss = 0.22699626\n",
            "Iteration 93, loss = 0.22475504\n",
            "Iteration 94, loss = 0.22694143\n",
            "Iteration 95, loss = 0.22370169\n",
            "Iteration 96, loss = 0.24403342\n",
            "Iteration 97, loss = 0.26438043\n",
            "Iteration 98, loss = 0.27980315\n",
            "Iteration 99, loss = 0.23370253\n",
            "Iteration 100, loss = 0.20622036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35094551\n",
            "Iteration 2, loss = 1.26693506\n",
            "Iteration 3, loss = 1.17407581\n",
            "Iteration 4, loss = 1.07966761\n",
            "Iteration 5, loss = 0.99205882\n",
            "Iteration 6, loss = 0.91220469\n",
            "Iteration 7, loss = 0.87230276\n",
            "Iteration 8, loss = 0.81782451\n",
            "Iteration 9, loss = 0.77207339\n",
            "Iteration 10, loss = 0.74844688\n",
            "Iteration 11, loss = 0.73859451\n",
            "Iteration 12, loss = 0.70980928\n",
            "Iteration 13, loss = 0.69162461\n",
            "Iteration 14, loss = 0.67589801\n",
            "Iteration 15, loss = 0.66545738\n",
            "Iteration 16, loss = 0.65489367\n",
            "Iteration 17, loss = 0.64031116\n",
            "Iteration 18, loss = 0.64248522\n",
            "Iteration 19, loss = 0.62305697\n",
            "Iteration 20, loss = 0.61417373\n",
            "Iteration 21, loss = 0.59969367\n",
            "Iteration 22, loss = 0.58738938\n",
            "Iteration 23, loss = 0.57720409\n",
            "Iteration 24, loss = 0.57318107\n",
            "Iteration 25, loss = 0.58604051\n",
            "Iteration 26, loss = 0.58828193\n",
            "Iteration 27, loss = 0.53796344\n",
            "Iteration 28, loss = 0.52838086\n",
            "Iteration 29, loss = 0.52136372\n",
            "Iteration 30, loss = 0.51121683\n",
            "Iteration 31, loss = 0.50276612\n",
            "Iteration 32, loss = 0.50775269\n",
            "Iteration 33, loss = 0.50515152\n",
            "Iteration 34, loss = 0.49569704\n",
            "Iteration 35, loss = 0.48425183\n",
            "Iteration 36, loss = 0.50295611\n",
            "Iteration 37, loss = 0.49597329\n",
            "Iteration 38, loss = 0.48731082\n",
            "Iteration 39, loss = 0.47704109\n",
            "Iteration 40, loss = 0.46012437\n",
            "Iteration 41, loss = 0.48011979\n",
            "Iteration 42, loss = 0.45640617\n",
            "Iteration 43, loss = 0.49283657\n",
            "Iteration 44, loss = 0.49664911\n",
            "Iteration 45, loss = 0.47881718\n",
            "Iteration 46, loss = 0.46371154\n",
            "Iteration 47, loss = 0.42587241\n",
            "Iteration 48, loss = 0.43986730\n",
            "Iteration 49, loss = 0.45710456\n",
            "Iteration 50, loss = 0.46921425\n",
            "Iteration 51, loss = 0.41373532\n",
            "Iteration 52, loss = 0.38975674\n",
            "Iteration 53, loss = 0.40680123\n",
            "Iteration 54, loss = 0.39725373\n",
            "Iteration 55, loss = 0.38427419\n",
            "Iteration 56, loss = 0.38133682\n",
            "Iteration 57, loss = 0.36534619\n",
            "Iteration 58, loss = 0.36016602\n",
            "Iteration 59, loss = 0.35812500\n",
            "Iteration 60, loss = 0.37067885\n",
            "Iteration 61, loss = 0.36500311\n",
            "Iteration 62, loss = 0.37728624\n",
            "Iteration 63, loss = 0.36792511\n",
            "Iteration 64, loss = 0.37209108\n",
            "Iteration 65, loss = 0.33465705\n",
            "Iteration 66, loss = 0.32954564\n",
            "Iteration 67, loss = 0.31948260\n",
            "Iteration 68, loss = 0.33059640\n",
            "Iteration 69, loss = 0.35247873\n",
            "Iteration 70, loss = 0.33825242\n",
            "Iteration 71, loss = 0.30100803\n",
            "Iteration 72, loss = 0.30028921\n",
            "Iteration 73, loss = 0.30316960\n",
            "Iteration 74, loss = 0.29112003\n",
            "Iteration 75, loss = 0.28693463\n",
            "Iteration 76, loss = 0.29420692\n",
            "Iteration 77, loss = 0.32722573\n",
            "Iteration 78, loss = 0.31054916\n",
            "Iteration 79, loss = 0.29190658\n",
            "Iteration 80, loss = 0.28173035\n",
            "Iteration 81, loss = 0.27346690\n",
            "Iteration 82, loss = 0.27518677\n",
            "Iteration 83, loss = 0.26666094\n",
            "Iteration 84, loss = 0.28181095\n",
            "Iteration 85, loss = 0.31122239\n",
            "Iteration 86, loss = 0.26340577\n",
            "Iteration 87, loss = 0.24026693\n",
            "Iteration 88, loss = 0.23360394\n",
            "Iteration 89, loss = 0.21827584\n",
            "Iteration 90, loss = 0.21935087\n",
            "Iteration 91, loss = 0.21970811\n",
            "Iteration 92, loss = 0.22149630\n",
            "Iteration 93, loss = 0.19898615\n",
            "Iteration 94, loss = 0.20096928\n",
            "Iteration 95, loss = 0.20526980\n",
            "Iteration 96, loss = 0.23388150\n",
            "Iteration 97, loss = 0.29037339\n",
            "Iteration 98, loss = 0.27614985\n",
            "Iteration 99, loss = 0.22415731\n",
            "Iteration 100, loss = 0.19956540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35115164\n",
            "Iteration 2, loss = 1.23542443\n",
            "Iteration 3, loss = 1.12807597\n",
            "Iteration 4, loss = 1.03728254\n",
            "Iteration 5, loss = 0.94307195\n",
            "Iteration 6, loss = 0.86994212\n",
            "Iteration 7, loss = 0.83951816\n",
            "Iteration 8, loss = 0.80365806\n",
            "Iteration 9, loss = 0.74771178\n",
            "Iteration 10, loss = 0.71926552\n",
            "Iteration 11, loss = 0.69693973\n",
            "Iteration 12, loss = 0.67860434\n",
            "Iteration 13, loss = 0.66140015\n",
            "Iteration 14, loss = 0.64534600\n",
            "Iteration 15, loss = 0.63357852\n",
            "Iteration 16, loss = 0.62804083\n",
            "Iteration 17, loss = 0.62084571\n",
            "Iteration 18, loss = 0.61495811\n",
            "Iteration 19, loss = 0.58448057\n",
            "Iteration 20, loss = 0.60039568\n",
            "Iteration 21, loss = 0.58021552\n",
            "Iteration 22, loss = 0.56213665\n",
            "Iteration 23, loss = 0.55610485\n",
            "Iteration 24, loss = 0.54236625\n",
            "Iteration 25, loss = 0.55739885\n",
            "Iteration 26, loss = 0.54699786\n",
            "Iteration 27, loss = 0.52681250\n",
            "Iteration 28, loss = 0.52596278\n",
            "Iteration 29, loss = 0.51764107\n",
            "Iteration 30, loss = 0.50756872\n",
            "Iteration 31, loss = 0.49837630\n",
            "Iteration 32, loss = 0.50169232\n",
            "Iteration 33, loss = 0.50196924\n",
            "Iteration 34, loss = 0.48732843\n",
            "Iteration 35, loss = 0.47099184\n",
            "Iteration 36, loss = 0.51065348\n",
            "Iteration 37, loss = 0.50392950\n",
            "Iteration 38, loss = 0.50155826\n",
            "Iteration 39, loss = 0.49734711\n",
            "Iteration 40, loss = 0.48847036\n",
            "Iteration 41, loss = 0.48083202\n",
            "Iteration 42, loss = 0.45736870\n",
            "Iteration 43, loss = 0.44993017\n",
            "Iteration 44, loss = 0.45692337\n",
            "Iteration 45, loss = 0.46129578\n",
            "Iteration 46, loss = 0.43401956\n",
            "Iteration 47, loss = 0.42612559\n",
            "Iteration 48, loss = 0.41992889\n",
            "Iteration 49, loss = 0.42457907\n",
            "Iteration 50, loss = 0.41380705\n",
            "Iteration 51, loss = 0.38836523\n",
            "Iteration 52, loss = 0.38933971\n",
            "Iteration 53, loss = 0.39599362\n",
            "Iteration 54, loss = 0.39251977\n",
            "Iteration 55, loss = 0.38950138\n",
            "Iteration 56, loss = 0.38181044\n",
            "Iteration 57, loss = 0.37210815\n",
            "Iteration 58, loss = 0.38044293\n",
            "Iteration 59, loss = 0.35956091\n",
            "Iteration 60, loss = 0.37915868\n",
            "Iteration 61, loss = 0.40291308\n",
            "Iteration 62, loss = 0.37645973\n",
            "Iteration 63, loss = 0.35137342\n",
            "Iteration 64, loss = 0.34376878\n",
            "Iteration 65, loss = 0.32931794\n",
            "Iteration 66, loss = 0.34154914\n",
            "Iteration 67, loss = 0.35150500\n",
            "Iteration 68, loss = 0.35658677\n",
            "Iteration 69, loss = 0.36390092\n",
            "Iteration 70, loss = 0.35936763\n",
            "Iteration 71, loss = 0.31979969\n",
            "Iteration 72, loss = 0.30750674\n",
            "Iteration 73, loss = 0.30817888\n",
            "Iteration 74, loss = 0.30207731\n",
            "Iteration 75, loss = 0.31260095\n",
            "Iteration 76, loss = 0.31525884\n",
            "Iteration 77, loss = 0.35105700\n",
            "Iteration 78, loss = 0.31422811\n",
            "Iteration 79, loss = 0.31385990\n",
            "Iteration 80, loss = 0.29019561\n",
            "Iteration 81, loss = 0.28162340\n",
            "Iteration 82, loss = 0.28066015\n",
            "Iteration 83, loss = 0.28712064\n",
            "Iteration 84, loss = 0.27336835\n",
            "Iteration 85, loss = 0.27799745\n",
            "Iteration 86, loss = 0.25273061\n",
            "Iteration 87, loss = 0.24558568\n",
            "Iteration 88, loss = 0.25577208\n",
            "Iteration 89, loss = 0.23657090\n",
            "Iteration 90, loss = 0.23802829\n",
            "Iteration 91, loss = 0.24288181\n",
            "Iteration 92, loss = 0.22551648\n",
            "Iteration 93, loss = 0.23055399\n",
            "Iteration 94, loss = 0.23501187\n",
            "Iteration 95, loss = 0.22540907\n",
            "Iteration 96, loss = 0.23121310\n",
            "Iteration 97, loss = 0.23261838\n",
            "Iteration 98, loss = 0.28657454\n",
            "Iteration 99, loss = 0.28132170\n",
            "Iteration 100, loss = 0.21260694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36225030\n",
            "Iteration 2, loss = 1.27177895\n",
            "Iteration 3, loss = 1.18571076\n",
            "Iteration 4, loss = 1.09586320\n",
            "Iteration 5, loss = 1.01519659\n",
            "Iteration 6, loss = 0.93877968\n",
            "Iteration 7, loss = 0.91241284\n",
            "Iteration 8, loss = 0.86332303\n",
            "Iteration 9, loss = 0.81640990\n",
            "Iteration 10, loss = 0.77903452\n",
            "Iteration 11, loss = 0.75133960\n",
            "Iteration 12, loss = 0.72220024\n",
            "Iteration 13, loss = 0.71190909\n",
            "Iteration 14, loss = 0.68311666\n",
            "Iteration 15, loss = 0.66075499\n",
            "Iteration 16, loss = 0.64177672\n",
            "Iteration 17, loss = 0.64294054\n",
            "Iteration 18, loss = 0.63717140\n",
            "Iteration 19, loss = 0.63920432\n",
            "Iteration 20, loss = 0.68158256\n",
            "Iteration 21, loss = 0.66085921\n",
            "Iteration 22, loss = 0.63630127\n",
            "Iteration 23, loss = 0.60338888\n",
            "Iteration 24, loss = 0.58164730\n",
            "Iteration 25, loss = 0.56294067\n",
            "Iteration 26, loss = 0.55667311\n",
            "Iteration 27, loss = 0.58873964\n",
            "Iteration 28, loss = 0.57237783\n",
            "Iteration 29, loss = 0.55491170\n",
            "Iteration 30, loss = 0.54338765\n",
            "Iteration 31, loss = 0.53176123\n",
            "Iteration 32, loss = 0.53435642\n",
            "Iteration 33, loss = 0.52547876\n",
            "Iteration 34, loss = 0.50766146\n",
            "Iteration 35, loss = 0.49536975\n",
            "Iteration 36, loss = 0.50803683\n",
            "Iteration 37, loss = 0.50133243\n",
            "Iteration 38, loss = 0.50382241\n",
            "Iteration 39, loss = 0.50685697\n",
            "Iteration 40, loss = 0.50072305\n",
            "Iteration 41, loss = 0.53489520\n",
            "Iteration 42, loss = 0.50785800\n",
            "Iteration 43, loss = 0.50684843\n",
            "Iteration 44, loss = 0.47001766\n",
            "Iteration 45, loss = 0.44624365\n",
            "Iteration 46, loss = 0.44440226\n",
            "Iteration 47, loss = 0.46105921\n",
            "Iteration 48, loss = 0.48063724\n",
            "Iteration 49, loss = 0.46378544\n",
            "Iteration 50, loss = 0.47726393\n",
            "Iteration 51, loss = 0.48302114\n",
            "Iteration 52, loss = 0.46886505\n",
            "Iteration 53, loss = 0.45655984\n",
            "Iteration 54, loss = 0.45006259\n",
            "Iteration 55, loss = 0.42764809\n",
            "Iteration 56, loss = 0.41860887\n",
            "Iteration 57, loss = 0.41045248\n",
            "Iteration 58, loss = 0.39512592\n",
            "Iteration 59, loss = 0.39612761\n",
            "Iteration 60, loss = 0.39333080\n",
            "Iteration 61, loss = 0.37683591\n",
            "Iteration 62, loss = 0.36724732\n",
            "Iteration 63, loss = 0.38599993\n",
            "Iteration 64, loss = 0.38402601\n",
            "Iteration 65, loss = 0.37209687\n",
            "Iteration 66, loss = 0.37231021\n",
            "Iteration 67, loss = 0.36325596\n",
            "Iteration 68, loss = 0.34788256\n",
            "Iteration 69, loss = 0.33807260\n",
            "Iteration 70, loss = 0.35686200\n",
            "Iteration 71, loss = 0.36245656\n",
            "Iteration 72, loss = 0.34280014\n",
            "Iteration 73, loss = 0.33288384\n",
            "Iteration 74, loss = 0.30698573\n",
            "Iteration 75, loss = 0.31847883\n",
            "Iteration 76, loss = 0.33833945\n",
            "Iteration 77, loss = 0.32446945\n",
            "Iteration 78, loss = 0.29525587\n",
            "Iteration 79, loss = 0.27891836\n",
            "Iteration 80, loss = 0.26798082\n",
            "Iteration 81, loss = 0.28810797\n",
            "Iteration 82, loss = 0.29700933\n",
            "Iteration 83, loss = 0.30018094\n",
            "Iteration 84, loss = 0.26423339\n",
            "Iteration 85, loss = 0.26515145\n",
            "Iteration 86, loss = 0.26181463\n",
            "Iteration 87, loss = 0.26174158\n",
            "Iteration 88, loss = 0.31860097\n",
            "Iteration 89, loss = 0.28671712\n",
            "Iteration 90, loss = 0.24310487\n",
            "Iteration 91, loss = 0.24520979\n",
            "Iteration 92, loss = 0.24397155\n",
            "Iteration 93, loss = 0.28137866\n",
            "Iteration 94, loss = 0.32110058\n",
            "Iteration 95, loss = 0.31568199\n",
            "Iteration 96, loss = 0.27136005\n",
            "Iteration 97, loss = 0.24614721\n",
            "Iteration 98, loss = 0.21436978\n",
            "Iteration 99, loss = 0.20373315\n",
            "Iteration 100, loss = 0.21243337\n",
            "33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38424839\n",
            "Iteration 2, loss = 1.35939965\n",
            "Iteration 3, loss = 1.34727832\n",
            "Iteration 4, loss = 1.33366048\n",
            "Iteration 5, loss = 1.32101869\n",
            "Iteration 6, loss = 1.30844443\n",
            "Iteration 7, loss = 1.30117669\n",
            "Iteration 8, loss = 1.29078672\n",
            "Iteration 9, loss = 1.28386486\n",
            "Iteration 10, loss = 1.28177513\n",
            "Iteration 11, loss = 1.26176134\n",
            "Iteration 12, loss = 1.27096755\n",
            "Iteration 13, loss = 1.25029160\n",
            "Iteration 14, loss = 1.24336015\n",
            "Iteration 15, loss = 1.24118901\n",
            "Iteration 16, loss = 1.23935977\n",
            "Iteration 17, loss = 1.23983181\n",
            "Iteration 18, loss = 1.22892761\n",
            "Iteration 19, loss = 1.23245391\n",
            "Iteration 20, loss = 1.22178208\n",
            "Iteration 21, loss = 1.21345539\n",
            "Iteration 22, loss = 1.20068350\n",
            "Iteration 23, loss = 1.19455607\n",
            "Iteration 24, loss = 1.18900838\n",
            "Iteration 25, loss = 1.20389758\n",
            "Iteration 26, loss = 1.18566325\n",
            "Iteration 27, loss = 1.17351055\n",
            "Iteration 28, loss = 1.17032141\n",
            "Iteration 29, loss = 1.15945409\n",
            "Iteration 30, loss = 1.16039926\n",
            "Iteration 31, loss = 1.14893868\n",
            "Iteration 32, loss = 1.15263216\n",
            "Iteration 33, loss = 1.13319266\n",
            "Iteration 34, loss = 1.16046213\n",
            "Iteration 35, loss = 1.14148999\n",
            "Iteration 36, loss = 1.13226837\n",
            "Iteration 37, loss = 1.10412160\n",
            "Iteration 38, loss = 1.10386968\n",
            "Iteration 39, loss = 1.11121410\n",
            "Iteration 40, loss = 1.09209231\n",
            "Iteration 41, loss = 1.08333089\n",
            "Iteration 42, loss = 1.10484472\n",
            "Iteration 43, loss = 1.08532027\n",
            "Iteration 44, loss = 1.10949364\n",
            "Iteration 45, loss = 1.10832713\n",
            "Iteration 46, loss = 1.08227744\n",
            "Iteration 47, loss = 1.04612278\n",
            "Iteration 48, loss = 1.05005607\n",
            "Iteration 49, loss = 1.02217771\n",
            "Iteration 50, loss = 1.01586948\n",
            "Iteration 51, loss = 1.02181178\n",
            "Iteration 52, loss = 1.00780789\n",
            "Iteration 53, loss = 0.99418523\n",
            "Iteration 54, loss = 0.97967037\n",
            "Iteration 55, loss = 1.01445242\n",
            "Iteration 56, loss = 0.99525326\n",
            "Iteration 57, loss = 1.00663190\n",
            "Iteration 58, loss = 0.97579531\n",
            "Iteration 59, loss = 0.95975678\n",
            "Iteration 60, loss = 0.96602985\n",
            "Iteration 61, loss = 0.93338370\n",
            "Iteration 62, loss = 0.93764234\n",
            "Iteration 63, loss = 0.91473290\n",
            "Iteration 64, loss = 0.89326641\n",
            "Iteration 65, loss = 0.90668927\n",
            "Iteration 66, loss = 0.91102357\n",
            "Iteration 67, loss = 0.89117492\n",
            "Iteration 68, loss = 0.86509263\n",
            "Iteration 69, loss = 0.86176480\n",
            "Iteration 70, loss = 0.85195732\n",
            "Iteration 71, loss = 0.84389663\n",
            "Iteration 72, loss = 0.84474820\n",
            "Iteration 73, loss = 0.83862319\n",
            "Iteration 74, loss = 0.83193469\n",
            "Iteration 75, loss = 0.82233993\n",
            "Iteration 76, loss = 0.83253839\n",
            "Iteration 77, loss = 0.83254602\n",
            "Iteration 78, loss = 0.78639834\n",
            "Iteration 79, loss = 0.78643490\n",
            "Iteration 80, loss = 0.77539302\n",
            "Iteration 81, loss = 0.77915992\n",
            "Iteration 82, loss = 0.74800064\n",
            "Iteration 83, loss = 0.74934451\n",
            "Iteration 84, loss = 0.74950980\n",
            "Iteration 85, loss = 0.75001545\n",
            "Iteration 86, loss = 0.74468339\n",
            "Iteration 87, loss = 0.73899470\n",
            "Iteration 88, loss = 0.72421385\n",
            "Iteration 89, loss = 0.70718237\n",
            "Iteration 90, loss = 0.68329946\n",
            "Iteration 91, loss = 0.68133376\n",
            "Iteration 92, loss = 0.66802373\n",
            "Iteration 93, loss = 0.68863274\n",
            "Iteration 94, loss = 0.67597081\n",
            "Iteration 95, loss = 0.65244458\n",
            "Iteration 96, loss = 0.62958639\n",
            "Iteration 97, loss = 0.63324735\n",
            "Iteration 98, loss = 0.64209257\n",
            "Iteration 99, loss = 0.65876829\n",
            "Iteration 100, loss = 0.68375825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38669268\n",
            "Iteration 2, loss = 1.36526541\n",
            "Iteration 3, loss = 1.34958205\n",
            "Iteration 4, loss = 1.33204656\n",
            "Iteration 5, loss = 1.31686009\n",
            "Iteration 6, loss = 1.30862371\n",
            "Iteration 7, loss = 1.31850269\n",
            "Iteration 8, loss = 1.30297096\n",
            "Iteration 9, loss = 1.29077412\n",
            "Iteration 10, loss = 1.27866477\n",
            "Iteration 11, loss = 1.26926575\n",
            "Iteration 12, loss = 1.26062687\n",
            "Iteration 13, loss = 1.25657762\n",
            "Iteration 14, loss = 1.24377569\n",
            "Iteration 15, loss = 1.26085903\n",
            "Iteration 16, loss = 1.24796696\n",
            "Iteration 17, loss = 1.25407741\n",
            "Iteration 18, loss = 1.23911462\n",
            "Iteration 19, loss = 1.23964644\n",
            "Iteration 20, loss = 1.22974231\n",
            "Iteration 21, loss = 1.21991690\n",
            "Iteration 22, loss = 1.21339904\n",
            "Iteration 23, loss = 1.20887405\n",
            "Iteration 24, loss = 1.20623504\n",
            "Iteration 25, loss = 1.21959537\n",
            "Iteration 26, loss = 1.19930197\n",
            "Iteration 27, loss = 1.18488533\n",
            "Iteration 28, loss = 1.18687581\n",
            "Iteration 29, loss = 1.16974493\n",
            "Iteration 30, loss = 1.17220683\n",
            "Iteration 31, loss = 1.17465385\n",
            "Iteration 32, loss = 1.15403860\n",
            "Iteration 33, loss = 1.15097230\n",
            "Iteration 34, loss = 1.17445807\n",
            "Iteration 35, loss = 1.15341773\n",
            "Iteration 36, loss = 1.13445421\n",
            "Iteration 37, loss = 1.12337399\n",
            "Iteration 38, loss = 1.12561867\n",
            "Iteration 39, loss = 1.13097310\n",
            "Iteration 40, loss = 1.11595635\n",
            "Iteration 41, loss = 1.10751233\n",
            "Iteration 42, loss = 1.09491346\n",
            "Iteration 43, loss = 1.08985063\n",
            "Iteration 44, loss = 1.11179553\n",
            "Iteration 45, loss = 1.09052578\n",
            "Iteration 46, loss = 1.09734059\n",
            "Iteration 47, loss = 1.05707657\n",
            "Iteration 48, loss = 1.05183056\n",
            "Iteration 49, loss = 1.04388401\n",
            "Iteration 50, loss = 1.02362291\n",
            "Iteration 51, loss = 1.05451263\n",
            "Iteration 52, loss = 1.03732318\n",
            "Iteration 53, loss = 0.99827416\n",
            "Iteration 54, loss = 0.99229335\n",
            "Iteration 55, loss = 1.02032248\n",
            "Iteration 56, loss = 0.97669510\n",
            "Iteration 57, loss = 0.97780709\n",
            "Iteration 58, loss = 0.97348901\n",
            "Iteration 59, loss = 0.94703778\n",
            "Iteration 60, loss = 1.00583030\n",
            "Iteration 61, loss = 0.95929438\n",
            "Iteration 62, loss = 0.94686508\n",
            "Iteration 63, loss = 0.95915071\n",
            "Iteration 64, loss = 0.95544902\n",
            "Iteration 65, loss = 0.92463321\n",
            "Iteration 66, loss = 0.90380405\n",
            "Iteration 67, loss = 0.89477227\n",
            "Iteration 68, loss = 0.88523165\n",
            "Iteration 69, loss = 0.88182740\n",
            "Iteration 70, loss = 0.85606487\n",
            "Iteration 71, loss = 0.84998754\n",
            "Iteration 72, loss = 0.87043760\n",
            "Iteration 73, loss = 0.83672093\n",
            "Iteration 74, loss = 0.85097799\n",
            "Iteration 75, loss = 0.86450100\n",
            "Iteration 76, loss = 0.84884863\n",
            "Iteration 77, loss = 0.85835771\n",
            "Iteration 78, loss = 0.81621735\n",
            "Iteration 79, loss = 0.82586474\n",
            "Iteration 80, loss = 0.81235489\n",
            "Iteration 81, loss = 0.81620225\n",
            "Iteration 82, loss = 0.79873465\n",
            "Iteration 83, loss = 0.80645258\n",
            "Iteration 84, loss = 0.79899550\n",
            "Iteration 85, loss = 0.77259576\n",
            "Iteration 86, loss = 0.79454285\n",
            "Iteration 87, loss = 0.75855485\n",
            "Iteration 88, loss = 0.74049744\n",
            "Iteration 89, loss = 0.73127892\n",
            "Iteration 90, loss = 0.73097024\n",
            "Iteration 91, loss = 0.73298114\n",
            "Iteration 92, loss = 0.71994081\n",
            "Iteration 93, loss = 0.72315717\n",
            "Iteration 94, loss = 0.70984891\n",
            "Iteration 95, loss = 0.70340747\n",
            "Iteration 96, loss = 0.69243372\n",
            "Iteration 97, loss = 0.68852815\n",
            "Iteration 98, loss = 0.67653625\n",
            "Iteration 99, loss = 0.71180458\n",
            "Iteration 100, loss = 0.73229776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39251447\n",
            "Iteration 2, loss = 1.38097632\n",
            "Iteration 3, loss = 1.36435258\n",
            "Iteration 4, loss = 1.34671423\n",
            "Iteration 5, loss = 1.33162566\n",
            "Iteration 6, loss = 1.31550335\n",
            "Iteration 7, loss = 1.31867677\n",
            "Iteration 8, loss = 1.30806111\n",
            "Iteration 9, loss = 1.30531261\n",
            "Iteration 10, loss = 1.29639353\n",
            "Iteration 11, loss = 1.28021983\n",
            "Iteration 12, loss = 1.26489816\n",
            "Iteration 13, loss = 1.26396362\n",
            "Iteration 14, loss = 1.25628136\n",
            "Iteration 15, loss = 1.26792069\n",
            "Iteration 16, loss = 1.26352018\n",
            "Iteration 17, loss = 1.26974577\n",
            "Iteration 18, loss = 1.25300254\n",
            "Iteration 19, loss = 1.25261149\n",
            "Iteration 20, loss = 1.23631161\n",
            "Iteration 21, loss = 1.23343779\n",
            "Iteration 22, loss = 1.21894404\n",
            "Iteration 23, loss = 1.21599486\n",
            "Iteration 24, loss = 1.21700657\n",
            "Iteration 25, loss = 1.23407689\n",
            "Iteration 26, loss = 1.21711471\n",
            "Iteration 27, loss = 1.20413674\n",
            "Iteration 28, loss = 1.20701915\n",
            "Iteration 29, loss = 1.19181593\n",
            "Iteration 30, loss = 1.19536944\n",
            "Iteration 31, loss = 1.20174850\n",
            "Iteration 32, loss = 1.18274193\n",
            "Iteration 33, loss = 1.17206230\n",
            "Iteration 34, loss = 1.18550532\n",
            "Iteration 35, loss = 1.16631244\n",
            "Iteration 36, loss = 1.14895778\n",
            "Iteration 37, loss = 1.13697031\n",
            "Iteration 38, loss = 1.14663677\n",
            "Iteration 39, loss = 1.14284659\n",
            "Iteration 40, loss = 1.12624572\n",
            "Iteration 41, loss = 1.11221969\n",
            "Iteration 42, loss = 1.11641958\n",
            "Iteration 43, loss = 1.11469903\n",
            "Iteration 44, loss = 1.14751257\n",
            "Iteration 45, loss = 1.13307949\n",
            "Iteration 46, loss = 1.13778056\n",
            "Iteration 47, loss = 1.10532385\n",
            "Iteration 48, loss = 1.08855297\n",
            "Iteration 49, loss = 1.07424235\n",
            "Iteration 50, loss = 1.06432089\n",
            "Iteration 51, loss = 1.06817815\n",
            "Iteration 52, loss = 1.05765643\n",
            "Iteration 53, loss = 1.03787707\n",
            "Iteration 54, loss = 1.02575285\n",
            "Iteration 55, loss = 1.04618193\n",
            "Iteration 56, loss = 1.01761454\n",
            "Iteration 57, loss = 1.00360322\n",
            "Iteration 58, loss = 1.01090618\n",
            "Iteration 59, loss = 1.01170579\n",
            "Iteration 60, loss = 1.01870334\n",
            "Iteration 61, loss = 0.98116970\n",
            "Iteration 62, loss = 0.99239476\n",
            "Iteration 63, loss = 1.00808276\n",
            "Iteration 64, loss = 0.99816943\n",
            "Iteration 65, loss = 0.97882253\n",
            "Iteration 66, loss = 0.96600519\n",
            "Iteration 67, loss = 0.93186159\n",
            "Iteration 68, loss = 0.93669236\n",
            "Iteration 69, loss = 0.92531302\n",
            "Iteration 70, loss = 0.94663689\n",
            "Iteration 71, loss = 0.93226147\n",
            "Iteration 72, loss = 0.90111601\n",
            "Iteration 73, loss = 0.89304128\n",
            "Iteration 74, loss = 0.89103889\n",
            "Iteration 75, loss = 0.89505059\n",
            "Iteration 76, loss = 0.90513413\n",
            "Iteration 77, loss = 0.90501229\n",
            "Iteration 78, loss = 0.85735278\n",
            "Iteration 79, loss = 0.85027678\n",
            "Iteration 80, loss = 0.85172679\n",
            "Iteration 81, loss = 0.85165891\n",
            "Iteration 82, loss = 0.82644494\n",
            "Iteration 83, loss = 0.81299409\n",
            "Iteration 84, loss = 0.82830339\n",
            "Iteration 85, loss = 0.80531745\n",
            "Iteration 86, loss = 0.81448435\n",
            "Iteration 87, loss = 0.83749114\n",
            "Iteration 88, loss = 0.79593643\n",
            "Iteration 89, loss = 0.79835645\n",
            "Iteration 90, loss = 0.80039013\n",
            "Iteration 91, loss = 0.77260027\n",
            "Iteration 92, loss = 0.75810522\n",
            "Iteration 93, loss = 0.75356954\n",
            "Iteration 94, loss = 0.76800102\n",
            "Iteration 95, loss = 0.75570254\n",
            "Iteration 96, loss = 0.71271192\n",
            "Iteration 97, loss = 0.71511596\n",
            "Iteration 98, loss = 0.71595922\n",
            "Iteration 99, loss = 0.73339189\n",
            "Iteration 100, loss = 0.72501807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39070243\n",
            "Iteration 2, loss = 1.37469966\n",
            "Iteration 3, loss = 1.35510394\n",
            "Iteration 4, loss = 1.34021724\n",
            "Iteration 5, loss = 1.32369385\n",
            "Iteration 6, loss = 1.31164076\n",
            "Iteration 7, loss = 1.31266001\n",
            "Iteration 8, loss = 1.29687737\n",
            "Iteration 9, loss = 1.29444032\n",
            "Iteration 10, loss = 1.29421021\n",
            "Iteration 11, loss = 1.28235038\n",
            "Iteration 12, loss = 1.26968807\n",
            "Iteration 13, loss = 1.26286996\n",
            "Iteration 14, loss = 1.25857281\n",
            "Iteration 15, loss = 1.26098804\n",
            "Iteration 16, loss = 1.25383775\n",
            "Iteration 17, loss = 1.24564074\n",
            "Iteration 18, loss = 1.24284026\n",
            "Iteration 19, loss = 1.25131752\n",
            "Iteration 20, loss = 1.26232871\n",
            "Iteration 21, loss = 1.24843096\n",
            "Iteration 22, loss = 1.23402683\n",
            "Iteration 23, loss = 1.22370576\n",
            "Iteration 24, loss = 1.21195190\n",
            "Iteration 25, loss = 1.21259837\n",
            "Iteration 26, loss = 1.20831362\n",
            "Iteration 27, loss = 1.19490611\n",
            "Iteration 28, loss = 1.20336264\n",
            "Iteration 29, loss = 1.18410821\n",
            "Iteration 30, loss = 1.18379553\n",
            "Iteration 31, loss = 1.18257820\n",
            "Iteration 32, loss = 1.17518139\n",
            "Iteration 33, loss = 1.18625415\n",
            "Iteration 34, loss = 1.19608749\n",
            "Iteration 35, loss = 1.15678190\n",
            "Iteration 36, loss = 1.15306574\n",
            "Iteration 37, loss = 1.13362039\n",
            "Iteration 38, loss = 1.13068250\n",
            "Iteration 39, loss = 1.12168304\n",
            "Iteration 40, loss = 1.11108415\n",
            "Iteration 41, loss = 1.09602329\n",
            "Iteration 42, loss = 1.09818259\n",
            "Iteration 43, loss = 1.10560078\n",
            "Iteration 44, loss = 1.12266313\n",
            "Iteration 45, loss = 1.10744343\n",
            "Iteration 46, loss = 1.11398394\n",
            "Iteration 47, loss = 1.07993561\n",
            "Iteration 48, loss = 1.07836643\n",
            "Iteration 49, loss = 1.05999883\n",
            "Iteration 50, loss = 1.05490507\n",
            "Iteration 51, loss = 1.04516063\n",
            "Iteration 52, loss = 1.03970579\n",
            "Iteration 53, loss = 1.05283786\n",
            "Iteration 54, loss = 1.02735044\n",
            "Iteration 55, loss = 0.99641007\n",
            "Iteration 56, loss = 0.99346795\n",
            "Iteration 57, loss = 0.99059073\n",
            "Iteration 58, loss = 1.03202903\n",
            "Iteration 59, loss = 0.99732784\n",
            "Iteration 60, loss = 0.97534368\n",
            "Iteration 61, loss = 0.95349654\n",
            "Iteration 62, loss = 0.97233080\n",
            "Iteration 63, loss = 0.96047844\n",
            "Iteration 64, loss = 0.94991487\n",
            "Iteration 65, loss = 0.92689128\n",
            "Iteration 66, loss = 0.93822109\n",
            "Iteration 67, loss = 0.92889694\n",
            "Iteration 68, loss = 0.92868419\n",
            "Iteration 69, loss = 0.93636956\n",
            "Iteration 70, loss = 0.94235409\n",
            "Iteration 71, loss = 0.90795848\n",
            "Iteration 72, loss = 0.88314279\n",
            "Iteration 73, loss = 0.88183691\n",
            "Iteration 74, loss = 0.89184749\n",
            "Iteration 75, loss = 0.91568166\n",
            "Iteration 76, loss = 0.89182221\n",
            "Iteration 77, loss = 0.85389903\n",
            "Iteration 78, loss = 0.84172355\n",
            "Iteration 79, loss = 0.85472662\n",
            "Iteration 80, loss = 0.83360871\n",
            "Iteration 81, loss = 0.81231247\n",
            "Iteration 82, loss = 0.83615521\n",
            "Iteration 83, loss = 0.79767233\n",
            "Iteration 84, loss = 0.82732925\n",
            "Iteration 85, loss = 0.78551116\n",
            "Iteration 86, loss = 0.78466159\n",
            "Iteration 87, loss = 0.77574967\n",
            "Iteration 88, loss = 0.76092708\n",
            "Iteration 89, loss = 0.75269154\n",
            "Iteration 90, loss = 0.75815364\n",
            "Iteration 91, loss = 0.75836296\n",
            "Iteration 92, loss = 0.73241404\n",
            "Iteration 93, loss = 0.74249391\n",
            "Iteration 94, loss = 0.73262615\n",
            "Iteration 95, loss = 0.72570361\n",
            "Iteration 96, loss = 0.70732107\n",
            "Iteration 97, loss = 0.73980076\n",
            "Iteration 98, loss = 0.72713095\n",
            "Iteration 99, loss = 0.73177262\n",
            "Iteration 100, loss = 0.71624202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38757035\n",
            "Iteration 2, loss = 1.36529927\n",
            "Iteration 3, loss = 1.35305258\n",
            "Iteration 4, loss = 1.33414302\n",
            "Iteration 5, loss = 1.32205178\n",
            "Iteration 6, loss = 1.31324479\n",
            "Iteration 7, loss = 1.29985462\n",
            "Iteration 8, loss = 1.29153617\n",
            "Iteration 9, loss = 1.28619533\n",
            "Iteration 10, loss = 1.28797295\n",
            "Iteration 11, loss = 1.27277337\n",
            "Iteration 12, loss = 1.26335624\n",
            "Iteration 13, loss = 1.25329297\n",
            "Iteration 14, loss = 1.24741219\n",
            "Iteration 15, loss = 1.24760661\n",
            "Iteration 16, loss = 1.24350878\n",
            "Iteration 17, loss = 1.22880168\n",
            "Iteration 18, loss = 1.22998063\n",
            "Iteration 19, loss = 1.23697854\n",
            "Iteration 20, loss = 1.24914538\n",
            "Iteration 21, loss = 1.23344711\n",
            "Iteration 22, loss = 1.22999795\n",
            "Iteration 23, loss = 1.21516405\n",
            "Iteration 24, loss = 1.20782236\n",
            "Iteration 25, loss = 1.20955012\n",
            "Iteration 26, loss = 1.20197690\n",
            "Iteration 27, loss = 1.19299820\n",
            "Iteration 28, loss = 1.19715350\n",
            "Iteration 29, loss = 1.18535071\n",
            "Iteration 30, loss = 1.18269688\n",
            "Iteration 31, loss = 1.19474166\n",
            "Iteration 32, loss = 1.17082441\n",
            "Iteration 33, loss = 1.18786613\n",
            "Iteration 34, loss = 1.19239301\n",
            "Iteration 35, loss = 1.15552256\n",
            "Iteration 36, loss = 1.14946058\n",
            "Iteration 37, loss = 1.14130797\n",
            "Iteration 38, loss = 1.13661698\n",
            "Iteration 39, loss = 1.13136080\n",
            "Iteration 40, loss = 1.12284131\n",
            "Iteration 41, loss = 1.10625894\n",
            "Iteration 42, loss = 1.11334606\n",
            "Iteration 43, loss = 1.12108331\n",
            "Iteration 44, loss = 1.12346134\n",
            "Iteration 45, loss = 1.11340139\n",
            "Iteration 46, loss = 1.10774005\n",
            "Iteration 47, loss = 1.08853342\n",
            "Iteration 48, loss = 1.09961281\n",
            "Iteration 49, loss = 1.09715998\n",
            "Iteration 50, loss = 1.07604906\n",
            "Iteration 51, loss = 1.04306528\n",
            "Iteration 52, loss = 1.04025580\n",
            "Iteration 53, loss = 1.04859818\n",
            "Iteration 54, loss = 1.02675900\n",
            "Iteration 55, loss = 1.02216801\n",
            "Iteration 56, loss = 1.01322469\n",
            "Iteration 57, loss = 1.02162000\n",
            "Iteration 58, loss = 1.03394959\n",
            "Iteration 59, loss = 1.01693650\n",
            "Iteration 60, loss = 1.01176998\n",
            "Iteration 61, loss = 0.98694711\n",
            "Iteration 62, loss = 0.99116179\n",
            "Iteration 63, loss = 0.97200837\n",
            "Iteration 64, loss = 0.96857342\n",
            "Iteration 65, loss = 0.94660138\n",
            "Iteration 66, loss = 0.95242828\n",
            "Iteration 67, loss = 0.94562870\n",
            "Iteration 68, loss = 0.96839815\n",
            "Iteration 69, loss = 0.93586350\n",
            "Iteration 70, loss = 0.92766807\n",
            "Iteration 71, loss = 0.92503058\n",
            "Iteration 72, loss = 0.92586096\n",
            "Iteration 73, loss = 0.89407100\n",
            "Iteration 74, loss = 0.91120433\n",
            "Iteration 75, loss = 0.97214959\n",
            "Iteration 76, loss = 0.95979784\n",
            "Iteration 77, loss = 0.92437549\n",
            "Iteration 78, loss = 0.90543763\n",
            "Iteration 79, loss = 0.87631653\n",
            "Iteration 80, loss = 0.86699271\n",
            "Iteration 81, loss = 0.86877976\n",
            "Iteration 82, loss = 0.88760854\n",
            "Iteration 83, loss = 0.85642701\n",
            "Iteration 84, loss = 0.87830905\n",
            "Iteration 85, loss = 0.85457227\n",
            "Iteration 86, loss = 0.87370306\n",
            "Iteration 87, loss = 0.84435174\n",
            "Iteration 88, loss = 0.84427320\n",
            "Iteration 89, loss = 0.81794239\n",
            "Iteration 90, loss = 0.82960677\n",
            "Iteration 91, loss = 0.85700569\n",
            "Iteration 92, loss = 0.82382531\n",
            "Iteration 93, loss = 0.78378002\n",
            "Iteration 94, loss = 0.78387799\n",
            "Iteration 95, loss = 0.78057531\n",
            "Iteration 96, loss = 0.75881805\n",
            "Iteration 97, loss = 0.75971275\n",
            "Iteration 98, loss = 0.76439018\n",
            "Iteration 99, loss = 0.75759692\n",
            "Iteration 100, loss = 0.77844844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38507629\n",
            "Iteration 2, loss = 1.36812082\n",
            "Iteration 3, loss = 1.35985920\n",
            "Iteration 4, loss = 1.33980661\n",
            "Iteration 5, loss = 1.33203667\n",
            "Iteration 6, loss = 1.32112579\n",
            "Iteration 7, loss = 1.31256206\n",
            "Iteration 8, loss = 1.30695561\n",
            "Iteration 9, loss = 1.30947192\n",
            "Iteration 10, loss = 1.30241218\n",
            "Iteration 11, loss = 1.29196941\n",
            "Iteration 12, loss = 1.27938356\n",
            "Iteration 13, loss = 1.27713886\n",
            "Iteration 14, loss = 1.26909790\n",
            "Iteration 15, loss = 1.26853746\n",
            "Iteration 16, loss = 1.27434577\n",
            "Iteration 17, loss = 1.26346985\n",
            "Iteration 18, loss = 1.26851142\n",
            "Iteration 19, loss = 1.27256688\n",
            "Iteration 20, loss = 1.26904053\n",
            "Iteration 21, loss = 1.25759313\n",
            "Iteration 22, loss = 1.25349335\n",
            "Iteration 23, loss = 1.23891268\n",
            "Iteration 24, loss = 1.23575424\n",
            "Iteration 25, loss = 1.24136998\n",
            "Iteration 26, loss = 1.24603560\n",
            "Iteration 27, loss = 1.24482323\n",
            "Iteration 28, loss = 1.23170150\n",
            "Iteration 29, loss = 1.21565520\n",
            "Iteration 30, loss = 1.21874540\n",
            "Iteration 31, loss = 1.20757795\n",
            "Iteration 32, loss = 1.22345227\n",
            "Iteration 33, loss = 1.21310281\n",
            "Iteration 34, loss = 1.19778066\n",
            "Iteration 35, loss = 1.18073432\n",
            "Iteration 36, loss = 1.18865615\n",
            "Iteration 37, loss = 1.18950793\n",
            "Iteration 38, loss = 1.19044051\n",
            "Iteration 39, loss = 1.18404259\n",
            "Iteration 40, loss = 1.19206715\n",
            "Iteration 41, loss = 1.16754667\n",
            "Iteration 42, loss = 1.16042091\n",
            "Iteration 43, loss = 1.15791053\n",
            "Iteration 44, loss = 1.20365478\n",
            "Iteration 45, loss = 1.15369766\n",
            "Iteration 46, loss = 1.14597918\n",
            "Iteration 47, loss = 1.13082068\n",
            "Iteration 48, loss = 1.14743540\n",
            "Iteration 49, loss = 1.13101006\n",
            "Iteration 50, loss = 1.13495206\n",
            "Iteration 51, loss = 1.11091805\n",
            "Iteration 52, loss = 1.10873629\n",
            "Iteration 53, loss = 1.08039206\n",
            "Iteration 54, loss = 1.07709700\n",
            "Iteration 55, loss = 1.08838976\n",
            "Iteration 56, loss = 1.07360206\n",
            "Iteration 57, loss = 1.05622003\n",
            "Iteration 58, loss = 1.05779248\n",
            "Iteration 59, loss = 1.05592825\n",
            "Iteration 60, loss = 1.05939255\n",
            "Iteration 61, loss = 1.03876825\n",
            "Iteration 62, loss = 1.03629371\n",
            "Iteration 63, loss = 1.04510581\n",
            "Iteration 64, loss = 1.02266308\n",
            "Iteration 65, loss = 1.00006375\n",
            "Iteration 66, loss = 0.98241499\n",
            "Iteration 67, loss = 0.97815255\n",
            "Iteration 68, loss = 0.99869213\n",
            "Iteration 69, loss = 0.96677215\n",
            "Iteration 70, loss = 0.96243384\n",
            "Iteration 71, loss = 0.94878542\n",
            "Iteration 72, loss = 0.96422250\n",
            "Iteration 73, loss = 0.96405422\n",
            "Iteration 74, loss = 0.95982389\n",
            "Iteration 75, loss = 1.03268353\n",
            "Iteration 76, loss = 0.99772371\n",
            "Iteration 77, loss = 0.95755545\n",
            "Iteration 78, loss = 0.93249394\n",
            "Iteration 79, loss = 0.90909782\n",
            "Iteration 80, loss = 0.90212906\n",
            "Iteration 81, loss = 0.90292399\n",
            "Iteration 82, loss = 0.89578972\n",
            "Iteration 83, loss = 0.87369931\n",
            "Iteration 84, loss = 0.87003554\n",
            "Iteration 85, loss = 0.87023798\n",
            "Iteration 86, loss = 0.87992339\n",
            "Iteration 87, loss = 0.86510195\n",
            "Iteration 88, loss = 0.85245842\n",
            "Iteration 89, loss = 0.83835714\n",
            "Iteration 90, loss = 0.84106804\n",
            "Iteration 91, loss = 0.85818560\n",
            "Iteration 92, loss = 0.82577245\n",
            "Iteration 93, loss = 0.82134883\n",
            "Iteration 94, loss = 0.81039101\n",
            "Iteration 95, loss = 0.79912129\n",
            "Iteration 96, loss = 0.79025529\n",
            "Iteration 97, loss = 0.77502101\n",
            "Iteration 98, loss = 0.82439030\n",
            "Iteration 99, loss = 0.79791570\n",
            "Iteration 100, loss = 0.79328084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37171725\n",
            "Iteration 2, loss = 1.35896551\n",
            "Iteration 3, loss = 1.34445562\n",
            "Iteration 4, loss = 1.32851408\n",
            "Iteration 5, loss = 1.31929582\n",
            "Iteration 6, loss = 1.30690769\n",
            "Iteration 7, loss = 1.30082243\n",
            "Iteration 8, loss = 1.29289307\n",
            "Iteration 9, loss = 1.29186201\n",
            "Iteration 10, loss = 1.28710473\n",
            "Iteration 11, loss = 1.28289492\n",
            "Iteration 12, loss = 1.27602697\n",
            "Iteration 13, loss = 1.26350281\n",
            "Iteration 14, loss = 1.25461736\n",
            "Iteration 15, loss = 1.25344663\n",
            "Iteration 16, loss = 1.25326423\n",
            "Iteration 17, loss = 1.23960750\n",
            "Iteration 18, loss = 1.25032672\n",
            "Iteration 19, loss = 1.24675777\n",
            "Iteration 20, loss = 1.23760173\n",
            "Iteration 21, loss = 1.23578041\n",
            "Iteration 22, loss = 1.23043635\n",
            "Iteration 23, loss = 1.22656247\n",
            "Iteration 24, loss = 1.22165212\n",
            "Iteration 25, loss = 1.21779881\n",
            "Iteration 26, loss = 1.23836831\n",
            "Iteration 27, loss = 1.22640457\n",
            "Iteration 28, loss = 1.22052091\n",
            "Iteration 29, loss = 1.19655125\n",
            "Iteration 30, loss = 1.18887247\n",
            "Iteration 31, loss = 1.18364822\n",
            "Iteration 32, loss = 1.19485996\n",
            "Iteration 33, loss = 1.20063635\n",
            "Iteration 34, loss = 1.18332712\n",
            "Iteration 35, loss = 1.16762328\n",
            "Iteration 36, loss = 1.16740341\n",
            "Iteration 37, loss = 1.17228182\n",
            "Iteration 38, loss = 1.16724085\n",
            "Iteration 39, loss = 1.16497040\n",
            "Iteration 40, loss = 1.15372867\n",
            "Iteration 41, loss = 1.14263579\n",
            "Iteration 42, loss = 1.14340394\n",
            "Iteration 43, loss = 1.13562682\n",
            "Iteration 44, loss = 1.16345140\n",
            "Iteration 45, loss = 1.14301740\n",
            "Iteration 46, loss = 1.12255980\n",
            "Iteration 47, loss = 1.11327201\n",
            "Iteration 48, loss = 1.12163695\n",
            "Iteration 49, loss = 1.11624840\n",
            "Iteration 50, loss = 1.09871272\n",
            "Iteration 51, loss = 1.07150172\n",
            "Iteration 52, loss = 1.07376397\n",
            "Iteration 53, loss = 1.06168531\n",
            "Iteration 54, loss = 1.05643149\n",
            "Iteration 55, loss = 1.05274503\n",
            "Iteration 56, loss = 1.06540026\n",
            "Iteration 57, loss = 1.06942129\n",
            "Iteration 58, loss = 1.06348592\n",
            "Iteration 59, loss = 1.03545159\n",
            "Iteration 60, loss = 1.01272039\n",
            "Iteration 61, loss = 1.00516801\n",
            "Iteration 62, loss = 1.01115696\n",
            "Iteration 63, loss = 1.01802531\n",
            "Iteration 64, loss = 0.99806690\n",
            "Iteration 65, loss = 0.97956893\n",
            "Iteration 66, loss = 1.00877179\n",
            "Iteration 67, loss = 0.98760426\n",
            "Iteration 68, loss = 0.98781524\n",
            "Iteration 69, loss = 0.96974977\n",
            "Iteration 70, loss = 0.95447562\n",
            "Iteration 71, loss = 0.95061964\n",
            "Iteration 72, loss = 0.94165493\n",
            "Iteration 73, loss = 0.92491332\n",
            "Iteration 74, loss = 0.92491764\n",
            "Iteration 75, loss = 0.94731306\n",
            "Iteration 76, loss = 0.90679144\n",
            "Iteration 77, loss = 0.88593934\n",
            "Iteration 78, loss = 0.88569715\n",
            "Iteration 79, loss = 0.87435139\n",
            "Iteration 80, loss = 0.90430443\n",
            "Iteration 81, loss = 0.89336009\n",
            "Iteration 82, loss = 0.89505769\n",
            "Iteration 83, loss = 0.90144067\n",
            "Iteration 84, loss = 0.91458618\n",
            "Iteration 85, loss = 0.87137492\n",
            "Iteration 86, loss = 0.85836662\n",
            "Iteration 87, loss = 0.84250103\n",
            "Iteration 88, loss = 0.85089631\n",
            "Iteration 89, loss = 0.84141065\n",
            "Iteration 90, loss = 0.82682684\n",
            "Iteration 91, loss = 0.85496168\n",
            "Iteration 92, loss = 0.81706715\n",
            "Iteration 93, loss = 0.78451456\n",
            "Iteration 94, loss = 0.81051318\n",
            "Iteration 95, loss = 0.79421986\n",
            "Iteration 96, loss = 0.80635796\n",
            "Iteration 97, loss = 0.81342875\n",
            "Iteration 98, loss = 0.79322082\n",
            "Iteration 99, loss = 0.77285218\n",
            "Iteration 100, loss = 0.76774965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38706166\n",
            "Iteration 2, loss = 1.37816428\n",
            "Iteration 3, loss = 1.36334724\n",
            "Iteration 4, loss = 1.34409227\n",
            "Iteration 5, loss = 1.34244981\n",
            "Iteration 6, loss = 1.33049585\n",
            "Iteration 7, loss = 1.31113787\n",
            "Iteration 8, loss = 1.30812996\n",
            "Iteration 9, loss = 1.29633912\n",
            "Iteration 10, loss = 1.28834746\n",
            "Iteration 11, loss = 1.27651200\n",
            "Iteration 12, loss = 1.27838223\n",
            "Iteration 13, loss = 1.26789585\n",
            "Iteration 14, loss = 1.26535757\n",
            "Iteration 15, loss = 1.25158263\n",
            "Iteration 16, loss = 1.24874067\n",
            "Iteration 17, loss = 1.24415377\n",
            "Iteration 18, loss = 1.25780825\n",
            "Iteration 19, loss = 1.24302967\n",
            "Iteration 20, loss = 1.24255711\n",
            "Iteration 21, loss = 1.24329471\n",
            "Iteration 22, loss = 1.22930809\n",
            "Iteration 23, loss = 1.21439316\n",
            "Iteration 24, loss = 1.20849868\n",
            "Iteration 25, loss = 1.22055399\n",
            "Iteration 26, loss = 1.20857622\n",
            "Iteration 27, loss = 1.20381157\n",
            "Iteration 28, loss = 1.18897380\n",
            "Iteration 29, loss = 1.18262000\n",
            "Iteration 30, loss = 1.18425684\n",
            "Iteration 31, loss = 1.18205015\n",
            "Iteration 32, loss = 1.18317271\n",
            "Iteration 33, loss = 1.16926240\n",
            "Iteration 34, loss = 1.18522038\n",
            "Iteration 35, loss = 1.17665934\n",
            "Iteration 36, loss = 1.16584501\n",
            "Iteration 37, loss = 1.15222148\n",
            "Iteration 38, loss = 1.15008337\n",
            "Iteration 39, loss = 1.14520553\n",
            "Iteration 40, loss = 1.12999092\n",
            "Iteration 41, loss = 1.14579012\n",
            "Iteration 42, loss = 1.14776048\n",
            "Iteration 43, loss = 1.13291919\n",
            "Iteration 44, loss = 1.15125814\n",
            "Iteration 45, loss = 1.14816251\n",
            "Iteration 46, loss = 1.12122440\n",
            "Iteration 47, loss = 1.10074543\n",
            "Iteration 48, loss = 1.10844445\n",
            "Iteration 49, loss = 1.10381738\n",
            "Iteration 50, loss = 1.07029554\n",
            "Iteration 51, loss = 1.05597895\n",
            "Iteration 52, loss = 1.05076401\n",
            "Iteration 53, loss = 1.04431959\n",
            "Iteration 54, loss = 1.03072097\n",
            "Iteration 55, loss = 1.03071835\n",
            "Iteration 56, loss = 1.01910001\n",
            "Iteration 57, loss = 1.03622870\n",
            "Iteration 58, loss = 1.04823082\n",
            "Iteration 59, loss = 1.01867602\n",
            "Iteration 60, loss = 0.99954482\n",
            "Iteration 61, loss = 0.98579424\n",
            "Iteration 62, loss = 0.99005742\n",
            "Iteration 63, loss = 0.98914011\n",
            "Iteration 64, loss = 0.98311194\n",
            "Iteration 65, loss = 0.96760263\n",
            "Iteration 66, loss = 0.96086944\n",
            "Iteration 67, loss = 0.96417752\n",
            "Iteration 68, loss = 1.02854849\n",
            "Iteration 69, loss = 0.97604168\n",
            "Iteration 70, loss = 0.94345297\n",
            "Iteration 71, loss = 0.93404738\n",
            "Iteration 72, loss = 0.92787621\n",
            "Iteration 73, loss = 0.89563103\n",
            "Iteration 74, loss = 0.90744733\n",
            "Iteration 75, loss = 0.90703081\n",
            "Iteration 76, loss = 0.91320461\n",
            "Iteration 77, loss = 0.91865166\n",
            "Iteration 78, loss = 0.89449337\n",
            "Iteration 79, loss = 0.85765065\n",
            "Iteration 80, loss = 0.85331094\n",
            "Iteration 81, loss = 0.85240218\n",
            "Iteration 82, loss = 0.84866309\n",
            "Iteration 83, loss = 0.84520692\n",
            "Iteration 84, loss = 0.83727616\n",
            "Iteration 85, loss = 0.83869975\n",
            "Iteration 86, loss = 0.82580946\n",
            "Iteration 87, loss = 0.81512615\n",
            "Iteration 88, loss = 0.82140907\n",
            "Iteration 89, loss = 0.79590827\n",
            "Iteration 90, loss = 0.80012225\n",
            "Iteration 91, loss = 0.80393278\n",
            "Iteration 92, loss = 0.78265954\n",
            "Iteration 93, loss = 0.77221821\n",
            "Iteration 94, loss = 0.77670731\n",
            "Iteration 95, loss = 0.77302160\n",
            "Iteration 96, loss = 0.76576205\n",
            "Iteration 97, loss = 0.76730632\n",
            "Iteration 98, loss = 0.76805600\n",
            "Iteration 99, loss = 0.76222668\n",
            "Iteration 100, loss = 0.76115465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38842638\n",
            "Iteration 2, loss = 1.36392953\n",
            "Iteration 3, loss = 1.34524204\n",
            "Iteration 4, loss = 1.33004227\n",
            "Iteration 5, loss = 1.32041692\n",
            "Iteration 6, loss = 1.31736917\n",
            "Iteration 7, loss = 1.31878020\n",
            "Iteration 8, loss = 1.31002597\n",
            "Iteration 9, loss = 1.28796126\n",
            "Iteration 10, loss = 1.28842589\n",
            "Iteration 11, loss = 1.28039145\n",
            "Iteration 12, loss = 1.27883990\n",
            "Iteration 13, loss = 1.26844522\n",
            "Iteration 14, loss = 1.26344737\n",
            "Iteration 15, loss = 1.25293468\n",
            "Iteration 16, loss = 1.24917646\n",
            "Iteration 17, loss = 1.23774845\n",
            "Iteration 18, loss = 1.23435453\n",
            "Iteration 19, loss = 1.24300959\n",
            "Iteration 20, loss = 1.26110186\n",
            "Iteration 21, loss = 1.25041296\n",
            "Iteration 22, loss = 1.22867271\n",
            "Iteration 23, loss = 1.21671401\n",
            "Iteration 24, loss = 1.20854119\n",
            "Iteration 25, loss = 1.21850557\n",
            "Iteration 26, loss = 1.20155232\n",
            "Iteration 27, loss = 1.20567642\n",
            "Iteration 28, loss = 1.18822033\n",
            "Iteration 29, loss = 1.18646596\n",
            "Iteration 30, loss = 1.18359335\n",
            "Iteration 31, loss = 1.17347344\n",
            "Iteration 32, loss = 1.17241163\n",
            "Iteration 33, loss = 1.17025805\n",
            "Iteration 34, loss = 1.18615288\n",
            "Iteration 35, loss = 1.17260254\n",
            "Iteration 36, loss = 1.15744106\n",
            "Iteration 37, loss = 1.15160009\n",
            "Iteration 38, loss = 1.12846505\n",
            "Iteration 39, loss = 1.12190600\n",
            "Iteration 40, loss = 1.11927231\n",
            "Iteration 41, loss = 1.12588215\n",
            "Iteration 42, loss = 1.12611804\n",
            "Iteration 43, loss = 1.10126056\n",
            "Iteration 44, loss = 1.09880218\n",
            "Iteration 45, loss = 1.09605748\n",
            "Iteration 46, loss = 1.08748115\n",
            "Iteration 47, loss = 1.07225095\n",
            "Iteration 48, loss = 1.05675223\n",
            "Iteration 49, loss = 1.06501900\n",
            "Iteration 50, loss = 1.05055467\n",
            "Iteration 51, loss = 1.04320621\n",
            "Iteration 52, loss = 1.04413488\n",
            "Iteration 53, loss = 1.03520335\n",
            "Iteration 54, loss = 1.02792455\n",
            "Iteration 55, loss = 1.02310644\n",
            "Iteration 56, loss = 1.01153713\n",
            "Iteration 57, loss = 1.01604677\n",
            "Iteration 58, loss = 1.01822031\n",
            "Iteration 59, loss = 0.99560841\n",
            "Iteration 60, loss = 0.97932857\n",
            "Iteration 61, loss = 0.96198217\n",
            "Iteration 62, loss = 0.98419802\n",
            "Iteration 63, loss = 0.99097354\n",
            "Iteration 64, loss = 0.97911036\n",
            "Iteration 65, loss = 0.97507250\n",
            "Iteration 66, loss = 0.96150929\n",
            "Iteration 67, loss = 0.94455799\n",
            "Iteration 68, loss = 0.93207162\n",
            "Iteration 69, loss = 0.91106997\n",
            "Iteration 70, loss = 0.91201707\n",
            "Iteration 71, loss = 0.90978606\n",
            "Iteration 72, loss = 0.91455055\n",
            "Iteration 73, loss = 0.95337407\n",
            "Iteration 74, loss = 0.93564844\n",
            "Iteration 75, loss = 0.91400385\n",
            "Iteration 76, loss = 0.89146230\n",
            "Iteration 77, loss = 0.91337053\n",
            "Iteration 78, loss = 0.87584049\n",
            "Iteration 79, loss = 0.86657992\n",
            "Iteration 80, loss = 0.86553012\n",
            "Iteration 81, loss = 0.85600753\n",
            "Iteration 82, loss = 0.86479414\n",
            "Iteration 83, loss = 0.85514629\n",
            "Iteration 84, loss = 0.83772305\n",
            "Iteration 85, loss = 0.81769158\n",
            "Iteration 86, loss = 0.81833818\n",
            "Iteration 87, loss = 0.80297323\n",
            "Iteration 88, loss = 0.79354706\n",
            "Iteration 89, loss = 0.79323926\n",
            "Iteration 90, loss = 0.78610059\n",
            "Iteration 91, loss = 0.78941126\n",
            "Iteration 92, loss = 0.77350574\n",
            "Iteration 93, loss = 0.76470449\n",
            "Iteration 94, loss = 0.77042542\n",
            "Iteration 95, loss = 0.75705272\n",
            "Iteration 96, loss = 0.75985102\n",
            "Iteration 97, loss = 0.72738047\n",
            "Iteration 98, loss = 0.73582780\n",
            "Iteration 99, loss = 0.73172410\n",
            "Iteration 100, loss = 0.73419839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39453468\n",
            "Iteration 2, loss = 1.35487566\n",
            "Iteration 3, loss = 1.33567167\n",
            "Iteration 4, loss = 1.32098941\n",
            "Iteration 5, loss = 1.30599058\n",
            "Iteration 6, loss = 1.30566075\n",
            "Iteration 7, loss = 1.30444559\n",
            "Iteration 8, loss = 1.29476567\n",
            "Iteration 9, loss = 1.27799953\n",
            "Iteration 10, loss = 1.27255260\n",
            "Iteration 11, loss = 1.26309200\n",
            "Iteration 12, loss = 1.26008230\n",
            "Iteration 13, loss = 1.25345489\n",
            "Iteration 14, loss = 1.24566881\n",
            "Iteration 15, loss = 1.24007738\n",
            "Iteration 16, loss = 1.24399369\n",
            "Iteration 17, loss = 1.23956072\n",
            "Iteration 18, loss = 1.22614663\n",
            "Iteration 19, loss = 1.24174111\n",
            "Iteration 20, loss = 1.23756771\n",
            "Iteration 21, loss = 1.22491753\n",
            "Iteration 22, loss = 1.21106407\n",
            "Iteration 23, loss = 1.20422620\n",
            "Iteration 24, loss = 1.19616009\n",
            "Iteration 25, loss = 1.19887981\n",
            "Iteration 26, loss = 1.20179016\n",
            "Iteration 27, loss = 1.21418752\n",
            "Iteration 28, loss = 1.17796760\n",
            "Iteration 29, loss = 1.18191562\n",
            "Iteration 30, loss = 1.17541669\n",
            "Iteration 31, loss = 1.16209792\n",
            "Iteration 32, loss = 1.17525363\n",
            "Iteration 33, loss = 1.15712227\n",
            "Iteration 34, loss = 1.15763598\n",
            "Iteration 35, loss = 1.15964984\n",
            "Iteration 36, loss = 1.14932791\n",
            "Iteration 37, loss = 1.12465907\n",
            "Iteration 38, loss = 1.12086850\n",
            "Iteration 39, loss = 1.11241093\n",
            "Iteration 40, loss = 1.09836447\n",
            "Iteration 41, loss = 1.10150627\n",
            "Iteration 42, loss = 1.09577617\n",
            "Iteration 43, loss = 1.09325189\n",
            "Iteration 44, loss = 1.08721019\n",
            "Iteration 45, loss = 1.07895244\n",
            "Iteration 46, loss = 1.08168931\n",
            "Iteration 47, loss = 1.07421699\n",
            "Iteration 48, loss = 1.05410035\n",
            "Iteration 49, loss = 1.06773582\n",
            "Iteration 50, loss = 1.05691349\n",
            "Iteration 51, loss = 1.03575650\n",
            "Iteration 52, loss = 1.02662921\n",
            "Iteration 53, loss = 1.01061073\n",
            "Iteration 54, loss = 1.01724424\n",
            "Iteration 55, loss = 1.02172134\n",
            "Iteration 56, loss = 1.01890295\n",
            "Iteration 57, loss = 1.00018011\n",
            "Iteration 58, loss = 0.99785800\n",
            "Iteration 59, loss = 0.97630383\n",
            "Iteration 60, loss = 0.96803514\n",
            "Iteration 61, loss = 0.96104550\n",
            "Iteration 62, loss = 1.01121309\n",
            "Iteration 63, loss = 0.97381196\n",
            "Iteration 64, loss = 0.97076071\n",
            "Iteration 65, loss = 0.95290685\n",
            "Iteration 66, loss = 0.94430519\n",
            "Iteration 67, loss = 0.92650714\n",
            "Iteration 68, loss = 0.97349043\n",
            "Iteration 69, loss = 0.95324596\n",
            "Iteration 70, loss = 0.93934104\n",
            "Iteration 71, loss = 0.91930356\n",
            "Iteration 72, loss = 0.89559764\n",
            "Iteration 73, loss = 0.90664100\n",
            "Iteration 74, loss = 0.91667831\n",
            "Iteration 75, loss = 0.93675327\n",
            "Iteration 76, loss = 0.95668430\n",
            "Iteration 77, loss = 0.91732158\n",
            "Iteration 78, loss = 0.91620159\n",
            "Iteration 79, loss = 0.87852739\n",
            "Iteration 80, loss = 0.88940033\n",
            "Iteration 81, loss = 0.87658569\n",
            "Iteration 82, loss = 0.85868787\n",
            "Iteration 83, loss = 0.86050082\n",
            "Iteration 84, loss = 0.84740847\n",
            "Iteration 85, loss = 0.84487126\n",
            "Iteration 86, loss = 0.83891298\n",
            "Iteration 87, loss = 0.83283023\n",
            "Iteration 88, loss = 0.82949662\n",
            "Iteration 89, loss = 0.81532591\n",
            "Iteration 90, loss = 0.80686776\n",
            "Iteration 91, loss = 0.79912923\n",
            "Iteration 92, loss = 0.78242053\n",
            "Iteration 93, loss = 0.77613133\n",
            "Iteration 94, loss = 0.79475367\n",
            "Iteration 95, loss = 0.81322854\n",
            "Iteration 96, loss = 0.79350012\n",
            "Iteration 97, loss = 0.77976310\n",
            "Iteration 98, loss = 0.74750614\n",
            "Iteration 99, loss = 0.74548258\n",
            "Iteration 100, loss = 0.74733672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "Iteration 1, loss = 1.35823714\n",
            "Iteration 2, loss = 1.27282670\n",
            "Iteration 3, loss = 1.18905805\n",
            "Iteration 4, loss = 1.09973000\n",
            "Iteration 5, loss = 0.99175279\n",
            "Iteration 6, loss = 0.92520398\n",
            "Iteration 7, loss = 0.86612940\n",
            "Iteration 8, loss = 0.82796070\n",
            "Iteration 9, loss = 0.79227575\n",
            "Iteration 10, loss = 0.74973271\n",
            "Iteration 11, loss = 0.73715927\n",
            "Iteration 12, loss = 0.71459751\n",
            "Iteration 13, loss = 0.69004314\n",
            "Iteration 14, loss = 0.68728347\n",
            "Iteration 15, loss = 0.66549889\n",
            "Iteration 16, loss = 0.66175002\n",
            "Iteration 17, loss = 0.64543870\n",
            "Iteration 18, loss = 0.62141654\n",
            "Iteration 19, loss = 0.65915648\n",
            "Iteration 20, loss = 0.64472137\n",
            "Iteration 21, loss = 0.60695082\n",
            "Iteration 22, loss = 0.59314425\n",
            "Iteration 23, loss = 0.57277268\n",
            "Iteration 24, loss = 0.57912224\n",
            "Iteration 25, loss = 0.59759443\n",
            "Iteration 26, loss = 0.57397273\n",
            "Iteration 27, loss = 0.54706806\n",
            "Iteration 28, loss = 0.53346819\n",
            "Iteration 29, loss = 0.53587506\n",
            "Iteration 30, loss = 0.53823129\n",
            "Iteration 31, loss = 0.53087843\n",
            "Iteration 32, loss = 0.51293016\n",
            "Iteration 33, loss = 0.50070904\n",
            "Iteration 34, loss = 0.50006240\n",
            "Iteration 35, loss = 0.49473125\n",
            "Iteration 36, loss = 0.47252556\n",
            "Iteration 37, loss = 0.47998814\n",
            "Iteration 38, loss = 0.45998438\n",
            "Iteration 39, loss = 0.47363014\n",
            "Iteration 40, loss = 0.46480167\n",
            "Iteration 41, loss = 0.45490811\n",
            "Iteration 42, loss = 0.49714664\n",
            "Iteration 43, loss = 0.51724287\n",
            "Iteration 44, loss = 0.52093409\n",
            "Iteration 45, loss = 0.47146994\n",
            "Iteration 46, loss = 0.45046458\n",
            "Iteration 47, loss = 0.42450620\n",
            "Iteration 48, loss = 0.42944789\n",
            "Iteration 49, loss = 0.43821560\n",
            "Iteration 50, loss = 0.44641788\n",
            "Iteration 51, loss = 0.41045782\n",
            "Iteration 52, loss = 0.39694609\n",
            "Iteration 53, loss = 0.38876122\n",
            "Iteration 54, loss = 0.39623815\n",
            "Iteration 55, loss = 0.39189027\n",
            "Iteration 56, loss = 0.37641512\n",
            "Iteration 57, loss = 0.40010331\n",
            "Iteration 58, loss = 0.37504323\n",
            "Iteration 59, loss = 0.37532722\n",
            "Iteration 60, loss = 0.35532896\n",
            "Iteration 61, loss = 0.35157526\n",
            "Iteration 62, loss = 0.37403953\n",
            "Iteration 63, loss = 0.41105718\n",
            "Iteration 64, loss = 0.34846178\n",
            "Iteration 65, loss = 0.34606363\n",
            "Iteration 66, loss = 0.32444319\n",
            "Iteration 67, loss = 0.32697743\n",
            "Iteration 68, loss = 0.31795963\n",
            "Iteration 69, loss = 0.32816376\n",
            "Iteration 70, loss = 0.34686038\n",
            "Iteration 71, loss = 0.32665775\n",
            "Iteration 72, loss = 0.32133030\n",
            "Iteration 73, loss = 0.33800799\n",
            "Iteration 74, loss = 0.32849899\n",
            "Iteration 75, loss = 0.35384997\n",
            "Iteration 76, loss = 0.33560821\n",
            "Iteration 77, loss = 0.29918289\n",
            "Iteration 78, loss = 0.30110768\n",
            "Iteration 79, loss = 0.30813514\n",
            "Iteration 80, loss = 0.28145070\n",
            "Iteration 81, loss = 0.29748969\n",
            "Iteration 82, loss = 0.29461927\n",
            "Iteration 83, loss = 0.28659884\n",
            "Iteration 84, loss = 0.29969892\n",
            "Iteration 85, loss = 0.28957731\n",
            "Iteration 86, loss = 0.28327528\n",
            "Iteration 87, loss = 0.25973250\n",
            "Iteration 88, loss = 0.27016060\n",
            "Iteration 89, loss = 0.25687709\n",
            "Iteration 90, loss = 0.24671304\n",
            "Iteration 91, loss = 0.23880742\n",
            "Iteration 92, loss = 0.22763363\n",
            "Iteration 93, loss = 0.22431864\n",
            "Iteration 94, loss = 0.24587961\n",
            "Iteration 95, loss = 0.23497153\n",
            "Iteration 96, loss = 0.22226228\n",
            "Iteration 97, loss = 0.22125572\n",
            "Iteration 98, loss = 0.26439121\n",
            "Iteration 99, loss = 0.23571251\n",
            "Iteration 100, loss = 0.22338438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36001099\n",
            "Iteration 2, loss = 1.26999463\n",
            "Iteration 3, loss = 1.18357278\n",
            "Iteration 4, loss = 1.08355183\n",
            "Iteration 5, loss = 0.98811022\n",
            "Iteration 6, loss = 0.92544796\n",
            "Iteration 7, loss = 0.88502351\n",
            "Iteration 8, loss = 0.86028914\n",
            "Iteration 9, loss = 0.80986867\n",
            "Iteration 10, loss = 0.77013978\n",
            "Iteration 11, loss = 0.74209314\n",
            "Iteration 12, loss = 0.71955676\n",
            "Iteration 13, loss = 0.71321825\n",
            "Iteration 14, loss = 0.70680913\n",
            "Iteration 15, loss = 0.67611890\n",
            "Iteration 16, loss = 0.65897627\n",
            "Iteration 17, loss = 0.64739574\n",
            "Iteration 18, loss = 0.63465442\n",
            "Iteration 19, loss = 0.65369527\n",
            "Iteration 20, loss = 0.64902540\n",
            "Iteration 21, loss = 0.61489487\n",
            "Iteration 22, loss = 0.59813205\n",
            "Iteration 23, loss = 0.58082637\n",
            "Iteration 24, loss = 0.59440729\n",
            "Iteration 25, loss = 0.63179190\n",
            "Iteration 26, loss = 0.59643607\n",
            "Iteration 27, loss = 0.56216761\n",
            "Iteration 28, loss = 0.54277473\n",
            "Iteration 29, loss = 0.54055311\n",
            "Iteration 30, loss = 0.55332333\n",
            "Iteration 31, loss = 0.55975554\n",
            "Iteration 32, loss = 0.56497203\n",
            "Iteration 33, loss = 0.55346026\n",
            "Iteration 34, loss = 0.53260974\n",
            "Iteration 35, loss = 0.51140907\n",
            "Iteration 36, loss = 0.50399127\n",
            "Iteration 37, loss = 0.50447732\n",
            "Iteration 38, loss = 0.48841846\n",
            "Iteration 39, loss = 0.50007283\n",
            "Iteration 40, loss = 0.49113615\n",
            "Iteration 41, loss = 0.48351160\n",
            "Iteration 42, loss = 0.50811909\n",
            "Iteration 43, loss = 0.50440036\n",
            "Iteration 44, loss = 0.49396763\n",
            "Iteration 45, loss = 0.47253417\n",
            "Iteration 46, loss = 0.46856374\n",
            "Iteration 47, loss = 0.47141254\n",
            "Iteration 48, loss = 0.46049162\n",
            "Iteration 49, loss = 0.46734657\n",
            "Iteration 50, loss = 0.44694732\n",
            "Iteration 51, loss = 0.42353959\n",
            "Iteration 52, loss = 0.43058249\n",
            "Iteration 53, loss = 0.42405970\n",
            "Iteration 54, loss = 0.43295627\n",
            "Iteration 55, loss = 0.42530231\n",
            "Iteration 56, loss = 0.40094783\n",
            "Iteration 57, loss = 0.42049337\n",
            "Iteration 58, loss = 0.39971210\n",
            "Iteration 59, loss = 0.40172013\n",
            "Iteration 60, loss = 0.38613905\n",
            "Iteration 61, loss = 0.39021278\n",
            "Iteration 62, loss = 0.38888608\n",
            "Iteration 63, loss = 0.38264730\n",
            "Iteration 64, loss = 0.36551936\n",
            "Iteration 65, loss = 0.37749430\n",
            "Iteration 66, loss = 0.38099207\n",
            "Iteration 67, loss = 0.37610175\n",
            "Iteration 68, loss = 0.37174212\n",
            "Iteration 69, loss = 0.35995213\n",
            "Iteration 70, loss = 0.36748277\n",
            "Iteration 71, loss = 0.37513032\n",
            "Iteration 72, loss = 0.33947150\n",
            "Iteration 73, loss = 0.33456971\n",
            "Iteration 74, loss = 0.33873983\n",
            "Iteration 75, loss = 0.34881040\n",
            "Iteration 76, loss = 0.33737564\n",
            "Iteration 77, loss = 0.32818717\n",
            "Iteration 78, loss = 0.34001088\n",
            "Iteration 79, loss = 0.34013571\n",
            "Iteration 80, loss = 0.34049707\n",
            "Iteration 81, loss = 0.32843005\n",
            "Iteration 82, loss = 0.30683754\n",
            "Iteration 83, loss = 0.31307683\n",
            "Iteration 84, loss = 0.33016423\n",
            "Iteration 85, loss = 0.30661819\n",
            "Iteration 86, loss = 0.32271293\n",
            "Iteration 87, loss = 0.32524919\n",
            "Iteration 88, loss = 0.34031612\n",
            "Iteration 89, loss = 0.31570743\n",
            "Iteration 90, loss = 0.30930172\n",
            "Iteration 91, loss = 0.32713758\n",
            "Iteration 92, loss = 0.30251283\n",
            "Iteration 93, loss = 0.28411440\n",
            "Iteration 94, loss = 0.27816695\n",
            "Iteration 95, loss = 0.28707856\n",
            "Iteration 96, loss = 0.27678883\n",
            "Iteration 97, loss = 0.28589183\n",
            "Iteration 98, loss = 0.26221160\n",
            "Iteration 99, loss = 0.23604007\n",
            "Iteration 100, loss = 0.24520135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36106550\n",
            "Iteration 2, loss = 1.27967598\n",
            "Iteration 3, loss = 1.19490091\n",
            "Iteration 4, loss = 1.08086732\n",
            "Iteration 5, loss = 0.99055162\n",
            "Iteration 6, loss = 0.92484131\n",
            "Iteration 7, loss = 0.89448497\n",
            "Iteration 8, loss = 0.87362847\n",
            "Iteration 9, loss = 0.82905321\n",
            "Iteration 10, loss = 0.78680566\n",
            "Iteration 11, loss = 0.75754445\n",
            "Iteration 12, loss = 0.73135301\n",
            "Iteration 13, loss = 0.71423203\n",
            "Iteration 14, loss = 0.70071626\n",
            "Iteration 15, loss = 0.67935482\n",
            "Iteration 16, loss = 0.66982611\n",
            "Iteration 17, loss = 0.64672046\n",
            "Iteration 18, loss = 0.64745383\n",
            "Iteration 19, loss = 0.66102424\n",
            "Iteration 20, loss = 0.63997984\n",
            "Iteration 21, loss = 0.61436122\n",
            "Iteration 22, loss = 0.60309009\n",
            "Iteration 23, loss = 0.59118465\n",
            "Iteration 24, loss = 0.59683691\n",
            "Iteration 25, loss = 0.63416224\n",
            "Iteration 26, loss = 0.59503524\n",
            "Iteration 27, loss = 0.56891231\n",
            "Iteration 28, loss = 0.55942325\n",
            "Iteration 29, loss = 0.55244928\n",
            "Iteration 30, loss = 0.56551303\n",
            "Iteration 31, loss = 0.60101845\n",
            "Iteration 32, loss = 0.56929230\n",
            "Iteration 33, loss = 0.54358341\n",
            "Iteration 34, loss = 0.51990088\n",
            "Iteration 35, loss = 0.51152844\n",
            "Iteration 36, loss = 0.50620882\n",
            "Iteration 37, loss = 0.50365464\n",
            "Iteration 38, loss = 0.49237073\n",
            "Iteration 39, loss = 0.48724062\n",
            "Iteration 40, loss = 0.48356935\n",
            "Iteration 41, loss = 0.47777057\n",
            "Iteration 42, loss = 0.46376262\n",
            "Iteration 43, loss = 0.48412174\n",
            "Iteration 44, loss = 0.49584450\n",
            "Iteration 45, loss = 0.46064367\n",
            "Iteration 46, loss = 0.46661876\n",
            "Iteration 47, loss = 0.46103330\n",
            "Iteration 48, loss = 0.46778179\n",
            "Iteration 49, loss = 0.45900638\n",
            "Iteration 50, loss = 0.44577320\n",
            "Iteration 51, loss = 0.44211256\n",
            "Iteration 52, loss = 0.43659914\n",
            "Iteration 53, loss = 0.43067189\n",
            "Iteration 54, loss = 0.43078211\n",
            "Iteration 55, loss = 0.43265938\n",
            "Iteration 56, loss = 0.39579757\n",
            "Iteration 57, loss = 0.40162346\n",
            "Iteration 58, loss = 0.39337534\n",
            "Iteration 59, loss = 0.39760023\n",
            "Iteration 60, loss = 0.38293962\n",
            "Iteration 61, loss = 0.38897186\n",
            "Iteration 62, loss = 0.39953032\n",
            "Iteration 63, loss = 0.37721330\n",
            "Iteration 64, loss = 0.36318270\n",
            "Iteration 65, loss = 0.36626898\n",
            "Iteration 66, loss = 0.40277173\n",
            "Iteration 67, loss = 0.37123194\n",
            "Iteration 68, loss = 0.35094208\n",
            "Iteration 69, loss = 0.33843231\n",
            "Iteration 70, loss = 0.33402506\n",
            "Iteration 71, loss = 0.34903405\n",
            "Iteration 72, loss = 0.34554830\n",
            "Iteration 73, loss = 0.34704302\n",
            "Iteration 74, loss = 0.36409375\n",
            "Iteration 75, loss = 0.37939674\n",
            "Iteration 76, loss = 0.36668184\n",
            "Iteration 77, loss = 0.36138142\n",
            "Iteration 78, loss = 0.35406249\n",
            "Iteration 79, loss = 0.32576212\n",
            "Iteration 80, loss = 0.32592080\n",
            "Iteration 81, loss = 0.30752708\n",
            "Iteration 82, loss = 0.30685059\n",
            "Iteration 83, loss = 0.30627301\n",
            "Iteration 84, loss = 0.32253622\n",
            "Iteration 85, loss = 0.29195346\n",
            "Iteration 86, loss = 0.29168617\n",
            "Iteration 87, loss = 0.29736294\n",
            "Iteration 88, loss = 0.29252568\n",
            "Iteration 89, loss = 0.26887535\n",
            "Iteration 90, loss = 0.26681590\n",
            "Iteration 91, loss = 0.27065879\n",
            "Iteration 92, loss = 0.27946089\n",
            "Iteration 93, loss = 0.28646528\n",
            "Iteration 94, loss = 0.35975766\n",
            "Iteration 95, loss = 0.36772467\n",
            "Iteration 96, loss = 0.31055204\n",
            "Iteration 97, loss = 0.29275725\n",
            "Iteration 98, loss = 0.26101525\n",
            "Iteration 99, loss = 0.23987712\n",
            "Iteration 100, loss = 0.24532786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36204639\n",
            "Iteration 2, loss = 1.28104756\n",
            "Iteration 3, loss = 1.18904915\n",
            "Iteration 4, loss = 1.08495817\n",
            "Iteration 5, loss = 0.98766412\n",
            "Iteration 6, loss = 0.90720114\n",
            "Iteration 7, loss = 0.89474328\n",
            "Iteration 8, loss = 0.84933798\n",
            "Iteration 9, loss = 0.80900859\n",
            "Iteration 10, loss = 0.77586393\n",
            "Iteration 11, loss = 0.74676991\n",
            "Iteration 12, loss = 0.73632936\n",
            "Iteration 13, loss = 0.71953559\n",
            "Iteration 14, loss = 0.70420728\n",
            "Iteration 15, loss = 0.67709090\n",
            "Iteration 16, loss = 0.66397178\n",
            "Iteration 17, loss = 0.65461191\n",
            "Iteration 18, loss = 0.66635117\n",
            "Iteration 19, loss = 0.64404555\n",
            "Iteration 20, loss = 0.66179941\n",
            "Iteration 21, loss = 0.64813724\n",
            "Iteration 22, loss = 0.62099173\n",
            "Iteration 23, loss = 0.59570946\n",
            "Iteration 24, loss = 0.58299542\n",
            "Iteration 25, loss = 0.61353836\n",
            "Iteration 26, loss = 0.60959107\n",
            "Iteration 27, loss = 0.59804125\n",
            "Iteration 28, loss = 0.55903970\n",
            "Iteration 29, loss = 0.56504212\n",
            "Iteration 30, loss = 0.58181125\n",
            "Iteration 31, loss = 0.58315141\n",
            "Iteration 32, loss = 0.56352184\n",
            "Iteration 33, loss = 0.53371365\n",
            "Iteration 34, loss = 0.51876939\n",
            "Iteration 35, loss = 0.51692785\n",
            "Iteration 36, loss = 0.50556359\n",
            "Iteration 37, loss = 0.50904904\n",
            "Iteration 38, loss = 0.51131729\n",
            "Iteration 39, loss = 0.50107229\n",
            "Iteration 40, loss = 0.48877649\n",
            "Iteration 41, loss = 0.48911718\n",
            "Iteration 42, loss = 0.49943888\n",
            "Iteration 43, loss = 0.50719428\n",
            "Iteration 44, loss = 0.53435725\n",
            "Iteration 45, loss = 0.49587444\n",
            "Iteration 46, loss = 0.49533894\n",
            "Iteration 47, loss = 0.46856603\n",
            "Iteration 48, loss = 0.46452092\n",
            "Iteration 49, loss = 0.46264302\n",
            "Iteration 50, loss = 0.46865394\n",
            "Iteration 51, loss = 0.45701574\n",
            "Iteration 52, loss = 0.45428066\n",
            "Iteration 53, loss = 0.44660500\n",
            "Iteration 54, loss = 0.46117437\n",
            "Iteration 55, loss = 0.45912051\n",
            "Iteration 56, loss = 0.45211556\n",
            "Iteration 57, loss = 0.46515247\n",
            "Iteration 58, loss = 0.44033847\n",
            "Iteration 59, loss = 0.42006506\n",
            "Iteration 60, loss = 0.40298248\n",
            "Iteration 61, loss = 0.41440659\n",
            "Iteration 62, loss = 0.42018099\n",
            "Iteration 63, loss = 0.41347502\n",
            "Iteration 64, loss = 0.40045984\n",
            "Iteration 65, loss = 0.38379975\n",
            "Iteration 66, loss = 0.42125652\n",
            "Iteration 67, loss = 0.43815035\n",
            "Iteration 68, loss = 0.42112137\n",
            "Iteration 69, loss = 0.37753266\n",
            "Iteration 70, loss = 0.37497457\n",
            "Iteration 71, loss = 0.36869916\n",
            "Iteration 72, loss = 0.36165359\n",
            "Iteration 73, loss = 0.36110120\n",
            "Iteration 74, loss = 0.37038560\n",
            "Iteration 75, loss = 0.35205479\n",
            "Iteration 76, loss = 0.34694967\n",
            "Iteration 77, loss = 0.35913465\n",
            "Iteration 78, loss = 0.34050506\n",
            "Iteration 79, loss = 0.34463509\n",
            "Iteration 80, loss = 0.34026610\n",
            "Iteration 81, loss = 0.32996683\n",
            "Iteration 82, loss = 0.31686598\n",
            "Iteration 83, loss = 0.31426714\n",
            "Iteration 84, loss = 0.34743890\n",
            "Iteration 85, loss = 0.35023379\n",
            "Iteration 86, loss = 0.31782607\n",
            "Iteration 87, loss = 0.32718235\n",
            "Iteration 88, loss = 0.31871355\n",
            "Iteration 89, loss = 0.30644117\n",
            "Iteration 90, loss = 0.28597912\n",
            "Iteration 91, loss = 0.28589466\n",
            "Iteration 92, loss = 0.28658597\n",
            "Iteration 93, loss = 0.29058819\n",
            "Iteration 94, loss = 0.29657042\n",
            "Iteration 95, loss = 0.29196743\n",
            "Iteration 96, loss = 0.26456839\n",
            "Iteration 97, loss = 0.29179654\n",
            "Iteration 98, loss = 0.36576991\n",
            "Iteration 99, loss = 0.32258684\n",
            "Iteration 100, loss = 0.26598295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36460967\n",
            "Iteration 2, loss = 1.28076544\n",
            "Iteration 3, loss = 1.18938953\n",
            "Iteration 4, loss = 1.09141246\n",
            "Iteration 5, loss = 0.98328393\n",
            "Iteration 6, loss = 0.89756547\n",
            "Iteration 7, loss = 0.83463116\n",
            "Iteration 8, loss = 0.80941688\n",
            "Iteration 9, loss = 0.75391210\n",
            "Iteration 10, loss = 0.72452526\n",
            "Iteration 11, loss = 0.71531248\n",
            "Iteration 12, loss = 0.70788692\n",
            "Iteration 13, loss = 0.68127419\n",
            "Iteration 14, loss = 0.64972351\n",
            "Iteration 15, loss = 0.63961528\n",
            "Iteration 16, loss = 0.63358181\n",
            "Iteration 17, loss = 0.63128272\n",
            "Iteration 18, loss = 0.64785920\n",
            "Iteration 19, loss = 0.63489003\n",
            "Iteration 20, loss = 0.63208701\n",
            "Iteration 21, loss = 0.60125475\n",
            "Iteration 22, loss = 0.58197259\n",
            "Iteration 23, loss = 0.56978349\n",
            "Iteration 24, loss = 0.56046993\n",
            "Iteration 25, loss = 0.60719526\n",
            "Iteration 26, loss = 0.61312324\n",
            "Iteration 27, loss = 0.57514042\n",
            "Iteration 28, loss = 0.54620788\n",
            "Iteration 29, loss = 0.53511568\n",
            "Iteration 30, loss = 0.54000755\n",
            "Iteration 31, loss = 0.52974246\n",
            "Iteration 32, loss = 0.53045253\n",
            "Iteration 33, loss = 0.50501528\n",
            "Iteration 34, loss = 0.50414405\n",
            "Iteration 35, loss = 0.49375098\n",
            "Iteration 36, loss = 0.48708035\n",
            "Iteration 37, loss = 0.48083564\n",
            "Iteration 38, loss = 0.48713193\n",
            "Iteration 39, loss = 0.47722109\n",
            "Iteration 40, loss = 0.47099999\n",
            "Iteration 41, loss = 0.47669481\n",
            "Iteration 42, loss = 0.49203413\n",
            "Iteration 43, loss = 0.46758099\n",
            "Iteration 44, loss = 0.45897399\n",
            "Iteration 45, loss = 0.46039201\n",
            "Iteration 46, loss = 0.45347226\n",
            "Iteration 47, loss = 0.44871554\n",
            "Iteration 48, loss = 0.43335245\n",
            "Iteration 49, loss = 0.46168378\n",
            "Iteration 50, loss = 0.46489461\n",
            "Iteration 51, loss = 0.45199738\n",
            "Iteration 52, loss = 0.42523089\n",
            "Iteration 53, loss = 0.43135715\n",
            "Iteration 54, loss = 0.42514895\n",
            "Iteration 55, loss = 0.40624488\n",
            "Iteration 56, loss = 0.39466789\n",
            "Iteration 57, loss = 0.40714814\n",
            "Iteration 58, loss = 0.38921544\n",
            "Iteration 59, loss = 0.40599113\n",
            "Iteration 60, loss = 0.37998860\n",
            "Iteration 61, loss = 0.40262712\n",
            "Iteration 62, loss = 0.43645076\n",
            "Iteration 63, loss = 0.39397834\n",
            "Iteration 64, loss = 0.38979290\n",
            "Iteration 65, loss = 0.37063223\n",
            "Iteration 66, loss = 0.39420723\n",
            "Iteration 67, loss = 0.39776676\n",
            "Iteration 68, loss = 0.36563116\n",
            "Iteration 69, loss = 0.35407105\n",
            "Iteration 70, loss = 0.34910253\n",
            "Iteration 71, loss = 0.37443047\n",
            "Iteration 72, loss = 0.35752329\n",
            "Iteration 73, loss = 0.33297131\n",
            "Iteration 74, loss = 0.33247271\n",
            "Iteration 75, loss = 0.31599793\n",
            "Iteration 76, loss = 0.32688732\n",
            "Iteration 77, loss = 0.34473221\n",
            "Iteration 78, loss = 0.32097929\n",
            "Iteration 79, loss = 0.32213172\n",
            "Iteration 80, loss = 0.31561598\n",
            "Iteration 81, loss = 0.30736114\n",
            "Iteration 82, loss = 0.29191202\n",
            "Iteration 83, loss = 0.29872009\n",
            "Iteration 84, loss = 0.30591421\n",
            "Iteration 85, loss = 0.32430799\n",
            "Iteration 86, loss = 0.29440135\n",
            "Iteration 87, loss = 0.29832958\n",
            "Iteration 88, loss = 0.28505065\n",
            "Iteration 89, loss = 0.27589891\n",
            "Iteration 90, loss = 0.26024407\n",
            "Iteration 91, loss = 0.26397509\n",
            "Iteration 92, loss = 0.26774559\n",
            "Iteration 93, loss = 0.26963695\n",
            "Iteration 94, loss = 0.27112398\n",
            "Iteration 95, loss = 0.26555764\n",
            "Iteration 96, loss = 0.24881695\n",
            "Iteration 97, loss = 0.28708089\n",
            "Iteration 98, loss = 0.37367199\n",
            "Iteration 99, loss = 0.30564949\n",
            "Iteration 100, loss = 0.27277762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35862055\n",
            "Iteration 2, loss = 1.28692940\n",
            "Iteration 3, loss = 1.20710459\n",
            "Iteration 4, loss = 1.11865208\n",
            "Iteration 5, loss = 1.02980452\n",
            "Iteration 6, loss = 0.95498316\n",
            "Iteration 7, loss = 0.91651155\n",
            "Iteration 8, loss = 0.87791606\n",
            "Iteration 9, loss = 0.82875248\n",
            "Iteration 10, loss = 0.81005889\n",
            "Iteration 11, loss = 0.78820203\n",
            "Iteration 12, loss = 0.75738329\n",
            "Iteration 13, loss = 0.73908525\n",
            "Iteration 14, loss = 0.71415926\n",
            "Iteration 15, loss = 0.69574387\n",
            "Iteration 16, loss = 0.68600170\n",
            "Iteration 17, loss = 0.68557553\n",
            "Iteration 18, loss = 0.68352917\n",
            "Iteration 19, loss = 0.70618228\n",
            "Iteration 20, loss = 0.68967198\n",
            "Iteration 21, loss = 0.65897624\n",
            "Iteration 22, loss = 0.63276135\n",
            "Iteration 23, loss = 0.60816628\n",
            "Iteration 24, loss = 0.62664127\n",
            "Iteration 25, loss = 0.66386285\n",
            "Iteration 26, loss = 0.66367650\n",
            "Iteration 27, loss = 0.63500412\n",
            "Iteration 28, loss = 0.59850647\n",
            "Iteration 29, loss = 0.58653556\n",
            "Iteration 30, loss = 0.58177490\n",
            "Iteration 31, loss = 0.57637015\n",
            "Iteration 32, loss = 0.57808804\n",
            "Iteration 33, loss = 0.55383404\n",
            "Iteration 34, loss = 0.55053281\n",
            "Iteration 35, loss = 0.53846640\n",
            "Iteration 36, loss = 0.53919325\n",
            "Iteration 37, loss = 0.52400648\n",
            "Iteration 38, loss = 0.52340052\n",
            "Iteration 39, loss = 0.51291089\n",
            "Iteration 40, loss = 0.50296130\n",
            "Iteration 41, loss = 0.51705534\n",
            "Iteration 42, loss = 0.53210269\n",
            "Iteration 43, loss = 0.51290333\n",
            "Iteration 44, loss = 0.50413019\n",
            "Iteration 45, loss = 0.51421788\n",
            "Iteration 46, loss = 0.50894443\n",
            "Iteration 47, loss = 0.48678410\n",
            "Iteration 48, loss = 0.47501694\n",
            "Iteration 49, loss = 0.49275399\n",
            "Iteration 50, loss = 0.50376757\n",
            "Iteration 51, loss = 0.49300541\n",
            "Iteration 52, loss = 0.47201507\n",
            "Iteration 53, loss = 0.45959514\n",
            "Iteration 54, loss = 0.45320079\n",
            "Iteration 55, loss = 0.43795204\n",
            "Iteration 56, loss = 0.42649617\n",
            "Iteration 57, loss = 0.42710158\n",
            "Iteration 58, loss = 0.41543355\n",
            "Iteration 59, loss = 0.41463027\n",
            "Iteration 60, loss = 0.41843079\n",
            "Iteration 61, loss = 0.44647281\n",
            "Iteration 62, loss = 0.45256476\n",
            "Iteration 63, loss = 0.41161172\n",
            "Iteration 64, loss = 0.39559765\n",
            "Iteration 65, loss = 0.38112866\n",
            "Iteration 66, loss = 0.39918250\n",
            "Iteration 67, loss = 0.42348920\n",
            "Iteration 68, loss = 0.42203614\n",
            "Iteration 69, loss = 0.39593246\n",
            "Iteration 70, loss = 0.37914758\n",
            "Iteration 71, loss = 0.38278315\n",
            "Iteration 72, loss = 0.37406500\n",
            "Iteration 73, loss = 0.34572926\n",
            "Iteration 74, loss = 0.35330547\n",
            "Iteration 75, loss = 0.33915427\n",
            "Iteration 76, loss = 0.34368571\n",
            "Iteration 77, loss = 0.36054433\n",
            "Iteration 78, loss = 0.35243388\n",
            "Iteration 79, loss = 0.33533164\n",
            "Iteration 80, loss = 0.32445839\n",
            "Iteration 81, loss = 0.32050806\n",
            "Iteration 82, loss = 0.33412557\n",
            "Iteration 83, loss = 0.32400926\n",
            "Iteration 84, loss = 0.33940587\n",
            "Iteration 85, loss = 0.32135423\n",
            "Iteration 86, loss = 0.32509087\n",
            "Iteration 87, loss = 0.31683063\n",
            "Iteration 88, loss = 0.32280736\n",
            "Iteration 89, loss = 0.31589750\n",
            "Iteration 90, loss = 0.27796581\n",
            "Iteration 91, loss = 0.28673805\n",
            "Iteration 92, loss = 0.27391837\n",
            "Iteration 93, loss = 0.26571426\n",
            "Iteration 94, loss = 0.26746000\n",
            "Iteration 95, loss = 0.31773508\n",
            "Iteration 96, loss = 0.31129673\n",
            "Iteration 97, loss = 0.33201920\n",
            "Iteration 98, loss = 0.34177226\n",
            "Iteration 99, loss = 0.32277981\n",
            "Iteration 100, loss = 0.27481070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34307401\n",
            "Iteration 2, loss = 1.27287923\n",
            "Iteration 3, loss = 1.19962821\n",
            "Iteration 4, loss = 1.11758044\n",
            "Iteration 5, loss = 1.04711636\n",
            "Iteration 6, loss = 0.97413192\n",
            "Iteration 7, loss = 0.95200527\n",
            "Iteration 8, loss = 0.91112124\n",
            "Iteration 9, loss = 0.85782469\n",
            "Iteration 10, loss = 0.84273820\n",
            "Iteration 11, loss = 0.80789788\n",
            "Iteration 12, loss = 0.77972829\n",
            "Iteration 13, loss = 0.75355692\n",
            "Iteration 14, loss = 0.74263480\n",
            "Iteration 15, loss = 0.72885692\n",
            "Iteration 16, loss = 0.72167417\n",
            "Iteration 17, loss = 0.72323783\n",
            "Iteration 18, loss = 0.72731436\n",
            "Iteration 19, loss = 0.70814931\n",
            "Iteration 20, loss = 0.69764801\n",
            "Iteration 21, loss = 0.68178129\n",
            "Iteration 22, loss = 0.66756806\n",
            "Iteration 23, loss = 0.65114551\n",
            "Iteration 24, loss = 0.64293652\n",
            "Iteration 25, loss = 0.67656461\n",
            "Iteration 26, loss = 0.71383835\n",
            "Iteration 27, loss = 0.68089026\n",
            "Iteration 28, loss = 0.62844830\n",
            "Iteration 29, loss = 0.61071710\n",
            "Iteration 30, loss = 0.60471469\n",
            "Iteration 31, loss = 0.60014847\n",
            "Iteration 32, loss = 0.59728028\n",
            "Iteration 33, loss = 0.58599900\n",
            "Iteration 34, loss = 0.57876860\n",
            "Iteration 35, loss = 0.57215136\n",
            "Iteration 36, loss = 0.58440737\n",
            "Iteration 37, loss = 0.58196926\n",
            "Iteration 38, loss = 0.57661150\n",
            "Iteration 39, loss = 0.54825524\n",
            "Iteration 40, loss = 0.54443363\n",
            "Iteration 41, loss = 0.56474994\n",
            "Iteration 42, loss = 0.56510701\n",
            "Iteration 43, loss = 0.54948663\n",
            "Iteration 44, loss = 0.56044451\n",
            "Iteration 45, loss = 0.56389100\n",
            "Iteration 46, loss = 0.54172590\n",
            "Iteration 47, loss = 0.54309070\n",
            "Iteration 48, loss = 0.52185495\n",
            "Iteration 49, loss = 0.53847227\n",
            "Iteration 50, loss = 0.54283688\n",
            "Iteration 51, loss = 0.52708885\n",
            "Iteration 52, loss = 0.51063733\n",
            "Iteration 53, loss = 0.52129691\n",
            "Iteration 54, loss = 0.49986172\n",
            "Iteration 55, loss = 0.48576590\n",
            "Iteration 56, loss = 0.46969504\n",
            "Iteration 57, loss = 0.46614007\n",
            "Iteration 58, loss = 0.45380047\n",
            "Iteration 59, loss = 0.45756143\n",
            "Iteration 60, loss = 0.44833417\n",
            "Iteration 61, loss = 0.44913200\n",
            "Iteration 62, loss = 0.45479070\n",
            "Iteration 63, loss = 0.44215165\n",
            "Iteration 64, loss = 0.43320987\n",
            "Iteration 65, loss = 0.42856528\n",
            "Iteration 66, loss = 0.45991396\n",
            "Iteration 67, loss = 0.47215762\n",
            "Iteration 68, loss = 0.42664413\n",
            "Iteration 69, loss = 0.42581702\n",
            "Iteration 70, loss = 0.43201686\n",
            "Iteration 71, loss = 0.40301991\n",
            "Iteration 72, loss = 0.40065967\n",
            "Iteration 73, loss = 0.39237354\n",
            "Iteration 74, loss = 0.40427749\n",
            "Iteration 75, loss = 0.39078837\n",
            "Iteration 76, loss = 0.39668144\n",
            "Iteration 77, loss = 0.43012230\n",
            "Iteration 78, loss = 0.41059518\n",
            "Iteration 79, loss = 0.39489098\n",
            "Iteration 80, loss = 0.38991751\n",
            "Iteration 81, loss = 0.37213577\n",
            "Iteration 82, loss = 0.34855776\n",
            "Iteration 83, loss = 0.35150469\n",
            "Iteration 84, loss = 0.37434469\n",
            "Iteration 85, loss = 0.36360535\n",
            "Iteration 86, loss = 0.34320231\n",
            "Iteration 87, loss = 0.33190529\n",
            "Iteration 88, loss = 0.33513745\n",
            "Iteration 89, loss = 0.32957958\n",
            "Iteration 90, loss = 0.33262440\n",
            "Iteration 91, loss = 0.33754618\n",
            "Iteration 92, loss = 0.30637508\n",
            "Iteration 93, loss = 0.31402660\n",
            "Iteration 94, loss = 0.32425664\n",
            "Iteration 95, loss = 0.35772775\n",
            "Iteration 96, loss = 0.32089263\n",
            "Iteration 97, loss = 0.38261751\n",
            "Iteration 98, loss = 0.37337485\n",
            "Iteration 99, loss = 0.32212837\n",
            "Iteration 100, loss = 0.30830809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35726326\n",
            "Iteration 2, loss = 1.29092206\n",
            "Iteration 3, loss = 1.19219318\n",
            "Iteration 4, loss = 1.09375395\n",
            "Iteration 5, loss = 1.01009858\n",
            "Iteration 6, loss = 0.93877659\n",
            "Iteration 7, loss = 0.91559069\n",
            "Iteration 8, loss = 0.84589815\n",
            "Iteration 9, loss = 0.81973797\n",
            "Iteration 10, loss = 0.79396254\n",
            "Iteration 11, loss = 0.77832151\n",
            "Iteration 12, loss = 0.74887815\n",
            "Iteration 13, loss = 0.73886318\n",
            "Iteration 14, loss = 0.70814684\n",
            "Iteration 15, loss = 0.69464197\n",
            "Iteration 16, loss = 0.68191152\n",
            "Iteration 17, loss = 0.68850998\n",
            "Iteration 18, loss = 0.69443097\n",
            "Iteration 19, loss = 0.64745346\n",
            "Iteration 20, loss = 0.66640809\n",
            "Iteration 21, loss = 0.65894604\n",
            "Iteration 22, loss = 0.63062484\n",
            "Iteration 23, loss = 0.61100904\n",
            "Iteration 24, loss = 0.60903700\n",
            "Iteration 25, loss = 0.61766437\n",
            "Iteration 26, loss = 0.62269947\n",
            "Iteration 27, loss = 0.59095564\n",
            "Iteration 28, loss = 0.57219229\n",
            "Iteration 29, loss = 0.56552744\n",
            "Iteration 30, loss = 0.55402480\n",
            "Iteration 31, loss = 0.55321170\n",
            "Iteration 32, loss = 0.55745000\n",
            "Iteration 33, loss = 0.54275443\n",
            "Iteration 34, loss = 0.53035838\n",
            "Iteration 35, loss = 0.52312444\n",
            "Iteration 36, loss = 0.53823897\n",
            "Iteration 37, loss = 0.51788207\n",
            "Iteration 38, loss = 0.51174524\n",
            "Iteration 39, loss = 0.50565136\n",
            "Iteration 40, loss = 0.50586234\n",
            "Iteration 41, loss = 0.50796747\n",
            "Iteration 42, loss = 0.51406148\n",
            "Iteration 43, loss = 0.51708111\n",
            "Iteration 44, loss = 0.55922083\n",
            "Iteration 45, loss = 0.52004367\n",
            "Iteration 46, loss = 0.49999954\n",
            "Iteration 47, loss = 0.50223639\n",
            "Iteration 48, loss = 0.48857967\n",
            "Iteration 49, loss = 0.48296525\n",
            "Iteration 50, loss = 0.47433272\n",
            "Iteration 51, loss = 0.45981880\n",
            "Iteration 52, loss = 0.46084542\n",
            "Iteration 53, loss = 0.47080959\n",
            "Iteration 54, loss = 0.45460983\n",
            "Iteration 55, loss = 0.44397908\n",
            "Iteration 56, loss = 0.42281794\n",
            "Iteration 57, loss = 0.41995383\n",
            "Iteration 58, loss = 0.41836362\n",
            "Iteration 59, loss = 0.42362930\n",
            "Iteration 60, loss = 0.43177482\n",
            "Iteration 61, loss = 0.41126605\n",
            "Iteration 62, loss = 0.41097635\n",
            "Iteration 63, loss = 0.40374236\n",
            "Iteration 64, loss = 0.40070552\n",
            "Iteration 65, loss = 0.40322226\n",
            "Iteration 66, loss = 0.42381771\n",
            "Iteration 67, loss = 0.41417479\n",
            "Iteration 68, loss = 0.39013511\n",
            "Iteration 69, loss = 0.38784972\n",
            "Iteration 70, loss = 0.40317180\n",
            "Iteration 71, loss = 0.39546497\n",
            "Iteration 72, loss = 0.39321464\n",
            "Iteration 73, loss = 0.36022384\n",
            "Iteration 74, loss = 0.35764871\n",
            "Iteration 75, loss = 0.35387123\n",
            "Iteration 76, loss = 0.36491537\n",
            "Iteration 77, loss = 0.38184095\n",
            "Iteration 78, loss = 0.41975730\n",
            "Iteration 79, loss = 0.37767784\n",
            "Iteration 80, loss = 0.37148047\n",
            "Iteration 81, loss = 0.38409041\n",
            "Iteration 82, loss = 0.34352602\n",
            "Iteration 83, loss = 0.33700348\n",
            "Iteration 84, loss = 0.34825111\n",
            "Iteration 85, loss = 0.33605370\n",
            "Iteration 86, loss = 0.32087325\n",
            "Iteration 87, loss = 0.30992840\n",
            "Iteration 88, loss = 0.31089915\n",
            "Iteration 89, loss = 0.31123147\n",
            "Iteration 90, loss = 0.31124190\n",
            "Iteration 91, loss = 0.32240032\n",
            "Iteration 92, loss = 0.32328750\n",
            "Iteration 93, loss = 0.33719592\n",
            "Iteration 94, loss = 0.34563672\n",
            "Iteration 95, loss = 0.34557894\n",
            "Iteration 96, loss = 0.33329573\n",
            "Iteration 97, loss = 0.37782602\n",
            "Iteration 98, loss = 0.36092081\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36410073\n",
            "Iteration 2, loss = 1.27891964\n",
            "Iteration 3, loss = 1.18964047\n",
            "Iteration 4, loss = 1.09745629\n",
            "Iteration 5, loss = 1.01030647\n",
            "Iteration 6, loss = 0.93008871\n",
            "Iteration 7, loss = 0.91667806\n",
            "Iteration 8, loss = 0.84288017\n",
            "Iteration 9, loss = 0.79207726\n",
            "Iteration 10, loss = 0.76873134\n",
            "Iteration 11, loss = 0.75818451\n",
            "Iteration 12, loss = 0.72450325\n",
            "Iteration 13, loss = 0.71960683\n",
            "Iteration 14, loss = 0.69376568\n",
            "Iteration 15, loss = 0.66978780\n",
            "Iteration 16, loss = 0.65831971\n",
            "Iteration 17, loss = 0.66204730\n",
            "Iteration 18, loss = 0.65844970\n",
            "Iteration 19, loss = 0.62926929\n",
            "Iteration 20, loss = 0.63185647\n",
            "Iteration 21, loss = 0.62585791\n",
            "Iteration 22, loss = 0.60218740\n",
            "Iteration 23, loss = 0.58891003\n",
            "Iteration 24, loss = 0.56924631\n",
            "Iteration 25, loss = 0.57588625\n",
            "Iteration 26, loss = 0.60057451\n",
            "Iteration 27, loss = 0.59001761\n",
            "Iteration 28, loss = 0.56313682\n",
            "Iteration 29, loss = 0.55254925\n",
            "Iteration 30, loss = 0.53809757\n",
            "Iteration 31, loss = 0.52493572\n",
            "Iteration 32, loss = 0.52782906\n",
            "Iteration 33, loss = 0.52548012\n",
            "Iteration 34, loss = 0.50909928\n",
            "Iteration 35, loss = 0.51018387\n",
            "Iteration 36, loss = 0.50812725\n",
            "Iteration 37, loss = 0.51372047\n",
            "Iteration 38, loss = 0.52347594\n",
            "Iteration 39, loss = 0.52343488\n",
            "Iteration 40, loss = 0.50983543\n",
            "Iteration 41, loss = 0.51825897\n",
            "Iteration 42, loss = 0.48615163\n",
            "Iteration 43, loss = 0.48814062\n",
            "Iteration 44, loss = 0.53041643\n",
            "Iteration 45, loss = 0.52665282\n",
            "Iteration 46, loss = 0.48240726\n",
            "Iteration 47, loss = 0.46734939\n",
            "Iteration 48, loss = 0.48222987\n",
            "Iteration 49, loss = 0.46818739\n",
            "Iteration 50, loss = 0.45911793\n",
            "Iteration 51, loss = 0.44203057\n",
            "Iteration 52, loss = 0.43487502\n",
            "Iteration 53, loss = 0.45954693\n",
            "Iteration 54, loss = 0.46371175\n",
            "Iteration 55, loss = 0.45861542\n",
            "Iteration 56, loss = 0.42499502\n",
            "Iteration 57, loss = 0.42479066\n",
            "Iteration 58, loss = 0.44002898\n",
            "Iteration 59, loss = 0.42365580\n",
            "Iteration 60, loss = 0.42173456\n",
            "Iteration 61, loss = 0.40922663\n",
            "Iteration 62, loss = 0.41427764\n",
            "Iteration 63, loss = 0.41753275\n",
            "Iteration 64, loss = 0.40207920\n",
            "Iteration 65, loss = 0.38665816\n",
            "Iteration 66, loss = 0.41085092\n",
            "Iteration 67, loss = 0.38623284\n",
            "Iteration 68, loss = 0.40350427\n",
            "Iteration 69, loss = 0.40595694\n",
            "Iteration 70, loss = 0.42341623\n",
            "Iteration 71, loss = 0.42109804\n",
            "Iteration 72, loss = 0.39019404\n",
            "Iteration 73, loss = 0.37443885\n",
            "Iteration 74, loss = 0.37366529\n",
            "Iteration 75, loss = 0.34633979\n",
            "Iteration 76, loss = 0.36332446\n",
            "Iteration 77, loss = 0.36187097\n",
            "Iteration 78, loss = 0.37076994\n",
            "Iteration 79, loss = 0.34982777\n",
            "Iteration 80, loss = 0.34658395\n",
            "Iteration 81, loss = 0.36048140\n",
            "Iteration 82, loss = 0.34487514\n",
            "Iteration 83, loss = 0.32899972\n",
            "Iteration 84, loss = 0.34119865\n",
            "Iteration 85, loss = 0.33458071\n",
            "Iteration 86, loss = 0.32047709\n",
            "Iteration 87, loss = 0.31813195\n",
            "Iteration 88, loss = 0.33912088\n",
            "Iteration 89, loss = 0.32118418\n",
            "Iteration 90, loss = 0.30473562\n",
            "Iteration 91, loss = 0.31745486\n",
            "Iteration 92, loss = 0.31834471\n",
            "Iteration 93, loss = 0.33355442\n",
            "Iteration 94, loss = 0.33232043\n",
            "Iteration 95, loss = 0.33194026\n",
            "Iteration 96, loss = 0.33883699\n",
            "Iteration 97, loss = 0.37108993\n",
            "Iteration 98, loss = 0.37866984\n",
            "Iteration 99, loss = 0.33005962\n",
            "Iteration 100, loss = 0.29615256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37062510\n",
            "Iteration 2, loss = 1.28633933\n",
            "Iteration 3, loss = 1.19394082\n",
            "Iteration 4, loss = 1.10603866\n",
            "Iteration 5, loss = 1.03387055\n",
            "Iteration 6, loss = 0.96870548\n",
            "Iteration 7, loss = 0.94899018\n",
            "Iteration 8, loss = 0.87492156\n",
            "Iteration 9, loss = 0.84721256\n",
            "Iteration 10, loss = 0.82129714\n",
            "Iteration 11, loss = 0.80006089\n",
            "Iteration 12, loss = 0.78837147\n",
            "Iteration 13, loss = 0.78008038\n",
            "Iteration 14, loss = 0.74959441\n",
            "Iteration 15, loss = 0.73065962\n",
            "Iteration 16, loss = 0.73109934\n",
            "Iteration 17, loss = 0.72638396\n",
            "Iteration 18, loss = 0.69487861\n",
            "Iteration 19, loss = 0.68045007\n",
            "Iteration 20, loss = 0.67874321\n",
            "Iteration 21, loss = 0.68258034\n",
            "Iteration 22, loss = 0.66294175\n",
            "Iteration 23, loss = 0.64497097\n",
            "Iteration 24, loss = 0.64607751\n",
            "Iteration 25, loss = 0.62765428\n",
            "Iteration 26, loss = 0.63636659\n",
            "Iteration 27, loss = 0.66392904\n",
            "Iteration 28, loss = 0.62044650\n",
            "Iteration 29, loss = 0.61380782\n",
            "Iteration 30, loss = 0.59097247\n",
            "Iteration 31, loss = 0.57687559\n",
            "Iteration 32, loss = 0.58622634\n",
            "Iteration 33, loss = 0.59472237\n",
            "Iteration 34, loss = 0.57550582\n",
            "Iteration 35, loss = 0.56350150\n",
            "Iteration 36, loss = 0.58175279\n",
            "Iteration 37, loss = 0.57135812\n",
            "Iteration 38, loss = 0.55796854\n",
            "Iteration 39, loss = 0.54013333\n",
            "Iteration 40, loss = 0.53838449\n",
            "Iteration 41, loss = 0.56471596\n",
            "Iteration 42, loss = 0.58983408\n",
            "Iteration 43, loss = 0.57004912\n",
            "Iteration 44, loss = 0.52884980\n",
            "Iteration 45, loss = 0.52063861\n",
            "Iteration 46, loss = 0.52493000\n",
            "Iteration 47, loss = 0.53799913\n",
            "Iteration 48, loss = 0.52655704\n",
            "Iteration 49, loss = 0.52187767\n",
            "Iteration 50, loss = 0.51864550\n",
            "Iteration 51, loss = 0.54206069\n",
            "Iteration 52, loss = 0.52725554\n",
            "Iteration 53, loss = 0.50582650\n",
            "Iteration 54, loss = 0.50327226\n",
            "Iteration 55, loss = 0.48504551\n",
            "Iteration 56, loss = 0.46927939\n",
            "Iteration 57, loss = 0.47223525\n",
            "Iteration 58, loss = 0.47305765\n",
            "Iteration 59, loss = 0.46999825\n",
            "Iteration 60, loss = 0.46905759\n",
            "Iteration 61, loss = 0.46169815\n",
            "Iteration 62, loss = 0.47228359\n",
            "Iteration 63, loss = 0.47262012\n",
            "Iteration 64, loss = 0.47210825\n",
            "Iteration 65, loss = 0.47096860\n",
            "Iteration 66, loss = 0.47814652\n",
            "Iteration 67, loss = 0.47961730\n",
            "Iteration 68, loss = 0.46350161\n",
            "Iteration 69, loss = 0.43835887\n",
            "Iteration 70, loss = 0.46794069\n",
            "Iteration 71, loss = 0.45394501\n",
            "Iteration 72, loss = 0.45112245\n",
            "Iteration 73, loss = 0.43643678\n",
            "Iteration 74, loss = 0.42471799\n",
            "Iteration 75, loss = 0.40051362\n",
            "Iteration 76, loss = 0.42106537\n",
            "Iteration 77, loss = 0.45516186\n",
            "Iteration 78, loss = 0.43888596\n",
            "Iteration 79, loss = 0.41391378\n",
            "Iteration 80, loss = 0.39527304\n",
            "Iteration 81, loss = 0.42384929\n",
            "Iteration 82, loss = 0.38966598\n",
            "Iteration 83, loss = 0.39243990\n",
            "Iteration 84, loss = 0.37382192\n",
            "Iteration 85, loss = 0.38410552\n",
            "Iteration 86, loss = 0.38482565\n",
            "Iteration 87, loss = 0.40195507\n",
            "Iteration 88, loss = 0.37595800\n",
            "Iteration 89, loss = 0.36566745\n",
            "Iteration 90, loss = 0.38914319\n",
            "Iteration 91, loss = 0.38829425\n",
            "Iteration 92, loss = 0.40475322\n",
            "Iteration 93, loss = 0.41469392\n",
            "Iteration 94, loss = 0.41558257\n",
            "Iteration 95, loss = 0.39771909\n",
            "Iteration 96, loss = 0.36041225\n",
            "Iteration 97, loss = 0.36954921\n",
            "Iteration 98, loss = 0.34583574\n",
            "Iteration 99, loss = 0.34734678\n",
            "Iteration 100, loss = 0.36300693\n",
            "35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39743017\n",
            "Iteration 2, loss = 1.37290872\n",
            "Iteration 3, loss = 1.34645234\n",
            "Iteration 4, loss = 1.32880235\n",
            "Iteration 5, loss = 1.32546919\n",
            "Iteration 6, loss = 1.30328777\n",
            "Iteration 7, loss = 1.30230909\n",
            "Iteration 8, loss = 1.29367935\n",
            "Iteration 9, loss = 1.27884522\n",
            "Iteration 10, loss = 1.27379377\n",
            "Iteration 11, loss = 1.25223170\n",
            "Iteration 12, loss = 1.23903130\n",
            "Iteration 13, loss = 1.24987604\n",
            "Iteration 14, loss = 1.20992081\n",
            "Iteration 15, loss = 1.19958297\n",
            "Iteration 16, loss = 1.24758201\n",
            "Iteration 17, loss = 1.19766157\n",
            "Iteration 18, loss = 1.16053156\n",
            "Iteration 19, loss = 1.14146362\n",
            "Iteration 20, loss = 1.13228816\n",
            "Iteration 21, loss = 1.14572743\n",
            "Iteration 22, loss = 1.12465005\n",
            "Iteration 23, loss = 1.12359053\n",
            "Iteration 24, loss = 1.08544896\n",
            "Iteration 25, loss = 1.10526273\n",
            "Iteration 26, loss = 1.11946286\n",
            "Iteration 27, loss = 1.12365295\n",
            "Iteration 28, loss = 1.08241497\n",
            "Iteration 29, loss = 1.05612112\n",
            "Iteration 30, loss = 1.04147066\n",
            "Iteration 31, loss = 1.02968506\n",
            "Iteration 32, loss = 1.05533056\n",
            "Iteration 33, loss = 1.01580913\n",
            "Iteration 34, loss = 1.04849008\n",
            "Iteration 35, loss = 1.04873511\n",
            "Iteration 36, loss = 1.02946349\n",
            "Iteration 37, loss = 1.00433764\n",
            "Iteration 38, loss = 0.97061561\n",
            "Iteration 39, loss = 0.96013143\n",
            "Iteration 40, loss = 0.94853692\n",
            "Iteration 41, loss = 0.92832172\n",
            "Iteration 42, loss = 0.92435917\n",
            "Iteration 43, loss = 0.90275403\n",
            "Iteration 44, loss = 0.89956133\n",
            "Iteration 45, loss = 0.87990601\n",
            "Iteration 46, loss = 0.90787520\n",
            "Iteration 47, loss = 0.91629494\n",
            "Iteration 48, loss = 0.85001608\n",
            "Iteration 49, loss = 0.84537124\n",
            "Iteration 50, loss = 0.85247387\n",
            "Iteration 51, loss = 0.86669358\n",
            "Iteration 52, loss = 0.87326735\n",
            "Iteration 53, loss = 0.83183782\n",
            "Iteration 54, loss = 0.83070402\n",
            "Iteration 55, loss = 0.83431602\n",
            "Iteration 56, loss = 0.87314330\n",
            "Iteration 57, loss = 0.83859971\n",
            "Iteration 58, loss = 0.80043099\n",
            "Iteration 59, loss = 0.79781111\n",
            "Iteration 60, loss = 0.80055923\n",
            "Iteration 61, loss = 0.78351956\n",
            "Iteration 62, loss = 0.84755023\n",
            "Iteration 63, loss = 0.81018869\n",
            "Iteration 64, loss = 0.75818020\n",
            "Iteration 65, loss = 0.73755509\n",
            "Iteration 66, loss = 0.76450731\n",
            "Iteration 67, loss = 0.75944527\n",
            "Iteration 68, loss = 0.74288418\n",
            "Iteration 69, loss = 0.77610229\n",
            "Iteration 70, loss = 0.74424179\n",
            "Iteration 71, loss = 0.73284482\n",
            "Iteration 72, loss = 0.72532764\n",
            "Iteration 73, loss = 0.69210983\n",
            "Iteration 74, loss = 0.70093055\n",
            "Iteration 75, loss = 0.68938405\n",
            "Iteration 76, loss = 0.65796272\n",
            "Iteration 77, loss = 0.65880100\n",
            "Iteration 78, loss = 0.66114051\n",
            "Iteration 79, loss = 0.67610989\n",
            "Iteration 80, loss = 0.68883987\n",
            "Iteration 81, loss = 0.68785799\n",
            "Iteration 82, loss = 0.68838199\n",
            "Iteration 83, loss = 0.70744026\n",
            "Iteration 84, loss = 0.74140200\n",
            "Iteration 85, loss = 0.70948626\n",
            "Iteration 86, loss = 0.67682294\n",
            "Iteration 87, loss = 0.74103435\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.40318322\n",
            "Iteration 2, loss = 1.37958738\n",
            "Iteration 3, loss = 1.36081119\n",
            "Iteration 4, loss = 1.34643165\n",
            "Iteration 5, loss = 1.33548302\n",
            "Iteration 6, loss = 1.31548490\n",
            "Iteration 7, loss = 1.32078498\n",
            "Iteration 8, loss = 1.31488467\n",
            "Iteration 9, loss = 1.29153335\n",
            "Iteration 10, loss = 1.27617354\n",
            "Iteration 11, loss = 1.27003992\n",
            "Iteration 12, loss = 1.25157675\n",
            "Iteration 13, loss = 1.25496989\n",
            "Iteration 14, loss = 1.24161024\n",
            "Iteration 15, loss = 1.22441087\n",
            "Iteration 16, loss = 1.26350665\n",
            "Iteration 17, loss = 1.22848863\n",
            "Iteration 18, loss = 1.19140493\n",
            "Iteration 19, loss = 1.17168009\n",
            "Iteration 20, loss = 1.16760328\n",
            "Iteration 21, loss = 1.14879100\n",
            "Iteration 22, loss = 1.14089923\n",
            "Iteration 23, loss = 1.14594898\n",
            "Iteration 24, loss = 1.11483108\n",
            "Iteration 25, loss = 1.14308844\n",
            "Iteration 26, loss = 1.15413479\n",
            "Iteration 27, loss = 1.13289455\n",
            "Iteration 28, loss = 1.09202432\n",
            "Iteration 29, loss = 1.09055800\n",
            "Iteration 30, loss = 1.06781592\n",
            "Iteration 31, loss = 1.05533806\n",
            "Iteration 32, loss = 1.08769799\n",
            "Iteration 33, loss = 1.04450616\n",
            "Iteration 34, loss = 1.04563836\n",
            "Iteration 35, loss = 1.09384444\n",
            "Iteration 36, loss = 1.07336746\n",
            "Iteration 37, loss = 1.03338333\n",
            "Iteration 38, loss = 1.00198192\n",
            "Iteration 39, loss = 1.00144136\n",
            "Iteration 40, loss = 0.99453970\n",
            "Iteration 41, loss = 0.98344468\n",
            "Iteration 42, loss = 0.97510702\n",
            "Iteration 43, loss = 0.95225251\n",
            "Iteration 44, loss = 0.95584155\n",
            "Iteration 45, loss = 0.95500191\n",
            "Iteration 46, loss = 0.94020029\n",
            "Iteration 47, loss = 0.97229750\n",
            "Iteration 48, loss = 0.95604157\n",
            "Iteration 49, loss = 0.93653317\n",
            "Iteration 50, loss = 0.91774700\n",
            "Iteration 51, loss = 0.92541475\n",
            "Iteration 52, loss = 0.89582890\n",
            "Iteration 53, loss = 0.88515442\n",
            "Iteration 54, loss = 0.85114124\n",
            "Iteration 55, loss = 0.85390377\n",
            "Iteration 56, loss = 0.85529368\n",
            "Iteration 57, loss = 0.85158101\n",
            "Iteration 58, loss = 0.84196703\n",
            "Iteration 59, loss = 0.82199589\n",
            "Iteration 60, loss = 0.82732129\n",
            "Iteration 61, loss = 0.83402901\n",
            "Iteration 62, loss = 0.86691678\n",
            "Iteration 63, loss = 0.82146470\n",
            "Iteration 64, loss = 0.80361173\n",
            "Iteration 65, loss = 0.78399471\n",
            "Iteration 66, loss = 0.77612776\n",
            "Iteration 67, loss = 0.78752683\n",
            "Iteration 68, loss = 0.78381772\n",
            "Iteration 69, loss = 0.74889180\n",
            "Iteration 70, loss = 0.75697058\n",
            "Iteration 71, loss = 0.72309490\n",
            "Iteration 72, loss = 0.74352841\n",
            "Iteration 73, loss = 0.75078375\n",
            "Iteration 74, loss = 0.72283498\n",
            "Iteration 75, loss = 0.75132404\n",
            "Iteration 76, loss = 0.80558617\n",
            "Iteration 77, loss = 0.80023675\n",
            "Iteration 78, loss = 0.81374541\n",
            "Iteration 79, loss = 0.73358245\n",
            "Iteration 80, loss = 0.71338816\n",
            "Iteration 81, loss = 0.70550674\n",
            "Iteration 82, loss = 0.78541305\n",
            "Iteration 83, loss = 0.73562605\n",
            "Iteration 84, loss = 0.76389756\n",
            "Iteration 85, loss = 0.74673454\n",
            "Iteration 86, loss = 0.79502295\n",
            "Iteration 87, loss = 0.82131538\n",
            "Iteration 88, loss = 0.75312686\n",
            "Iteration 89, loss = 0.67532368\n",
            "Iteration 90, loss = 0.67417143\n",
            "Iteration 91, loss = 0.66984626\n",
            "Iteration 92, loss = 0.67440796\n",
            "Iteration 93, loss = 0.64859556\n",
            "Iteration 94, loss = 0.62716915\n",
            "Iteration 95, loss = 0.67623635\n",
            "Iteration 96, loss = 0.66090682\n",
            "Iteration 97, loss = 0.62895830\n",
            "Iteration 98, loss = 0.61092822\n",
            "Iteration 99, loss = 0.61982487\n",
            "Iteration 100, loss = 0.64856275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39165973\n",
            "Iteration 2, loss = 1.35289377\n",
            "Iteration 3, loss = 1.35403196\n",
            "Iteration 4, loss = 1.32624668\n",
            "Iteration 5, loss = 1.31766277\n",
            "Iteration 6, loss = 1.29873512\n",
            "Iteration 7, loss = 1.30072467\n",
            "Iteration 8, loss = 1.28784939\n",
            "Iteration 9, loss = 1.26048158\n",
            "Iteration 10, loss = 1.24715821\n",
            "Iteration 11, loss = 1.22923359\n",
            "Iteration 12, loss = 1.20646842\n",
            "Iteration 13, loss = 1.22934290\n",
            "Iteration 14, loss = 1.21411368\n",
            "Iteration 15, loss = 1.18329696\n",
            "Iteration 16, loss = 1.21945638\n",
            "Iteration 17, loss = 1.18257716\n",
            "Iteration 18, loss = 1.14973899\n",
            "Iteration 19, loss = 1.13469035\n",
            "Iteration 20, loss = 1.15105849\n",
            "Iteration 21, loss = 1.10016398\n",
            "Iteration 22, loss = 1.09039627\n",
            "Iteration 23, loss = 1.10712006\n",
            "Iteration 24, loss = 1.10944488\n",
            "Iteration 25, loss = 1.12401956\n",
            "Iteration 26, loss = 1.14193850\n",
            "Iteration 27, loss = 1.10208415\n",
            "Iteration 28, loss = 1.06206828\n",
            "Iteration 29, loss = 1.04215464\n",
            "Iteration 30, loss = 1.06947699\n",
            "Iteration 31, loss = 1.04866448\n",
            "Iteration 32, loss = 1.06410733\n",
            "Iteration 33, loss = 1.05611727\n",
            "Iteration 34, loss = 1.07004009\n",
            "Iteration 35, loss = 1.03162469\n",
            "Iteration 36, loss = 1.00360195\n",
            "Iteration 37, loss = 0.97847487\n",
            "Iteration 38, loss = 0.97309468\n",
            "Iteration 39, loss = 0.96284553\n",
            "Iteration 40, loss = 0.96542417\n",
            "Iteration 41, loss = 0.92336582\n",
            "Iteration 42, loss = 0.93079002\n",
            "Iteration 43, loss = 0.92283571\n",
            "Iteration 44, loss = 0.93424619\n",
            "Iteration 45, loss = 0.96562948\n",
            "Iteration 46, loss = 0.89479127\n",
            "Iteration 47, loss = 0.88169457\n",
            "Iteration 48, loss = 0.90565171\n",
            "Iteration 49, loss = 0.89962482\n",
            "Iteration 50, loss = 0.88186302\n",
            "Iteration 51, loss = 0.87883422\n",
            "Iteration 52, loss = 0.85627055\n",
            "Iteration 53, loss = 0.83620020\n",
            "Iteration 54, loss = 0.81552770\n",
            "Iteration 55, loss = 0.84600360\n",
            "Iteration 56, loss = 0.87877699\n",
            "Iteration 57, loss = 0.83197961\n",
            "Iteration 58, loss = 0.80340916\n",
            "Iteration 59, loss = 0.77604124\n",
            "Iteration 60, loss = 0.78683122\n",
            "Iteration 61, loss = 0.81587293\n",
            "Iteration 62, loss = 0.81258836\n",
            "Iteration 63, loss = 0.78906370\n",
            "Iteration 64, loss = 0.78539891\n",
            "Iteration 65, loss = 0.76912126\n",
            "Iteration 66, loss = 0.75028045\n",
            "Iteration 67, loss = 0.79799105\n",
            "Iteration 68, loss = 0.78427654\n",
            "Iteration 69, loss = 0.72820793\n",
            "Iteration 70, loss = 0.71040681\n",
            "Iteration 71, loss = 0.70624240\n",
            "Iteration 72, loss = 0.74103285\n",
            "Iteration 73, loss = 0.69665839\n",
            "Iteration 74, loss = 0.70123978\n",
            "Iteration 75, loss = 0.72443690\n",
            "Iteration 76, loss = 0.80421033\n",
            "Iteration 77, loss = 0.76726876\n",
            "Iteration 78, loss = 0.79229560\n",
            "Iteration 79, loss = 0.72470711\n",
            "Iteration 80, loss = 0.70498708\n",
            "Iteration 81, loss = 0.68010150\n",
            "Iteration 82, loss = 0.67239785\n",
            "Iteration 83, loss = 0.67029237\n",
            "Iteration 84, loss = 0.65365339\n",
            "Iteration 85, loss = 0.66676689\n",
            "Iteration 86, loss = 0.69630942\n",
            "Iteration 87, loss = 0.65197181\n",
            "Iteration 88, loss = 0.67369265\n",
            "Iteration 89, loss = 0.60699949\n",
            "Iteration 90, loss = 0.66622675\n",
            "Iteration 91, loss = 0.67319327\n",
            "Iteration 92, loss = 0.61511798\n",
            "Iteration 93, loss = 0.62830497\n",
            "Iteration 94, loss = 0.66078168\n",
            "Iteration 95, loss = 0.67475075\n",
            "Iteration 96, loss = 0.63015125\n",
            "Iteration 97, loss = 0.58988338\n",
            "Iteration 98, loss = 0.59009008\n",
            "Iteration 99, loss = 0.62574694\n",
            "Iteration 100, loss = 0.63487438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39872123\n",
            "Iteration 2, loss = 1.37067448\n",
            "Iteration 3, loss = 1.35161491\n",
            "Iteration 4, loss = 1.33339278\n",
            "Iteration 5, loss = 1.33052121\n",
            "Iteration 6, loss = 1.31157885\n",
            "Iteration 7, loss = 1.31487099\n",
            "Iteration 8, loss = 1.30579281\n",
            "Iteration 9, loss = 1.29116131\n",
            "Iteration 10, loss = 1.27240276\n",
            "Iteration 11, loss = 1.25407336\n",
            "Iteration 12, loss = 1.24027698\n",
            "Iteration 13, loss = 1.23809339\n",
            "Iteration 14, loss = 1.21832007\n",
            "Iteration 15, loss = 1.19893098\n",
            "Iteration 16, loss = 1.20234371\n",
            "Iteration 17, loss = 1.19576138\n",
            "Iteration 18, loss = 1.18669014\n",
            "Iteration 19, loss = 1.16274827\n",
            "Iteration 20, loss = 1.20230335\n",
            "Iteration 21, loss = 1.16383044\n",
            "Iteration 22, loss = 1.15982040\n",
            "Iteration 23, loss = 1.13718561\n",
            "Iteration 24, loss = 1.12341242\n",
            "Iteration 25, loss = 1.10935681\n",
            "Iteration 26, loss = 1.14136231\n",
            "Iteration 27, loss = 1.13234365\n",
            "Iteration 28, loss = 1.08080873\n",
            "Iteration 29, loss = 1.08297162\n",
            "Iteration 30, loss = 1.06802154\n",
            "Iteration 31, loss = 1.04669183\n",
            "Iteration 32, loss = 1.04127323\n",
            "Iteration 33, loss = 1.05031153\n",
            "Iteration 34, loss = 1.08118605\n",
            "Iteration 35, loss = 1.06671968\n",
            "Iteration 36, loss = 1.02521267\n",
            "Iteration 37, loss = 1.00308609\n",
            "Iteration 38, loss = 0.98601928\n",
            "Iteration 39, loss = 0.97950030\n",
            "Iteration 40, loss = 0.97706581\n",
            "Iteration 41, loss = 0.95018735\n",
            "Iteration 42, loss = 0.96793391\n",
            "Iteration 43, loss = 0.96025012\n",
            "Iteration 44, loss = 0.96014561\n",
            "Iteration 45, loss = 0.92879560\n",
            "Iteration 46, loss = 0.93459572\n",
            "Iteration 47, loss = 0.90699786\n",
            "Iteration 48, loss = 0.89502982\n",
            "Iteration 49, loss = 0.87815627\n",
            "Iteration 50, loss = 0.85350128\n",
            "Iteration 51, loss = 0.88339813\n",
            "Iteration 52, loss = 0.88396785\n",
            "Iteration 53, loss = 0.86921312\n",
            "Iteration 54, loss = 0.81620996\n",
            "Iteration 55, loss = 0.85119592\n",
            "Iteration 56, loss = 0.87583282\n",
            "Iteration 57, loss = 0.87606874\n",
            "Iteration 58, loss = 0.88918033\n",
            "Iteration 59, loss = 0.86024853\n",
            "Iteration 60, loss = 0.79525213\n",
            "Iteration 61, loss = 0.78969377\n",
            "Iteration 62, loss = 0.78597402\n",
            "Iteration 63, loss = 0.78500909\n",
            "Iteration 64, loss = 0.76775388\n",
            "Iteration 65, loss = 0.81147955\n",
            "Iteration 66, loss = 0.75539214\n",
            "Iteration 67, loss = 0.76926457\n",
            "Iteration 68, loss = 0.74291726\n",
            "Iteration 69, loss = 0.71727284\n",
            "Iteration 70, loss = 0.71556624\n",
            "Iteration 71, loss = 0.73753433\n",
            "Iteration 72, loss = 0.72696295\n",
            "Iteration 73, loss = 0.71560864\n",
            "Iteration 74, loss = 0.72475894\n",
            "Iteration 75, loss = 0.74148421\n",
            "Iteration 76, loss = 0.79378739\n",
            "Iteration 77, loss = 0.77252119\n",
            "Iteration 78, loss = 0.77426440\n",
            "Iteration 79, loss = 0.72618192\n",
            "Iteration 80, loss = 0.66540575\n",
            "Iteration 81, loss = 0.65453828\n",
            "Iteration 82, loss = 0.66426714\n",
            "Iteration 83, loss = 0.66167172\n",
            "Iteration 84, loss = 0.68758001\n",
            "Iteration 85, loss = 0.65608746\n",
            "Iteration 86, loss = 0.70235992\n",
            "Iteration 87, loss = 0.76919797\n",
            "Iteration 88, loss = 0.77622201\n",
            "Iteration 89, loss = 0.71369538\n",
            "Iteration 90, loss = 0.68724247\n",
            "Iteration 91, loss = 0.67961786\n",
            "Iteration 92, loss = 0.66133276\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38439212\n",
            "Iteration 2, loss = 1.36367864\n",
            "Iteration 3, loss = 1.34283027\n",
            "Iteration 4, loss = 1.33115879\n",
            "Iteration 5, loss = 1.32041125\n",
            "Iteration 6, loss = 1.30544098\n",
            "Iteration 7, loss = 1.31020626\n",
            "Iteration 8, loss = 1.29448706\n",
            "Iteration 9, loss = 1.28423332\n",
            "Iteration 10, loss = 1.26800012\n",
            "Iteration 11, loss = 1.25196185\n",
            "Iteration 12, loss = 1.23984762\n",
            "Iteration 13, loss = 1.23041135\n",
            "Iteration 14, loss = 1.21710171\n",
            "Iteration 15, loss = 1.21249505\n",
            "Iteration 16, loss = 1.22036123\n",
            "Iteration 17, loss = 1.16749381\n",
            "Iteration 18, loss = 1.15071805\n",
            "Iteration 19, loss = 1.14916345\n",
            "Iteration 20, loss = 1.17070148\n",
            "Iteration 21, loss = 1.14237159\n",
            "Iteration 22, loss = 1.11557174\n",
            "Iteration 23, loss = 1.09482790\n",
            "Iteration 24, loss = 1.08386439\n",
            "Iteration 25, loss = 1.08409980\n",
            "Iteration 26, loss = 1.08339987\n",
            "Iteration 27, loss = 1.11391831\n",
            "Iteration 28, loss = 1.07410338\n",
            "Iteration 29, loss = 1.05464098\n",
            "Iteration 30, loss = 1.01883432\n",
            "Iteration 31, loss = 0.99837858\n",
            "Iteration 32, loss = 1.02101394\n",
            "Iteration 33, loss = 1.00560228\n",
            "Iteration 34, loss = 1.03902898\n",
            "Iteration 35, loss = 1.00009076\n",
            "Iteration 36, loss = 0.96578996\n",
            "Iteration 37, loss = 0.94833948\n",
            "Iteration 38, loss = 0.93275020\n",
            "Iteration 39, loss = 0.94549495\n",
            "Iteration 40, loss = 0.91936328\n",
            "Iteration 41, loss = 0.92584655\n",
            "Iteration 42, loss = 0.98917151\n",
            "Iteration 43, loss = 0.93748748\n",
            "Iteration 44, loss = 0.88252597\n",
            "Iteration 45, loss = 0.87552364\n",
            "Iteration 46, loss = 0.87324534\n",
            "Iteration 47, loss = 0.85766892\n",
            "Iteration 48, loss = 0.94507206\n",
            "Iteration 49, loss = 0.88160943\n",
            "Iteration 50, loss = 0.86492885\n",
            "Iteration 51, loss = 0.83701114\n",
            "Iteration 52, loss = 0.83696006\n",
            "Iteration 53, loss = 0.80817334\n",
            "Iteration 54, loss = 0.81996019\n",
            "Iteration 55, loss = 0.83547185\n",
            "Iteration 56, loss = 0.80046117\n",
            "Iteration 57, loss = 0.80887680\n",
            "Iteration 58, loss = 0.94008737\n",
            "Iteration 59, loss = 0.83758865\n",
            "Iteration 60, loss = 0.78466950\n",
            "Iteration 61, loss = 0.78324885\n",
            "Iteration 62, loss = 0.77632346\n",
            "Iteration 63, loss = 0.74966888\n",
            "Iteration 64, loss = 0.81126133\n",
            "Iteration 65, loss = 0.75371959\n",
            "Iteration 66, loss = 0.72945662\n",
            "Iteration 67, loss = 0.76139213\n",
            "Iteration 68, loss = 0.78213583\n",
            "Iteration 69, loss = 0.74802062\n",
            "Iteration 70, loss = 0.76789193\n",
            "Iteration 71, loss = 0.78123027\n",
            "Iteration 72, loss = 0.73863861\n",
            "Iteration 73, loss = 0.72414569\n",
            "Iteration 74, loss = 0.70356125\n",
            "Iteration 75, loss = 0.71227765\n",
            "Iteration 76, loss = 0.69443962\n",
            "Iteration 77, loss = 0.68744425\n",
            "Iteration 78, loss = 0.65782353\n",
            "Iteration 79, loss = 0.64928539\n",
            "Iteration 80, loss = 0.66917455\n",
            "Iteration 81, loss = 0.66742431\n",
            "Iteration 82, loss = 0.64245143\n",
            "Iteration 83, loss = 0.68554868\n",
            "Iteration 84, loss = 0.71065677\n",
            "Iteration 85, loss = 0.67217432\n",
            "Iteration 86, loss = 0.65273958\n",
            "Iteration 87, loss = 0.64580251\n",
            "Iteration 88, loss = 0.64801938\n",
            "Iteration 89, loss = 0.60301697\n",
            "Iteration 90, loss = 0.63108976\n",
            "Iteration 91, loss = 0.63763257\n",
            "Iteration 92, loss = 0.71554853\n",
            "Iteration 93, loss = 0.63875874\n",
            "Iteration 94, loss = 0.64468577\n",
            "Iteration 95, loss = 0.69585116\n",
            "Iteration 96, loss = 0.71302018\n",
            "Iteration 97, loss = 0.65443748\n",
            "Iteration 98, loss = 0.59172238\n",
            "Iteration 99, loss = 0.60370220\n",
            "Iteration 100, loss = 0.61981674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39439843\n",
            "Iteration 2, loss = 1.36062477\n",
            "Iteration 3, loss = 1.35185477\n",
            "Iteration 4, loss = 1.34319127\n",
            "Iteration 5, loss = 1.33595018\n",
            "Iteration 6, loss = 1.32341342\n",
            "Iteration 7, loss = 1.31742324\n",
            "Iteration 8, loss = 1.31204107\n",
            "Iteration 9, loss = 1.30196328\n",
            "Iteration 10, loss = 1.28574014\n",
            "Iteration 11, loss = 1.27475312\n",
            "Iteration 12, loss = 1.26420984\n",
            "Iteration 13, loss = 1.25095845\n",
            "Iteration 14, loss = 1.23715050\n",
            "Iteration 15, loss = 1.23509509\n",
            "Iteration 16, loss = 1.22057618\n",
            "Iteration 17, loss = 1.19251561\n",
            "Iteration 18, loss = 1.18686288\n",
            "Iteration 19, loss = 1.18352566\n",
            "Iteration 20, loss = 1.17947498\n",
            "Iteration 21, loss = 1.16741478\n",
            "Iteration 22, loss = 1.14577658\n",
            "Iteration 23, loss = 1.12466217\n",
            "Iteration 24, loss = 1.12189564\n",
            "Iteration 25, loss = 1.08837185\n",
            "Iteration 26, loss = 1.08196348\n",
            "Iteration 27, loss = 1.13992476\n",
            "Iteration 28, loss = 1.09960318\n",
            "Iteration 29, loss = 1.06748188\n",
            "Iteration 30, loss = 1.05027362\n",
            "Iteration 31, loss = 1.02895322\n",
            "Iteration 32, loss = 1.03820718\n",
            "Iteration 33, loss = 1.05657517\n",
            "Iteration 34, loss = 1.05310921\n",
            "Iteration 35, loss = 1.03456946\n",
            "Iteration 36, loss = 0.98956097\n",
            "Iteration 37, loss = 0.97660478\n",
            "Iteration 38, loss = 0.97341116\n",
            "Iteration 39, loss = 0.95115282\n",
            "Iteration 40, loss = 0.96242183\n",
            "Iteration 41, loss = 0.94439555\n",
            "Iteration 42, loss = 0.98824805\n",
            "Iteration 43, loss = 0.98183973\n",
            "Iteration 44, loss = 0.96500211\n",
            "Iteration 45, loss = 0.98891770\n",
            "Iteration 46, loss = 0.93662890\n",
            "Iteration 47, loss = 0.91821603\n",
            "Iteration 48, loss = 0.97129008\n",
            "Iteration 49, loss = 0.91489223\n",
            "Iteration 50, loss = 0.87592213\n",
            "Iteration 51, loss = 0.87101331\n",
            "Iteration 52, loss = 0.84416295\n",
            "Iteration 53, loss = 0.85706760\n",
            "Iteration 54, loss = 0.89187701\n",
            "Iteration 55, loss = 0.95878278\n",
            "Iteration 56, loss = 0.94698167\n",
            "Iteration 57, loss = 0.87701114\n",
            "Iteration 58, loss = 0.88613122\n",
            "Iteration 59, loss = 0.86449321\n",
            "Iteration 60, loss = 0.82443832\n",
            "Iteration 61, loss = 0.89708763\n",
            "Iteration 62, loss = 0.93732935\n",
            "Iteration 63, loss = 0.86307294\n",
            "Iteration 64, loss = 0.83406083\n",
            "Iteration 65, loss = 0.79636914\n",
            "Iteration 66, loss = 0.79945541\n",
            "Iteration 67, loss = 0.80116719\n",
            "Iteration 68, loss = 0.82391403\n",
            "Iteration 69, loss = 0.77412054\n",
            "Iteration 70, loss = 0.77705112\n",
            "Iteration 71, loss = 0.77754222\n",
            "Iteration 72, loss = 0.75094100\n",
            "Iteration 73, loss = 0.75860638\n",
            "Iteration 74, loss = 0.74534306\n",
            "Iteration 75, loss = 0.80045150\n",
            "Iteration 76, loss = 0.74724683\n",
            "Iteration 77, loss = 0.73196672\n",
            "Iteration 78, loss = 0.72123687\n",
            "Iteration 79, loss = 0.71629071\n",
            "Iteration 80, loss = 0.75025282\n",
            "Iteration 81, loss = 0.74254282\n",
            "Iteration 82, loss = 0.70486549\n",
            "Iteration 83, loss = 0.69369643\n",
            "Iteration 84, loss = 0.70529188\n",
            "Iteration 85, loss = 0.69756052\n",
            "Iteration 86, loss = 0.71854091\n",
            "Iteration 87, loss = 0.73972897\n",
            "Iteration 88, loss = 0.72447988\n",
            "Iteration 89, loss = 0.69395168\n",
            "Iteration 90, loss = 0.67518288\n",
            "Iteration 91, loss = 0.65138222\n",
            "Iteration 92, loss = 0.64548019\n",
            "Iteration 93, loss = 0.66012996\n",
            "Iteration 94, loss = 0.70722067\n",
            "Iteration 95, loss = 0.76917686\n",
            "Iteration 96, loss = 0.72654095\n",
            "Iteration 97, loss = 0.69553588\n",
            "Iteration 98, loss = 0.63788653\n",
            "Iteration 99, loss = 0.66941687\n",
            "Iteration 100, loss = 0.64895986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38460133\n",
            "Iteration 2, loss = 1.34842585\n",
            "Iteration 3, loss = 1.33352319\n",
            "Iteration 4, loss = 1.33434806\n",
            "Iteration 5, loss = 1.32917715\n",
            "Iteration 6, loss = 1.31681520\n",
            "Iteration 7, loss = 1.30757664\n",
            "Iteration 8, loss = 1.29992985\n",
            "Iteration 9, loss = 1.29354251\n",
            "Iteration 10, loss = 1.27662343\n",
            "Iteration 11, loss = 1.26602473\n",
            "Iteration 12, loss = 1.25783381\n",
            "Iteration 13, loss = 1.25267445\n",
            "Iteration 14, loss = 1.23150264\n",
            "Iteration 15, loss = 1.23016964\n",
            "Iteration 16, loss = 1.22915757\n",
            "Iteration 17, loss = 1.19021748\n",
            "Iteration 18, loss = 1.18545892\n",
            "Iteration 19, loss = 1.18365509\n",
            "Iteration 20, loss = 1.17853336\n",
            "Iteration 21, loss = 1.15330976\n",
            "Iteration 22, loss = 1.15834543\n",
            "Iteration 23, loss = 1.13534792\n",
            "Iteration 24, loss = 1.12532015\n",
            "Iteration 25, loss = 1.11460615\n",
            "Iteration 26, loss = 1.10340129\n",
            "Iteration 27, loss = 1.11586090\n",
            "Iteration 28, loss = 1.09776804\n",
            "Iteration 29, loss = 1.05705349\n",
            "Iteration 30, loss = 1.05772876\n",
            "Iteration 31, loss = 1.05340235\n",
            "Iteration 32, loss = 1.06130389\n",
            "Iteration 33, loss = 1.06384673\n",
            "Iteration 34, loss = 1.05420555\n",
            "Iteration 35, loss = 1.04634205\n",
            "Iteration 36, loss = 1.02684881\n",
            "Iteration 37, loss = 0.99246574\n",
            "Iteration 38, loss = 0.98618418\n",
            "Iteration 39, loss = 0.97330371\n",
            "Iteration 40, loss = 0.98011897\n",
            "Iteration 41, loss = 0.97806400\n",
            "Iteration 42, loss = 0.99921568\n",
            "Iteration 43, loss = 1.00324738\n",
            "Iteration 44, loss = 0.96353174\n",
            "Iteration 45, loss = 0.91368604\n",
            "Iteration 46, loss = 0.92534342\n",
            "Iteration 47, loss = 0.94199566\n",
            "Iteration 48, loss = 0.97471371\n",
            "Iteration 49, loss = 0.94493024\n",
            "Iteration 50, loss = 0.90294881\n",
            "Iteration 51, loss = 0.91371005\n",
            "Iteration 52, loss = 0.87667027\n",
            "Iteration 53, loss = 0.87762544\n",
            "Iteration 54, loss = 0.89263572\n",
            "Iteration 55, loss = 1.00077140\n",
            "Iteration 56, loss = 0.98953607\n",
            "Iteration 57, loss = 0.96050570\n",
            "Iteration 58, loss = 0.91743402\n",
            "Iteration 59, loss = 0.85091078\n",
            "Iteration 60, loss = 0.84557032\n",
            "Iteration 61, loss = 0.92926297\n",
            "Iteration 62, loss = 0.91608291\n",
            "Iteration 63, loss = 0.86496884\n",
            "Iteration 64, loss = 0.83406362\n",
            "Iteration 65, loss = 0.81913300\n",
            "Iteration 66, loss = 0.83328958\n",
            "Iteration 67, loss = 0.81581789\n",
            "Iteration 68, loss = 0.82978494\n",
            "Iteration 69, loss = 0.78646470\n",
            "Iteration 70, loss = 0.78617428\n",
            "Iteration 71, loss = 0.78754861\n",
            "Iteration 72, loss = 0.78316550\n",
            "Iteration 73, loss = 0.76811431\n",
            "Iteration 74, loss = 0.75816580\n",
            "Iteration 75, loss = 0.81004161\n",
            "Iteration 76, loss = 0.75943930\n",
            "Iteration 77, loss = 0.78835813\n",
            "Iteration 78, loss = 0.75587968\n",
            "Iteration 79, loss = 0.75325205\n",
            "Iteration 80, loss = 0.77068300\n",
            "Iteration 81, loss = 0.75184805\n",
            "Iteration 82, loss = 0.72860041\n",
            "Iteration 83, loss = 0.73549566\n",
            "Iteration 84, loss = 0.69774228\n",
            "Iteration 85, loss = 0.68866784\n",
            "Iteration 86, loss = 0.72218570\n",
            "Iteration 87, loss = 0.72161574\n",
            "Iteration 88, loss = 0.76256031\n",
            "Iteration 89, loss = 0.78720083\n",
            "Iteration 90, loss = 0.69243758\n",
            "Iteration 91, loss = 0.74827372\n",
            "Iteration 92, loss = 0.70824271\n",
            "Iteration 93, loss = 0.69587656\n",
            "Iteration 94, loss = 0.72562355\n",
            "Iteration 95, loss = 0.77225486\n",
            "Iteration 96, loss = 0.80508144\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38448505\n",
            "Iteration 2, loss = 1.36358482\n",
            "Iteration 3, loss = 1.34963465\n",
            "Iteration 4, loss = 1.34523856\n",
            "Iteration 5, loss = 1.34183239\n",
            "Iteration 6, loss = 1.31186288\n",
            "Iteration 7, loss = 1.30670686\n",
            "Iteration 8, loss = 1.30901160\n",
            "Iteration 9, loss = 1.28439274\n",
            "Iteration 10, loss = 1.26133266\n",
            "Iteration 11, loss = 1.25878149\n",
            "Iteration 12, loss = 1.25318546\n",
            "Iteration 13, loss = 1.23347302\n",
            "Iteration 14, loss = 1.22458497\n",
            "Iteration 15, loss = 1.23908090\n",
            "Iteration 16, loss = 1.20637548\n",
            "Iteration 17, loss = 1.17497896\n",
            "Iteration 18, loss = 1.17283963\n",
            "Iteration 19, loss = 1.17367514\n",
            "Iteration 20, loss = 1.17026165\n",
            "Iteration 21, loss = 1.15360265\n",
            "Iteration 22, loss = 1.16463238\n",
            "Iteration 23, loss = 1.12452211\n",
            "Iteration 24, loss = 1.10288883\n",
            "Iteration 25, loss = 1.09316074\n",
            "Iteration 26, loss = 1.08090221\n",
            "Iteration 27, loss = 1.15126422\n",
            "Iteration 28, loss = 1.09739867\n",
            "Iteration 29, loss = 1.08037532\n",
            "Iteration 30, loss = 1.05233578\n",
            "Iteration 31, loss = 1.05478199\n",
            "Iteration 32, loss = 1.03903745\n",
            "Iteration 33, loss = 1.04771047\n",
            "Iteration 34, loss = 1.02773005\n",
            "Iteration 35, loss = 1.00112310\n",
            "Iteration 36, loss = 0.98845051\n",
            "Iteration 37, loss = 0.98522632\n",
            "Iteration 38, loss = 0.97562109\n",
            "Iteration 39, loss = 0.97009334\n",
            "Iteration 40, loss = 0.95957472\n",
            "Iteration 41, loss = 0.99035178\n",
            "Iteration 42, loss = 1.02288636\n",
            "Iteration 43, loss = 0.97683454\n",
            "Iteration 44, loss = 0.97606752\n",
            "Iteration 45, loss = 0.92452444\n",
            "Iteration 46, loss = 0.91045252\n",
            "Iteration 47, loss = 0.89403428\n",
            "Iteration 48, loss = 0.90322818\n",
            "Iteration 49, loss = 0.87866247\n",
            "Iteration 50, loss = 0.87412994\n",
            "Iteration 51, loss = 0.89108858\n",
            "Iteration 52, loss = 0.88863180\n",
            "Iteration 53, loss = 0.90399090\n",
            "Iteration 54, loss = 0.84470027\n",
            "Iteration 55, loss = 0.87937632\n",
            "Iteration 56, loss = 0.90114019\n",
            "Iteration 57, loss = 0.85449729\n",
            "Iteration 58, loss = 0.82556694\n",
            "Iteration 59, loss = 0.82637521\n",
            "Iteration 60, loss = 0.80566170\n",
            "Iteration 61, loss = 0.91549787\n",
            "Iteration 62, loss = 0.87294721\n",
            "Iteration 63, loss = 0.86521654\n",
            "Iteration 64, loss = 0.82679358\n",
            "Iteration 65, loss = 0.79006138\n",
            "Iteration 66, loss = 0.81853784\n",
            "Iteration 67, loss = 0.87563974\n",
            "Iteration 68, loss = 0.85252325\n",
            "Iteration 69, loss = 0.78759249\n",
            "Iteration 70, loss = 0.78879347\n",
            "Iteration 71, loss = 0.76229876\n",
            "Iteration 72, loss = 0.73974012\n",
            "Iteration 73, loss = 0.72799844\n",
            "Iteration 74, loss = 0.73580293\n",
            "Iteration 75, loss = 0.77427233\n",
            "Iteration 76, loss = 0.76896310\n",
            "Iteration 77, loss = 0.72919244\n",
            "Iteration 78, loss = 0.72915913\n",
            "Iteration 79, loss = 0.72940235\n",
            "Iteration 80, loss = 0.69125616\n",
            "Iteration 81, loss = 0.71295019\n",
            "Iteration 82, loss = 0.70056594\n",
            "Iteration 83, loss = 0.70057212\n",
            "Iteration 84, loss = 0.69049704\n",
            "Iteration 85, loss = 0.69790809\n",
            "Iteration 86, loss = 0.70213807\n",
            "Iteration 87, loss = 0.69431982\n",
            "Iteration 88, loss = 0.68819404\n",
            "Iteration 89, loss = 0.64827641\n",
            "Iteration 90, loss = 0.68119483\n",
            "Iteration 91, loss = 0.69022521\n",
            "Iteration 92, loss = 0.66691521\n",
            "Iteration 93, loss = 0.66402963\n",
            "Iteration 94, loss = 0.69931066\n",
            "Iteration 95, loss = 0.69397057\n",
            "Iteration 96, loss = 0.74708219\n",
            "Iteration 97, loss = 0.69652879\n",
            "Iteration 98, loss = 0.65122344\n",
            "Iteration 99, loss = 0.65895133\n",
            "Iteration 100, loss = 0.64897889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38152482\n",
            "Iteration 2, loss = 1.36204013\n",
            "Iteration 3, loss = 1.34463504\n",
            "Iteration 4, loss = 1.33480337\n",
            "Iteration 5, loss = 1.32399059\n",
            "Iteration 6, loss = 1.30070916\n",
            "Iteration 7, loss = 1.28546635\n",
            "Iteration 8, loss = 1.28464736\n",
            "Iteration 9, loss = 1.26373282\n",
            "Iteration 10, loss = 1.25072768\n",
            "Iteration 11, loss = 1.24379904\n",
            "Iteration 12, loss = 1.21556796\n",
            "Iteration 13, loss = 1.19793512\n",
            "Iteration 14, loss = 1.20916651\n",
            "Iteration 15, loss = 1.22128936\n",
            "Iteration 16, loss = 1.20195921\n",
            "Iteration 17, loss = 1.16391572\n",
            "Iteration 18, loss = 1.14483991\n",
            "Iteration 19, loss = 1.12519433\n",
            "Iteration 20, loss = 1.13407060\n",
            "Iteration 21, loss = 1.11124921\n",
            "Iteration 22, loss = 1.11530969\n",
            "Iteration 23, loss = 1.13543939\n",
            "Iteration 24, loss = 1.08904279\n",
            "Iteration 25, loss = 1.09359283\n",
            "Iteration 26, loss = 1.09146127\n",
            "Iteration 27, loss = 1.11863496\n",
            "Iteration 28, loss = 1.05719976\n",
            "Iteration 29, loss = 1.05299274\n",
            "Iteration 30, loss = 1.03373425\n",
            "Iteration 31, loss = 1.04241942\n",
            "Iteration 32, loss = 1.03168887\n",
            "Iteration 33, loss = 1.03959861\n",
            "Iteration 34, loss = 1.02052828\n",
            "Iteration 35, loss = 1.00889337\n",
            "Iteration 36, loss = 0.99255234\n",
            "Iteration 37, loss = 0.98314507\n",
            "Iteration 38, loss = 0.97601205\n",
            "Iteration 39, loss = 0.98302886\n",
            "Iteration 40, loss = 0.96695228\n",
            "Iteration 41, loss = 0.97649030\n",
            "Iteration 42, loss = 0.96066642\n",
            "Iteration 43, loss = 0.93366604\n",
            "Iteration 44, loss = 0.91402205\n",
            "Iteration 45, loss = 0.91754985\n",
            "Iteration 46, loss = 0.91379974\n",
            "Iteration 47, loss = 0.89694238\n",
            "Iteration 48, loss = 0.88864958\n",
            "Iteration 49, loss = 0.89876669\n",
            "Iteration 50, loss = 0.90224182\n",
            "Iteration 51, loss = 0.93221404\n",
            "Iteration 52, loss = 0.94466263\n",
            "Iteration 53, loss = 0.92083668\n",
            "Iteration 54, loss = 0.91706016\n",
            "Iteration 55, loss = 0.86480745\n",
            "Iteration 56, loss = 0.85446416\n",
            "Iteration 57, loss = 0.85477644\n",
            "Iteration 58, loss = 0.85572409\n",
            "Iteration 59, loss = 0.84574694\n",
            "Iteration 60, loss = 0.86342903\n",
            "Iteration 61, loss = 0.85942389\n",
            "Iteration 62, loss = 0.80120966\n",
            "Iteration 63, loss = 0.82458118\n",
            "Iteration 64, loss = 0.79065038\n",
            "Iteration 65, loss = 0.78541154\n",
            "Iteration 66, loss = 0.79928958\n",
            "Iteration 67, loss = 0.92400208\n",
            "Iteration 68, loss = 0.84359404\n",
            "Iteration 69, loss = 0.78077294\n",
            "Iteration 70, loss = 0.77103280\n",
            "Iteration 71, loss = 0.74991188\n",
            "Iteration 72, loss = 0.75054995\n",
            "Iteration 73, loss = 0.72793822\n",
            "Iteration 74, loss = 0.75217468\n",
            "Iteration 75, loss = 0.80867602\n",
            "Iteration 76, loss = 0.82130886\n",
            "Iteration 77, loss = 0.77014687\n",
            "Iteration 78, loss = 0.73897882\n",
            "Iteration 79, loss = 0.74169450\n",
            "Iteration 80, loss = 0.71325006\n",
            "Iteration 81, loss = 0.74465804\n",
            "Iteration 82, loss = 0.72324115\n",
            "Iteration 83, loss = 0.69723001\n",
            "Iteration 84, loss = 0.70087780\n",
            "Iteration 85, loss = 0.68196005\n",
            "Iteration 86, loss = 0.66878492\n",
            "Iteration 87, loss = 0.66317518\n",
            "Iteration 88, loss = 0.67530858\n",
            "Iteration 89, loss = 0.66487880\n",
            "Iteration 90, loss = 0.67929166\n",
            "Iteration 91, loss = 0.67540666\n",
            "Iteration 92, loss = 0.68071593\n",
            "Iteration 93, loss = 0.74004557\n",
            "Iteration 94, loss = 0.71488006\n",
            "Iteration 95, loss = 0.64423628\n",
            "Iteration 96, loss = 0.69534563\n",
            "Iteration 97, loss = 0.66794621\n",
            "Iteration 98, loss = 0.62237005\n",
            "Iteration 99, loss = 0.61516830\n",
            "Iteration 100, loss = 0.62375070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38033134\n",
            "Iteration 2, loss = 1.35064616\n",
            "Iteration 3, loss = 1.33925575\n",
            "Iteration 4, loss = 1.33049650\n",
            "Iteration 5, loss = 1.32344340\n",
            "Iteration 6, loss = 1.31163523\n",
            "Iteration 7, loss = 1.29803321\n",
            "Iteration 8, loss = 1.28004743\n",
            "Iteration 9, loss = 1.28217769\n",
            "Iteration 10, loss = 1.26919841\n",
            "Iteration 11, loss = 1.25268229\n",
            "Iteration 12, loss = 1.23751599\n",
            "Iteration 13, loss = 1.22031645\n",
            "Iteration 14, loss = 1.24140616\n",
            "Iteration 15, loss = 1.18825051\n",
            "Iteration 16, loss = 1.19226147\n",
            "Iteration 17, loss = 1.17122346\n",
            "Iteration 18, loss = 1.15102051\n",
            "Iteration 19, loss = 1.15802739\n",
            "Iteration 20, loss = 1.16537754\n",
            "Iteration 21, loss = 1.10464552\n",
            "Iteration 22, loss = 1.11955213\n",
            "Iteration 23, loss = 1.12943820\n",
            "Iteration 24, loss = 1.10596712\n",
            "Iteration 25, loss = 1.07596818\n",
            "Iteration 26, loss = 1.07602561\n",
            "Iteration 27, loss = 1.07070590\n",
            "Iteration 28, loss = 1.04003107\n",
            "Iteration 29, loss = 1.03448319\n",
            "Iteration 30, loss = 1.02634618\n",
            "Iteration 31, loss = 1.02366113\n",
            "Iteration 32, loss = 1.00767784\n",
            "Iteration 33, loss = 1.06665380\n",
            "Iteration 34, loss = 0.99745556\n",
            "Iteration 35, loss = 0.99659415\n",
            "Iteration 36, loss = 0.99103059\n",
            "Iteration 37, loss = 0.97298094\n",
            "Iteration 38, loss = 0.95824415\n",
            "Iteration 39, loss = 0.96773099\n",
            "Iteration 40, loss = 0.95191092\n",
            "Iteration 41, loss = 0.96405270\n",
            "Iteration 42, loss = 0.94416070\n",
            "Iteration 43, loss = 0.95161589\n",
            "Iteration 44, loss = 0.93848449\n",
            "Iteration 45, loss = 0.90766459\n",
            "Iteration 46, loss = 0.91047705\n",
            "Iteration 47, loss = 0.90860765\n",
            "Iteration 48, loss = 0.87884005\n",
            "Iteration 49, loss = 0.89811132\n",
            "Iteration 50, loss = 0.90453944\n",
            "Iteration 51, loss = 0.96845385\n",
            "Iteration 52, loss = 0.90210491\n",
            "Iteration 53, loss = 0.87584823\n",
            "Iteration 54, loss = 0.90015656\n",
            "Iteration 55, loss = 0.89717568\n",
            "Iteration 56, loss = 0.87406759\n",
            "Iteration 57, loss = 0.86869508\n",
            "Iteration 58, loss = 0.85399193\n",
            "Iteration 59, loss = 0.81391296\n",
            "Iteration 60, loss = 0.82174066\n",
            "Iteration 61, loss = 0.82789730\n",
            "Iteration 62, loss = 0.78897697\n",
            "Iteration 63, loss = 0.79805015\n",
            "Iteration 64, loss = 0.79895569\n",
            "Iteration 65, loss = 0.84366560\n",
            "Iteration 66, loss = 0.81691314\n",
            "Iteration 67, loss = 0.94816924\n",
            "Iteration 68, loss = 0.84680572\n",
            "Iteration 69, loss = 0.88183006\n",
            "Iteration 70, loss = 0.83180235\n",
            "Iteration 71, loss = 0.79961814\n",
            "Iteration 72, loss = 0.75857458\n",
            "Iteration 73, loss = 0.73854682\n",
            "Iteration 74, loss = 0.74223681\n",
            "Iteration 75, loss = 0.80574443\n",
            "Iteration 76, loss = 0.82546964\n",
            "Iteration 77, loss = 0.75033489\n",
            "Iteration 78, loss = 0.76797350\n",
            "Iteration 79, loss = 0.81980224\n",
            "Iteration 80, loss = 0.76196196\n",
            "Iteration 81, loss = 0.73298552\n",
            "Iteration 82, loss = 0.74026964\n",
            "Iteration 83, loss = 0.70888304\n",
            "Iteration 84, loss = 0.68527495\n",
            "Iteration 85, loss = 0.69999742\n",
            "Iteration 86, loss = 0.69705664\n",
            "Iteration 87, loss = 0.71030283\n",
            "Iteration 88, loss = 0.68433833\n",
            "Iteration 89, loss = 0.68059327\n",
            "Iteration 90, loss = 0.68365166\n",
            "Iteration 91, loss = 0.72799707\n",
            "Iteration 92, loss = 0.69873119\n",
            "Iteration 93, loss = 0.72910504\n",
            "Iteration 94, loss = 0.71019938\n",
            "Iteration 95, loss = 0.68892734\n",
            "Iteration 96, loss = 0.71248445\n",
            "Iteration 97, loss = 0.69272523\n",
            "Iteration 98, loss = 0.65045556\n",
            "Iteration 99, loss = 0.64886931\n",
            "Iteration 100, loss = 0.64240505\n",
            "36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37263043\n",
            "Iteration 2, loss = 1.20351162\n",
            "Iteration 3, loss = 1.00731040\n",
            "Iteration 4, loss = 0.84209200\n",
            "Iteration 5, loss = 0.70828667\n",
            "Iteration 6, loss = 0.55675483\n",
            "Iteration 7, loss = 0.40570259\n",
            "Iteration 8, loss = 0.28611579\n",
            "Iteration 9, loss = 0.20528177\n",
            "Iteration 10, loss = 0.15198367\n",
            "Iteration 11, loss = 0.11778080\n",
            "Iteration 12, loss = 0.09095081\n",
            "Iteration 13, loss = 0.06876358\n",
            "Iteration 14, loss = 0.05291809\n",
            "Iteration 15, loss = 0.04276969\n",
            "Iteration 16, loss = 0.03369763\n",
            "Iteration 17, loss = 0.02821512\n",
            "Iteration 18, loss = 0.02229848\n",
            "Iteration 19, loss = 0.01953617\n",
            "Iteration 20, loss = 0.01582269\n",
            "Iteration 21, loss = 0.01363231\n",
            "Iteration 22, loss = 0.01166594\n",
            "Iteration 23, loss = 0.01016472\n",
            "Iteration 24, loss = 0.00900028\n",
            "Iteration 25, loss = 0.00794587\n",
            "Iteration 26, loss = 0.00692631\n",
            "Iteration 27, loss = 0.00621609\n",
            "Iteration 28, loss = 0.00570377\n",
            "Iteration 29, loss = 0.00529391\n",
            "Iteration 30, loss = 0.00488121\n",
            "Iteration 31, loss = 0.00432338\n",
            "Iteration 32, loss = 0.00398425\n",
            "Iteration 33, loss = 0.00372177\n",
            "Iteration 34, loss = 0.00342532\n",
            "Iteration 35, loss = 0.00316495\n",
            "Iteration 36, loss = 0.00295665\n",
            "Iteration 37, loss = 0.00278969\n",
            "Iteration 38, loss = 0.00258008\n",
            "Iteration 39, loss = 0.00244078\n",
            "Iteration 40, loss = 0.00248048\n",
            "Iteration 41, loss = 0.00243824\n",
            "Iteration 42, loss = 0.00196913\n",
            "Iteration 43, loss = 0.00212371\n",
            "Iteration 44, loss = 0.00197407\n",
            "Iteration 45, loss = 0.00184047\n",
            "Iteration 46, loss = 0.00173719\n",
            "Iteration 47, loss = 0.00177098\n",
            "Iteration 48, loss = 0.00166921\n",
            "Iteration 49, loss = 0.00154500\n",
            "Iteration 50, loss = 0.00147723\n",
            "Iteration 51, loss = 0.00146374\n",
            "Iteration 52, loss = 0.00143079\n",
            "Iteration 53, loss = 0.00134755\n",
            "Iteration 54, loss = 0.00134691\n",
            "Iteration 55, loss = 0.00139439\n",
            "Iteration 56, loss = 0.00125087\n",
            "Iteration 57, loss = 0.00127066\n",
            "Iteration 58, loss = 0.00132311\n",
            "Iteration 59, loss = 0.00116560\n",
            "Iteration 60, loss = 0.00124554\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37349785\n",
            "Iteration 2, loss = 1.20500795\n",
            "Iteration 3, loss = 1.00904312\n",
            "Iteration 4, loss = 0.84273696\n",
            "Iteration 5, loss = 0.70908567\n",
            "Iteration 6, loss = 0.56111473\n",
            "Iteration 7, loss = 0.41489705\n",
            "Iteration 8, loss = 0.29786061\n",
            "Iteration 9, loss = 0.22032391\n",
            "Iteration 10, loss = 0.17052849\n",
            "Iteration 11, loss = 0.13393654\n",
            "Iteration 12, loss = 0.10444481\n",
            "Iteration 13, loss = 0.08564695\n",
            "Iteration 14, loss = 0.07187664\n",
            "Iteration 15, loss = 0.05901390\n",
            "Iteration 16, loss = 0.04866725\n",
            "Iteration 17, loss = 0.04217140\n",
            "Iteration 18, loss = 0.03524257\n",
            "Iteration 19, loss = 0.02910573\n",
            "Iteration 20, loss = 0.02481957\n",
            "Iteration 21, loss = 0.02183988\n",
            "Iteration 22, loss = 0.01884198\n",
            "Iteration 23, loss = 0.01636288\n",
            "Iteration 24, loss = 0.01244323\n",
            "Iteration 25, loss = 0.01137956\n",
            "Iteration 26, loss = 0.00966720\n",
            "Iteration 27, loss = 0.00853165\n",
            "Iteration 28, loss = 0.00739623\n",
            "Iteration 29, loss = 0.00643760\n",
            "Iteration 30, loss = 0.00585049\n",
            "Iteration 31, loss = 0.00518791\n",
            "Iteration 32, loss = 0.00471295\n",
            "Iteration 33, loss = 0.00434008\n",
            "Iteration 34, loss = 0.00399484\n",
            "Iteration 35, loss = 0.00377803\n",
            "Iteration 36, loss = 0.00328769\n",
            "Iteration 37, loss = 0.00321559\n",
            "Iteration 38, loss = 0.00284345\n",
            "Iteration 39, loss = 0.00269252\n",
            "Iteration 40, loss = 0.00262328\n",
            "Iteration 41, loss = 0.00269313\n",
            "Iteration 42, loss = 0.00214209\n",
            "Iteration 43, loss = 0.00227430\n",
            "Iteration 44, loss = 0.00212287\n",
            "Iteration 45, loss = 0.00194617\n",
            "Iteration 46, loss = 0.00184974\n",
            "Iteration 47, loss = 0.00183911\n",
            "Iteration 48, loss = 0.00172726\n",
            "Iteration 49, loss = 0.00160994\n",
            "Iteration 50, loss = 0.00156847\n",
            "Iteration 51, loss = 0.00152911\n",
            "Iteration 52, loss = 0.00149735\n",
            "Iteration 53, loss = 0.00138966\n",
            "Iteration 54, loss = 0.00141166\n",
            "Iteration 55, loss = 0.00147906\n",
            "Iteration 56, loss = 0.00132308\n",
            "Iteration 57, loss = 0.00131365\n",
            "Iteration 58, loss = 0.00136630\n",
            "Iteration 59, loss = 0.00119598\n",
            "Iteration 60, loss = 0.00129538\n",
            "Iteration 61, loss = 0.00123226\n",
            "Iteration 62, loss = 0.00120594\n",
            "Iteration 63, loss = 0.00110838\n",
            "Iteration 64, loss = 0.00107529\n",
            "Iteration 65, loss = 0.00108363\n",
            "Iteration 66, loss = 0.00119401\n",
            "Iteration 67, loss = 0.00113690\n",
            "Iteration 68, loss = 0.00117579\n",
            "Iteration 69, loss = 0.00108407\n",
            "Iteration 70, loss = 0.00091638\n",
            "Iteration 71, loss = 0.00136310\n",
            "Iteration 72, loss = 0.00069510\n",
            "Iteration 73, loss = 0.00113408\n",
            "Iteration 74, loss = 0.00100359\n",
            "Iteration 75, loss = 0.00092423\n",
            "Iteration 76, loss = 0.00096366\n",
            "Iteration 77, loss = 0.00087045\n",
            "Iteration 78, loss = 0.00084743\n",
            "Iteration 79, loss = 0.00087360\n",
            "Iteration 80, loss = 0.00084790\n",
            "Iteration 81, loss = 0.00092590\n",
            "Iteration 82, loss = 0.00080832\n",
            "Iteration 83, loss = 0.00083510\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37666022\n",
            "Iteration 2, loss = 1.20720848\n",
            "Iteration 3, loss = 1.01202738\n",
            "Iteration 4, loss = 0.84572527\n",
            "Iteration 5, loss = 0.71093572\n",
            "Iteration 6, loss = 0.56315831\n",
            "Iteration 7, loss = 0.41338770\n",
            "Iteration 8, loss = 0.29723606\n",
            "Iteration 9, loss = 0.21647178\n",
            "Iteration 10, loss = 0.16338934\n",
            "Iteration 11, loss = 0.12677683\n",
            "Iteration 12, loss = 0.10101799\n",
            "Iteration 13, loss = 0.08186367\n",
            "Iteration 14, loss = 0.06830028\n",
            "Iteration 15, loss = 0.05598653\n",
            "Iteration 16, loss = 0.04907458\n",
            "Iteration 17, loss = 0.04207122\n",
            "Iteration 18, loss = 0.03142425\n",
            "Iteration 19, loss = 0.02602266\n",
            "Iteration 20, loss = 0.02092918\n",
            "Iteration 21, loss = 0.01711419\n",
            "Iteration 22, loss = 0.01504121\n",
            "Iteration 23, loss = 0.01248159\n",
            "Iteration 24, loss = 0.01072236\n",
            "Iteration 25, loss = 0.00945848\n",
            "Iteration 26, loss = 0.00833407\n",
            "Iteration 27, loss = 0.00726430\n",
            "Iteration 28, loss = 0.00643595\n",
            "Iteration 29, loss = 0.00595637\n",
            "Iteration 30, loss = 0.00548387\n",
            "Iteration 31, loss = 0.00488573\n",
            "Iteration 32, loss = 0.00439731\n",
            "Iteration 33, loss = 0.00404814\n",
            "Iteration 34, loss = 0.00383550\n",
            "Iteration 35, loss = 0.00365932\n",
            "Iteration 36, loss = 0.00312991\n",
            "Iteration 37, loss = 0.00311329\n",
            "Iteration 38, loss = 0.00273591\n",
            "Iteration 39, loss = 0.00259874\n",
            "Iteration 40, loss = 0.00245331\n",
            "Iteration 41, loss = 0.00246000\n",
            "Iteration 42, loss = 0.00209648\n",
            "Iteration 43, loss = 0.00214083\n",
            "Iteration 44, loss = 0.00205533\n",
            "Iteration 45, loss = 0.00184121\n",
            "Iteration 46, loss = 0.00176671\n",
            "Iteration 47, loss = 0.00171774\n",
            "Iteration 48, loss = 0.00168010\n",
            "Iteration 49, loss = 0.00153501\n",
            "Iteration 50, loss = 0.00148007\n",
            "Iteration 51, loss = 0.00144184\n",
            "Iteration 52, loss = 0.00140118\n",
            "Iteration 53, loss = 0.00133534\n",
            "Iteration 54, loss = 0.00132317\n",
            "Iteration 55, loss = 0.00136000\n",
            "Iteration 56, loss = 0.00124252\n",
            "Iteration 57, loss = 0.00122193\n",
            "Iteration 58, loss = 0.00121359\n",
            "Iteration 59, loss = 0.00110991\n",
            "Iteration 60, loss = 0.00114463\n",
            "Iteration 61, loss = 0.00112258\n",
            "Iteration 62, loss = 0.00109120\n",
            "Iteration 63, loss = 0.00102231\n",
            "Iteration 64, loss = 0.00098711\n",
            "Iteration 65, loss = 0.00099024\n",
            "Iteration 66, loss = 0.00107262\n",
            "Iteration 67, loss = 0.00101002\n",
            "Iteration 68, loss = 0.00103213\n",
            "Iteration 69, loss = 0.00095502\n",
            "Iteration 70, loss = 0.00086033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38086353\n",
            "Iteration 2, loss = 1.21262332\n",
            "Iteration 3, loss = 1.01927099\n",
            "Iteration 4, loss = 0.85225612\n",
            "Iteration 5, loss = 0.71821086\n",
            "Iteration 6, loss = 0.56798626\n",
            "Iteration 7, loss = 0.41312646\n",
            "Iteration 8, loss = 0.29552292\n",
            "Iteration 9, loss = 0.21925488\n",
            "Iteration 10, loss = 0.17025441\n",
            "Iteration 11, loss = 0.13580424\n",
            "Iteration 12, loss = 0.10663838\n",
            "Iteration 13, loss = 0.08532446\n",
            "Iteration 14, loss = 0.06858103\n",
            "Iteration 15, loss = 0.05579767\n",
            "Iteration 16, loss = 0.04586988\n",
            "Iteration 17, loss = 0.03927883\n",
            "Iteration 18, loss = 0.03129703\n",
            "Iteration 19, loss = 0.02555117\n",
            "Iteration 20, loss = 0.02139681\n",
            "Iteration 21, loss = 0.01790171\n",
            "Iteration 22, loss = 0.01549742\n",
            "Iteration 23, loss = 0.01337283\n",
            "Iteration 24, loss = 0.01140233\n",
            "Iteration 25, loss = 0.01007850\n",
            "Iteration 26, loss = 0.00879848\n",
            "Iteration 27, loss = 0.00770407\n",
            "Iteration 28, loss = 0.00687604\n",
            "Iteration 29, loss = 0.00616748\n",
            "Iteration 30, loss = 0.00564638\n",
            "Iteration 31, loss = 0.00507307\n",
            "Iteration 32, loss = 0.00466138\n",
            "Iteration 33, loss = 0.00435355\n",
            "Iteration 34, loss = 0.00411139\n",
            "Iteration 35, loss = 0.00392161\n",
            "Iteration 36, loss = 0.00339413\n",
            "Iteration 37, loss = 0.00336169\n",
            "Iteration 38, loss = 0.00297501\n",
            "Iteration 39, loss = 0.00286419\n",
            "Iteration 40, loss = 0.00268674\n",
            "Iteration 41, loss = 0.00268859\n",
            "Iteration 42, loss = 0.00231623\n",
            "Iteration 43, loss = 0.00237599\n",
            "Iteration 44, loss = 0.00226611\n",
            "Iteration 45, loss = 0.00205655\n",
            "Iteration 46, loss = 0.00197367\n",
            "Iteration 47, loss = 0.00195576\n",
            "Iteration 48, loss = 0.00187878\n",
            "Iteration 49, loss = 0.00171887\n",
            "Iteration 50, loss = 0.00166543\n",
            "Iteration 51, loss = 0.00162971\n",
            "Iteration 52, loss = 0.00158332\n",
            "Iteration 53, loss = 0.00149965\n",
            "Iteration 54, loss = 0.00150093\n",
            "Iteration 55, loss = 0.00155584\n",
            "Iteration 56, loss = 0.00141486\n",
            "Iteration 57, loss = 0.00139453\n",
            "Iteration 58, loss = 0.00141152\n",
            "Iteration 59, loss = 0.00125462\n",
            "Iteration 60, loss = 0.00131814\n",
            "Iteration 61, loss = 0.00127933\n",
            "Iteration 62, loss = 0.00124177\n",
            "Iteration 63, loss = 0.00115449\n",
            "Iteration 64, loss = 0.00111478\n",
            "Iteration 65, loss = 0.00111151\n",
            "Iteration 66, loss = 0.00121541\n",
            "Iteration 67, loss = 0.00114198\n",
            "Iteration 68, loss = 0.00118059\n",
            "Iteration 69, loss = 0.00108297\n",
            "Iteration 70, loss = 0.00095293\n",
            "Iteration 71, loss = 0.00135919\n",
            "Iteration 72, loss = 0.00071595\n",
            "Iteration 73, loss = 0.00114388\n",
            "Iteration 74, loss = 0.00100862\n",
            "Iteration 75, loss = 0.00094003\n",
            "Iteration 76, loss = 0.00094311\n",
            "Iteration 77, loss = 0.00089469\n",
            "Iteration 78, loss = 0.00084795\n",
            "Iteration 79, loss = 0.00088281\n",
            "Iteration 80, loss = 0.00084698\n",
            "Iteration 81, loss = 0.00091905\n",
            "Iteration 82, loss = 0.00081761\n",
            "Iteration 83, loss = 0.00085660\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37829511\n",
            "Iteration 2, loss = 1.21026676\n",
            "Iteration 3, loss = 1.01576659\n",
            "Iteration 4, loss = 0.84911621\n",
            "Iteration 5, loss = 0.71151451\n",
            "Iteration 6, loss = 0.55594157\n",
            "Iteration 7, loss = 0.40044384\n",
            "Iteration 8, loss = 0.28475457\n",
            "Iteration 9, loss = 0.20895595\n",
            "Iteration 10, loss = 0.15931633\n",
            "Iteration 11, loss = 0.12057261\n",
            "Iteration 12, loss = 0.09413384\n",
            "Iteration 13, loss = 0.07318329\n",
            "Iteration 14, loss = 0.05675816\n",
            "Iteration 15, loss = 0.04455859\n",
            "Iteration 16, loss = 0.03626775\n",
            "Iteration 17, loss = 0.03065702\n",
            "Iteration 18, loss = 0.02433182\n",
            "Iteration 19, loss = 0.01863375\n",
            "Iteration 20, loss = 0.01491111\n",
            "Iteration 21, loss = 0.01265109\n",
            "Iteration 22, loss = 0.01031799\n",
            "Iteration 23, loss = 0.00835310\n",
            "Iteration 24, loss = 0.00736131\n",
            "Iteration 25, loss = 0.00622388\n",
            "Iteration 26, loss = 0.00548820\n",
            "Iteration 27, loss = 0.00487150\n",
            "Iteration 28, loss = 0.00434927\n",
            "Iteration 29, loss = 0.00389432\n",
            "Iteration 30, loss = 0.00349160\n",
            "Iteration 31, loss = 0.00317488\n",
            "Iteration 32, loss = 0.00292680\n",
            "Iteration 33, loss = 0.00270075\n",
            "Iteration 34, loss = 0.00243074\n",
            "Iteration 35, loss = 0.00226596\n",
            "Iteration 36, loss = 0.00208393\n",
            "Iteration 37, loss = 0.00192980\n",
            "Iteration 38, loss = 0.00180551\n",
            "Iteration 39, loss = 0.00171137\n",
            "Iteration 40, loss = 0.00158489\n",
            "Iteration 41, loss = 0.00149715\n",
            "Iteration 42, loss = 0.00140977\n",
            "Iteration 43, loss = 0.00134828\n",
            "Iteration 44, loss = 0.00126261\n",
            "Iteration 45, loss = 0.00120170\n",
            "Iteration 46, loss = 0.00113414\n",
            "Iteration 47, loss = 0.00108105\n",
            "Iteration 48, loss = 0.00104009\n",
            "Iteration 49, loss = 0.00099243\n",
            "Iteration 50, loss = 0.00094951\n",
            "Iteration 51, loss = 0.00090870\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37872161\n",
            "Iteration 2, loss = 1.23544747\n",
            "Iteration 3, loss = 1.05423008\n",
            "Iteration 4, loss = 0.87549564\n",
            "Iteration 5, loss = 0.71089920\n",
            "Iteration 6, loss = 0.53647379\n",
            "Iteration 7, loss = 0.38088991\n",
            "Iteration 8, loss = 0.27582319\n",
            "Iteration 9, loss = 0.20677186\n",
            "Iteration 10, loss = 0.16285233\n",
            "Iteration 11, loss = 0.12836265\n",
            "Iteration 12, loss = 0.10686839\n",
            "Iteration 13, loss = 0.08326753\n",
            "Iteration 14, loss = 0.06668073\n",
            "Iteration 15, loss = 0.05580276\n",
            "Iteration 16, loss = 0.04648029\n",
            "Iteration 17, loss = 0.03836833\n",
            "Iteration 18, loss = 0.03142386\n",
            "Iteration 19, loss = 0.02560140\n",
            "Iteration 20, loss = 0.02148389\n",
            "Iteration 21, loss = 0.01822104\n",
            "Iteration 22, loss = 0.01580698\n",
            "Iteration 23, loss = 0.01339374\n",
            "Iteration 24, loss = 0.01165089\n",
            "Iteration 25, loss = 0.01000803\n",
            "Iteration 26, loss = 0.00871960\n",
            "Iteration 27, loss = 0.00775162\n",
            "Iteration 28, loss = 0.00686245\n",
            "Iteration 29, loss = 0.00625313\n",
            "Iteration 30, loss = 0.00533069\n",
            "Iteration 31, loss = 0.00491140\n",
            "Iteration 32, loss = 0.00461707\n",
            "Iteration 33, loss = 0.00416003\n",
            "Iteration 34, loss = 0.00387591\n",
            "Iteration 35, loss = 0.00363193\n",
            "Iteration 36, loss = 0.00304291\n",
            "Iteration 37, loss = 0.00283645\n",
            "Iteration 38, loss = 0.00263362\n",
            "Iteration 39, loss = 0.00233557\n",
            "Iteration 40, loss = 0.00219673\n",
            "Iteration 41, loss = 0.00201369\n",
            "Iteration 42, loss = 0.00185098\n",
            "Iteration 43, loss = 0.00174633\n",
            "Iteration 44, loss = 0.00165712\n",
            "Iteration 45, loss = 0.00152035\n",
            "Iteration 46, loss = 0.00144989\n",
            "Iteration 47, loss = 0.00135317\n",
            "Iteration 48, loss = 0.00127601\n",
            "Iteration 49, loss = 0.00123980\n",
            "Iteration 50, loss = 0.00116398\n",
            "Iteration 51, loss = 0.00110033\n",
            "Iteration 52, loss = 0.00102923\n",
            "Iteration 53, loss = 0.00098468\n",
            "Iteration 54, loss = 0.00095994\n",
            "Iteration 55, loss = 0.00091919\n",
            "Iteration 56, loss = 0.00087253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37728815\n",
            "Iteration 2, loss = 1.24871783\n",
            "Iteration 3, loss = 1.08178483\n",
            "Iteration 4, loss = 0.89502184\n",
            "Iteration 5, loss = 0.71347710\n",
            "Iteration 6, loss = 0.52745131\n",
            "Iteration 7, loss = 0.37266549\n",
            "Iteration 8, loss = 0.27201956\n",
            "Iteration 9, loss = 0.20568426\n",
            "Iteration 10, loss = 0.16401635\n",
            "Iteration 11, loss = 0.13035111\n",
            "Iteration 12, loss = 0.10986878\n",
            "Iteration 13, loss = 0.08630325\n",
            "Iteration 14, loss = 0.06865011\n",
            "Iteration 15, loss = 0.05776484\n",
            "Iteration 16, loss = 0.04812323\n",
            "Iteration 17, loss = 0.03998539\n",
            "Iteration 18, loss = 0.03263345\n",
            "Iteration 19, loss = 0.02642705\n",
            "Iteration 20, loss = 0.02228958\n",
            "Iteration 21, loss = 0.01886432\n",
            "Iteration 22, loss = 0.01639735\n",
            "Iteration 23, loss = 0.01382993\n",
            "Iteration 24, loss = 0.01212820\n",
            "Iteration 25, loss = 0.01040179\n",
            "Iteration 26, loss = 0.00913087\n",
            "Iteration 27, loss = 0.00811888\n",
            "Iteration 28, loss = 0.00725854\n",
            "Iteration 29, loss = 0.00666507\n",
            "Iteration 30, loss = 0.00584241\n",
            "Iteration 31, loss = 0.00524167\n",
            "Iteration 32, loss = 0.00493151\n",
            "Iteration 33, loss = 0.00464655\n",
            "Iteration 34, loss = 0.00474971\n",
            "Iteration 35, loss = 0.00464419\n",
            "Iteration 36, loss = 0.00359086\n",
            "Iteration 37, loss = 0.00341721\n",
            "Iteration 38, loss = 0.00329920\n",
            "Iteration 39, loss = 0.00280787\n",
            "Iteration 40, loss = 0.00268816\n",
            "Iteration 41, loss = 0.00247650\n",
            "Iteration 42, loss = 0.00232088\n",
            "Iteration 43, loss = 0.00222405\n",
            "Iteration 44, loss = 0.00217416\n",
            "Iteration 45, loss = 0.00205690\n",
            "Iteration 46, loss = 0.00196375\n",
            "Iteration 47, loss = 0.00186218\n",
            "Iteration 48, loss = 0.00184176\n",
            "Iteration 49, loss = 0.00187092\n",
            "Iteration 50, loss = 0.00185613\n",
            "Iteration 51, loss = 0.00174530\n",
            "Iteration 52, loss = 0.00152181\n",
            "Iteration 53, loss = 0.00149388\n",
            "Iteration 54, loss = 0.00149339\n",
            "Iteration 55, loss = 0.00147780\n",
            "Iteration 56, loss = 0.00134274\n",
            "Iteration 57, loss = 0.00136336\n",
            "Iteration 58, loss = 0.00127685\n",
            "Iteration 59, loss = 0.00136303\n",
            "Iteration 60, loss = 0.00139138\n",
            "Iteration 61, loss = 0.00126768\n",
            "Iteration 62, loss = 0.00119275\n",
            "Iteration 63, loss = 0.00132300\n",
            "Iteration 64, loss = 0.00116264\n",
            "Iteration 65, loss = 0.00119122\n",
            "Iteration 66, loss = 0.00106353\n",
            "Iteration 67, loss = 0.00111741\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35925182\n",
            "Iteration 2, loss = 1.21283590\n",
            "Iteration 3, loss = 1.02602520\n",
            "Iteration 4, loss = 0.85491836\n",
            "Iteration 5, loss = 0.70370088\n",
            "Iteration 6, loss = 0.53850167\n",
            "Iteration 7, loss = 0.39271174\n",
            "Iteration 8, loss = 0.28957863\n",
            "Iteration 9, loss = 0.21703938\n",
            "Iteration 10, loss = 0.17291799\n",
            "Iteration 11, loss = 0.13786484\n",
            "Iteration 12, loss = 0.11390771\n",
            "Iteration 13, loss = 0.09084052\n",
            "Iteration 14, loss = 0.07283650\n",
            "Iteration 15, loss = 0.06250781\n",
            "Iteration 16, loss = 0.05071851\n",
            "Iteration 17, loss = 0.04214861\n",
            "Iteration 18, loss = 0.03446477\n",
            "Iteration 19, loss = 0.02855094\n",
            "Iteration 20, loss = 0.02428864\n",
            "Iteration 21, loss = 0.02082045\n",
            "Iteration 22, loss = 0.01820284\n",
            "Iteration 23, loss = 0.01512171\n",
            "Iteration 24, loss = 0.01343424\n",
            "Iteration 25, loss = 0.01158385\n",
            "Iteration 26, loss = 0.01017805\n",
            "Iteration 27, loss = 0.00913608\n",
            "Iteration 28, loss = 0.00821256\n",
            "Iteration 29, loss = 0.00771449\n",
            "Iteration 30, loss = 0.00669910\n",
            "Iteration 31, loss = 0.00603565\n",
            "Iteration 32, loss = 0.00579306\n",
            "Iteration 33, loss = 0.00545785\n",
            "Iteration 34, loss = 0.00577241\n",
            "Iteration 35, loss = 0.00582811\n",
            "Iteration 36, loss = 0.00455874\n",
            "Iteration 37, loss = 0.00397047\n",
            "Iteration 38, loss = 0.00405374\n",
            "Iteration 39, loss = 0.00340386\n",
            "Iteration 40, loss = 0.00321428\n",
            "Iteration 41, loss = 0.00298694\n",
            "Iteration 42, loss = 0.00277007\n",
            "Iteration 43, loss = 0.00265336\n",
            "Iteration 44, loss = 0.00259085\n",
            "Iteration 45, loss = 0.00243206\n",
            "Iteration 46, loss = 0.00234214\n",
            "Iteration 47, loss = 0.00218528\n",
            "Iteration 48, loss = 0.00218745\n",
            "Iteration 49, loss = 0.00225680\n",
            "Iteration 50, loss = 0.00218610\n",
            "Iteration 51, loss = 0.00210431\n",
            "Iteration 52, loss = 0.00179047\n",
            "Iteration 53, loss = 0.00177179\n",
            "Iteration 54, loss = 0.00177900\n",
            "Iteration 55, loss = 0.00174500\n",
            "Iteration 56, loss = 0.00159653\n",
            "Iteration 57, loss = 0.00160089\n",
            "Iteration 58, loss = 0.00149440\n",
            "Iteration 59, loss = 0.00160393\n",
            "Iteration 60, loss = 0.00161844\n",
            "Iteration 61, loss = 0.00148564\n",
            "Iteration 62, loss = 0.00139871\n",
            "Iteration 63, loss = 0.00149972\n",
            "Iteration 64, loss = 0.00134875\n",
            "Iteration 65, loss = 0.00135094\n",
            "Iteration 66, loss = 0.00124062\n",
            "Iteration 67, loss = 0.00127190\n",
            "Iteration 68, loss = 0.00121988\n",
            "Iteration 69, loss = 0.00123916\n",
            "Iteration 70, loss = 0.00115811\n",
            "Iteration 71, loss = 0.00113571\n",
            "Iteration 72, loss = 0.00114547\n",
            "Iteration 73, loss = 0.00113232\n",
            "Iteration 74, loss = 0.00115152\n",
            "Iteration 75, loss = 0.00101321\n",
            "Iteration 76, loss = 0.00104959\n",
            "Iteration 77, loss = 0.00099131\n",
            "Iteration 78, loss = 0.00107330\n",
            "Iteration 79, loss = 0.00111041\n",
            "Iteration 80, loss = 0.00104455\n",
            "Iteration 81, loss = 0.00092562\n",
            "Iteration 82, loss = 0.00090682\n",
            "Iteration 83, loss = 0.00093592\n",
            "Iteration 84, loss = 0.00089922\n",
            "Iteration 85, loss = 0.00087136\n",
            "Iteration 86, loss = 0.00089057\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36773391\n",
            "Iteration 2, loss = 1.19959263\n",
            "Iteration 3, loss = 1.00731580\n",
            "Iteration 4, loss = 0.84327888\n",
            "Iteration 5, loss = 0.71765966\n",
            "Iteration 6, loss = 0.57730128\n",
            "Iteration 7, loss = 0.43608075\n",
            "Iteration 8, loss = 0.32580733\n",
            "Iteration 9, loss = 0.24467284\n",
            "Iteration 10, loss = 0.19058442\n",
            "Iteration 11, loss = 0.15221388\n",
            "Iteration 12, loss = 0.12393442\n",
            "Iteration 13, loss = 0.09979222\n",
            "Iteration 14, loss = 0.08102459\n",
            "Iteration 15, loss = 0.06732210\n",
            "Iteration 16, loss = 0.05477979\n",
            "Iteration 17, loss = 0.04557463\n",
            "Iteration 18, loss = 0.03665198\n",
            "Iteration 19, loss = 0.02914753\n",
            "Iteration 20, loss = 0.02489038\n",
            "Iteration 21, loss = 0.02084645\n",
            "Iteration 22, loss = 0.01861740\n",
            "Iteration 23, loss = 0.01500565\n",
            "Iteration 24, loss = 0.01300001\n",
            "Iteration 25, loss = 0.01121165\n",
            "Iteration 26, loss = 0.00979189\n",
            "Iteration 27, loss = 0.00872939\n",
            "Iteration 28, loss = 0.00787025\n",
            "Iteration 29, loss = 0.00738320\n",
            "Iteration 30, loss = 0.00648889\n",
            "Iteration 31, loss = 0.00581043\n",
            "Iteration 32, loss = 0.00547008\n",
            "Iteration 33, loss = 0.00528039\n",
            "Iteration 34, loss = 0.00544618\n",
            "Iteration 35, loss = 0.00573486\n",
            "Iteration 36, loss = 0.00429806\n",
            "Iteration 37, loss = 0.00395498\n",
            "Iteration 38, loss = 0.00390602\n",
            "Iteration 39, loss = 0.00340400\n",
            "Iteration 40, loss = 0.00331283\n",
            "Iteration 41, loss = 0.00297282\n",
            "Iteration 42, loss = 0.00276212\n",
            "Iteration 43, loss = 0.00260179\n",
            "Iteration 44, loss = 0.00254740\n",
            "Iteration 45, loss = 0.00238866\n",
            "Iteration 46, loss = 0.00233846\n",
            "Iteration 47, loss = 0.00222633\n",
            "Iteration 48, loss = 0.00216607\n",
            "Iteration 49, loss = 0.00229491\n",
            "Iteration 50, loss = 0.00218621\n",
            "Iteration 51, loss = 0.00217520\n",
            "Iteration 52, loss = 0.00179445\n",
            "Iteration 53, loss = 0.00176695\n",
            "Iteration 54, loss = 0.00178628\n",
            "Iteration 55, loss = 0.00173587\n",
            "Iteration 56, loss = 0.00162329\n",
            "Iteration 57, loss = 0.00161297\n",
            "Iteration 58, loss = 0.00152440\n",
            "Iteration 59, loss = 0.00161910\n",
            "Iteration 60, loss = 0.00162956\n",
            "Iteration 61, loss = 0.00151278\n",
            "Iteration 62, loss = 0.00141796\n",
            "Iteration 63, loss = 0.00154460\n",
            "Iteration 64, loss = 0.00138906\n",
            "Iteration 65, loss = 0.00135755\n",
            "Iteration 66, loss = 0.00125421\n",
            "Iteration 67, loss = 0.00129137\n",
            "Iteration 68, loss = 0.00125371\n",
            "Iteration 69, loss = 0.00124820\n",
            "Iteration 70, loss = 0.00119267\n",
            "Iteration 71, loss = 0.00116956\n",
            "Iteration 72, loss = 0.00117114\n",
            "Iteration 73, loss = 0.00115041\n",
            "Iteration 74, loss = 0.00120344\n",
            "Iteration 75, loss = 0.00105609\n",
            "Iteration 76, loss = 0.00109208\n",
            "Iteration 77, loss = 0.00103830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.41882114\n",
            "Iteration 2, loss = 1.22849843\n",
            "Iteration 3, loss = 1.03194184\n",
            "Iteration 4, loss = 0.86163170\n",
            "Iteration 5, loss = 0.73605932\n",
            "Iteration 6, loss = 0.60899492\n",
            "Iteration 7, loss = 0.46937857\n",
            "Iteration 8, loss = 0.34421821\n",
            "Iteration 9, loss = 0.25609739\n",
            "Iteration 10, loss = 0.20093251\n",
            "Iteration 11, loss = 0.15997735\n",
            "Iteration 12, loss = 0.12852828\n",
            "Iteration 13, loss = 0.10090945\n",
            "Iteration 14, loss = 0.08326700\n",
            "Iteration 15, loss = 0.06847097\n",
            "Iteration 16, loss = 0.05504423\n",
            "Iteration 17, loss = 0.04535108\n",
            "Iteration 18, loss = 0.03634936\n",
            "Iteration 19, loss = 0.02943426\n",
            "Iteration 20, loss = 0.02513765\n",
            "Iteration 21, loss = 0.02117342\n",
            "Iteration 22, loss = 0.01855828\n",
            "Iteration 23, loss = 0.01505133\n",
            "Iteration 24, loss = 0.01295840\n",
            "Iteration 25, loss = 0.01120532\n",
            "Iteration 26, loss = 0.00973037\n",
            "Iteration 27, loss = 0.00871794\n",
            "Iteration 28, loss = 0.00782147\n",
            "Iteration 29, loss = 0.00717545\n",
            "Iteration 30, loss = 0.00629032\n",
            "Iteration 31, loss = 0.00573661\n",
            "Iteration 32, loss = 0.00539464\n",
            "Iteration 33, loss = 0.00509223\n",
            "Iteration 34, loss = 0.00522461\n",
            "Iteration 35, loss = 0.00523220\n",
            "Iteration 36, loss = 0.00406096\n",
            "Iteration 37, loss = 0.00375102\n",
            "Iteration 38, loss = 0.00367098\n",
            "Iteration 39, loss = 0.00322101\n",
            "Iteration 40, loss = 0.00307724\n",
            "Iteration 41, loss = 0.00281754\n",
            "Iteration 42, loss = 0.00264172\n",
            "Iteration 43, loss = 0.00250788\n",
            "Iteration 44, loss = 0.00244343\n",
            "Iteration 45, loss = 0.00233252\n",
            "Iteration 46, loss = 0.00225523\n",
            "Iteration 47, loss = 0.00214929\n",
            "Iteration 48, loss = 0.00208704\n",
            "Iteration 49, loss = 0.00215921\n",
            "Iteration 50, loss = 0.00209837\n",
            "Iteration 51, loss = 0.00205992\n",
            "Iteration 52, loss = 0.00173751\n",
            "Iteration 53, loss = 0.00171316\n",
            "Iteration 54, loss = 0.00170400\n",
            "Iteration 55, loss = 0.00166622\n",
            "Iteration 56, loss = 0.00154467\n",
            "Iteration 57, loss = 0.00153696\n",
            "Iteration 58, loss = 0.00148417\n",
            "Iteration 59, loss = 0.00154426\n",
            "Iteration 60, loss = 0.00158218\n",
            "Iteration 61, loss = 0.00144812\n",
            "Iteration 62, loss = 0.00136588\n",
            "Iteration 63, loss = 0.00148869\n",
            "Iteration 64, loss = 0.00133313\n",
            "Iteration 65, loss = 0.00132578\n",
            "Iteration 66, loss = 0.00122452\n",
            "Iteration 67, loss = 0.00125786\n",
            "Iteration 68, loss = 0.00121675\n",
            "Iteration 69, loss = 0.00122928\n",
            "Iteration 70, loss = 0.00115336\n",
            "Iteration 71, loss = 0.00113481\n",
            "Iteration 72, loss = 0.00115180\n",
            "Iteration 73, loss = 0.00111803\n",
            "Iteration 74, loss = 0.00117756\n",
            "Iteration 75, loss = 0.00103400\n",
            "Iteration 76, loss = 0.00106252\n",
            "Iteration 77, loss = 0.00101854\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "37\n",
            "Iteration 1, loss = 1.33992734\n",
            "Iteration 2, loss = 1.24088217\n",
            "Iteration 3, loss = 1.20556890\n",
            "Iteration 4, loss = 1.19185930\n",
            "Iteration 5, loss = 1.17682756\n",
            "Iteration 6, loss = 1.16931564\n",
            "Iteration 7, loss = 1.17729742\n",
            "Iteration 8, loss = 1.16738298\n",
            "Iteration 9, loss = 1.14389294\n",
            "Iteration 10, loss = 1.14472922\n",
            "Iteration 11, loss = 1.15273629\n",
            "Iteration 12, loss = 1.15028152\n",
            "Iteration 13, loss = 1.12105496\n",
            "Iteration 14, loss = 1.11976867\n",
            "Iteration 15, loss = 1.11908204\n",
            "Iteration 16, loss = 1.09792804\n",
            "Iteration 17, loss = 1.09581967\n",
            "Iteration 18, loss = 1.08296431\n",
            "Iteration 19, loss = 1.08163879\n",
            "Iteration 20, loss = 1.07787291\n",
            "Iteration 21, loss = 1.06644469\n",
            "Iteration 22, loss = 1.05463930\n",
            "Iteration 23, loss = 1.05379135\n",
            "Iteration 24, loss = 1.03936760\n",
            "Iteration 25, loss = 1.04085891\n",
            "Iteration 26, loss = 1.03144726\n",
            "Iteration 27, loss = 1.02405948\n",
            "Iteration 28, loss = 1.02110125\n",
            "Iteration 29, loss = 1.02123237\n",
            "Iteration 30, loss = 1.01258368\n",
            "Iteration 31, loss = 1.01456254\n",
            "Iteration 32, loss = 1.00571684\n",
            "Iteration 33, loss = 1.00860687\n",
            "Iteration 34, loss = 1.00866841\n",
            "Iteration 35, loss = 1.00102276\n",
            "Iteration 36, loss = 0.97130908\n",
            "Iteration 37, loss = 0.96267130\n",
            "Iteration 38, loss = 0.96120877\n",
            "Iteration 39, loss = 0.94978810\n",
            "Iteration 40, loss = 0.93450407\n",
            "Iteration 41, loss = 0.93250828\n",
            "Iteration 42, loss = 0.91452137\n",
            "Iteration 43, loss = 0.93479857\n",
            "Iteration 44, loss = 0.93596129\n",
            "Iteration 45, loss = 0.93762498\n",
            "Iteration 46, loss = 0.91274351\n",
            "Iteration 47, loss = 0.89418567\n",
            "Iteration 48, loss = 0.88864323\n",
            "Iteration 49, loss = 0.88912301\n",
            "Iteration 50, loss = 0.86837192\n",
            "Iteration 51, loss = 0.90650282\n",
            "Iteration 52, loss = 0.88831104\n",
            "Iteration 53, loss = 0.89010044\n",
            "Iteration 54, loss = 0.87165975\n",
            "Iteration 55, loss = 0.88099301\n",
            "Iteration 56, loss = 0.86744095\n",
            "Iteration 57, loss = 0.84686793\n",
            "Iteration 58, loss = 0.83058592\n",
            "Iteration 59, loss = 0.85220341\n",
            "Iteration 60, loss = 0.83898647\n",
            "Iteration 61, loss = 0.81763052\n",
            "Iteration 62, loss = 0.82608210\n",
            "Iteration 63, loss = 0.83225796\n",
            "Iteration 64, loss = 0.82238514\n",
            "Iteration 65, loss = 0.83282579\n",
            "Iteration 66, loss = 0.82535533\n",
            "Iteration 67, loss = 0.81829493\n",
            "Iteration 68, loss = 0.82107163\n",
            "Iteration 69, loss = 0.86375242\n",
            "Iteration 70, loss = 0.85484433\n",
            "Iteration 71, loss = 0.87952361\n",
            "Iteration 72, loss = 0.85727453\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34074859\n",
            "Iteration 2, loss = 1.24250566\n",
            "Iteration 3, loss = 1.20414839\n",
            "Iteration 4, loss = 1.19466278\n",
            "Iteration 5, loss = 1.17757717\n",
            "Iteration 6, loss = 1.16364433\n",
            "Iteration 7, loss = 1.17551590\n",
            "Iteration 8, loss = 1.16407859\n",
            "Iteration 9, loss = 1.15890774\n",
            "Iteration 10, loss = 1.15427526\n",
            "Iteration 11, loss = 1.15216372\n",
            "Iteration 12, loss = 1.12742646\n",
            "Iteration 13, loss = 1.11697305\n",
            "Iteration 14, loss = 1.11413654\n",
            "Iteration 15, loss = 1.10406566\n",
            "Iteration 16, loss = 1.08462558\n",
            "Iteration 17, loss = 1.08967489\n",
            "Iteration 18, loss = 1.07440693\n",
            "Iteration 19, loss = 1.08103863\n",
            "Iteration 20, loss = 1.07745568\n",
            "Iteration 21, loss = 1.07407093\n",
            "Iteration 22, loss = 1.05362160\n",
            "Iteration 23, loss = 1.05113055\n",
            "Iteration 24, loss = 1.02968983\n",
            "Iteration 25, loss = 1.03065700\n",
            "Iteration 26, loss = 1.02764749\n",
            "Iteration 27, loss = 1.00595347\n",
            "Iteration 28, loss = 1.00067086\n",
            "Iteration 29, loss = 1.00179524\n",
            "Iteration 30, loss = 0.99479358\n",
            "Iteration 31, loss = 1.00073117\n",
            "Iteration 32, loss = 1.00322409\n",
            "Iteration 33, loss = 0.98135029\n",
            "Iteration 34, loss = 0.97265966\n",
            "Iteration 35, loss = 0.96643141\n",
            "Iteration 36, loss = 0.98606828\n",
            "Iteration 37, loss = 0.96400668\n",
            "Iteration 38, loss = 0.94673114\n",
            "Iteration 39, loss = 0.93661726\n",
            "Iteration 40, loss = 0.91810216\n",
            "Iteration 41, loss = 0.93107452\n",
            "Iteration 42, loss = 0.90711411\n",
            "Iteration 43, loss = 0.92356261\n",
            "Iteration 44, loss = 0.90804833\n",
            "Iteration 45, loss = 0.91051868\n",
            "Iteration 46, loss = 0.89159762\n",
            "Iteration 47, loss = 0.89432656\n",
            "Iteration 48, loss = 0.89425528\n",
            "Iteration 49, loss = 0.90683298\n",
            "Iteration 50, loss = 0.86177667\n",
            "Iteration 51, loss = 0.89078937\n",
            "Iteration 52, loss = 0.86811175\n",
            "Iteration 53, loss = 0.87114147\n",
            "Iteration 54, loss = 0.87795548\n",
            "Iteration 55, loss = 0.86074911\n",
            "Iteration 56, loss = 0.84297082\n",
            "Iteration 57, loss = 0.82193443\n",
            "Iteration 58, loss = 0.81119417\n",
            "Iteration 59, loss = 0.83091650\n",
            "Iteration 60, loss = 0.80380681\n",
            "Iteration 61, loss = 0.81080945\n",
            "Iteration 62, loss = 0.80970265\n",
            "Iteration 63, loss = 0.79067772\n",
            "Iteration 64, loss = 0.77963329\n",
            "Iteration 65, loss = 0.77408827\n",
            "Iteration 66, loss = 0.78258008\n",
            "Iteration 67, loss = 0.77163194\n",
            "Iteration 68, loss = 0.77422954\n",
            "Iteration 69, loss = 0.86121848\n",
            "Iteration 70, loss = 0.82793607\n",
            "Iteration 71, loss = 0.86096422\n",
            "Iteration 72, loss = 0.84240411\n",
            "Iteration 73, loss = 0.79861981\n",
            "Iteration 74, loss = 0.75489975\n",
            "Iteration 75, loss = 0.74217563\n",
            "Iteration 76, loss = 0.74622699\n",
            "Iteration 77, loss = 0.73657271\n",
            "Iteration 78, loss = 0.71430623\n",
            "Iteration 79, loss = 0.71123338\n",
            "Iteration 80, loss = 0.69227413\n",
            "Iteration 81, loss = 0.70154827\n",
            "Iteration 82, loss = 0.69041159\n",
            "Iteration 83, loss = 0.68614812\n",
            "Iteration 84, loss = 0.68893994\n",
            "Iteration 85, loss = 0.68124038\n",
            "Iteration 86, loss = 0.68124991\n",
            "Iteration 87, loss = 0.68594663\n",
            "Iteration 88, loss = 0.67033948\n",
            "Iteration 89, loss = 0.65322257\n",
            "Iteration 90, loss = 0.68363269\n",
            "Iteration 91, loss = 0.68456800\n",
            "Iteration 92, loss = 0.64718091\n",
            "Iteration 93, loss = 0.65075156\n",
            "Iteration 94, loss = 0.70336481\n",
            "Iteration 95, loss = 0.68402868\n",
            "Iteration 96, loss = 0.67202722\n",
            "Iteration 97, loss = 0.64325932\n",
            "Iteration 98, loss = 0.64309767\n",
            "Iteration 99, loss = 0.62152270\n",
            "Iteration 100, loss = 0.60710184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34177426\n",
            "Iteration 2, loss = 1.25743893\n",
            "Iteration 3, loss = 1.21491431\n",
            "Iteration 4, loss = 1.20781906\n",
            "Iteration 5, loss = 1.18365914\n",
            "Iteration 6, loss = 1.17350245\n",
            "Iteration 7, loss = 1.19284559\n",
            "Iteration 8, loss = 1.17898396\n",
            "Iteration 9, loss = 1.17306558\n",
            "Iteration 10, loss = 1.17107619\n",
            "Iteration 11, loss = 1.16045901\n",
            "Iteration 12, loss = 1.13040790\n",
            "Iteration 13, loss = 1.11944186\n",
            "Iteration 14, loss = 1.11547575\n",
            "Iteration 15, loss = 1.11798020\n",
            "Iteration 16, loss = 1.08929694\n",
            "Iteration 17, loss = 1.09405068\n",
            "Iteration 18, loss = 1.10079109\n",
            "Iteration 19, loss = 1.09621365\n",
            "Iteration 20, loss = 1.09913040\n",
            "Iteration 21, loss = 1.08219582\n",
            "Iteration 22, loss = 1.06365862\n",
            "Iteration 23, loss = 1.05824209\n",
            "Iteration 24, loss = 1.06556775\n",
            "Iteration 25, loss = 1.03809907\n",
            "Iteration 26, loss = 1.03242890\n",
            "Iteration 27, loss = 1.02194450\n",
            "Iteration 28, loss = 1.01761812\n",
            "Iteration 29, loss = 1.01563541\n",
            "Iteration 30, loss = 1.00740651\n",
            "Iteration 31, loss = 1.01411254\n",
            "Iteration 32, loss = 1.01831855\n",
            "Iteration 33, loss = 1.01025032\n",
            "Iteration 34, loss = 1.00151007\n",
            "Iteration 35, loss = 0.98057764\n",
            "Iteration 36, loss = 0.99358964\n",
            "Iteration 37, loss = 0.98260651\n",
            "Iteration 38, loss = 0.96956778\n",
            "Iteration 39, loss = 0.95179746\n",
            "Iteration 40, loss = 0.93926858\n",
            "Iteration 41, loss = 0.96033699\n",
            "Iteration 42, loss = 0.94084100\n",
            "Iteration 43, loss = 0.93972508\n",
            "Iteration 44, loss = 0.92538140\n",
            "Iteration 45, loss = 0.93402687\n",
            "Iteration 46, loss = 0.91281185\n",
            "Iteration 47, loss = 0.91605551\n",
            "Iteration 48, loss = 0.89948194\n",
            "Iteration 49, loss = 0.88991397\n",
            "Iteration 50, loss = 0.86905664\n",
            "Iteration 51, loss = 0.91170673\n",
            "Iteration 52, loss = 0.87357950\n",
            "Iteration 53, loss = 0.89535651\n",
            "Iteration 54, loss = 0.86836163\n",
            "Iteration 55, loss = 0.84072316\n",
            "Iteration 56, loss = 0.83976128\n",
            "Iteration 57, loss = 0.82615009\n",
            "Iteration 58, loss = 0.81959537\n",
            "Iteration 59, loss = 0.83538614\n",
            "Iteration 60, loss = 0.82070124\n",
            "Iteration 61, loss = 0.81830572\n",
            "Iteration 62, loss = 0.79238897\n",
            "Iteration 63, loss = 0.81830805\n",
            "Iteration 64, loss = 0.78663563\n",
            "Iteration 65, loss = 0.77133547\n",
            "Iteration 66, loss = 0.78352073\n",
            "Iteration 67, loss = 0.77655053\n",
            "Iteration 68, loss = 0.78121633\n",
            "Iteration 69, loss = 0.81730868\n",
            "Iteration 70, loss = 0.77648112\n",
            "Iteration 71, loss = 0.81726234\n",
            "Iteration 72, loss = 0.81904054\n",
            "Iteration 73, loss = 0.78605202\n",
            "Iteration 74, loss = 0.75129027\n",
            "Iteration 75, loss = 0.73681560\n",
            "Iteration 76, loss = 0.73271439\n",
            "Iteration 77, loss = 0.71304732\n",
            "Iteration 78, loss = 0.71606390\n",
            "Iteration 79, loss = 0.70610793\n",
            "Iteration 80, loss = 0.71409907\n",
            "Iteration 81, loss = 0.70607154\n",
            "Iteration 82, loss = 0.69792488\n",
            "Iteration 83, loss = 0.68699282\n",
            "Iteration 84, loss = 0.69180028\n",
            "Iteration 85, loss = 0.67701782\n",
            "Iteration 86, loss = 0.66137109\n",
            "Iteration 87, loss = 0.67211792\n",
            "Iteration 88, loss = 0.66585547\n",
            "Iteration 89, loss = 0.68432189\n",
            "Iteration 90, loss = 0.68424017\n",
            "Iteration 91, loss = 0.67743701\n",
            "Iteration 92, loss = 0.67282683\n",
            "Iteration 93, loss = 0.63664846\n",
            "Iteration 94, loss = 0.67265671\n",
            "Iteration 95, loss = 0.71314015\n",
            "Iteration 96, loss = 0.66150232\n",
            "Iteration 97, loss = 0.64379792\n",
            "Iteration 98, loss = 0.61362701\n",
            "Iteration 99, loss = 0.62605035\n",
            "Iteration 100, loss = 0.61175612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34480613\n",
            "Iteration 2, loss = 1.26622217\n",
            "Iteration 3, loss = 1.21711187\n",
            "Iteration 4, loss = 1.21354758\n",
            "Iteration 5, loss = 1.18547834\n",
            "Iteration 6, loss = 1.16818236\n",
            "Iteration 7, loss = 1.17693888\n",
            "Iteration 8, loss = 1.18573290\n",
            "Iteration 9, loss = 1.14969000\n",
            "Iteration 10, loss = 1.14808425\n",
            "Iteration 11, loss = 1.13742015\n",
            "Iteration 12, loss = 1.15035907\n",
            "Iteration 13, loss = 1.13312592\n",
            "Iteration 14, loss = 1.11532501\n",
            "Iteration 15, loss = 1.11454118\n",
            "Iteration 16, loss = 1.09299662\n",
            "Iteration 17, loss = 1.09683973\n",
            "Iteration 18, loss = 1.10250633\n",
            "Iteration 19, loss = 1.09173891\n",
            "Iteration 20, loss = 1.10145734\n",
            "Iteration 21, loss = 1.07531865\n",
            "Iteration 22, loss = 1.06091429\n",
            "Iteration 23, loss = 1.04550383\n",
            "Iteration 24, loss = 1.06359380\n",
            "Iteration 25, loss = 1.04323000\n",
            "Iteration 26, loss = 1.02320886\n",
            "Iteration 27, loss = 1.01764465\n",
            "Iteration 28, loss = 1.02815061\n",
            "Iteration 29, loss = 1.01686105\n",
            "Iteration 30, loss = 1.00521416\n",
            "Iteration 31, loss = 0.99731453\n",
            "Iteration 32, loss = 0.99302366\n",
            "Iteration 33, loss = 0.98910606\n",
            "Iteration 34, loss = 0.97553944\n",
            "Iteration 35, loss = 0.98862765\n",
            "Iteration 36, loss = 1.00536576\n",
            "Iteration 37, loss = 0.99681032\n",
            "Iteration 38, loss = 0.98599344\n",
            "Iteration 39, loss = 0.96866385\n",
            "Iteration 40, loss = 0.95465472\n",
            "Iteration 41, loss = 0.95897396\n",
            "Iteration 42, loss = 0.96004511\n",
            "Iteration 43, loss = 0.95728889\n",
            "Iteration 44, loss = 0.94024654\n",
            "Iteration 45, loss = 0.95002973\n",
            "Iteration 46, loss = 0.92041970\n",
            "Iteration 47, loss = 0.91273876\n",
            "Iteration 48, loss = 0.90229031\n",
            "Iteration 49, loss = 0.88903268\n",
            "Iteration 50, loss = 0.87250718\n",
            "Iteration 51, loss = 0.91505638\n",
            "Iteration 52, loss = 0.90001625\n",
            "Iteration 53, loss = 0.88740641\n",
            "Iteration 54, loss = 0.87116892\n",
            "Iteration 55, loss = 0.85471981\n",
            "Iteration 56, loss = 0.87593600\n",
            "Iteration 57, loss = 0.85734505\n",
            "Iteration 58, loss = 0.84446146\n",
            "Iteration 59, loss = 0.84202941\n",
            "Iteration 60, loss = 0.84347748\n",
            "Iteration 61, loss = 0.82448053\n",
            "Iteration 62, loss = 0.81212210\n",
            "Iteration 63, loss = 0.82005812\n",
            "Iteration 64, loss = 0.80456910\n",
            "Iteration 65, loss = 0.80237408\n",
            "Iteration 66, loss = 0.79237916\n",
            "Iteration 67, loss = 0.79994143\n",
            "Iteration 68, loss = 0.78151395\n",
            "Iteration 69, loss = 0.84460715\n",
            "Iteration 70, loss = 0.82596454\n",
            "Iteration 71, loss = 0.83436708\n",
            "Iteration 72, loss = 0.85957089\n",
            "Iteration 73, loss = 0.80831772\n",
            "Iteration 74, loss = 0.78645832\n",
            "Iteration 75, loss = 0.77668804\n",
            "Iteration 76, loss = 0.76211668\n",
            "Iteration 77, loss = 0.74464493\n",
            "Iteration 78, loss = 0.73398228\n",
            "Iteration 79, loss = 0.75980396\n",
            "Iteration 80, loss = 0.74298028\n",
            "Iteration 81, loss = 0.72858145\n",
            "Iteration 82, loss = 0.71222634\n",
            "Iteration 83, loss = 0.70361423\n",
            "Iteration 84, loss = 0.68859210\n",
            "Iteration 85, loss = 0.70983863\n",
            "Iteration 86, loss = 0.71026614\n",
            "Iteration 87, loss = 0.71232247\n",
            "Iteration 88, loss = 0.70634381\n",
            "Iteration 89, loss = 0.68725560\n",
            "Iteration 90, loss = 0.71838743\n",
            "Iteration 91, loss = 0.71894342\n",
            "Iteration 92, loss = 0.68633740\n",
            "Iteration 93, loss = 0.68196778\n",
            "Iteration 94, loss = 0.67380642\n",
            "Iteration 95, loss = 0.67343343\n",
            "Iteration 96, loss = 0.66383666\n",
            "Iteration 97, loss = 0.66263952\n",
            "Iteration 98, loss = 0.64480811\n",
            "Iteration 99, loss = 0.66828216\n",
            "Iteration 100, loss = 0.66520365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34499174\n",
            "Iteration 2, loss = 1.26135467\n",
            "Iteration 3, loss = 1.21481258\n",
            "Iteration 4, loss = 1.20881560\n",
            "Iteration 5, loss = 1.17938654\n",
            "Iteration 6, loss = 1.16805483\n",
            "Iteration 7, loss = 1.18255385\n",
            "Iteration 8, loss = 1.18415832\n",
            "Iteration 9, loss = 1.17111893\n",
            "Iteration 10, loss = 1.15790466\n",
            "Iteration 11, loss = 1.13211980\n",
            "Iteration 12, loss = 1.11902778\n",
            "Iteration 13, loss = 1.11902682\n",
            "Iteration 14, loss = 1.10665591\n",
            "Iteration 15, loss = 1.10289810\n",
            "Iteration 16, loss = 1.08956726\n",
            "Iteration 17, loss = 1.09252769\n",
            "Iteration 18, loss = 1.08459993\n",
            "Iteration 19, loss = 1.07759310\n",
            "Iteration 20, loss = 1.09054554\n",
            "Iteration 21, loss = 1.07126601\n",
            "Iteration 22, loss = 1.04745950\n",
            "Iteration 23, loss = 1.03362232\n",
            "Iteration 24, loss = 1.05336679\n",
            "Iteration 25, loss = 1.04364519\n",
            "Iteration 26, loss = 1.02899036\n",
            "Iteration 27, loss = 1.01093025\n",
            "Iteration 28, loss = 1.01654055\n",
            "Iteration 29, loss = 1.01618805\n",
            "Iteration 30, loss = 0.99614453\n",
            "Iteration 31, loss = 1.01226997\n",
            "Iteration 32, loss = 1.00185047\n",
            "Iteration 33, loss = 1.00570381\n",
            "Iteration 34, loss = 0.98532550\n",
            "Iteration 35, loss = 0.99235748\n",
            "Iteration 36, loss = 1.02663129\n",
            "Iteration 37, loss = 1.00424186\n",
            "Iteration 38, loss = 0.97491808\n",
            "Iteration 39, loss = 0.96921698\n",
            "Iteration 40, loss = 0.95977978\n",
            "Iteration 41, loss = 0.97548212\n",
            "Iteration 42, loss = 0.95136573\n",
            "Iteration 43, loss = 0.94799228\n",
            "Iteration 44, loss = 0.93865520\n",
            "Iteration 45, loss = 0.93988769\n",
            "Iteration 46, loss = 0.91854089\n",
            "Iteration 47, loss = 0.90845121\n",
            "Iteration 48, loss = 0.90203622\n",
            "Iteration 49, loss = 0.89987259\n",
            "Iteration 50, loss = 0.89086127\n",
            "Iteration 51, loss = 0.90217944\n",
            "Iteration 52, loss = 0.88760449\n",
            "Iteration 53, loss = 0.87437210\n",
            "Iteration 54, loss = 0.88678161\n",
            "Iteration 55, loss = 0.93546056\n",
            "Iteration 56, loss = 0.91330993\n",
            "Iteration 57, loss = 0.87263739\n",
            "Iteration 58, loss = 0.87342306\n",
            "Iteration 59, loss = 0.88021945\n",
            "Iteration 60, loss = 0.84977333\n",
            "Iteration 61, loss = 0.85835476\n",
            "Iteration 62, loss = 0.84734579\n",
            "Iteration 63, loss = 0.87385561\n",
            "Iteration 64, loss = 0.82640210\n",
            "Iteration 65, loss = 0.83101246\n",
            "Iteration 66, loss = 0.81728910\n",
            "Iteration 67, loss = 0.81506963\n",
            "Iteration 68, loss = 0.80098024\n",
            "Iteration 69, loss = 0.84184891\n",
            "Iteration 70, loss = 0.84708965\n",
            "Iteration 71, loss = 0.88407439\n",
            "Iteration 72, loss = 0.86836539\n",
            "Iteration 73, loss = 0.83843713\n",
            "Iteration 74, loss = 0.82734204\n",
            "Iteration 75, loss = 0.80747119\n",
            "Iteration 76, loss = 0.80923430\n",
            "Iteration 77, loss = 0.75792738\n",
            "Iteration 78, loss = 0.74899472\n",
            "Iteration 79, loss = 0.75661385\n",
            "Iteration 80, loss = 0.74687491\n",
            "Iteration 81, loss = 0.73412490\n",
            "Iteration 82, loss = 0.72840508\n",
            "Iteration 83, loss = 0.71053465\n",
            "Iteration 84, loss = 0.72131217\n",
            "Iteration 85, loss = 0.72536432\n",
            "Iteration 86, loss = 0.71148637\n",
            "Iteration 87, loss = 0.69656839\n",
            "Iteration 88, loss = 0.71430508\n",
            "Iteration 89, loss = 0.69577794\n",
            "Iteration 90, loss = 0.71958734\n",
            "Iteration 91, loss = 0.73700332\n",
            "Iteration 92, loss = 0.68754079\n",
            "Iteration 93, loss = 0.69160608\n",
            "Iteration 94, loss = 0.69071237\n",
            "Iteration 95, loss = 0.67724552\n",
            "Iteration 96, loss = 0.66354336\n",
            "Iteration 97, loss = 0.66865116\n",
            "Iteration 98, loss = 0.65065887\n",
            "Iteration 99, loss = 0.65884852\n",
            "Iteration 100, loss = 0.68133667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36246571\n",
            "Iteration 2, loss = 1.29040764\n",
            "Iteration 3, loss = 1.24565638\n",
            "Iteration 4, loss = 1.24199475\n",
            "Iteration 5, loss = 1.22288424\n",
            "Iteration 6, loss = 1.19923858\n",
            "Iteration 7, loss = 1.20950125\n",
            "Iteration 8, loss = 1.20336833\n",
            "Iteration 9, loss = 1.20116889\n",
            "Iteration 10, loss = 1.18662965\n",
            "Iteration 11, loss = 1.16341077\n",
            "Iteration 12, loss = 1.15648742\n",
            "Iteration 13, loss = 1.16195150\n",
            "Iteration 14, loss = 1.14401666\n",
            "Iteration 15, loss = 1.14836750\n",
            "Iteration 16, loss = 1.13334509\n",
            "Iteration 17, loss = 1.13294039\n",
            "Iteration 18, loss = 1.12097879\n",
            "Iteration 19, loss = 1.10880707\n",
            "Iteration 20, loss = 1.11406434\n",
            "Iteration 21, loss = 1.10541780\n",
            "Iteration 22, loss = 1.09843777\n",
            "Iteration 23, loss = 1.08892884\n",
            "Iteration 24, loss = 1.09029128\n",
            "Iteration 25, loss = 1.07954027\n",
            "Iteration 26, loss = 1.06594938\n",
            "Iteration 27, loss = 1.05974787\n",
            "Iteration 28, loss = 1.06023467\n",
            "Iteration 29, loss = 1.04689656\n",
            "Iteration 30, loss = 1.03653174\n",
            "Iteration 31, loss = 1.03714617\n",
            "Iteration 32, loss = 1.03126092\n",
            "Iteration 33, loss = 1.02995425\n",
            "Iteration 34, loss = 1.03796669\n",
            "Iteration 35, loss = 1.02562218\n",
            "Iteration 36, loss = 1.01362339\n",
            "Iteration 37, loss = 1.02214688\n",
            "Iteration 38, loss = 1.01162669\n",
            "Iteration 39, loss = 1.00665543\n",
            "Iteration 40, loss = 0.99332398\n",
            "Iteration 41, loss = 0.99477762\n",
            "Iteration 42, loss = 1.02179916\n",
            "Iteration 43, loss = 1.01861422\n",
            "Iteration 44, loss = 1.00092063\n",
            "Iteration 45, loss = 0.99141735\n",
            "Iteration 46, loss = 0.96364669\n",
            "Iteration 47, loss = 0.95055756\n",
            "Iteration 48, loss = 0.94131255\n",
            "Iteration 49, loss = 0.94412989\n",
            "Iteration 50, loss = 0.93223637\n",
            "Iteration 51, loss = 0.98638544\n",
            "Iteration 52, loss = 0.96624541\n",
            "Iteration 53, loss = 0.94744507\n",
            "Iteration 54, loss = 0.93233151\n",
            "Iteration 55, loss = 0.92535185\n",
            "Iteration 56, loss = 0.92613781\n",
            "Iteration 57, loss = 0.93239630\n",
            "Iteration 58, loss = 0.89691821\n",
            "Iteration 59, loss = 0.89066514\n",
            "Iteration 60, loss = 0.89123118\n",
            "Iteration 61, loss = 0.87448710\n",
            "Iteration 62, loss = 0.86260581\n",
            "Iteration 63, loss = 0.89955481\n",
            "Iteration 64, loss = 0.85517175\n",
            "Iteration 65, loss = 0.85891929\n",
            "Iteration 66, loss = 0.85085117\n",
            "Iteration 67, loss = 0.84435658\n",
            "Iteration 68, loss = 0.83753724\n",
            "Iteration 69, loss = 0.82366031\n",
            "Iteration 70, loss = 0.81795964\n",
            "Iteration 71, loss = 0.86935779\n",
            "Iteration 72, loss = 0.87612573\n",
            "Iteration 73, loss = 0.80913542\n",
            "Iteration 74, loss = 0.79797485\n",
            "Iteration 75, loss = 0.80967120\n",
            "Iteration 76, loss = 0.80529384\n",
            "Iteration 77, loss = 0.79068581\n",
            "Iteration 78, loss = 0.78264545\n",
            "Iteration 79, loss = 0.80109145\n",
            "Iteration 80, loss = 0.80453416\n",
            "Iteration 81, loss = 0.78332555\n",
            "Iteration 82, loss = 0.77289983\n",
            "Iteration 83, loss = 0.74759633\n",
            "Iteration 84, loss = 0.74800915\n",
            "Iteration 85, loss = 0.78292501\n",
            "Iteration 86, loss = 0.76520730\n",
            "Iteration 87, loss = 0.76050676\n",
            "Iteration 88, loss = 0.74458071\n",
            "Iteration 89, loss = 0.72263023\n",
            "Iteration 90, loss = 0.70836479\n",
            "Iteration 91, loss = 0.71210166\n",
            "Iteration 92, loss = 0.71979186\n",
            "Iteration 93, loss = 0.72571956\n",
            "Iteration 94, loss = 0.76468024\n",
            "Iteration 95, loss = 0.73346353\n",
            "Iteration 96, loss = 0.73231294\n",
            "Iteration 97, loss = 0.72772424\n",
            "Iteration 98, loss = 0.72047180\n",
            "Iteration 99, loss = 0.71855197\n",
            "Iteration 100, loss = 0.73424536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35424626\n",
            "Iteration 2, loss = 1.29032339\n",
            "Iteration 3, loss = 1.25651466\n",
            "Iteration 4, loss = 1.24926044\n",
            "Iteration 5, loss = 1.22232121\n",
            "Iteration 6, loss = 1.20713061\n",
            "Iteration 7, loss = 1.21594196\n",
            "Iteration 8, loss = 1.21775089\n",
            "Iteration 9, loss = 1.20802320\n",
            "Iteration 10, loss = 1.18977834\n",
            "Iteration 11, loss = 1.17426535\n",
            "Iteration 12, loss = 1.17053267\n",
            "Iteration 13, loss = 1.17409596\n",
            "Iteration 14, loss = 1.15438225\n",
            "Iteration 15, loss = 1.14910031\n",
            "Iteration 16, loss = 1.14436301\n",
            "Iteration 17, loss = 1.13913136\n",
            "Iteration 18, loss = 1.13433271\n",
            "Iteration 19, loss = 1.12268856\n",
            "Iteration 20, loss = 1.12989917\n",
            "Iteration 21, loss = 1.12019895\n",
            "Iteration 22, loss = 1.10822703\n",
            "Iteration 23, loss = 1.09310368\n",
            "Iteration 24, loss = 1.09459321\n",
            "Iteration 25, loss = 1.07520811\n",
            "Iteration 26, loss = 1.07151071\n",
            "Iteration 27, loss = 1.06449811\n",
            "Iteration 28, loss = 1.06503116\n",
            "Iteration 29, loss = 1.04805125\n",
            "Iteration 30, loss = 1.04408662\n",
            "Iteration 31, loss = 1.03816793\n",
            "Iteration 32, loss = 1.03154127\n",
            "Iteration 33, loss = 1.04761972\n",
            "Iteration 34, loss = 1.03827388\n",
            "Iteration 35, loss = 1.06334992\n",
            "Iteration 36, loss = 1.04448050\n",
            "Iteration 37, loss = 1.04431331\n",
            "Iteration 38, loss = 1.02474603\n",
            "Iteration 39, loss = 1.02615519\n",
            "Iteration 40, loss = 1.01108719\n",
            "Iteration 41, loss = 1.00164841\n",
            "Iteration 42, loss = 1.01141404\n",
            "Iteration 43, loss = 1.01484049\n",
            "Iteration 44, loss = 0.99487670\n",
            "Iteration 45, loss = 0.97743401\n",
            "Iteration 46, loss = 0.97220796\n",
            "Iteration 47, loss = 0.96222833\n",
            "Iteration 48, loss = 0.94724969\n",
            "Iteration 49, loss = 0.94036664\n",
            "Iteration 50, loss = 0.93564147\n",
            "Iteration 51, loss = 0.95273214\n",
            "Iteration 52, loss = 0.92213116\n",
            "Iteration 53, loss = 0.92494162\n",
            "Iteration 54, loss = 0.91856380\n",
            "Iteration 55, loss = 0.91290556\n",
            "Iteration 56, loss = 0.92463359\n",
            "Iteration 57, loss = 0.94545098\n",
            "Iteration 58, loss = 0.88907370\n",
            "Iteration 59, loss = 0.88764622\n",
            "Iteration 60, loss = 0.87790806\n",
            "Iteration 61, loss = 0.88225665\n",
            "Iteration 62, loss = 0.88163448\n",
            "Iteration 63, loss = 0.90595812\n",
            "Iteration 64, loss = 0.87887149\n",
            "Iteration 65, loss = 0.87426080\n",
            "Iteration 66, loss = 0.84883450\n",
            "Iteration 67, loss = 0.84884025\n",
            "Iteration 68, loss = 0.84313640\n",
            "Iteration 69, loss = 0.81748805\n",
            "Iteration 70, loss = 0.81530314\n",
            "Iteration 71, loss = 0.86940126\n",
            "Iteration 72, loss = 0.82459861\n",
            "Iteration 73, loss = 0.83587213\n",
            "Iteration 74, loss = 0.85280996\n",
            "Iteration 75, loss = 0.86722738\n",
            "Iteration 76, loss = 0.85203546\n",
            "Iteration 77, loss = 0.82744861\n",
            "Iteration 78, loss = 0.80085792\n",
            "Iteration 79, loss = 0.79629959\n",
            "Iteration 80, loss = 0.79411742\n",
            "Iteration 81, loss = 0.78433261\n",
            "Iteration 82, loss = 0.77363338\n",
            "Iteration 83, loss = 0.74827270\n",
            "Iteration 84, loss = 0.74763028\n",
            "Iteration 85, loss = 0.78439969\n",
            "Iteration 86, loss = 0.75318485\n",
            "Iteration 87, loss = 0.73967070\n",
            "Iteration 88, loss = 0.72745968\n",
            "Iteration 89, loss = 0.73132358\n",
            "Iteration 90, loss = 0.71361416\n",
            "Iteration 91, loss = 0.72384586\n",
            "Iteration 92, loss = 0.71039441\n",
            "Iteration 93, loss = 0.72105533\n",
            "Iteration 94, loss = 0.72741087\n",
            "Iteration 95, loss = 0.71241627\n",
            "Iteration 96, loss = 0.72942448\n",
            "Iteration 97, loss = 0.71047856\n",
            "Iteration 98, loss = 0.68481824\n",
            "Iteration 99, loss = 0.69310299\n",
            "Iteration 100, loss = 0.67637240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35519554\n",
            "Iteration 2, loss = 1.27892954\n",
            "Iteration 3, loss = 1.23607217\n",
            "Iteration 4, loss = 1.21541048\n",
            "Iteration 5, loss = 1.19543013\n",
            "Iteration 6, loss = 1.17940467\n",
            "Iteration 7, loss = 1.20373609\n",
            "Iteration 8, loss = 1.21742206\n",
            "Iteration 9, loss = 1.19221775\n",
            "Iteration 10, loss = 1.16737715\n",
            "Iteration 11, loss = 1.15066890\n",
            "Iteration 12, loss = 1.15147840\n",
            "Iteration 13, loss = 1.14345549\n",
            "Iteration 14, loss = 1.13043623\n",
            "Iteration 15, loss = 1.13314637\n",
            "Iteration 16, loss = 1.13014650\n",
            "Iteration 17, loss = 1.12203224\n",
            "Iteration 18, loss = 1.11032301\n",
            "Iteration 19, loss = 1.09630420\n",
            "Iteration 20, loss = 1.12634603\n",
            "Iteration 21, loss = 1.10487572\n",
            "Iteration 22, loss = 1.09256392\n",
            "Iteration 23, loss = 1.07821044\n",
            "Iteration 24, loss = 1.07057054\n",
            "Iteration 25, loss = 1.06360033\n",
            "Iteration 26, loss = 1.05989463\n",
            "Iteration 27, loss = 1.04494486\n",
            "Iteration 28, loss = 1.05347251\n",
            "Iteration 29, loss = 1.04027319\n",
            "Iteration 30, loss = 1.02748805\n",
            "Iteration 31, loss = 1.01719329\n",
            "Iteration 32, loss = 1.01636946\n",
            "Iteration 33, loss = 1.02423558\n",
            "Iteration 34, loss = 1.01996334\n",
            "Iteration 35, loss = 1.03054405\n",
            "Iteration 36, loss = 1.01507113\n",
            "Iteration 37, loss = 1.01349453\n",
            "Iteration 38, loss = 1.00213520\n",
            "Iteration 39, loss = 0.98718212\n",
            "Iteration 40, loss = 0.97397369\n",
            "Iteration 41, loss = 0.98327973\n",
            "Iteration 42, loss = 0.97495266\n",
            "Iteration 43, loss = 0.99790502\n",
            "Iteration 44, loss = 0.99673104\n",
            "Iteration 45, loss = 0.96834857\n",
            "Iteration 46, loss = 0.96957517\n",
            "Iteration 47, loss = 0.95128076\n",
            "Iteration 48, loss = 0.94042910\n",
            "Iteration 49, loss = 0.94628038\n",
            "Iteration 50, loss = 0.93003911\n",
            "Iteration 51, loss = 0.95260991\n",
            "Iteration 52, loss = 0.94270517\n",
            "Iteration 53, loss = 0.94487610\n",
            "Iteration 54, loss = 0.92207379\n",
            "Iteration 55, loss = 0.92631300\n",
            "Iteration 56, loss = 0.90882754\n",
            "Iteration 57, loss = 0.91658020\n",
            "Iteration 58, loss = 0.88751580\n",
            "Iteration 59, loss = 0.87814892\n",
            "Iteration 60, loss = 0.86704603\n",
            "Iteration 61, loss = 0.87169425\n",
            "Iteration 62, loss = 0.87987726\n",
            "Iteration 63, loss = 0.88564039\n",
            "Iteration 64, loss = 0.86408027\n",
            "Iteration 65, loss = 0.87836899\n",
            "Iteration 66, loss = 0.86503824\n",
            "Iteration 67, loss = 0.85446472\n",
            "Iteration 68, loss = 0.84783068\n",
            "Iteration 69, loss = 0.82491409\n",
            "Iteration 70, loss = 0.81851045\n",
            "Iteration 71, loss = 0.82318736\n",
            "Iteration 72, loss = 0.82375598\n",
            "Iteration 73, loss = 0.81704605\n",
            "Iteration 74, loss = 0.86023167\n",
            "Iteration 75, loss = 0.81895303\n",
            "Iteration 76, loss = 0.82209571\n",
            "Iteration 77, loss = 0.79123775\n",
            "Iteration 78, loss = 0.78020010\n",
            "Iteration 79, loss = 0.78020598\n",
            "Iteration 80, loss = 0.78425475\n",
            "Iteration 81, loss = 0.80064606\n",
            "Iteration 82, loss = 0.78306819\n",
            "Iteration 83, loss = 0.76165289\n",
            "Iteration 84, loss = 0.75486441\n",
            "Iteration 85, loss = 0.78750445\n",
            "Iteration 86, loss = 0.75191222\n",
            "Iteration 87, loss = 0.73480963\n",
            "Iteration 88, loss = 0.73063724\n",
            "Iteration 89, loss = 0.75037412\n",
            "Iteration 90, loss = 0.74444129\n",
            "Iteration 91, loss = 0.76073726\n",
            "Iteration 92, loss = 0.74962805\n",
            "Iteration 93, loss = 0.72653872\n",
            "Iteration 94, loss = 0.74995728\n",
            "Iteration 95, loss = 0.72389619\n",
            "Iteration 96, loss = 0.73773474\n",
            "Iteration 97, loss = 0.73768704\n",
            "Iteration 98, loss = 0.74067107\n",
            "Iteration 99, loss = 0.72356237\n",
            "Iteration 100, loss = 0.71141423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33424615\n",
            "Iteration 2, loss = 1.26195719\n",
            "Iteration 3, loss = 1.23270559\n",
            "Iteration 4, loss = 1.21660571\n",
            "Iteration 5, loss = 1.18535999\n",
            "Iteration 6, loss = 1.18020510\n",
            "Iteration 7, loss = 1.19061780\n",
            "Iteration 8, loss = 1.19415757\n",
            "Iteration 9, loss = 1.15803638\n",
            "Iteration 10, loss = 1.14411589\n",
            "Iteration 11, loss = 1.13562473\n",
            "Iteration 12, loss = 1.13579453\n",
            "Iteration 13, loss = 1.13212177\n",
            "Iteration 14, loss = 1.12346668\n",
            "Iteration 15, loss = 1.11442831\n",
            "Iteration 16, loss = 1.10155248\n",
            "Iteration 17, loss = 1.08947659\n",
            "Iteration 18, loss = 1.09873126\n",
            "Iteration 19, loss = 1.08216146\n",
            "Iteration 20, loss = 1.09066130\n",
            "Iteration 21, loss = 1.06770736\n",
            "Iteration 22, loss = 1.06247234\n",
            "Iteration 23, loss = 1.04250195\n",
            "Iteration 24, loss = 1.06626137\n",
            "Iteration 25, loss = 1.05136049\n",
            "Iteration 26, loss = 1.05434913\n",
            "Iteration 27, loss = 1.03104594\n",
            "Iteration 28, loss = 1.02821260\n",
            "Iteration 29, loss = 1.02591340\n",
            "Iteration 30, loss = 1.00683038\n",
            "Iteration 31, loss = 0.99903715\n",
            "Iteration 32, loss = 0.99609838\n",
            "Iteration 33, loss = 0.99779005\n",
            "Iteration 34, loss = 1.00347105\n",
            "Iteration 35, loss = 0.99001789\n",
            "Iteration 36, loss = 0.99525376\n",
            "Iteration 37, loss = 0.98974659\n",
            "Iteration 38, loss = 0.96952254\n",
            "Iteration 39, loss = 0.95600937\n",
            "Iteration 40, loss = 0.94662552\n",
            "Iteration 41, loss = 0.94709177\n",
            "Iteration 42, loss = 0.95313862\n",
            "Iteration 43, loss = 0.95294362\n",
            "Iteration 44, loss = 0.94751528\n",
            "Iteration 45, loss = 0.93519271\n",
            "Iteration 46, loss = 0.93681993\n",
            "Iteration 47, loss = 0.92530449\n",
            "Iteration 48, loss = 0.90456323\n",
            "Iteration 49, loss = 0.90314012\n",
            "Iteration 50, loss = 0.89511556\n",
            "Iteration 51, loss = 0.89538833\n",
            "Iteration 52, loss = 0.88747197\n",
            "Iteration 53, loss = 0.88739224\n",
            "Iteration 54, loss = 0.87176064\n",
            "Iteration 55, loss = 0.92006185\n",
            "Iteration 56, loss = 0.90672981\n",
            "Iteration 57, loss = 0.90563748\n",
            "Iteration 58, loss = 0.87056571\n",
            "Iteration 59, loss = 0.85476534\n",
            "Iteration 60, loss = 0.84890452\n",
            "Iteration 61, loss = 0.84799126\n",
            "Iteration 62, loss = 0.89612404\n",
            "Iteration 63, loss = 0.92556725\n",
            "Iteration 64, loss = 0.86259180\n",
            "Iteration 65, loss = 0.85191883\n",
            "Iteration 66, loss = 0.83483052\n",
            "Iteration 67, loss = 0.84221661\n",
            "Iteration 68, loss = 0.84387798\n",
            "Iteration 69, loss = 0.81100452\n",
            "Iteration 70, loss = 0.80316577\n",
            "Iteration 71, loss = 0.80433594\n",
            "Iteration 72, loss = 0.82201298\n",
            "Iteration 73, loss = 0.79505515\n",
            "Iteration 74, loss = 0.79771394\n",
            "Iteration 75, loss = 0.77466096\n",
            "Iteration 76, loss = 0.78480000\n",
            "Iteration 77, loss = 0.76069630\n",
            "Iteration 78, loss = 0.76859288\n",
            "Iteration 79, loss = 0.76829385\n",
            "Iteration 80, loss = 0.75923706\n",
            "Iteration 81, loss = 0.77366750\n",
            "Iteration 82, loss = 0.76045701\n",
            "Iteration 83, loss = 0.75183420\n",
            "Iteration 84, loss = 0.75024018\n",
            "Iteration 85, loss = 0.76064307\n",
            "Iteration 86, loss = 0.73382034\n",
            "Iteration 87, loss = 0.72539038\n",
            "Iteration 88, loss = 0.71083806\n",
            "Iteration 89, loss = 0.73632107\n",
            "Iteration 90, loss = 0.70501505\n",
            "Iteration 91, loss = 0.72416892\n",
            "Iteration 92, loss = 0.74112495\n",
            "Iteration 93, loss = 0.70078739\n",
            "Iteration 94, loss = 0.72020237\n",
            "Iteration 95, loss = 0.71106146\n",
            "Iteration 96, loss = 0.68438654\n",
            "Iteration 97, loss = 0.73087116\n",
            "Iteration 98, loss = 0.74353040\n",
            "Iteration 99, loss = 0.73350144\n",
            "Iteration 100, loss = 0.72313955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33355845\n",
            "Iteration 2, loss = 1.25360673\n",
            "Iteration 3, loss = 1.23493471\n",
            "Iteration 4, loss = 1.21897021\n",
            "Iteration 5, loss = 1.19010255\n",
            "Iteration 6, loss = 1.18403249\n",
            "Iteration 7, loss = 1.19364415\n",
            "Iteration 8, loss = 1.19013112\n",
            "Iteration 9, loss = 1.15648138\n",
            "Iteration 10, loss = 1.14657246\n",
            "Iteration 11, loss = 1.14543478\n",
            "Iteration 12, loss = 1.14514045\n",
            "Iteration 13, loss = 1.14661934\n",
            "Iteration 14, loss = 1.13822452\n",
            "Iteration 15, loss = 1.12560491\n",
            "Iteration 16, loss = 1.11992285\n",
            "Iteration 17, loss = 1.10469134\n",
            "Iteration 18, loss = 1.12296167\n",
            "Iteration 19, loss = 1.09556608\n",
            "Iteration 20, loss = 1.09907377\n",
            "Iteration 21, loss = 1.08282893\n",
            "Iteration 22, loss = 1.07116911\n",
            "Iteration 23, loss = 1.07071480\n",
            "Iteration 24, loss = 1.09392977\n",
            "Iteration 25, loss = 1.07951060\n",
            "Iteration 26, loss = 1.07208816\n",
            "Iteration 27, loss = 1.04908067\n",
            "Iteration 28, loss = 1.05022265\n",
            "Iteration 29, loss = 1.05258161\n",
            "Iteration 30, loss = 1.03241638\n",
            "Iteration 31, loss = 1.02236408\n",
            "Iteration 32, loss = 1.01235484\n",
            "Iteration 33, loss = 1.02557184\n",
            "Iteration 34, loss = 1.02375528\n",
            "Iteration 35, loss = 1.01506163\n",
            "Iteration 36, loss = 1.03641027\n",
            "Iteration 37, loss = 1.02104233\n",
            "Iteration 38, loss = 0.99962922\n",
            "Iteration 39, loss = 0.99157195\n",
            "Iteration 40, loss = 0.97462217\n",
            "Iteration 41, loss = 0.96925821\n",
            "Iteration 42, loss = 0.96597901\n",
            "Iteration 43, loss = 0.95687742\n",
            "Iteration 44, loss = 0.95927151\n",
            "Iteration 45, loss = 0.94984504\n",
            "Iteration 46, loss = 0.96895549\n",
            "Iteration 47, loss = 0.96366358\n",
            "Iteration 48, loss = 0.94222426\n",
            "Iteration 49, loss = 0.91744877\n",
            "Iteration 50, loss = 0.90761038\n",
            "Iteration 51, loss = 0.91515390\n",
            "Iteration 52, loss = 0.91091226\n",
            "Iteration 53, loss = 0.88602429\n",
            "Iteration 54, loss = 0.88628947\n",
            "Iteration 55, loss = 0.93970955\n",
            "Iteration 56, loss = 0.92917928\n",
            "Iteration 57, loss = 0.89328381\n",
            "Iteration 58, loss = 0.86832877\n",
            "Iteration 59, loss = 0.86989240\n",
            "Iteration 60, loss = 0.87754378\n",
            "Iteration 61, loss = 0.87556042\n",
            "Iteration 62, loss = 0.88925171\n",
            "Iteration 63, loss = 0.91045078\n",
            "Iteration 64, loss = 0.86036128\n",
            "Iteration 65, loss = 0.85808434\n",
            "Iteration 66, loss = 0.85945263\n",
            "Iteration 67, loss = 0.83986925\n",
            "Iteration 68, loss = 0.85744701\n",
            "Iteration 69, loss = 0.86066613\n",
            "Iteration 70, loss = 0.86322796\n",
            "Iteration 71, loss = 0.85789031\n",
            "Iteration 72, loss = 0.82806363\n",
            "Iteration 73, loss = 0.81690960\n",
            "Iteration 74, loss = 0.79042630\n",
            "Iteration 75, loss = 0.78101545\n",
            "Iteration 76, loss = 0.78819953\n",
            "Iteration 77, loss = 0.78191084\n",
            "Iteration 78, loss = 0.80207593\n",
            "Iteration 79, loss = 0.76960140\n",
            "Iteration 80, loss = 0.78697974\n",
            "Iteration 81, loss = 0.78454593\n",
            "Iteration 82, loss = 0.76779523\n",
            "Iteration 83, loss = 0.75039356\n",
            "Iteration 84, loss = 0.73378836\n",
            "Iteration 85, loss = 0.76428454\n",
            "Iteration 86, loss = 0.75905275\n",
            "Iteration 87, loss = 0.75159093\n",
            "Iteration 88, loss = 0.72931248\n",
            "Iteration 89, loss = 0.71347656\n",
            "Iteration 90, loss = 0.70070245\n",
            "Iteration 91, loss = 0.71913912\n",
            "Iteration 92, loss = 0.70985398\n",
            "Iteration 93, loss = 0.69346363\n",
            "Iteration 94, loss = 0.67137579\n",
            "Iteration 95, loss = 0.68587295\n",
            "Iteration 96, loss = 0.67103851\n",
            "Iteration 97, loss = 0.68518356\n",
            "Iteration 98, loss = 0.70705475\n",
            "Iteration 99, loss = 0.67240901\n",
            "Iteration 100, loss = 0.68332669\n",
            "38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41838789\n",
            "Iteration 2, loss = 1.37500723\n",
            "Iteration 3, loss = 1.36334185\n",
            "Iteration 4, loss = 1.35640553\n",
            "Iteration 5, loss = 1.34981155\n",
            "Iteration 6, loss = 1.34322664\n",
            "Iteration 7, loss = 1.33573996\n",
            "Iteration 8, loss = 1.32827605\n",
            "Iteration 9, loss = 1.32445856\n",
            "Iteration 10, loss = 1.31650562\n",
            "Iteration 11, loss = 1.30918471\n",
            "Iteration 12, loss = 1.30052832\n",
            "Iteration 13, loss = 1.29087967\n",
            "Iteration 14, loss = 1.28888065\n",
            "Iteration 15, loss = 1.27960669\n",
            "Iteration 16, loss = 1.27703450\n",
            "Iteration 17, loss = 1.25349032\n",
            "Iteration 18, loss = 1.24872843\n",
            "Iteration 19, loss = 1.23824572\n",
            "Iteration 20, loss = 1.28068419\n",
            "Iteration 21, loss = 1.24662568\n",
            "Iteration 22, loss = 1.22422340\n",
            "Iteration 23, loss = 1.21492357\n",
            "Iteration 24, loss = 1.24352081\n",
            "Iteration 25, loss = 1.23114327\n",
            "Iteration 26, loss = 1.22084476\n",
            "Iteration 27, loss = 1.19645465\n",
            "Iteration 28, loss = 1.18884402\n",
            "Iteration 29, loss = 1.18173854\n",
            "Iteration 30, loss = 1.17800261\n",
            "Iteration 31, loss = 1.20968644\n",
            "Iteration 32, loss = 1.17949758\n",
            "Iteration 33, loss = 1.17232278\n",
            "Iteration 34, loss = 1.17272096\n",
            "Iteration 35, loss = 1.18582015\n",
            "Iteration 36, loss = 1.17877175\n",
            "Iteration 37, loss = 1.16073811\n",
            "Iteration 38, loss = 1.14680613\n",
            "Iteration 39, loss = 1.13779591\n",
            "Iteration 40, loss = 1.13022504\n",
            "Iteration 41, loss = 1.13939580\n",
            "Iteration 42, loss = 1.12868996\n",
            "Iteration 43, loss = 1.14161936\n",
            "Iteration 44, loss = 1.14352132\n",
            "Iteration 45, loss = 1.12389045\n",
            "Iteration 46, loss = 1.11781914\n",
            "Iteration 47, loss = 1.10497842\n",
            "Iteration 48, loss = 1.10511811\n",
            "Iteration 49, loss = 1.10653162\n",
            "Iteration 50, loss = 1.09555178\n",
            "Iteration 51, loss = 1.08888952\n",
            "Iteration 52, loss = 1.08880582\n",
            "Iteration 53, loss = 1.08327667\n",
            "Iteration 54, loss = 1.08012931\n",
            "Iteration 55, loss = 1.08779294\n",
            "Iteration 56, loss = 1.11233350\n",
            "Iteration 57, loss = 1.08374262\n",
            "Iteration 58, loss = 1.10056018\n",
            "Iteration 59, loss = 1.08184269\n",
            "Iteration 60, loss = 1.06558508\n",
            "Iteration 61, loss = 1.06763495\n",
            "Iteration 62, loss = 1.07154506\n",
            "Iteration 63, loss = 1.07152947\n",
            "Iteration 64, loss = 1.07799355\n",
            "Iteration 65, loss = 1.06127272\n",
            "Iteration 66, loss = 1.07633146\n",
            "Iteration 67, loss = 1.05350902\n",
            "Iteration 68, loss = 1.04446332\n",
            "Iteration 69, loss = 1.03720882\n",
            "Iteration 70, loss = 1.05059744\n",
            "Iteration 71, loss = 1.03513441\n",
            "Iteration 72, loss = 1.03377258\n",
            "Iteration 73, loss = 1.04221868\n",
            "Iteration 74, loss = 1.03401885\n",
            "Iteration 75, loss = 1.03911548\n",
            "Iteration 76, loss = 1.04057409\n",
            "Iteration 77, loss = 1.04430339\n",
            "Iteration 78, loss = 1.02752740\n",
            "Iteration 79, loss = 1.00851591\n",
            "Iteration 80, loss = 1.02295514\n",
            "Iteration 81, loss = 1.01194228\n",
            "Iteration 82, loss = 1.02458563\n",
            "Iteration 83, loss = 1.00206975\n",
            "Iteration 84, loss = 1.00531194\n",
            "Iteration 85, loss = 0.99829356\n",
            "Iteration 86, loss = 0.99777788\n",
            "Iteration 87, loss = 1.04312898\n",
            "Iteration 88, loss = 1.00123835\n",
            "Iteration 89, loss = 1.01586390\n",
            "Iteration 90, loss = 0.99321249\n",
            "Iteration 91, loss = 0.98058611\n",
            "Iteration 92, loss = 0.98907089\n",
            "Iteration 93, loss = 0.98127856\n",
            "Iteration 94, loss = 0.98343944\n",
            "Iteration 95, loss = 0.98801657\n",
            "Iteration 96, loss = 0.99367483\n",
            "Iteration 97, loss = 1.00224343\n",
            "Iteration 98, loss = 1.01222706\n",
            "Iteration 99, loss = 1.00330676\n",
            "Iteration 100, loss = 0.99041804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.42278986\n",
            "Iteration 2, loss = 1.37283128\n",
            "Iteration 3, loss = 1.36253745\n",
            "Iteration 4, loss = 1.35580789\n",
            "Iteration 5, loss = 1.34611329\n",
            "Iteration 6, loss = 1.34161826\n",
            "Iteration 7, loss = 1.33495328\n",
            "Iteration 8, loss = 1.32617847\n",
            "Iteration 9, loss = 1.32117593\n",
            "Iteration 10, loss = 1.31286228\n",
            "Iteration 11, loss = 1.30375534\n",
            "Iteration 12, loss = 1.29445381\n",
            "Iteration 13, loss = 1.28448965\n",
            "Iteration 14, loss = 1.27873889\n",
            "Iteration 15, loss = 1.27838014\n",
            "Iteration 16, loss = 1.25742414\n",
            "Iteration 17, loss = 1.25947738\n",
            "Iteration 18, loss = 1.25816459\n",
            "Iteration 19, loss = 1.24036329\n",
            "Iteration 20, loss = 1.25224560\n",
            "Iteration 21, loss = 1.22748568\n",
            "Iteration 22, loss = 1.22074987\n",
            "Iteration 23, loss = 1.20848852\n",
            "Iteration 24, loss = 1.23390602\n",
            "Iteration 25, loss = 1.21693047\n",
            "Iteration 26, loss = 1.22110156\n",
            "Iteration 27, loss = 1.20090986\n",
            "Iteration 28, loss = 1.18217776\n",
            "Iteration 29, loss = 1.16905173\n",
            "Iteration 30, loss = 1.16570752\n",
            "Iteration 31, loss = 1.18789456\n",
            "Iteration 32, loss = 1.17773116\n",
            "Iteration 33, loss = 1.16607019\n",
            "Iteration 34, loss = 1.17036706\n",
            "Iteration 35, loss = 1.17602080\n",
            "Iteration 36, loss = 1.18572082\n",
            "Iteration 37, loss = 1.15486425\n",
            "Iteration 38, loss = 1.14347697\n",
            "Iteration 39, loss = 1.13539148\n",
            "Iteration 40, loss = 1.12673087\n",
            "Iteration 41, loss = 1.13702783\n",
            "Iteration 42, loss = 1.12163458\n",
            "Iteration 43, loss = 1.13706592\n",
            "Iteration 44, loss = 1.12861924\n",
            "Iteration 45, loss = 1.10990023\n",
            "Iteration 46, loss = 1.10591555\n",
            "Iteration 47, loss = 1.10552955\n",
            "Iteration 48, loss = 1.10526681\n",
            "Iteration 49, loss = 1.11757695\n",
            "Iteration 50, loss = 1.09139953\n",
            "Iteration 51, loss = 1.08282146\n",
            "Iteration 52, loss = 1.08010754\n",
            "Iteration 53, loss = 1.07449262\n",
            "Iteration 54, loss = 1.07342854\n",
            "Iteration 55, loss = 1.06888994\n",
            "Iteration 56, loss = 1.07644642\n",
            "Iteration 57, loss = 1.06480634\n",
            "Iteration 58, loss = 1.07867202\n",
            "Iteration 59, loss = 1.09204111\n",
            "Iteration 60, loss = 1.09308207\n",
            "Iteration 61, loss = 1.09590443\n",
            "Iteration 62, loss = 1.09344484\n",
            "Iteration 63, loss = 1.10723717\n",
            "Iteration 64, loss = 1.11407558\n",
            "Iteration 65, loss = 1.07891884\n",
            "Iteration 66, loss = 1.04066097\n",
            "Iteration 67, loss = 1.06532142\n",
            "Iteration 68, loss = 1.06532853\n",
            "Iteration 69, loss = 1.04103081\n",
            "Iteration 70, loss = 1.05578959\n",
            "Iteration 71, loss = 1.03554867\n",
            "Iteration 72, loss = 1.03616123\n",
            "Iteration 73, loss = 1.03081459\n",
            "Iteration 74, loss = 1.01243762\n",
            "Iteration 75, loss = 1.02144553\n",
            "Iteration 76, loss = 1.02325495\n",
            "Iteration 77, loss = 1.01894464\n",
            "Iteration 78, loss = 1.00803050\n",
            "Iteration 79, loss = 1.00155650\n",
            "Iteration 80, loss = 1.01541157\n",
            "Iteration 81, loss = 1.01493092\n",
            "Iteration 82, loss = 1.02817014\n",
            "Iteration 83, loss = 1.00418093\n",
            "Iteration 84, loss = 1.00895440\n",
            "Iteration 85, loss = 0.98243622\n",
            "Iteration 86, loss = 0.98902634\n",
            "Iteration 87, loss = 1.00180111\n",
            "Iteration 88, loss = 0.98237382\n",
            "Iteration 89, loss = 0.97511903\n",
            "Iteration 90, loss = 0.97492932\n",
            "Iteration 91, loss = 0.98413650\n",
            "Iteration 92, loss = 0.95569723\n",
            "Iteration 93, loss = 0.96136307\n",
            "Iteration 94, loss = 0.96030161\n",
            "Iteration 95, loss = 0.96017647\n",
            "Iteration 96, loss = 0.95846960\n",
            "Iteration 97, loss = 0.95597257\n",
            "Iteration 98, loss = 0.97107089\n",
            "Iteration 99, loss = 0.98930406\n",
            "Iteration 100, loss = 1.06597749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41710294\n",
            "Iteration 2, loss = 1.37241754\n",
            "Iteration 3, loss = 1.36432157\n",
            "Iteration 4, loss = 1.35880539\n",
            "Iteration 5, loss = 1.34680597\n",
            "Iteration 6, loss = 1.34730762\n",
            "Iteration 7, loss = 1.33945570\n",
            "Iteration 8, loss = 1.33010355\n",
            "Iteration 9, loss = 1.32599094\n",
            "Iteration 10, loss = 1.31781729\n",
            "Iteration 11, loss = 1.31030441\n",
            "Iteration 12, loss = 1.30609255\n",
            "Iteration 13, loss = 1.29989276\n",
            "Iteration 14, loss = 1.29142101\n",
            "Iteration 15, loss = 1.28034195\n",
            "Iteration 16, loss = 1.27130525\n",
            "Iteration 17, loss = 1.26185294\n",
            "Iteration 18, loss = 1.26736694\n",
            "Iteration 19, loss = 1.26506061\n",
            "Iteration 20, loss = 1.28251823\n",
            "Iteration 21, loss = 1.24504937\n",
            "Iteration 22, loss = 1.23787362\n",
            "Iteration 23, loss = 1.22712708\n",
            "Iteration 24, loss = 1.26811941\n",
            "Iteration 25, loss = 1.24135809\n",
            "Iteration 26, loss = 1.23121415\n",
            "Iteration 27, loss = 1.20684983\n",
            "Iteration 28, loss = 1.19738620\n",
            "Iteration 29, loss = 1.18649860\n",
            "Iteration 30, loss = 1.18128788\n",
            "Iteration 31, loss = 1.19756116\n",
            "Iteration 32, loss = 1.18015356\n",
            "Iteration 33, loss = 1.16666336\n",
            "Iteration 34, loss = 1.17540988\n",
            "Iteration 35, loss = 1.18553492\n",
            "Iteration 36, loss = 1.19836889\n",
            "Iteration 37, loss = 1.16201022\n",
            "Iteration 38, loss = 1.15016264\n",
            "Iteration 39, loss = 1.14341306\n",
            "Iteration 40, loss = 1.14039556\n",
            "Iteration 41, loss = 1.14777493\n",
            "Iteration 42, loss = 1.12509287\n",
            "Iteration 43, loss = 1.13962298\n",
            "Iteration 44, loss = 1.13914247\n",
            "Iteration 45, loss = 1.13115810\n",
            "Iteration 46, loss = 1.11911995\n",
            "Iteration 47, loss = 1.11737792\n",
            "Iteration 48, loss = 1.13019745\n",
            "Iteration 49, loss = 1.13930547\n",
            "Iteration 50, loss = 1.10963043\n",
            "Iteration 51, loss = 1.09720956\n",
            "Iteration 52, loss = 1.09189063\n",
            "Iteration 53, loss = 1.09210393\n",
            "Iteration 54, loss = 1.08994407\n",
            "Iteration 55, loss = 1.09540905\n",
            "Iteration 56, loss = 1.10414442\n",
            "Iteration 57, loss = 1.09063649\n",
            "Iteration 58, loss = 1.07963086\n",
            "Iteration 59, loss = 1.07791799\n",
            "Iteration 60, loss = 1.07401264\n",
            "Iteration 61, loss = 1.07323465\n",
            "Iteration 62, loss = 1.10154355\n",
            "Iteration 63, loss = 1.11757946\n",
            "Iteration 64, loss = 1.13697022\n",
            "Iteration 65, loss = 1.09678693\n",
            "Iteration 66, loss = 1.08194144\n",
            "Iteration 67, loss = 1.05151640\n",
            "Iteration 68, loss = 1.06644854\n",
            "Iteration 69, loss = 1.05523168\n",
            "Iteration 70, loss = 1.07125440\n",
            "Iteration 71, loss = 1.04938065\n",
            "Iteration 72, loss = 1.04850169\n",
            "Iteration 73, loss = 1.04541810\n",
            "Iteration 74, loss = 1.03234738\n",
            "Iteration 75, loss = 1.04687652\n",
            "Iteration 76, loss = 1.06105229\n",
            "Iteration 77, loss = 1.03137570\n",
            "Iteration 78, loss = 1.01959649\n",
            "Iteration 79, loss = 1.03553638\n",
            "Iteration 80, loss = 1.01266873\n",
            "Iteration 81, loss = 1.01189556\n",
            "Iteration 82, loss = 1.01070594\n",
            "Iteration 83, loss = 1.01518320\n",
            "Iteration 84, loss = 1.00679085\n",
            "Iteration 85, loss = 0.99514686\n",
            "Iteration 86, loss = 1.00610331\n",
            "Iteration 87, loss = 0.99714799\n",
            "Iteration 88, loss = 0.99111786\n",
            "Iteration 89, loss = 0.99117009\n",
            "Iteration 90, loss = 0.99047408\n",
            "Iteration 91, loss = 1.00335228\n",
            "Iteration 92, loss = 0.97770234\n",
            "Iteration 93, loss = 0.97458145\n",
            "Iteration 94, loss = 0.98062569\n",
            "Iteration 95, loss = 0.99191294\n",
            "Iteration 96, loss = 0.98743105\n",
            "Iteration 97, loss = 0.98087551\n",
            "Iteration 98, loss = 0.98513496\n",
            "Iteration 99, loss = 1.00064707\n",
            "Iteration 100, loss = 1.09121098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41911517\n",
            "Iteration 2, loss = 1.37305960\n",
            "Iteration 3, loss = 1.36364623\n",
            "Iteration 4, loss = 1.35824485\n",
            "Iteration 5, loss = 1.34854453\n",
            "Iteration 6, loss = 1.34311285\n",
            "Iteration 7, loss = 1.33885944\n",
            "Iteration 8, loss = 1.32978365\n",
            "Iteration 9, loss = 1.32457925\n",
            "Iteration 10, loss = 1.31610539\n",
            "Iteration 11, loss = 1.31049765\n",
            "Iteration 12, loss = 1.30885354\n",
            "Iteration 13, loss = 1.29713053\n",
            "Iteration 14, loss = 1.29194658\n",
            "Iteration 15, loss = 1.28276219\n",
            "Iteration 16, loss = 1.27262187\n",
            "Iteration 17, loss = 1.26375672\n",
            "Iteration 18, loss = 1.26495153\n",
            "Iteration 19, loss = 1.26075106\n",
            "Iteration 20, loss = 1.28192378\n",
            "Iteration 21, loss = 1.24020371\n",
            "Iteration 22, loss = 1.23222164\n",
            "Iteration 23, loss = 1.22132503\n",
            "Iteration 24, loss = 1.23601630\n",
            "Iteration 25, loss = 1.21931673\n",
            "Iteration 26, loss = 1.21101683\n",
            "Iteration 27, loss = 1.20716788\n",
            "Iteration 28, loss = 1.19782146\n",
            "Iteration 29, loss = 1.19209523\n",
            "Iteration 30, loss = 1.17968304\n",
            "Iteration 31, loss = 1.17778347\n",
            "Iteration 32, loss = 1.18253167\n",
            "Iteration 33, loss = 1.15606529\n",
            "Iteration 34, loss = 1.15801047\n",
            "Iteration 35, loss = 1.18692488\n",
            "Iteration 36, loss = 1.19031748\n",
            "Iteration 37, loss = 1.15780514\n",
            "Iteration 38, loss = 1.14698059\n",
            "Iteration 39, loss = 1.13546647\n",
            "Iteration 40, loss = 1.13246622\n",
            "Iteration 41, loss = 1.14566635\n",
            "Iteration 42, loss = 1.14818107\n",
            "Iteration 43, loss = 1.12976525\n",
            "Iteration 44, loss = 1.12816442\n",
            "Iteration 45, loss = 1.14688486\n",
            "Iteration 46, loss = 1.12795343\n",
            "Iteration 47, loss = 1.11118674\n",
            "Iteration 48, loss = 1.13672195\n",
            "Iteration 49, loss = 1.16640802\n",
            "Iteration 50, loss = 1.12450966\n",
            "Iteration 51, loss = 1.10474830\n",
            "Iteration 52, loss = 1.10145458\n",
            "Iteration 53, loss = 1.10175222\n",
            "Iteration 54, loss = 1.08792986\n",
            "Iteration 55, loss = 1.09566720\n",
            "Iteration 56, loss = 1.09731235\n",
            "Iteration 57, loss = 1.08428229\n",
            "Iteration 58, loss = 1.08851935\n",
            "Iteration 59, loss = 1.09267692\n",
            "Iteration 60, loss = 1.08986683\n",
            "Iteration 61, loss = 1.08672184\n",
            "Iteration 62, loss = 1.08269120\n",
            "Iteration 63, loss = 1.08876139\n",
            "Iteration 64, loss = 1.10091644\n",
            "Iteration 65, loss = 1.08264069\n",
            "Iteration 66, loss = 1.06516793\n",
            "Iteration 67, loss = 1.06674808\n",
            "Iteration 68, loss = 1.08410777\n",
            "Iteration 69, loss = 1.07651912\n",
            "Iteration 70, loss = 1.06147483\n",
            "Iteration 71, loss = 1.05740697\n",
            "Iteration 72, loss = 1.05814426\n",
            "Iteration 73, loss = 1.04615665\n",
            "Iteration 74, loss = 1.03574885\n",
            "Iteration 75, loss = 1.04561016\n",
            "Iteration 76, loss = 1.04481375\n",
            "Iteration 77, loss = 1.03645212\n",
            "Iteration 78, loss = 1.03572776\n",
            "Iteration 79, loss = 1.02923049\n",
            "Iteration 80, loss = 1.03754011\n",
            "Iteration 81, loss = 1.02798830\n",
            "Iteration 82, loss = 1.01824234\n",
            "Iteration 83, loss = 1.01736068\n",
            "Iteration 84, loss = 1.02621946\n",
            "Iteration 85, loss = 1.00297480\n",
            "Iteration 86, loss = 1.01635693\n",
            "Iteration 87, loss = 1.00954424\n",
            "Iteration 88, loss = 0.99867772\n",
            "Iteration 89, loss = 0.99769330\n",
            "Iteration 90, loss = 1.00502498\n",
            "Iteration 91, loss = 1.00285592\n",
            "Iteration 92, loss = 0.99625891\n",
            "Iteration 93, loss = 0.98947715\n",
            "Iteration 94, loss = 0.98945515\n",
            "Iteration 95, loss = 0.98451240\n",
            "Iteration 96, loss = 0.99587344\n",
            "Iteration 97, loss = 0.99074382\n",
            "Iteration 98, loss = 0.99404922\n",
            "Iteration 99, loss = 0.97057729\n",
            "Iteration 100, loss = 0.99890773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40688098\n",
            "Iteration 2, loss = 1.37272560\n",
            "Iteration 3, loss = 1.36218121\n",
            "Iteration 4, loss = 1.35650492\n",
            "Iteration 5, loss = 1.34560279\n",
            "Iteration 6, loss = 1.34387661\n",
            "Iteration 7, loss = 1.33321231\n",
            "Iteration 8, loss = 1.32495596\n",
            "Iteration 9, loss = 1.32245895\n",
            "Iteration 10, loss = 1.31126909\n",
            "Iteration 11, loss = 1.30435980\n",
            "Iteration 12, loss = 1.30872351\n",
            "Iteration 13, loss = 1.29434922\n",
            "Iteration 14, loss = 1.28254955\n",
            "Iteration 15, loss = 1.27359663\n",
            "Iteration 16, loss = 1.26354001\n",
            "Iteration 17, loss = 1.26402524\n",
            "Iteration 18, loss = 1.27069476\n",
            "Iteration 19, loss = 1.27119353\n",
            "Iteration 20, loss = 1.25579295\n",
            "Iteration 21, loss = 1.22876492\n",
            "Iteration 22, loss = 1.22689336\n",
            "Iteration 23, loss = 1.21355978\n",
            "Iteration 24, loss = 1.22385058\n",
            "Iteration 25, loss = 1.21291993\n",
            "Iteration 26, loss = 1.19629877\n",
            "Iteration 27, loss = 1.18544225\n",
            "Iteration 28, loss = 1.17917132\n",
            "Iteration 29, loss = 1.17606414\n",
            "Iteration 30, loss = 1.17819043\n",
            "Iteration 31, loss = 1.17529000\n",
            "Iteration 32, loss = 1.16643912\n",
            "Iteration 33, loss = 1.16948789\n",
            "Iteration 34, loss = 1.14951943\n",
            "Iteration 35, loss = 1.15191897\n",
            "Iteration 36, loss = 1.17376359\n",
            "Iteration 37, loss = 1.14486436\n",
            "Iteration 38, loss = 1.13148894\n",
            "Iteration 39, loss = 1.14419491\n",
            "Iteration 40, loss = 1.13185993\n",
            "Iteration 41, loss = 1.12875032\n",
            "Iteration 42, loss = 1.12774683\n",
            "Iteration 43, loss = 1.16098695\n",
            "Iteration 44, loss = 1.14687049\n",
            "Iteration 45, loss = 1.13539067\n",
            "Iteration 46, loss = 1.12633427\n",
            "Iteration 47, loss = 1.11952453\n",
            "Iteration 48, loss = 1.12692017\n",
            "Iteration 49, loss = 1.15397150\n",
            "Iteration 50, loss = 1.12569582\n",
            "Iteration 51, loss = 1.10439221\n",
            "Iteration 52, loss = 1.09249465\n",
            "Iteration 53, loss = 1.09035998\n",
            "Iteration 54, loss = 1.08285413\n",
            "Iteration 55, loss = 1.08994626\n",
            "Iteration 56, loss = 1.10021322\n",
            "Iteration 57, loss = 1.08587626\n",
            "Iteration 58, loss = 1.08109778\n",
            "Iteration 59, loss = 1.10773234\n",
            "Iteration 60, loss = 1.11094449\n",
            "Iteration 61, loss = 1.08607048\n",
            "Iteration 62, loss = 1.11259587\n",
            "Iteration 63, loss = 1.11194106\n",
            "Iteration 64, loss = 1.09841342\n",
            "Iteration 65, loss = 1.07881764\n",
            "Iteration 66, loss = 1.06110554\n",
            "Iteration 67, loss = 1.06567334\n",
            "Iteration 68, loss = 1.08975497\n",
            "Iteration 69, loss = 1.06243543\n",
            "Iteration 70, loss = 1.05538535\n",
            "Iteration 71, loss = 1.07065518\n",
            "Iteration 72, loss = 1.04581375\n",
            "Iteration 73, loss = 1.04046217\n",
            "Iteration 74, loss = 1.03655152\n",
            "Iteration 75, loss = 1.06223579\n",
            "Iteration 76, loss = 1.06597209\n",
            "Iteration 77, loss = 1.05747608\n",
            "Iteration 78, loss = 1.04510539\n",
            "Iteration 79, loss = 1.03087383\n",
            "Iteration 80, loss = 1.03391819\n",
            "Iteration 81, loss = 1.03115259\n",
            "Iteration 82, loss = 1.02904238\n",
            "Iteration 83, loss = 1.02777589\n",
            "Iteration 84, loss = 1.03182801\n",
            "Iteration 85, loss = 1.01260916\n",
            "Iteration 86, loss = 1.03794362\n",
            "Iteration 87, loss = 1.02251339\n",
            "Iteration 88, loss = 1.01210818\n",
            "Iteration 89, loss = 1.02673868\n",
            "Iteration 90, loss = 1.02512609\n",
            "Iteration 91, loss = 1.02103207\n",
            "Iteration 92, loss = 1.00382581\n",
            "Iteration 93, loss = 1.03230610\n",
            "Iteration 94, loss = 1.00489660\n",
            "Iteration 95, loss = 1.00433146\n",
            "Iteration 96, loss = 1.00258218\n",
            "Iteration 97, loss = 1.00256767\n",
            "Iteration 98, loss = 1.01123518\n",
            "Iteration 99, loss = 1.01690951\n",
            "Iteration 100, loss = 1.04439960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39936429\n",
            "Iteration 2, loss = 1.36835215\n",
            "Iteration 3, loss = 1.35961103\n",
            "Iteration 4, loss = 1.35732068\n",
            "Iteration 5, loss = 1.34509909\n",
            "Iteration 6, loss = 1.34748767\n",
            "Iteration 7, loss = 1.34625854\n",
            "Iteration 8, loss = 1.33336712\n",
            "Iteration 9, loss = 1.32715616\n",
            "Iteration 10, loss = 1.32039803\n",
            "Iteration 11, loss = 1.31499918\n",
            "Iteration 12, loss = 1.32301236\n",
            "Iteration 13, loss = 1.31341542\n",
            "Iteration 14, loss = 1.30180415\n",
            "Iteration 15, loss = 1.29764931\n",
            "Iteration 16, loss = 1.29435122\n",
            "Iteration 17, loss = 1.29952294\n",
            "Iteration 18, loss = 1.28251896\n",
            "Iteration 19, loss = 1.26827102\n",
            "Iteration 20, loss = 1.27139039\n",
            "Iteration 21, loss = 1.26157730\n",
            "Iteration 22, loss = 1.26642618\n",
            "Iteration 23, loss = 1.26036055\n",
            "Iteration 24, loss = 1.26617031\n",
            "Iteration 25, loss = 1.24595611\n",
            "Iteration 26, loss = 1.23499356\n",
            "Iteration 27, loss = 1.22863329\n",
            "Iteration 28, loss = 1.22079772\n",
            "Iteration 29, loss = 1.20884628\n",
            "Iteration 30, loss = 1.19744629\n",
            "Iteration 31, loss = 1.22261272\n",
            "Iteration 32, loss = 1.21090441\n",
            "Iteration 33, loss = 1.20464448\n",
            "Iteration 34, loss = 1.20703725\n",
            "Iteration 35, loss = 1.19412634\n",
            "Iteration 36, loss = 1.19482881\n",
            "Iteration 37, loss = 1.16876985\n",
            "Iteration 38, loss = 1.15672758\n",
            "Iteration 39, loss = 1.15511882\n",
            "Iteration 40, loss = 1.16380271\n",
            "Iteration 41, loss = 1.16362909\n",
            "Iteration 42, loss = 1.15609739\n",
            "Iteration 43, loss = 1.14493066\n",
            "Iteration 44, loss = 1.16199131\n",
            "Iteration 45, loss = 1.15110153\n",
            "Iteration 46, loss = 1.14962650\n",
            "Iteration 47, loss = 1.13309290\n",
            "Iteration 48, loss = 1.15165354\n",
            "Iteration 49, loss = 1.19761857\n",
            "Iteration 50, loss = 1.17411215\n",
            "Iteration 51, loss = 1.15752434\n",
            "Iteration 52, loss = 1.14311366\n",
            "Iteration 53, loss = 1.13036707\n",
            "Iteration 54, loss = 1.11134761\n",
            "Iteration 55, loss = 1.12649969\n",
            "Iteration 56, loss = 1.10880843\n",
            "Iteration 57, loss = 1.11630998\n",
            "Iteration 58, loss = 1.11992054\n",
            "Iteration 59, loss = 1.12470517\n",
            "Iteration 60, loss = 1.10182347\n",
            "Iteration 61, loss = 1.10572109\n",
            "Iteration 62, loss = 1.13039533\n",
            "Iteration 63, loss = 1.11847441\n",
            "Iteration 64, loss = 1.09436766\n",
            "Iteration 65, loss = 1.09617164\n",
            "Iteration 66, loss = 1.11254732\n",
            "Iteration 67, loss = 1.09883994\n",
            "Iteration 68, loss = 1.09403582\n",
            "Iteration 69, loss = 1.08902941\n",
            "Iteration 70, loss = 1.10369174\n",
            "Iteration 71, loss = 1.08149676\n",
            "Iteration 72, loss = 1.08634451\n",
            "Iteration 73, loss = 1.07963901\n",
            "Iteration 74, loss = 1.07122465\n",
            "Iteration 75, loss = 1.12785827\n",
            "Iteration 76, loss = 1.10934032\n",
            "Iteration 77, loss = 1.07702015\n",
            "Iteration 78, loss = 1.07009191\n",
            "Iteration 79, loss = 1.06692725\n",
            "Iteration 80, loss = 1.06267213\n",
            "Iteration 81, loss = 1.04889699\n",
            "Iteration 82, loss = 1.04427447\n",
            "Iteration 83, loss = 1.04269164\n",
            "Iteration 84, loss = 1.03715170\n",
            "Iteration 85, loss = 1.03109418\n",
            "Iteration 86, loss = 1.03264217\n",
            "Iteration 87, loss = 1.03669526\n",
            "Iteration 88, loss = 1.03224255\n",
            "Iteration 89, loss = 1.03670336\n",
            "Iteration 90, loss = 1.03573225\n",
            "Iteration 91, loss = 1.03750941\n",
            "Iteration 92, loss = 1.02523283\n",
            "Iteration 93, loss = 1.03352063\n",
            "Iteration 94, loss = 1.03279995\n",
            "Iteration 95, loss = 1.02048370\n",
            "Iteration 96, loss = 1.02229096\n",
            "Iteration 97, loss = 1.02809375\n",
            "Iteration 98, loss = 1.02782380\n",
            "Iteration 99, loss = 1.03496441\n",
            "Iteration 100, loss = 1.02425229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38795803\n",
            "Iteration 2, loss = 1.35609536\n",
            "Iteration 3, loss = 1.35095413\n",
            "Iteration 4, loss = 1.34303018\n",
            "Iteration 5, loss = 1.33416085\n",
            "Iteration 6, loss = 1.33514197\n",
            "Iteration 7, loss = 1.32954505\n",
            "Iteration 8, loss = 1.32001077\n",
            "Iteration 9, loss = 1.31994127\n",
            "Iteration 10, loss = 1.30927453\n",
            "Iteration 11, loss = 1.30598217\n",
            "Iteration 12, loss = 1.31275272\n",
            "Iteration 13, loss = 1.30509802\n",
            "Iteration 14, loss = 1.29486695\n",
            "Iteration 15, loss = 1.29019037\n",
            "Iteration 16, loss = 1.28406401\n",
            "Iteration 17, loss = 1.28755114\n",
            "Iteration 18, loss = 1.26865166\n",
            "Iteration 19, loss = 1.26454412\n",
            "Iteration 20, loss = 1.26132917\n",
            "Iteration 21, loss = 1.24665909\n",
            "Iteration 22, loss = 1.25990250\n",
            "Iteration 23, loss = 1.25857940\n",
            "Iteration 24, loss = 1.26796753\n",
            "Iteration 25, loss = 1.24312832\n",
            "Iteration 26, loss = 1.23571108\n",
            "Iteration 27, loss = 1.24767354\n",
            "Iteration 28, loss = 1.24143084\n",
            "Iteration 29, loss = 1.22423114\n",
            "Iteration 30, loss = 1.21686610\n",
            "Iteration 31, loss = 1.20404588\n",
            "Iteration 32, loss = 1.19590631\n",
            "Iteration 33, loss = 1.19520826\n",
            "Iteration 34, loss = 1.18504518\n",
            "Iteration 35, loss = 1.18949550\n",
            "Iteration 36, loss = 1.18487507\n",
            "Iteration 37, loss = 1.17252820\n",
            "Iteration 38, loss = 1.16090465\n",
            "Iteration 39, loss = 1.16984648\n",
            "Iteration 40, loss = 1.16620926\n",
            "Iteration 41, loss = 1.16276446\n",
            "Iteration 42, loss = 1.14550080\n",
            "Iteration 43, loss = 1.15353917\n",
            "Iteration 44, loss = 1.15358782\n",
            "Iteration 45, loss = 1.13724419\n",
            "Iteration 46, loss = 1.14459622\n",
            "Iteration 47, loss = 1.14806878\n",
            "Iteration 48, loss = 1.15396942\n",
            "Iteration 49, loss = 1.20658082\n",
            "Iteration 50, loss = 1.20281904\n",
            "Iteration 51, loss = 1.17858747\n",
            "Iteration 52, loss = 1.14639099\n",
            "Iteration 53, loss = 1.13138838\n",
            "Iteration 54, loss = 1.12728301\n",
            "Iteration 55, loss = 1.14429122\n",
            "Iteration 56, loss = 1.11917109\n",
            "Iteration 57, loss = 1.10536891\n",
            "Iteration 58, loss = 1.12224880\n",
            "Iteration 59, loss = 1.12778404\n",
            "Iteration 60, loss = 1.11305850\n",
            "Iteration 61, loss = 1.11198827\n",
            "Iteration 62, loss = 1.12816210\n",
            "Iteration 63, loss = 1.12743984\n",
            "Iteration 64, loss = 1.10887880\n",
            "Iteration 65, loss = 1.11334713\n",
            "Iteration 66, loss = 1.12726635\n",
            "Iteration 67, loss = 1.10349985\n",
            "Iteration 68, loss = 1.07992745\n",
            "Iteration 69, loss = 1.08357259\n",
            "Iteration 70, loss = 1.11478881\n",
            "Iteration 71, loss = 1.09665867\n",
            "Iteration 72, loss = 1.08453351\n",
            "Iteration 73, loss = 1.08516195\n",
            "Iteration 74, loss = 1.08302382\n",
            "Iteration 75, loss = 1.13904468\n",
            "Iteration 76, loss = 1.11652514\n",
            "Iteration 77, loss = 1.08452717\n",
            "Iteration 78, loss = 1.06767689\n",
            "Iteration 79, loss = 1.07231876\n",
            "Iteration 80, loss = 1.06729583\n",
            "Iteration 81, loss = 1.06151818\n",
            "Iteration 82, loss = 1.05450932\n",
            "Iteration 83, loss = 1.05301222\n",
            "Iteration 84, loss = 1.04994041\n",
            "Iteration 85, loss = 1.03248115\n",
            "Iteration 86, loss = 1.04506540\n",
            "Iteration 87, loss = 1.03322591\n",
            "Iteration 88, loss = 1.04266967\n",
            "Iteration 89, loss = 1.04785838\n",
            "Iteration 90, loss = 1.05023559\n",
            "Iteration 91, loss = 1.04829563\n",
            "Iteration 92, loss = 1.03205993\n",
            "Iteration 93, loss = 1.04446556\n",
            "Iteration 94, loss = 1.03454296\n",
            "Iteration 95, loss = 1.03994602\n",
            "Iteration 96, loss = 1.05023385\n",
            "Iteration 97, loss = 1.02420786\n",
            "Iteration 98, loss = 1.01735318\n",
            "Iteration 99, loss = 1.02026293\n",
            "Iteration 100, loss = 1.02216947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39733749\n",
            "Iteration 2, loss = 1.36556608\n",
            "Iteration 3, loss = 1.35947037\n",
            "Iteration 4, loss = 1.35112836\n",
            "Iteration 5, loss = 1.34675754\n",
            "Iteration 6, loss = 1.35220522\n",
            "Iteration 7, loss = 1.33913972\n",
            "Iteration 8, loss = 1.33461144\n",
            "Iteration 9, loss = 1.34076950\n",
            "Iteration 10, loss = 1.32159605\n",
            "Iteration 11, loss = 1.32225453\n",
            "Iteration 12, loss = 1.32900237\n",
            "Iteration 13, loss = 1.32240952\n",
            "Iteration 14, loss = 1.31010533\n",
            "Iteration 15, loss = 1.30225806\n",
            "Iteration 16, loss = 1.29539723\n",
            "Iteration 17, loss = 1.28831148\n",
            "Iteration 18, loss = 1.29176151\n",
            "Iteration 19, loss = 1.28831146\n",
            "Iteration 20, loss = 1.27227695\n",
            "Iteration 21, loss = 1.25414362\n",
            "Iteration 22, loss = 1.24841664\n",
            "Iteration 23, loss = 1.24213325\n",
            "Iteration 24, loss = 1.23559862\n",
            "Iteration 25, loss = 1.23747019\n",
            "Iteration 26, loss = 1.23201126\n",
            "Iteration 27, loss = 1.25084439\n",
            "Iteration 28, loss = 1.22204089\n",
            "Iteration 29, loss = 1.20426157\n",
            "Iteration 30, loss = 1.19632775\n",
            "Iteration 31, loss = 1.19470452\n",
            "Iteration 32, loss = 1.21233784\n",
            "Iteration 33, loss = 1.19662320\n",
            "Iteration 34, loss = 1.20478350\n",
            "Iteration 35, loss = 1.19962760\n",
            "Iteration 36, loss = 1.18873984\n",
            "Iteration 37, loss = 1.16922123\n",
            "Iteration 38, loss = 1.15659960\n",
            "Iteration 39, loss = 1.15633615\n",
            "Iteration 40, loss = 1.15178956\n",
            "Iteration 41, loss = 1.15241350\n",
            "Iteration 42, loss = 1.14656829\n",
            "Iteration 43, loss = 1.14714080\n",
            "Iteration 44, loss = 1.14180535\n",
            "Iteration 45, loss = 1.13031779\n",
            "Iteration 46, loss = 1.12537411\n",
            "Iteration 47, loss = 1.14112994\n",
            "Iteration 48, loss = 1.13799569\n",
            "Iteration 49, loss = 1.18122817\n",
            "Iteration 50, loss = 1.15969283\n",
            "Iteration 51, loss = 1.13730779\n",
            "Iteration 52, loss = 1.11705203\n",
            "Iteration 53, loss = 1.11276378\n",
            "Iteration 54, loss = 1.12063949\n",
            "Iteration 55, loss = 1.15862753\n",
            "Iteration 56, loss = 1.13217852\n",
            "Iteration 57, loss = 1.11929725\n",
            "Iteration 58, loss = 1.12482260\n",
            "Iteration 59, loss = 1.11124046\n",
            "Iteration 60, loss = 1.10902040\n",
            "Iteration 61, loss = 1.09580007\n",
            "Iteration 62, loss = 1.09517760\n",
            "Iteration 63, loss = 1.09248331\n",
            "Iteration 64, loss = 1.09007585\n",
            "Iteration 65, loss = 1.08288139\n",
            "Iteration 66, loss = 1.13667081\n",
            "Iteration 67, loss = 1.09489112\n",
            "Iteration 68, loss = 1.07681548\n",
            "Iteration 69, loss = 1.07616047\n",
            "Iteration 70, loss = 1.08516828\n",
            "Iteration 71, loss = 1.07798869\n",
            "Iteration 72, loss = 1.06828533\n",
            "Iteration 73, loss = 1.07484053\n",
            "Iteration 74, loss = 1.07069034\n",
            "Iteration 75, loss = 1.13195655\n",
            "Iteration 76, loss = 1.09619059\n",
            "Iteration 77, loss = 1.06364401\n",
            "Iteration 78, loss = 1.05305460\n",
            "Iteration 79, loss = 1.05190541\n",
            "Iteration 80, loss = 1.05478511\n",
            "Iteration 81, loss = 1.04386981\n",
            "Iteration 82, loss = 1.05080487\n",
            "Iteration 83, loss = 1.04705446\n",
            "Iteration 84, loss = 1.03336508\n",
            "Iteration 85, loss = 1.03404063\n",
            "Iteration 86, loss = 1.03390431\n",
            "Iteration 87, loss = 1.03544700\n",
            "Iteration 88, loss = 1.04613791\n",
            "Iteration 89, loss = 1.04227123\n",
            "Iteration 90, loss = 1.04132163\n",
            "Iteration 91, loss = 1.03082373\n",
            "Iteration 92, loss = 1.01780583\n",
            "Iteration 93, loss = 1.04116515\n",
            "Iteration 94, loss = 1.03304945\n",
            "Iteration 95, loss = 1.03097339\n",
            "Iteration 96, loss = 1.03097742\n",
            "Iteration 97, loss = 1.01385824\n",
            "Iteration 98, loss = 1.03271078\n",
            "Iteration 99, loss = 1.01669851\n",
            "Iteration 100, loss = 1.01224893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39938709\n",
            "Iteration 2, loss = 1.36125519\n",
            "Iteration 3, loss = 1.35346601\n",
            "Iteration 4, loss = 1.34225639\n",
            "Iteration 5, loss = 1.33850863\n",
            "Iteration 6, loss = 1.35441809\n",
            "Iteration 7, loss = 1.33673342\n",
            "Iteration 8, loss = 1.33378348\n",
            "Iteration 9, loss = 1.33310118\n",
            "Iteration 10, loss = 1.31356596\n",
            "Iteration 11, loss = 1.31382808\n",
            "Iteration 12, loss = 1.31937479\n",
            "Iteration 13, loss = 1.30854151\n",
            "Iteration 14, loss = 1.30257631\n",
            "Iteration 15, loss = 1.29063561\n",
            "Iteration 16, loss = 1.28554296\n",
            "Iteration 17, loss = 1.27468377\n",
            "Iteration 18, loss = 1.27630394\n",
            "Iteration 19, loss = 1.26785307\n",
            "Iteration 20, loss = 1.25860872\n",
            "Iteration 21, loss = 1.24399538\n",
            "Iteration 22, loss = 1.24070201\n",
            "Iteration 23, loss = 1.23584351\n",
            "Iteration 24, loss = 1.22632445\n",
            "Iteration 25, loss = 1.22036786\n",
            "Iteration 26, loss = 1.21136169\n",
            "Iteration 27, loss = 1.21711072\n",
            "Iteration 28, loss = 1.18761303\n",
            "Iteration 29, loss = 1.18356686\n",
            "Iteration 30, loss = 1.19238817\n",
            "Iteration 31, loss = 1.21175886\n",
            "Iteration 32, loss = 1.21942144\n",
            "Iteration 33, loss = 1.19977274\n",
            "Iteration 34, loss = 1.19604156\n",
            "Iteration 35, loss = 1.17970790\n",
            "Iteration 36, loss = 1.16748612\n",
            "Iteration 37, loss = 1.14235448\n",
            "Iteration 38, loss = 1.13861250\n",
            "Iteration 39, loss = 1.13165394\n",
            "Iteration 40, loss = 1.13485148\n",
            "Iteration 41, loss = 1.13090617\n",
            "Iteration 42, loss = 1.12849815\n",
            "Iteration 43, loss = 1.14402048\n",
            "Iteration 44, loss = 1.13396368\n",
            "Iteration 45, loss = 1.11932654\n",
            "Iteration 46, loss = 1.11636347\n",
            "Iteration 47, loss = 1.12873040\n",
            "Iteration 48, loss = 1.12662604\n",
            "Iteration 49, loss = 1.18556182\n",
            "Iteration 50, loss = 1.16226146\n",
            "Iteration 51, loss = 1.15882218\n",
            "Iteration 52, loss = 1.13331880\n",
            "Iteration 53, loss = 1.10176968\n",
            "Iteration 54, loss = 1.10704875\n",
            "Iteration 55, loss = 1.11811556\n",
            "Iteration 56, loss = 1.09357951\n",
            "Iteration 57, loss = 1.07386318\n",
            "Iteration 58, loss = 1.07958957\n",
            "Iteration 59, loss = 1.08094967\n",
            "Iteration 60, loss = 1.07129821\n",
            "Iteration 61, loss = 1.08091873\n",
            "Iteration 62, loss = 1.09431446\n",
            "Iteration 63, loss = 1.10704895\n",
            "Iteration 64, loss = 1.10000018\n",
            "Iteration 65, loss = 1.10140534\n",
            "Iteration 66, loss = 1.12247271\n",
            "Iteration 67, loss = 1.08239468\n",
            "Iteration 68, loss = 1.06497017\n",
            "Iteration 69, loss = 1.06597080\n",
            "Iteration 70, loss = 1.05670236\n",
            "Iteration 71, loss = 1.05507504\n",
            "Iteration 72, loss = 1.04679138\n",
            "Iteration 73, loss = 1.04860091\n",
            "Iteration 74, loss = 1.06283121\n",
            "Iteration 75, loss = 1.11411799\n",
            "Iteration 76, loss = 1.09970060\n",
            "Iteration 77, loss = 1.07577177\n",
            "Iteration 78, loss = 1.04482773\n",
            "Iteration 79, loss = 1.03377395\n",
            "Iteration 80, loss = 1.03883502\n",
            "Iteration 81, loss = 1.03648898\n",
            "Iteration 82, loss = 1.03527366\n",
            "Iteration 83, loss = 1.04685427\n",
            "Iteration 84, loss = 1.02478352\n",
            "Iteration 85, loss = 1.01686251\n",
            "Iteration 86, loss = 1.01562525\n",
            "Iteration 87, loss = 1.00888458\n",
            "Iteration 88, loss = 1.01611994\n",
            "Iteration 89, loss = 1.03688490\n",
            "Iteration 90, loss = 1.02718434\n",
            "Iteration 91, loss = 1.01790825\n",
            "Iteration 92, loss = 1.00702006\n",
            "Iteration 93, loss = 1.01782166\n",
            "Iteration 94, loss = 1.01285346\n",
            "Iteration 95, loss = 1.00663923\n",
            "Iteration 96, loss = 1.02016835\n",
            "Iteration 97, loss = 1.00160648\n",
            "Iteration 98, loss = 1.00912496\n",
            "Iteration 99, loss = 1.01565081\n",
            "Iteration 100, loss = 1.01143632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39556203\n",
            "Iteration 2, loss = 1.34914045\n",
            "Iteration 3, loss = 1.33719602\n",
            "Iteration 4, loss = 1.32943587\n",
            "Iteration 5, loss = 1.32344846\n",
            "Iteration 6, loss = 1.33011951\n",
            "Iteration 7, loss = 1.33041077\n",
            "Iteration 8, loss = 1.30819537\n",
            "Iteration 9, loss = 1.30337856\n",
            "Iteration 10, loss = 1.29497301\n",
            "Iteration 11, loss = 1.29829849\n",
            "Iteration 12, loss = 1.30408193\n",
            "Iteration 13, loss = 1.28382783\n",
            "Iteration 14, loss = 1.27594036\n",
            "Iteration 15, loss = 1.26828368\n",
            "Iteration 16, loss = 1.27106555\n",
            "Iteration 17, loss = 1.26683562\n",
            "Iteration 18, loss = 1.25712979\n",
            "Iteration 19, loss = 1.24740363\n",
            "Iteration 20, loss = 1.24680531\n",
            "Iteration 21, loss = 1.22373166\n",
            "Iteration 22, loss = 1.22499902\n",
            "Iteration 23, loss = 1.22669212\n",
            "Iteration 24, loss = 1.19981041\n",
            "Iteration 25, loss = 1.20346189\n",
            "Iteration 26, loss = 1.18322840\n",
            "Iteration 27, loss = 1.19752145\n",
            "Iteration 28, loss = 1.20603628\n",
            "Iteration 29, loss = 1.18828246\n",
            "Iteration 30, loss = 1.17328297\n",
            "Iteration 31, loss = 1.19705229\n",
            "Iteration 32, loss = 1.21785666\n",
            "Iteration 33, loss = 1.19383823\n",
            "Iteration 34, loss = 1.19800500\n",
            "Iteration 35, loss = 1.19252149\n",
            "Iteration 36, loss = 1.16448123\n",
            "Iteration 37, loss = 1.15131191\n",
            "Iteration 38, loss = 1.14511262\n",
            "Iteration 39, loss = 1.13877712\n",
            "Iteration 40, loss = 1.14140583\n",
            "Iteration 41, loss = 1.12914258\n",
            "Iteration 42, loss = 1.12469888\n",
            "Iteration 43, loss = 1.15524163\n",
            "Iteration 44, loss = 1.15422190\n",
            "Iteration 45, loss = 1.13045933\n",
            "Iteration 46, loss = 1.11200355\n",
            "Iteration 47, loss = 1.11935628\n",
            "Iteration 48, loss = 1.11276733\n",
            "Iteration 49, loss = 1.13823054\n",
            "Iteration 50, loss = 1.12378085\n",
            "Iteration 51, loss = 1.13936402\n",
            "Iteration 52, loss = 1.10243906\n",
            "Iteration 53, loss = 1.09844184\n",
            "Iteration 54, loss = 1.12185695\n",
            "Iteration 55, loss = 1.14156804\n",
            "Iteration 56, loss = 1.12279938\n",
            "Iteration 57, loss = 1.10156240\n",
            "Iteration 58, loss = 1.10492802\n",
            "Iteration 59, loss = 1.10529872\n",
            "Iteration 60, loss = 1.08400487\n",
            "Iteration 61, loss = 1.08502389\n",
            "Iteration 62, loss = 1.09689862\n",
            "Iteration 63, loss = 1.10984571\n",
            "Iteration 64, loss = 1.10428165\n",
            "Iteration 65, loss = 1.09944325\n",
            "Iteration 66, loss = 1.12794844\n",
            "Iteration 67, loss = 1.07520068\n",
            "Iteration 68, loss = 1.06838101\n",
            "Iteration 69, loss = 1.06647974\n",
            "Iteration 70, loss = 1.05934969\n",
            "Iteration 71, loss = 1.05888882\n",
            "Iteration 72, loss = 1.04894216\n",
            "Iteration 73, loss = 1.05597900\n",
            "Iteration 74, loss = 1.06622643\n",
            "Iteration 75, loss = 1.10177748\n",
            "Iteration 76, loss = 1.08727715\n",
            "Iteration 77, loss = 1.10455518\n",
            "Iteration 78, loss = 1.06075683\n",
            "Iteration 79, loss = 1.04207291\n",
            "Iteration 80, loss = 1.04317189\n",
            "Iteration 81, loss = 1.03908825\n",
            "Iteration 82, loss = 1.04636935\n",
            "Iteration 83, loss = 1.05097248\n",
            "Iteration 84, loss = 1.02156151\n",
            "Iteration 85, loss = 1.02016850\n",
            "Iteration 86, loss = 1.03907566\n",
            "Iteration 87, loss = 1.02050514\n",
            "Iteration 88, loss = 1.02038970\n",
            "Iteration 89, loss = 1.02841295\n",
            "Iteration 90, loss = 1.05002322\n",
            "Iteration 91, loss = 1.03472146\n",
            "Iteration 92, loss = 1.01544958\n",
            "Iteration 93, loss = 1.04606527\n",
            "Iteration 94, loss = 1.03448439\n",
            "Iteration 95, loss = 1.01317066\n",
            "Iteration 96, loss = 1.01990691\n",
            "Iteration 97, loss = 1.01044599\n",
            "Iteration 98, loss = 1.00967718\n",
            "Iteration 99, loss = 1.04216774\n",
            "Iteration 100, loss = 1.01667393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "Iteration 1, loss = 1.36026526\n",
            "Iteration 2, loss = 1.26869014\n",
            "Iteration 3, loss = 1.16226481\n",
            "Iteration 4, loss = 1.07448001\n",
            "Iteration 5, loss = 0.97662855\n",
            "Iteration 6, loss = 0.90796221\n",
            "Iteration 7, loss = 0.84251017\n",
            "Iteration 8, loss = 0.79537833\n",
            "Iteration 9, loss = 0.77166289\n",
            "Iteration 10, loss = 0.74583994\n",
            "Iteration 11, loss = 0.70742566\n",
            "Iteration 12, loss = 0.68054954\n",
            "Iteration 13, loss = 0.65247912\n",
            "Iteration 14, loss = 0.64301174\n",
            "Iteration 15, loss = 0.63718050\n",
            "Iteration 16, loss = 0.61329876\n",
            "Iteration 17, loss = 0.60679129\n",
            "Iteration 18, loss = 0.57449837\n",
            "Iteration 19, loss = 0.57446974\n",
            "Iteration 20, loss = 0.58402475\n",
            "Iteration 21, loss = 0.58395901\n",
            "Iteration 22, loss = 0.56665775\n",
            "Iteration 23, loss = 0.54123367\n",
            "Iteration 24, loss = 0.52582090\n",
            "Iteration 25, loss = 0.51252509\n",
            "Iteration 26, loss = 0.52453832\n",
            "Iteration 27, loss = 0.51103792\n",
            "Iteration 28, loss = 0.53532678\n",
            "Iteration 29, loss = 0.53363155\n",
            "Iteration 30, loss = 0.49750089\n",
            "Iteration 31, loss = 0.47741144\n",
            "Iteration 32, loss = 0.49380995\n",
            "Iteration 33, loss = 0.52175380\n",
            "Iteration 34, loss = 0.49132112\n",
            "Iteration 35, loss = 0.48049954\n",
            "Iteration 36, loss = 0.48384627\n",
            "Iteration 37, loss = 0.46932011\n",
            "Iteration 38, loss = 0.44049772\n",
            "Iteration 39, loss = 0.44458173\n",
            "Iteration 40, loss = 0.43548183\n",
            "Iteration 41, loss = 0.43090683\n",
            "Iteration 42, loss = 0.40635384\n",
            "Iteration 43, loss = 0.43716632\n",
            "Iteration 44, loss = 0.42181731\n",
            "Iteration 45, loss = 0.41600853\n",
            "Iteration 46, loss = 0.38257595\n",
            "Iteration 47, loss = 0.38585855\n",
            "Iteration 48, loss = 0.40545127\n",
            "Iteration 49, loss = 0.41591414\n",
            "Iteration 50, loss = 0.40910953\n",
            "Iteration 51, loss = 0.38330588\n",
            "Iteration 52, loss = 0.37150973\n",
            "Iteration 53, loss = 0.36978808\n",
            "Iteration 54, loss = 0.35241739\n",
            "Iteration 55, loss = 0.34592127\n",
            "Iteration 56, loss = 0.34968666\n",
            "Iteration 57, loss = 0.33701422\n",
            "Iteration 58, loss = 0.32334377\n",
            "Iteration 59, loss = 0.31758481\n",
            "Iteration 60, loss = 0.32918954\n",
            "Iteration 61, loss = 0.31373591\n",
            "Iteration 62, loss = 0.29987176\n",
            "Iteration 63, loss = 0.30427004\n",
            "Iteration 64, loss = 0.31884564\n",
            "Iteration 65, loss = 0.32473556\n",
            "Iteration 66, loss = 0.32357628\n",
            "Iteration 67, loss = 0.33557611\n",
            "Iteration 68, loss = 0.30330290\n",
            "Iteration 69, loss = 0.27633376\n",
            "Iteration 70, loss = 0.28638312\n",
            "Iteration 71, loss = 0.27008583\n",
            "Iteration 72, loss = 0.27332083\n",
            "Iteration 73, loss = 0.25724315\n",
            "Iteration 74, loss = 0.25438430\n",
            "Iteration 75, loss = 0.24817510\n",
            "Iteration 76, loss = 0.23943305\n",
            "Iteration 77, loss = 0.25507856\n",
            "Iteration 78, loss = 0.25970485\n",
            "Iteration 79, loss = 0.26606298\n",
            "Iteration 80, loss = 0.25586487\n",
            "Iteration 81, loss = 0.22774027\n",
            "Iteration 82, loss = 0.21248433\n",
            "Iteration 83, loss = 0.20599489\n",
            "Iteration 84, loss = 0.22011303\n",
            "Iteration 85, loss = 0.20112653\n",
            "Iteration 86, loss = 0.20181167\n",
            "Iteration 87, loss = 0.21124606\n",
            "Iteration 88, loss = 0.21627796\n",
            "Iteration 89, loss = 0.19328724\n",
            "Iteration 90, loss = 0.18452785\n",
            "Iteration 91, loss = 0.19944271\n",
            "Iteration 92, loss = 0.18451477\n",
            "Iteration 93, loss = 0.19023266\n",
            "Iteration 94, loss = 0.18098464\n",
            "Iteration 95, loss = 0.19491490\n",
            "Iteration 96, loss = 0.23113991\n",
            "Iteration 97, loss = 0.21301617\n",
            "Iteration 98, loss = 0.17623774\n",
            "Iteration 99, loss = 0.15956747\n",
            "Iteration 100, loss = 0.15005425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35759984\n",
            "Iteration 2, loss = 1.28442600\n",
            "Iteration 3, loss = 1.20358798\n",
            "Iteration 4, loss = 1.11072058\n",
            "Iteration 5, loss = 1.00854750\n",
            "Iteration 6, loss = 0.92386198\n",
            "Iteration 7, loss = 0.84911237\n",
            "Iteration 8, loss = 0.79305290\n",
            "Iteration 9, loss = 0.76131280\n",
            "Iteration 10, loss = 0.73814818\n",
            "Iteration 11, loss = 0.72291019\n",
            "Iteration 12, loss = 0.69323636\n",
            "Iteration 13, loss = 0.66934373\n",
            "Iteration 14, loss = 0.63508771\n",
            "Iteration 15, loss = 0.62855128\n",
            "Iteration 16, loss = 0.61897714\n",
            "Iteration 17, loss = 0.60230190\n",
            "Iteration 18, loss = 0.57338419\n",
            "Iteration 19, loss = 0.57725873\n",
            "Iteration 20, loss = 0.59221924\n",
            "Iteration 21, loss = 0.60904260\n",
            "Iteration 22, loss = 0.55610813\n",
            "Iteration 23, loss = 0.54771119\n",
            "Iteration 24, loss = 0.53631598\n",
            "Iteration 25, loss = 0.52638912\n",
            "Iteration 26, loss = 0.52999800\n",
            "Iteration 27, loss = 0.52813697\n",
            "Iteration 28, loss = 0.52808398\n",
            "Iteration 29, loss = 0.52310335\n",
            "Iteration 30, loss = 0.48753254\n",
            "Iteration 31, loss = 0.49691652\n",
            "Iteration 32, loss = 0.47537917\n",
            "Iteration 33, loss = 0.51684322\n",
            "Iteration 34, loss = 0.50801757\n",
            "Iteration 35, loss = 0.49536367\n",
            "Iteration 36, loss = 0.47800491\n",
            "Iteration 37, loss = 0.47374167\n",
            "Iteration 38, loss = 0.45144069\n",
            "Iteration 39, loss = 0.45749486\n",
            "Iteration 40, loss = 0.44040557\n",
            "Iteration 41, loss = 0.43066672\n",
            "Iteration 42, loss = 0.42086142\n",
            "Iteration 43, loss = 0.43161844\n",
            "Iteration 44, loss = 0.42766181\n",
            "Iteration 45, loss = 0.41628237\n",
            "Iteration 46, loss = 0.40930357\n",
            "Iteration 47, loss = 0.43397594\n",
            "Iteration 48, loss = 0.41918778\n",
            "Iteration 49, loss = 0.44966164\n",
            "Iteration 50, loss = 0.48879493\n",
            "Iteration 51, loss = 0.45699127\n",
            "Iteration 52, loss = 0.43971850\n",
            "Iteration 53, loss = 0.43103513\n",
            "Iteration 54, loss = 0.38841031\n",
            "Iteration 55, loss = 0.36339828\n",
            "Iteration 56, loss = 0.35966976\n",
            "Iteration 57, loss = 0.37962924\n",
            "Iteration 58, loss = 0.36410197\n",
            "Iteration 59, loss = 0.36790661\n",
            "Iteration 60, loss = 0.36954128\n",
            "Iteration 61, loss = 0.37065057\n",
            "Iteration 62, loss = 0.36251526\n",
            "Iteration 63, loss = 0.37045987\n",
            "Iteration 64, loss = 0.34541372\n",
            "Iteration 65, loss = 0.33436662\n",
            "Iteration 66, loss = 0.33927626\n",
            "Iteration 67, loss = 0.33525635\n",
            "Iteration 68, loss = 0.32215756\n",
            "Iteration 69, loss = 0.30614192\n",
            "Iteration 70, loss = 0.30729786\n",
            "Iteration 71, loss = 0.33614713\n",
            "Iteration 72, loss = 0.32038572\n",
            "Iteration 73, loss = 0.29649744\n",
            "Iteration 74, loss = 0.31205728\n",
            "Iteration 75, loss = 0.28120482\n",
            "Iteration 76, loss = 0.28963389\n",
            "Iteration 77, loss = 0.30461592\n",
            "Iteration 78, loss = 0.29593762\n",
            "Iteration 79, loss = 0.30981558\n",
            "Iteration 80, loss = 0.26730483\n",
            "Iteration 81, loss = 0.27249564\n",
            "Iteration 82, loss = 0.28114407\n",
            "Iteration 83, loss = 0.26864906\n",
            "Iteration 84, loss = 0.26218147\n",
            "Iteration 85, loss = 0.24137080\n",
            "Iteration 86, loss = 0.23308914\n",
            "Iteration 87, loss = 0.24064144\n",
            "Iteration 88, loss = 0.22889570\n",
            "Iteration 89, loss = 0.23632515\n",
            "Iteration 90, loss = 0.22388402\n",
            "Iteration 91, loss = 0.23690282\n",
            "Iteration 92, loss = 0.22754330\n",
            "Iteration 93, loss = 0.22193506\n",
            "Iteration 94, loss = 0.22830455\n",
            "Iteration 95, loss = 0.22781842\n",
            "Iteration 96, loss = 0.24074033\n",
            "Iteration 97, loss = 0.25241995\n",
            "Iteration 98, loss = 0.22479441\n",
            "Iteration 99, loss = 0.22257355\n",
            "Iteration 100, loss = 0.22804816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36200652\n",
            "Iteration 2, loss = 1.28707133\n",
            "Iteration 3, loss = 1.19354236\n",
            "Iteration 4, loss = 1.10766933\n",
            "Iteration 5, loss = 0.99449855\n",
            "Iteration 6, loss = 0.90957169\n",
            "Iteration 7, loss = 0.83693320\n",
            "Iteration 8, loss = 0.80029579\n",
            "Iteration 9, loss = 0.75518732\n",
            "Iteration 10, loss = 0.73142515\n",
            "Iteration 11, loss = 0.70470032\n",
            "Iteration 12, loss = 0.67463547\n",
            "Iteration 13, loss = 0.65259077\n",
            "Iteration 14, loss = 0.62688199\n",
            "Iteration 15, loss = 0.61381151\n",
            "Iteration 16, loss = 0.62254687\n",
            "Iteration 17, loss = 0.61125200\n",
            "Iteration 18, loss = 0.58104030\n",
            "Iteration 19, loss = 0.56882114\n",
            "Iteration 20, loss = 0.59943695\n",
            "Iteration 21, loss = 0.59361332\n",
            "Iteration 22, loss = 0.54697249\n",
            "Iteration 23, loss = 0.52721449\n",
            "Iteration 24, loss = 0.52485686\n",
            "Iteration 25, loss = 0.51916562\n",
            "Iteration 26, loss = 0.51524555\n",
            "Iteration 27, loss = 0.51665084\n",
            "Iteration 28, loss = 0.51365766\n",
            "Iteration 29, loss = 0.51280064\n",
            "Iteration 30, loss = 0.49333938\n",
            "Iteration 31, loss = 0.48422336\n",
            "Iteration 32, loss = 0.45375773\n",
            "Iteration 33, loss = 0.49148888\n",
            "Iteration 34, loss = 0.50474274\n",
            "Iteration 35, loss = 0.45576661\n",
            "Iteration 36, loss = 0.43716175\n",
            "Iteration 37, loss = 0.42707524\n",
            "Iteration 38, loss = 0.43487475\n",
            "Iteration 39, loss = 0.43793557\n",
            "Iteration 40, loss = 0.43622832\n",
            "Iteration 41, loss = 0.42708768\n",
            "Iteration 42, loss = 0.41144775\n",
            "Iteration 43, loss = 0.40799820\n",
            "Iteration 44, loss = 0.39899991\n",
            "Iteration 45, loss = 0.40091515\n",
            "Iteration 46, loss = 0.40237317\n",
            "Iteration 47, loss = 0.38781524\n",
            "Iteration 48, loss = 0.38254664\n",
            "Iteration 49, loss = 0.41999955\n",
            "Iteration 50, loss = 0.40513291\n",
            "Iteration 51, loss = 0.40342017\n",
            "Iteration 52, loss = 0.36967525\n",
            "Iteration 53, loss = 0.36950115\n",
            "Iteration 54, loss = 0.32904114\n",
            "Iteration 55, loss = 0.32642385\n",
            "Iteration 56, loss = 0.31929005\n",
            "Iteration 57, loss = 0.32557333\n",
            "Iteration 58, loss = 0.30742329\n",
            "Iteration 59, loss = 0.30122354\n",
            "Iteration 60, loss = 0.30618598\n",
            "Iteration 61, loss = 0.32500934\n",
            "Iteration 62, loss = 0.34445250\n",
            "Iteration 63, loss = 0.29480280\n",
            "Iteration 64, loss = 0.28208098\n",
            "Iteration 65, loss = 0.26940738\n",
            "Iteration 66, loss = 0.25522375\n",
            "Iteration 67, loss = 0.26396958\n",
            "Iteration 68, loss = 0.25683335\n",
            "Iteration 69, loss = 0.25902312\n",
            "Iteration 70, loss = 0.24179493\n",
            "Iteration 71, loss = 0.25167805\n",
            "Iteration 72, loss = 0.23376932\n",
            "Iteration 73, loss = 0.22280712\n",
            "Iteration 74, loss = 0.22334108\n",
            "Iteration 75, loss = 0.20661893\n",
            "Iteration 76, loss = 0.20878945\n",
            "Iteration 77, loss = 0.24229454\n",
            "Iteration 78, loss = 0.23870895\n",
            "Iteration 79, loss = 0.25236375\n",
            "Iteration 80, loss = 0.23055018\n",
            "Iteration 81, loss = 0.19029868\n",
            "Iteration 82, loss = 0.24439199\n",
            "Iteration 83, loss = 0.21815693\n",
            "Iteration 84, loss = 0.24776590\n",
            "Iteration 85, loss = 0.24229179\n",
            "Iteration 86, loss = 0.19179477\n",
            "Iteration 87, loss = 0.17914130\n",
            "Iteration 88, loss = 0.15862013\n",
            "Iteration 89, loss = 0.16084264\n",
            "Iteration 90, loss = 0.15370330\n",
            "Iteration 91, loss = 0.15003505\n",
            "Iteration 92, loss = 0.15835412\n",
            "Iteration 93, loss = 0.13729981\n",
            "Iteration 94, loss = 0.14730395\n",
            "Iteration 95, loss = 0.15598909\n",
            "Iteration 96, loss = 0.15501581\n",
            "Iteration 97, loss = 0.14700812\n",
            "Iteration 98, loss = 0.13612843\n",
            "Iteration 99, loss = 0.12011579\n",
            "Iteration 100, loss = 0.12116747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36221049\n",
            "Iteration 2, loss = 1.27741423\n",
            "Iteration 3, loss = 1.17593793\n",
            "Iteration 4, loss = 1.10015572\n",
            "Iteration 5, loss = 0.98150918\n",
            "Iteration 6, loss = 0.89527161\n",
            "Iteration 7, loss = 0.83465625\n",
            "Iteration 8, loss = 0.79704919\n",
            "Iteration 9, loss = 0.76027055\n",
            "Iteration 10, loss = 0.73127751\n",
            "Iteration 11, loss = 0.72015686\n",
            "Iteration 12, loss = 0.69299909\n",
            "Iteration 13, loss = 0.66022748\n",
            "Iteration 14, loss = 0.63436974\n",
            "Iteration 15, loss = 0.61452640\n",
            "Iteration 16, loss = 0.61364228\n",
            "Iteration 17, loss = 0.60266849\n",
            "Iteration 18, loss = 0.59006700\n",
            "Iteration 19, loss = 0.57010186\n",
            "Iteration 20, loss = 0.60728155\n",
            "Iteration 21, loss = 0.64823807\n",
            "Iteration 22, loss = 0.58779864\n",
            "Iteration 23, loss = 0.56095932\n",
            "Iteration 24, loss = 0.54488122\n",
            "Iteration 25, loss = 0.53767838\n",
            "Iteration 26, loss = 0.53108499\n",
            "Iteration 27, loss = 0.52297337\n",
            "Iteration 28, loss = 0.53410779\n",
            "Iteration 29, loss = 0.53770803\n",
            "Iteration 30, loss = 0.50379468\n",
            "Iteration 31, loss = 0.49997550\n",
            "Iteration 32, loss = 0.48050977\n",
            "Iteration 33, loss = 0.52896906\n",
            "Iteration 34, loss = 0.49811994\n",
            "Iteration 35, loss = 0.47157728\n",
            "Iteration 36, loss = 0.45257855\n",
            "Iteration 37, loss = 0.44822005\n",
            "Iteration 38, loss = 0.46507852\n",
            "Iteration 39, loss = 0.48716554\n",
            "Iteration 40, loss = 0.47286949\n",
            "Iteration 41, loss = 0.44504818\n",
            "Iteration 42, loss = 0.41695072\n",
            "Iteration 43, loss = 0.42502784\n",
            "Iteration 44, loss = 0.42855888\n",
            "Iteration 45, loss = 0.43547745\n",
            "Iteration 46, loss = 0.45701607\n",
            "Iteration 47, loss = 0.43206980\n",
            "Iteration 48, loss = 0.41063947\n",
            "Iteration 49, loss = 0.41553520\n",
            "Iteration 50, loss = 0.38907132\n",
            "Iteration 51, loss = 0.38125496\n",
            "Iteration 52, loss = 0.38366425\n",
            "Iteration 53, loss = 0.41743387\n",
            "Iteration 54, loss = 0.37244283\n",
            "Iteration 55, loss = 0.35833374\n",
            "Iteration 56, loss = 0.35329481\n",
            "Iteration 57, loss = 0.35670337\n",
            "Iteration 58, loss = 0.34423172\n",
            "Iteration 59, loss = 0.32880842\n",
            "Iteration 60, loss = 0.32200182\n",
            "Iteration 61, loss = 0.32716382\n",
            "Iteration 62, loss = 0.31918048\n",
            "Iteration 63, loss = 0.30274640\n",
            "Iteration 64, loss = 0.30227754\n",
            "Iteration 65, loss = 0.30099580\n",
            "Iteration 66, loss = 0.29673450\n",
            "Iteration 67, loss = 0.30921106\n",
            "Iteration 68, loss = 0.31464444\n",
            "Iteration 69, loss = 0.30207244\n",
            "Iteration 70, loss = 0.28072400\n",
            "Iteration 71, loss = 0.26474448\n",
            "Iteration 72, loss = 0.25516259\n",
            "Iteration 73, loss = 0.24863235\n",
            "Iteration 74, loss = 0.25387517\n",
            "Iteration 75, loss = 0.25012525\n",
            "Iteration 76, loss = 0.25754688\n",
            "Iteration 77, loss = 0.24454020\n",
            "Iteration 78, loss = 0.23483888\n",
            "Iteration 79, loss = 0.25132012\n",
            "Iteration 80, loss = 0.24025316\n",
            "Iteration 81, loss = 0.21580158\n",
            "Iteration 82, loss = 0.20872123\n",
            "Iteration 83, loss = 0.22776445\n",
            "Iteration 84, loss = 0.21847398\n",
            "Iteration 85, loss = 0.19231606\n",
            "Iteration 86, loss = 0.20216387\n",
            "Iteration 87, loss = 0.19786382\n",
            "Iteration 88, loss = 0.17904383\n",
            "Iteration 89, loss = 0.17328939\n",
            "Iteration 90, loss = 0.18429112\n",
            "Iteration 91, loss = 0.24806356\n",
            "Iteration 92, loss = 0.18718702\n",
            "Iteration 93, loss = 0.17301355\n",
            "Iteration 94, loss = 0.15960922\n",
            "Iteration 95, loss = 0.16521037\n",
            "Iteration 96, loss = 0.16419044\n",
            "Iteration 97, loss = 0.18623001\n",
            "Iteration 98, loss = 0.20326889\n",
            "Iteration 99, loss = 0.20010339\n",
            "Iteration 100, loss = 0.13483881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36726357\n",
            "Iteration 2, loss = 1.28967461\n",
            "Iteration 3, loss = 1.17135540\n",
            "Iteration 4, loss = 1.08713187\n",
            "Iteration 5, loss = 0.97012682\n",
            "Iteration 6, loss = 0.89715401\n",
            "Iteration 7, loss = 0.83365574\n",
            "Iteration 8, loss = 0.78956465\n",
            "Iteration 9, loss = 0.74863441\n",
            "Iteration 10, loss = 0.73142318\n",
            "Iteration 11, loss = 0.71240390\n",
            "Iteration 12, loss = 0.67780258\n",
            "Iteration 13, loss = 0.66279638\n",
            "Iteration 14, loss = 0.65898973\n",
            "Iteration 15, loss = 0.63758618\n",
            "Iteration 16, loss = 0.60678057\n",
            "Iteration 17, loss = 0.59614947\n",
            "Iteration 18, loss = 0.58208076\n",
            "Iteration 19, loss = 0.60065069\n",
            "Iteration 20, loss = 0.61077094\n",
            "Iteration 21, loss = 0.62844527\n",
            "Iteration 22, loss = 0.57480430\n",
            "Iteration 23, loss = 0.54134024\n",
            "Iteration 24, loss = 0.54292568\n",
            "Iteration 25, loss = 0.53387502\n",
            "Iteration 26, loss = 0.52733481\n",
            "Iteration 27, loss = 0.54251348\n",
            "Iteration 28, loss = 0.55190294\n",
            "Iteration 29, loss = 0.55305010\n",
            "Iteration 30, loss = 0.52677120\n",
            "Iteration 31, loss = 0.51534066\n",
            "Iteration 32, loss = 0.48794842\n",
            "Iteration 33, loss = 0.50600788\n",
            "Iteration 34, loss = 0.50181689\n",
            "Iteration 35, loss = 0.49716915\n",
            "Iteration 36, loss = 0.47113475\n",
            "Iteration 37, loss = 0.46728699\n",
            "Iteration 38, loss = 0.46822067\n",
            "Iteration 39, loss = 0.47961447\n",
            "Iteration 40, loss = 0.48150621\n",
            "Iteration 41, loss = 0.47662803\n",
            "Iteration 42, loss = 0.46182010\n",
            "Iteration 43, loss = 0.45961493\n",
            "Iteration 44, loss = 0.45230182\n",
            "Iteration 45, loss = 0.45460016\n",
            "Iteration 46, loss = 0.45688515\n",
            "Iteration 47, loss = 0.46385648\n",
            "Iteration 48, loss = 0.45206138\n",
            "Iteration 49, loss = 0.43535377\n",
            "Iteration 50, loss = 0.41434363\n",
            "Iteration 51, loss = 0.42200713\n",
            "Iteration 52, loss = 0.41724233\n",
            "Iteration 53, loss = 0.45542357\n",
            "Iteration 54, loss = 0.41332211\n",
            "Iteration 55, loss = 0.39082379\n",
            "Iteration 56, loss = 0.38565804\n",
            "Iteration 57, loss = 0.37700710\n",
            "Iteration 58, loss = 0.36707537\n",
            "Iteration 59, loss = 0.35246450\n",
            "Iteration 60, loss = 0.36129139\n",
            "Iteration 61, loss = 0.36827205\n",
            "Iteration 62, loss = 0.36422070\n",
            "Iteration 63, loss = 0.35055442\n",
            "Iteration 64, loss = 0.35889740\n",
            "Iteration 65, loss = 0.35547231\n",
            "Iteration 66, loss = 0.33122889\n",
            "Iteration 67, loss = 0.33919584\n",
            "Iteration 68, loss = 0.33228225\n",
            "Iteration 69, loss = 0.33191073\n",
            "Iteration 70, loss = 0.32774656\n",
            "Iteration 71, loss = 0.31696075\n",
            "Iteration 72, loss = 0.30124798\n",
            "Iteration 73, loss = 0.29680091\n",
            "Iteration 74, loss = 0.31789281\n",
            "Iteration 75, loss = 0.29267760\n",
            "Iteration 76, loss = 0.29475484\n",
            "Iteration 77, loss = 0.32548438\n",
            "Iteration 78, loss = 0.29592628\n",
            "Iteration 79, loss = 0.28343103\n",
            "Iteration 80, loss = 0.28884946\n",
            "Iteration 81, loss = 0.27951604\n",
            "Iteration 82, loss = 0.27476503\n",
            "Iteration 83, loss = 0.25056580\n",
            "Iteration 84, loss = 0.25070886\n",
            "Iteration 85, loss = 0.26334064\n",
            "Iteration 86, loss = 0.25409919\n",
            "Iteration 87, loss = 0.23879398\n",
            "Iteration 88, loss = 0.22788237\n",
            "Iteration 89, loss = 0.24325048\n",
            "Iteration 90, loss = 0.24934284\n",
            "Iteration 91, loss = 0.27363251\n",
            "Iteration 92, loss = 0.26262827\n",
            "Iteration 93, loss = 0.22729515\n",
            "Iteration 94, loss = 0.23433117\n",
            "Iteration 95, loss = 0.21250074\n",
            "Iteration 96, loss = 0.19866189\n",
            "Iteration 97, loss = 0.21109918\n",
            "Iteration 98, loss = 0.19856032\n",
            "Iteration 99, loss = 0.18993694\n",
            "Iteration 100, loss = 0.18393286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38023500\n",
            "Iteration 2, loss = 1.31571807\n",
            "Iteration 3, loss = 1.22910253\n",
            "Iteration 4, loss = 1.15148760\n",
            "Iteration 5, loss = 1.04201343\n",
            "Iteration 6, loss = 0.96313193\n",
            "Iteration 7, loss = 0.89192956\n",
            "Iteration 8, loss = 0.83951933\n",
            "Iteration 9, loss = 0.79566111\n",
            "Iteration 10, loss = 0.75604735\n",
            "Iteration 11, loss = 0.73728838\n",
            "Iteration 12, loss = 0.73381498\n",
            "Iteration 13, loss = 0.69327406\n",
            "Iteration 14, loss = 0.69896371\n",
            "Iteration 15, loss = 0.66796341\n",
            "Iteration 16, loss = 0.65294565\n",
            "Iteration 17, loss = 0.64172856\n",
            "Iteration 18, loss = 0.62102990\n",
            "Iteration 19, loss = 0.62064167\n",
            "Iteration 20, loss = 0.61852637\n",
            "Iteration 21, loss = 0.64482274\n",
            "Iteration 22, loss = 0.58467861\n",
            "Iteration 23, loss = 0.59106133\n",
            "Iteration 24, loss = 0.60494924\n",
            "Iteration 25, loss = 0.57352671\n",
            "Iteration 26, loss = 0.55379667\n",
            "Iteration 27, loss = 0.55722553\n",
            "Iteration 28, loss = 0.56926657\n",
            "Iteration 29, loss = 0.56992003\n",
            "Iteration 30, loss = 0.53497939\n",
            "Iteration 31, loss = 0.52179173\n",
            "Iteration 32, loss = 0.51699547\n",
            "Iteration 33, loss = 0.54907568\n",
            "Iteration 34, loss = 0.52003123\n",
            "Iteration 35, loss = 0.52808298\n",
            "Iteration 36, loss = 0.50959277\n",
            "Iteration 37, loss = 0.49003169\n",
            "Iteration 38, loss = 0.50376427\n",
            "Iteration 39, loss = 0.48455924\n",
            "Iteration 40, loss = 0.48407893\n",
            "Iteration 41, loss = 0.47505818\n",
            "Iteration 42, loss = 0.47525383\n",
            "Iteration 43, loss = 0.50611500\n",
            "Iteration 44, loss = 0.50124425\n",
            "Iteration 45, loss = 0.50088473\n",
            "Iteration 46, loss = 0.46966293\n",
            "Iteration 47, loss = 0.46577447\n",
            "Iteration 48, loss = 0.45398757\n",
            "Iteration 49, loss = 0.47715323\n",
            "Iteration 50, loss = 0.46582542\n",
            "Iteration 51, loss = 0.46703921\n",
            "Iteration 52, loss = 0.43839708\n",
            "Iteration 53, loss = 0.46016718\n",
            "Iteration 54, loss = 0.42810617\n",
            "Iteration 55, loss = 0.43528467\n",
            "Iteration 56, loss = 0.40975195\n",
            "Iteration 57, loss = 0.42269593\n",
            "Iteration 58, loss = 0.39707889\n",
            "Iteration 59, loss = 0.38100031\n",
            "Iteration 60, loss = 0.38437404\n",
            "Iteration 61, loss = 0.38791924\n",
            "Iteration 62, loss = 0.38644738\n",
            "Iteration 63, loss = 0.37948482\n",
            "Iteration 64, loss = 0.39190655\n",
            "Iteration 65, loss = 0.40241189\n",
            "Iteration 66, loss = 0.38826184\n",
            "Iteration 67, loss = 0.38668100\n",
            "Iteration 68, loss = 0.37138944\n",
            "Iteration 69, loss = 0.36719586\n",
            "Iteration 70, loss = 0.33965617\n",
            "Iteration 71, loss = 0.32735268\n",
            "Iteration 72, loss = 0.32313585\n",
            "Iteration 73, loss = 0.32830589\n",
            "Iteration 74, loss = 0.33888399\n",
            "Iteration 75, loss = 0.33439726\n",
            "Iteration 76, loss = 0.32670316\n",
            "Iteration 77, loss = 0.36296473\n",
            "Iteration 78, loss = 0.33424823\n",
            "Iteration 79, loss = 0.30509537\n",
            "Iteration 80, loss = 0.30068077\n",
            "Iteration 81, loss = 0.28371335\n",
            "Iteration 82, loss = 0.29059740\n",
            "Iteration 83, loss = 0.29467935\n",
            "Iteration 84, loss = 0.32089837\n",
            "Iteration 85, loss = 0.37915893\n",
            "Iteration 86, loss = 0.36615678\n",
            "Iteration 87, loss = 0.29492689\n",
            "Iteration 88, loss = 0.27391661\n",
            "Iteration 89, loss = 0.33859035\n",
            "Iteration 90, loss = 0.28799846\n",
            "Iteration 91, loss = 0.27869445\n",
            "Iteration 92, loss = 0.28817626\n",
            "Iteration 93, loss = 0.25350309\n",
            "Iteration 94, loss = 0.24739451\n",
            "Iteration 95, loss = 0.24708672\n",
            "Iteration 96, loss = 0.23093612\n",
            "Iteration 97, loss = 0.25045976\n",
            "Iteration 98, loss = 0.24354180\n",
            "Iteration 99, loss = 0.29896128\n",
            "Iteration 100, loss = 0.27998317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37066711\n",
            "Iteration 2, loss = 1.30758240\n",
            "Iteration 3, loss = 1.23369475\n",
            "Iteration 4, loss = 1.18863481\n",
            "Iteration 5, loss = 1.09601918\n",
            "Iteration 6, loss = 1.02949575\n",
            "Iteration 7, loss = 0.96282947\n",
            "Iteration 8, loss = 0.90579722\n",
            "Iteration 9, loss = 0.85151446\n",
            "Iteration 10, loss = 0.80634560\n",
            "Iteration 11, loss = 0.78557364\n",
            "Iteration 12, loss = 0.78537517\n",
            "Iteration 13, loss = 0.73974045\n",
            "Iteration 14, loss = 0.74486464\n",
            "Iteration 15, loss = 0.70377403\n",
            "Iteration 16, loss = 0.68805145\n",
            "Iteration 17, loss = 0.68237989\n",
            "Iteration 18, loss = 0.67129180\n",
            "Iteration 19, loss = 0.67032183\n",
            "Iteration 20, loss = 0.65865650\n",
            "Iteration 21, loss = 0.68440928\n",
            "Iteration 22, loss = 0.63152975\n",
            "Iteration 23, loss = 0.63360655\n",
            "Iteration 24, loss = 0.63098767\n",
            "Iteration 25, loss = 0.60373281\n",
            "Iteration 26, loss = 0.59328777\n",
            "Iteration 27, loss = 0.59550165\n",
            "Iteration 28, loss = 0.61848207\n",
            "Iteration 29, loss = 0.61350383\n",
            "Iteration 30, loss = 0.56982879\n",
            "Iteration 31, loss = 0.56021017\n",
            "Iteration 32, loss = 0.56600441\n",
            "Iteration 33, loss = 0.61394794\n",
            "Iteration 34, loss = 0.57080400\n",
            "Iteration 35, loss = 0.54784649\n",
            "Iteration 36, loss = 0.53974633\n",
            "Iteration 37, loss = 0.51981492\n",
            "Iteration 38, loss = 0.52004639\n",
            "Iteration 39, loss = 0.51325989\n",
            "Iteration 40, loss = 0.52315716\n",
            "Iteration 41, loss = 0.49759896\n",
            "Iteration 42, loss = 0.48731123\n",
            "Iteration 43, loss = 0.50132958\n",
            "Iteration 44, loss = 0.49669970\n",
            "Iteration 45, loss = 0.49896526\n",
            "Iteration 46, loss = 0.50233732\n",
            "Iteration 47, loss = 0.51403246\n",
            "Iteration 48, loss = 0.50282852\n",
            "Iteration 49, loss = 0.51458663\n",
            "Iteration 50, loss = 0.51400466\n",
            "Iteration 51, loss = 0.48287962\n",
            "Iteration 52, loss = 0.46639522\n",
            "Iteration 53, loss = 0.51621037\n",
            "Iteration 54, loss = 0.44299948\n",
            "Iteration 55, loss = 0.43266392\n",
            "Iteration 56, loss = 0.40955450\n",
            "Iteration 57, loss = 0.41067231\n",
            "Iteration 58, loss = 0.39619776\n",
            "Iteration 59, loss = 0.40022319\n",
            "Iteration 60, loss = 0.41053783\n",
            "Iteration 61, loss = 0.39190321\n",
            "Iteration 62, loss = 0.38840220\n",
            "Iteration 63, loss = 0.37561669\n",
            "Iteration 64, loss = 0.37809453\n",
            "Iteration 65, loss = 0.39583532\n",
            "Iteration 66, loss = 0.36046211\n",
            "Iteration 67, loss = 0.35999991\n",
            "Iteration 68, loss = 0.34303130\n",
            "Iteration 69, loss = 0.34274807\n",
            "Iteration 70, loss = 0.31888819\n",
            "Iteration 71, loss = 0.31530952\n",
            "Iteration 72, loss = 0.31508366\n",
            "Iteration 73, loss = 0.33428859\n",
            "Iteration 74, loss = 0.33348951\n",
            "Iteration 75, loss = 0.30879682\n",
            "Iteration 76, loss = 0.31793542\n",
            "Iteration 77, loss = 0.36289398\n",
            "Iteration 78, loss = 0.33851390\n",
            "Iteration 79, loss = 0.34240020\n",
            "Iteration 80, loss = 0.32970984\n",
            "Iteration 81, loss = 0.30214086\n",
            "Iteration 82, loss = 0.32492789\n",
            "Iteration 83, loss = 0.28261182\n",
            "Iteration 84, loss = 0.29142733\n",
            "Iteration 85, loss = 0.28089866\n",
            "Iteration 86, loss = 0.27350948\n",
            "Iteration 87, loss = 0.24202718\n",
            "Iteration 88, loss = 0.24170339\n",
            "Iteration 89, loss = 0.26231179\n",
            "Iteration 90, loss = 0.25390703\n",
            "Iteration 91, loss = 0.24987398\n",
            "Iteration 92, loss = 0.26752238\n",
            "Iteration 93, loss = 0.24389430\n",
            "Iteration 94, loss = 0.23424224\n",
            "Iteration 95, loss = 0.22592543\n",
            "Iteration 96, loss = 0.20911954\n",
            "Iteration 97, loss = 0.22758888\n",
            "Iteration 98, loss = 0.22886442\n",
            "Iteration 99, loss = 0.21818229\n",
            "Iteration 100, loss = 0.19177196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37246618\n",
            "Iteration 2, loss = 1.30622635\n",
            "Iteration 3, loss = 1.22752373\n",
            "Iteration 4, loss = 1.13875279\n",
            "Iteration 5, loss = 1.01914981\n",
            "Iteration 6, loss = 0.92591776\n",
            "Iteration 7, loss = 0.85608525\n",
            "Iteration 8, loss = 0.80990388\n",
            "Iteration 9, loss = 0.77639388\n",
            "Iteration 10, loss = 0.73854962\n",
            "Iteration 11, loss = 0.72074130\n",
            "Iteration 12, loss = 0.70662297\n",
            "Iteration 13, loss = 0.69395142\n",
            "Iteration 14, loss = 0.66683821\n",
            "Iteration 15, loss = 0.65722374\n",
            "Iteration 16, loss = 0.64669553\n",
            "Iteration 17, loss = 0.62003999\n",
            "Iteration 18, loss = 0.60450102\n",
            "Iteration 19, loss = 0.60889869\n",
            "Iteration 20, loss = 0.61650872\n",
            "Iteration 21, loss = 0.65406788\n",
            "Iteration 22, loss = 0.59046563\n",
            "Iteration 23, loss = 0.58061487\n",
            "Iteration 24, loss = 0.57734652\n",
            "Iteration 25, loss = 0.55965771\n",
            "Iteration 26, loss = 0.54932170\n",
            "Iteration 27, loss = 0.54244199\n",
            "Iteration 28, loss = 0.54505036\n",
            "Iteration 29, loss = 0.54865076\n",
            "Iteration 30, loss = 0.52864057\n",
            "Iteration 31, loss = 0.51288619\n",
            "Iteration 32, loss = 0.50366616\n",
            "Iteration 33, loss = 0.50147594\n",
            "Iteration 34, loss = 0.51074030\n",
            "Iteration 35, loss = 0.52156509\n",
            "Iteration 36, loss = 0.49869387\n",
            "Iteration 37, loss = 0.48973023\n",
            "Iteration 38, loss = 0.50817737\n",
            "Iteration 39, loss = 0.47330966\n",
            "Iteration 40, loss = 0.49378486\n",
            "Iteration 41, loss = 0.45207688\n",
            "Iteration 42, loss = 0.44698775\n",
            "Iteration 43, loss = 0.45992089\n",
            "Iteration 44, loss = 0.46324008\n",
            "Iteration 45, loss = 0.48746197\n",
            "Iteration 46, loss = 0.43721344\n",
            "Iteration 47, loss = 0.43562845\n",
            "Iteration 48, loss = 0.42156215\n",
            "Iteration 49, loss = 0.43514893\n",
            "Iteration 50, loss = 0.42117463\n",
            "Iteration 51, loss = 0.42228448\n",
            "Iteration 52, loss = 0.40618466\n",
            "Iteration 53, loss = 0.41620819\n",
            "Iteration 54, loss = 0.40485057\n",
            "Iteration 55, loss = 0.42839706\n",
            "Iteration 56, loss = 0.38208722\n",
            "Iteration 57, loss = 0.39913400\n",
            "Iteration 58, loss = 0.39201496\n",
            "Iteration 59, loss = 0.37849375\n",
            "Iteration 60, loss = 0.39385480\n",
            "Iteration 61, loss = 0.38145487\n",
            "Iteration 62, loss = 0.36229947\n",
            "Iteration 63, loss = 0.36935599\n",
            "Iteration 64, loss = 0.39244941\n",
            "Iteration 65, loss = 0.37189184\n",
            "Iteration 66, loss = 0.35886182\n",
            "Iteration 67, loss = 0.34536647\n",
            "Iteration 68, loss = 0.33110475\n",
            "Iteration 69, loss = 0.33261131\n",
            "Iteration 70, loss = 0.31465647\n",
            "Iteration 71, loss = 0.29896755\n",
            "Iteration 72, loss = 0.29408478\n",
            "Iteration 73, loss = 0.31578300\n",
            "Iteration 74, loss = 0.33218867\n",
            "Iteration 75, loss = 0.30788931\n",
            "Iteration 76, loss = 0.30537892\n",
            "Iteration 77, loss = 0.31913017\n",
            "Iteration 78, loss = 0.27112143\n",
            "Iteration 79, loss = 0.26863059\n",
            "Iteration 80, loss = 0.25903915\n",
            "Iteration 81, loss = 0.26491441\n",
            "Iteration 82, loss = 0.26834694\n",
            "Iteration 83, loss = 0.24594408\n",
            "Iteration 84, loss = 0.25524231\n",
            "Iteration 85, loss = 0.32646987\n",
            "Iteration 86, loss = 0.28709306\n",
            "Iteration 87, loss = 0.26543524\n",
            "Iteration 88, loss = 0.24237721\n",
            "Iteration 89, loss = 0.26557398\n",
            "Iteration 90, loss = 0.24768609\n",
            "Iteration 91, loss = 0.21145944\n",
            "Iteration 92, loss = 0.21862426\n",
            "Iteration 93, loss = 0.21462295\n",
            "Iteration 94, loss = 0.23083335\n",
            "Iteration 95, loss = 0.19732488\n",
            "Iteration 96, loss = 0.19623787\n",
            "Iteration 97, loss = 0.22310848\n",
            "Iteration 98, loss = 0.22072861\n",
            "Iteration 99, loss = 0.26106158\n",
            "Iteration 100, loss = 0.23483684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35848505\n",
            "Iteration 2, loss = 1.27175017\n",
            "Iteration 3, loss = 1.17027214\n",
            "Iteration 4, loss = 1.07493011\n",
            "Iteration 5, loss = 0.95649656\n",
            "Iteration 6, loss = 0.87292362\n",
            "Iteration 7, loss = 0.81077613\n",
            "Iteration 8, loss = 0.77041993\n",
            "Iteration 9, loss = 0.73829859\n",
            "Iteration 10, loss = 0.72589507\n",
            "Iteration 11, loss = 0.70425283\n",
            "Iteration 12, loss = 0.68262705\n",
            "Iteration 13, loss = 0.65925788\n",
            "Iteration 14, loss = 0.63784218\n",
            "Iteration 15, loss = 0.62571253\n",
            "Iteration 16, loss = 0.60977863\n",
            "Iteration 17, loss = 0.59440965\n",
            "Iteration 18, loss = 0.58425170\n",
            "Iteration 19, loss = 0.59213723\n",
            "Iteration 20, loss = 0.58097919\n",
            "Iteration 21, loss = 0.60102360\n",
            "Iteration 22, loss = 0.56096174\n",
            "Iteration 23, loss = 0.53837894\n",
            "Iteration 24, loss = 0.54592516\n",
            "Iteration 25, loss = 0.52753109\n",
            "Iteration 26, loss = 0.52022837\n",
            "Iteration 27, loss = 0.51854209\n",
            "Iteration 28, loss = 0.52586337\n",
            "Iteration 29, loss = 0.54021482\n",
            "Iteration 30, loss = 0.50708772\n",
            "Iteration 31, loss = 0.49683695\n",
            "Iteration 32, loss = 0.48967458\n",
            "Iteration 33, loss = 0.48050098\n",
            "Iteration 34, loss = 0.47825469\n",
            "Iteration 35, loss = 0.50566993\n",
            "Iteration 36, loss = 0.47984315\n",
            "Iteration 37, loss = 0.46552017\n",
            "Iteration 38, loss = 0.47166376\n",
            "Iteration 39, loss = 0.46955594\n",
            "Iteration 40, loss = 0.45140211\n",
            "Iteration 41, loss = 0.44202727\n",
            "Iteration 42, loss = 0.44701365\n",
            "Iteration 43, loss = 0.45265961\n",
            "Iteration 44, loss = 0.46109974\n",
            "Iteration 45, loss = 0.44063807\n",
            "Iteration 46, loss = 0.43284051\n",
            "Iteration 47, loss = 0.42911761\n",
            "Iteration 48, loss = 0.42961769\n",
            "Iteration 49, loss = 0.43665110\n",
            "Iteration 50, loss = 0.46081404\n",
            "Iteration 51, loss = 0.41907955\n",
            "Iteration 52, loss = 0.42892748\n",
            "Iteration 53, loss = 0.42526778\n",
            "Iteration 54, loss = 0.38438869\n",
            "Iteration 55, loss = 0.40839436\n",
            "Iteration 56, loss = 0.37711059\n",
            "Iteration 57, loss = 0.38862929\n",
            "Iteration 58, loss = 0.37632433\n",
            "Iteration 59, loss = 0.38345773\n",
            "Iteration 60, loss = 0.40219096\n",
            "Iteration 61, loss = 0.38932634\n",
            "Iteration 62, loss = 0.36102711\n",
            "Iteration 63, loss = 0.36064373\n",
            "Iteration 64, loss = 0.39545352\n",
            "Iteration 65, loss = 0.36391995\n",
            "Iteration 66, loss = 0.33675010\n",
            "Iteration 67, loss = 0.33289819\n",
            "Iteration 68, loss = 0.33891893\n",
            "Iteration 69, loss = 0.32440349\n",
            "Iteration 70, loss = 0.31388024\n",
            "Iteration 71, loss = 0.31473931\n",
            "Iteration 72, loss = 0.31078301\n",
            "Iteration 73, loss = 0.33416939\n",
            "Iteration 74, loss = 0.32481006\n",
            "Iteration 75, loss = 0.29548046\n",
            "Iteration 76, loss = 0.31575558\n",
            "Iteration 77, loss = 0.29864173\n",
            "Iteration 78, loss = 0.30242442\n",
            "Iteration 79, loss = 0.29764979\n",
            "Iteration 80, loss = 0.27829895\n",
            "Iteration 81, loss = 0.29872591\n",
            "Iteration 82, loss = 0.30927815\n",
            "Iteration 83, loss = 0.27628250\n",
            "Iteration 84, loss = 0.30052516\n",
            "Iteration 85, loss = 0.28017697\n",
            "Iteration 86, loss = 0.25799425\n",
            "Iteration 87, loss = 0.24951180\n",
            "Iteration 88, loss = 0.24963724\n",
            "Iteration 89, loss = 0.26572393\n",
            "Iteration 90, loss = 0.24604017\n",
            "Iteration 91, loss = 0.25503433\n",
            "Iteration 92, loss = 0.24353069\n",
            "Iteration 93, loss = 0.22634446\n",
            "Iteration 94, loss = 0.22270467\n",
            "Iteration 95, loss = 0.23320112\n",
            "Iteration 96, loss = 0.21461834\n",
            "Iteration 97, loss = 0.25498187\n",
            "Iteration 98, loss = 0.24228004\n",
            "Iteration 99, loss = 0.24138832\n",
            "Iteration 100, loss = 0.21393197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35911752\n",
            "Iteration 2, loss = 1.27918135\n",
            "Iteration 3, loss = 1.17161283\n",
            "Iteration 4, loss = 1.08285679\n",
            "Iteration 5, loss = 0.96890607\n",
            "Iteration 6, loss = 0.89898217\n",
            "Iteration 7, loss = 0.85528384\n",
            "Iteration 8, loss = 0.81021799\n",
            "Iteration 9, loss = 0.77404460\n",
            "Iteration 10, loss = 0.75781849\n",
            "Iteration 11, loss = 0.73442433\n",
            "Iteration 12, loss = 0.72343820\n",
            "Iteration 13, loss = 0.69015079\n",
            "Iteration 14, loss = 0.67544295\n",
            "Iteration 15, loss = 0.67475759\n",
            "Iteration 16, loss = 0.68042943\n",
            "Iteration 17, loss = 0.65536067\n",
            "Iteration 18, loss = 0.64248373\n",
            "Iteration 19, loss = 0.64660471\n",
            "Iteration 20, loss = 0.60940926\n",
            "Iteration 21, loss = 0.63137184\n",
            "Iteration 22, loss = 0.61567449\n",
            "Iteration 23, loss = 0.59352427\n",
            "Iteration 24, loss = 0.59500342\n",
            "Iteration 25, loss = 0.58047713\n",
            "Iteration 26, loss = 0.57024193\n",
            "Iteration 27, loss = 0.55452614\n",
            "Iteration 28, loss = 0.55131534\n",
            "Iteration 29, loss = 0.54882469\n",
            "Iteration 30, loss = 0.53678315\n",
            "Iteration 31, loss = 0.53836148\n",
            "Iteration 32, loss = 0.55255800\n",
            "Iteration 33, loss = 0.53076348\n",
            "Iteration 34, loss = 0.52926585\n",
            "Iteration 35, loss = 0.52575369\n",
            "Iteration 36, loss = 0.49145726\n",
            "Iteration 37, loss = 0.48917289\n",
            "Iteration 38, loss = 0.48673436\n",
            "Iteration 39, loss = 0.47830594\n",
            "Iteration 40, loss = 0.48323704\n",
            "Iteration 41, loss = 0.46553614\n",
            "Iteration 42, loss = 0.45823326\n",
            "Iteration 43, loss = 0.45406422\n",
            "Iteration 44, loss = 0.48078980\n",
            "Iteration 45, loss = 0.47017518\n",
            "Iteration 46, loss = 0.47981970\n",
            "Iteration 47, loss = 0.45904665\n",
            "Iteration 48, loss = 0.43399526\n",
            "Iteration 49, loss = 0.45219765\n",
            "Iteration 50, loss = 0.45274991\n",
            "Iteration 51, loss = 0.44351564\n",
            "Iteration 52, loss = 0.44481298\n",
            "Iteration 53, loss = 0.42846923\n",
            "Iteration 54, loss = 0.41145355\n",
            "Iteration 55, loss = 0.44742614\n",
            "Iteration 56, loss = 0.40900357\n",
            "Iteration 57, loss = 0.39034701\n",
            "Iteration 58, loss = 0.39176352\n",
            "Iteration 59, loss = 0.37443308\n",
            "Iteration 60, loss = 0.40068613\n",
            "Iteration 61, loss = 0.40250997\n",
            "Iteration 62, loss = 0.37648592\n",
            "Iteration 63, loss = 0.35517565\n",
            "Iteration 64, loss = 0.36645632\n",
            "Iteration 65, loss = 0.35383325\n",
            "Iteration 66, loss = 0.33890460\n",
            "Iteration 67, loss = 0.36296240\n",
            "Iteration 68, loss = 0.40942461\n",
            "Iteration 69, loss = 0.36621493\n",
            "Iteration 70, loss = 0.32445017\n",
            "Iteration 71, loss = 0.31529577\n",
            "Iteration 72, loss = 0.32492042\n",
            "Iteration 73, loss = 0.32293844\n",
            "Iteration 74, loss = 0.32121545\n",
            "Iteration 75, loss = 0.31103815\n",
            "Iteration 76, loss = 0.34356281\n",
            "Iteration 77, loss = 0.31783248\n",
            "Iteration 78, loss = 0.29899989\n",
            "Iteration 79, loss = 0.31989860\n",
            "Iteration 80, loss = 0.29537689\n",
            "Iteration 81, loss = 0.28732318\n",
            "Iteration 82, loss = 0.26541793\n",
            "Iteration 83, loss = 0.26808441\n",
            "Iteration 84, loss = 0.27828769\n",
            "Iteration 85, loss = 0.27986395\n",
            "Iteration 86, loss = 0.25531033\n",
            "Iteration 87, loss = 0.24124047\n",
            "Iteration 88, loss = 0.25115147\n",
            "Iteration 89, loss = 0.27505525\n",
            "Iteration 90, loss = 0.28127175\n",
            "Iteration 91, loss = 0.24812834\n",
            "Iteration 92, loss = 0.23632283\n",
            "Iteration 93, loss = 0.22492526\n",
            "Iteration 94, loss = 0.25681623\n",
            "Iteration 95, loss = 0.25331636\n",
            "Iteration 96, loss = 0.22452897\n",
            "Iteration 97, loss = 0.21231512\n",
            "Iteration 98, loss = 0.21645642\n",
            "Iteration 99, loss = 0.22122910\n",
            "Iteration 100, loss = 0.21781066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "Iteration 1, loss = 1.39900053\n",
            "Iteration 2, loss = 1.36341961\n",
            "Iteration 3, loss = 1.34831775\n",
            "Iteration 4, loss = 1.32467890\n",
            "Iteration 5, loss = 1.30560173\n",
            "Iteration 6, loss = 1.27633072\n",
            "Iteration 7, loss = 1.26549488\n",
            "Iteration 8, loss = 1.25872113\n",
            "Iteration 9, loss = 1.26731226\n",
            "Iteration 10, loss = 1.23646123\n",
            "Iteration 11, loss = 1.22971054\n",
            "Iteration 12, loss = 1.22702746\n",
            "Iteration 13, loss = 1.22056230\n",
            "Iteration 14, loss = 1.22108453\n",
            "Iteration 15, loss = 1.21101226\n",
            "Iteration 16, loss = 1.20330264\n",
            "Iteration 17, loss = 1.19184328\n",
            "Iteration 18, loss = 1.19609977\n",
            "Iteration 19, loss = 1.20580076\n",
            "Iteration 20, loss = 1.20481194\n",
            "Iteration 21, loss = 1.18633933\n",
            "Iteration 22, loss = 1.17157376\n",
            "Iteration 23, loss = 1.17249999\n",
            "Iteration 24, loss = 1.16253199\n",
            "Iteration 25, loss = 1.15125442\n",
            "Iteration 26, loss = 1.14061518\n",
            "Iteration 27, loss = 1.13403481\n",
            "Iteration 28, loss = 1.13957977\n",
            "Iteration 29, loss = 1.12397183\n",
            "Iteration 30, loss = 1.12194494\n",
            "Iteration 31, loss = 1.11169739\n",
            "Iteration 32, loss = 1.09803741\n",
            "Iteration 33, loss = 1.09434085\n",
            "Iteration 34, loss = 1.08611194\n",
            "Iteration 35, loss = 1.08230321\n",
            "Iteration 36, loss = 1.08776147\n",
            "Iteration 37, loss = 1.07285373\n",
            "Iteration 38, loss = 1.06635377\n",
            "Iteration 39, loss = 1.06026618\n",
            "Iteration 40, loss = 1.05738655\n",
            "Iteration 41, loss = 1.04872204\n",
            "Iteration 42, loss = 1.04328450\n",
            "Iteration 43, loss = 1.06909047\n",
            "Iteration 44, loss = 1.09253367\n",
            "Iteration 45, loss = 1.02910287\n",
            "Iteration 46, loss = 1.03585808\n",
            "Iteration 47, loss = 1.00497338\n",
            "Iteration 48, loss = 0.99358245\n",
            "Iteration 49, loss = 0.98195409\n",
            "Iteration 50, loss = 0.97831593\n",
            "Iteration 51, loss = 0.96484438\n",
            "Iteration 52, loss = 0.94917721\n",
            "Iteration 53, loss = 0.94008175\n",
            "Iteration 54, loss = 0.93647594\n",
            "Iteration 55, loss = 0.92348768\n",
            "Iteration 56, loss = 0.91695843\n",
            "Iteration 57, loss = 0.91698501\n",
            "Iteration 58, loss = 0.91026466\n",
            "Iteration 59, loss = 0.89110264\n",
            "Iteration 60, loss = 0.91485315\n",
            "Iteration 61, loss = 0.91867563\n",
            "Iteration 62, loss = 0.90531753\n",
            "Iteration 63, loss = 0.88377943\n",
            "Iteration 64, loss = 0.87166410\n",
            "Iteration 65, loss = 0.85827353\n",
            "Iteration 66, loss = 0.86627830\n",
            "Iteration 67, loss = 0.85907839\n",
            "Iteration 68, loss = 0.86861946\n",
            "Iteration 69, loss = 0.85115329\n",
            "Iteration 70, loss = 0.87426485\n",
            "Iteration 71, loss = 0.85784781\n",
            "Iteration 72, loss = 0.84229504\n",
            "Iteration 73, loss = 0.80705356\n",
            "Iteration 74, loss = 0.78603964\n",
            "Iteration 75, loss = 0.75938051\n",
            "Iteration 76, loss = 0.76327414\n",
            "Iteration 77, loss = 0.76350890\n",
            "Iteration 78, loss = 0.74445938\n",
            "Iteration 79, loss = 0.73549844\n",
            "Iteration 80, loss = 0.73994653\n",
            "Iteration 81, loss = 0.72042150\n",
            "Iteration 82, loss = 0.72148996\n",
            "Iteration 83, loss = 0.70897539\n",
            "Iteration 84, loss = 0.71072607\n",
            "Iteration 85, loss = 0.75120625\n",
            "Iteration 86, loss = 0.75459321\n",
            "Iteration 87, loss = 0.69171014\n",
            "Iteration 88, loss = 0.66768589\n",
            "Iteration 89, loss = 0.66187262\n",
            "Iteration 90, loss = 0.64703087\n",
            "Iteration 91, loss = 0.68434086\n",
            "Iteration 92, loss = 0.64646708\n",
            "Iteration 93, loss = 0.64887993\n",
            "Iteration 94, loss = 0.63315023\n",
            "Iteration 95, loss = 0.64860607\n",
            "Iteration 96, loss = 0.62337282\n",
            "Iteration 97, loss = 0.61240634\n",
            "Iteration 98, loss = 0.61182438\n",
            "Iteration 99, loss = 0.59027524\n",
            "Iteration 100, loss = 0.59747687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39878723\n",
            "Iteration 2, loss = 1.36389445\n",
            "Iteration 3, loss = 1.34405715\n",
            "Iteration 4, loss = 1.32103156\n",
            "Iteration 5, loss = 1.29645366\n",
            "Iteration 6, loss = 1.28035223\n",
            "Iteration 7, loss = 1.28352254\n",
            "Iteration 8, loss = 1.26666757\n",
            "Iteration 9, loss = 1.25984056\n",
            "Iteration 10, loss = 1.24518616\n",
            "Iteration 11, loss = 1.24621422\n",
            "Iteration 12, loss = 1.23272733\n",
            "Iteration 13, loss = 1.23028719\n",
            "Iteration 14, loss = 1.22316239\n",
            "Iteration 15, loss = 1.21515247\n",
            "Iteration 16, loss = 1.22343170\n",
            "Iteration 17, loss = 1.21712805\n",
            "Iteration 18, loss = 1.20874295\n",
            "Iteration 19, loss = 1.21588384\n",
            "Iteration 20, loss = 1.21295000\n",
            "Iteration 21, loss = 1.19851976\n",
            "Iteration 22, loss = 1.18849662\n",
            "Iteration 23, loss = 1.18285273\n",
            "Iteration 24, loss = 1.17394026\n",
            "Iteration 25, loss = 1.17037576\n",
            "Iteration 26, loss = 1.16291758\n",
            "Iteration 27, loss = 1.15774753\n",
            "Iteration 28, loss = 1.16360958\n",
            "Iteration 29, loss = 1.14607916\n",
            "Iteration 30, loss = 1.14743041\n",
            "Iteration 31, loss = 1.13508285\n",
            "Iteration 32, loss = 1.12729399\n",
            "Iteration 33, loss = 1.13425337\n",
            "Iteration 34, loss = 1.11942483\n",
            "Iteration 35, loss = 1.11327439\n",
            "Iteration 36, loss = 1.11022840\n",
            "Iteration 37, loss = 1.09834868\n",
            "Iteration 38, loss = 1.09693093\n",
            "Iteration 39, loss = 1.08369049\n",
            "Iteration 40, loss = 1.08872784\n",
            "Iteration 41, loss = 1.09635964\n",
            "Iteration 42, loss = 1.07282314\n",
            "Iteration 43, loss = 1.06770622\n",
            "Iteration 44, loss = 1.07459649\n",
            "Iteration 45, loss = 1.06578527\n",
            "Iteration 46, loss = 1.07509365\n",
            "Iteration 47, loss = 1.05329210\n",
            "Iteration 48, loss = 1.04062611\n",
            "Iteration 49, loss = 1.01730160\n",
            "Iteration 50, loss = 1.01462023\n",
            "Iteration 51, loss = 1.00078042\n",
            "Iteration 52, loss = 0.99306458\n",
            "Iteration 53, loss = 0.98903445\n",
            "Iteration 54, loss = 0.98484975\n",
            "Iteration 55, loss = 0.97274633\n",
            "Iteration 56, loss = 0.96242828\n",
            "Iteration 57, loss = 0.95707621\n",
            "Iteration 58, loss = 0.94121436\n",
            "Iteration 59, loss = 0.94231841\n",
            "Iteration 60, loss = 0.94566151\n",
            "Iteration 61, loss = 0.94321701\n",
            "Iteration 62, loss = 0.92200909\n",
            "Iteration 63, loss = 0.92020342\n",
            "Iteration 64, loss = 0.92413125\n",
            "Iteration 65, loss = 0.92546768\n",
            "Iteration 66, loss = 0.93071349\n",
            "Iteration 67, loss = 0.91469552\n",
            "Iteration 68, loss = 0.91445330\n",
            "Iteration 69, loss = 0.89343160\n",
            "Iteration 70, loss = 0.93801272\n",
            "Iteration 71, loss = 0.93096086\n",
            "Iteration 72, loss = 0.90604524\n",
            "Iteration 73, loss = 0.86554415\n",
            "Iteration 74, loss = 0.84053035\n",
            "Iteration 75, loss = 0.83422840\n",
            "Iteration 76, loss = 0.82106936\n",
            "Iteration 77, loss = 0.82525234\n",
            "Iteration 78, loss = 0.81072480\n",
            "Iteration 79, loss = 0.79712451\n",
            "Iteration 80, loss = 0.80979647\n",
            "Iteration 81, loss = 0.78802599\n",
            "Iteration 82, loss = 0.77438210\n",
            "Iteration 83, loss = 0.78194638\n",
            "Iteration 84, loss = 0.77210546\n",
            "Iteration 85, loss = 0.77861928\n",
            "Iteration 86, loss = 0.79895604\n",
            "Iteration 87, loss = 0.75967384\n",
            "Iteration 88, loss = 0.74966788\n",
            "Iteration 89, loss = 0.72715769\n",
            "Iteration 90, loss = 0.71984179\n",
            "Iteration 91, loss = 0.70152041\n",
            "Iteration 92, loss = 0.68948505\n",
            "Iteration 93, loss = 0.69190328\n",
            "Iteration 94, loss = 0.71566018\n",
            "Iteration 95, loss = 0.70558462\n",
            "Iteration 96, loss = 0.68814815\n",
            "Iteration 97, loss = 0.65285376\n",
            "Iteration 98, loss = 0.67317903\n",
            "Iteration 99, loss = 0.67357252\n",
            "Iteration 100, loss = 0.66024664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40355004\n",
            "Iteration 2, loss = 1.36652886\n",
            "Iteration 3, loss = 1.35424320\n",
            "Iteration 4, loss = 1.33329366\n",
            "Iteration 5, loss = 1.31689178\n",
            "Iteration 6, loss = 1.29400823\n",
            "Iteration 7, loss = 1.28866092\n",
            "Iteration 8, loss = 1.27590795\n",
            "Iteration 9, loss = 1.26908254\n",
            "Iteration 10, loss = 1.25164702\n",
            "Iteration 11, loss = 1.25151175\n",
            "Iteration 12, loss = 1.23854779\n",
            "Iteration 13, loss = 1.23132484\n",
            "Iteration 14, loss = 1.21969110\n",
            "Iteration 15, loss = 1.21359585\n",
            "Iteration 16, loss = 1.22411040\n",
            "Iteration 17, loss = 1.21144663\n",
            "Iteration 18, loss = 1.20672045\n",
            "Iteration 19, loss = 1.22322276\n",
            "Iteration 20, loss = 1.21784837\n",
            "Iteration 21, loss = 1.18615674\n",
            "Iteration 22, loss = 1.17770446\n",
            "Iteration 23, loss = 1.17474811\n",
            "Iteration 24, loss = 1.16887921\n",
            "Iteration 25, loss = 1.16053267\n",
            "Iteration 26, loss = 1.14769984\n",
            "Iteration 27, loss = 1.14817377\n",
            "Iteration 28, loss = 1.15281366\n",
            "Iteration 29, loss = 1.13248695\n",
            "Iteration 30, loss = 1.13217311\n",
            "Iteration 31, loss = 1.12180407\n",
            "Iteration 32, loss = 1.11126711\n",
            "Iteration 33, loss = 1.11493435\n",
            "Iteration 34, loss = 1.09864606\n",
            "Iteration 35, loss = 1.10202440\n",
            "Iteration 36, loss = 1.08502868\n",
            "Iteration 37, loss = 1.07738290\n",
            "Iteration 38, loss = 1.07750844\n",
            "Iteration 39, loss = 1.05739260\n",
            "Iteration 40, loss = 1.05806964\n",
            "Iteration 41, loss = 1.09148218\n",
            "Iteration 42, loss = 1.08047344\n",
            "Iteration 43, loss = 1.05774126\n",
            "Iteration 44, loss = 1.05637052\n",
            "Iteration 45, loss = 1.03822482\n",
            "Iteration 46, loss = 1.03119700\n",
            "Iteration 47, loss = 1.04080601\n",
            "Iteration 48, loss = 1.00090746\n",
            "Iteration 49, loss = 1.00164152\n",
            "Iteration 50, loss = 1.00492808\n",
            "Iteration 51, loss = 0.98388193\n",
            "Iteration 52, loss = 0.97545823\n",
            "Iteration 53, loss = 0.96258820\n",
            "Iteration 54, loss = 0.95912141\n",
            "Iteration 55, loss = 0.95977766\n",
            "Iteration 56, loss = 0.95325895\n",
            "Iteration 57, loss = 0.95399132\n",
            "Iteration 58, loss = 0.92005755\n",
            "Iteration 59, loss = 0.91673183\n",
            "Iteration 60, loss = 0.91125402\n",
            "Iteration 61, loss = 0.89455473\n",
            "Iteration 62, loss = 0.88326186\n",
            "Iteration 63, loss = 0.88714722\n",
            "Iteration 64, loss = 0.88572657\n",
            "Iteration 65, loss = 0.88126080\n",
            "Iteration 66, loss = 0.90647400\n",
            "Iteration 67, loss = 0.89171005\n",
            "Iteration 68, loss = 0.86663626\n",
            "Iteration 69, loss = 0.84282533\n",
            "Iteration 70, loss = 0.88365574\n",
            "Iteration 71, loss = 0.87390292\n",
            "Iteration 72, loss = 0.84213854\n",
            "Iteration 73, loss = 0.81929886\n",
            "Iteration 74, loss = 0.80463471\n",
            "Iteration 75, loss = 0.78942118\n",
            "Iteration 76, loss = 0.78617485\n",
            "Iteration 77, loss = 0.78936571\n",
            "Iteration 78, loss = 0.77722667\n",
            "Iteration 79, loss = 0.76582759\n",
            "Iteration 80, loss = 0.75601399\n",
            "Iteration 81, loss = 0.74323737\n",
            "Iteration 82, loss = 0.72743702\n",
            "Iteration 83, loss = 0.73574935\n",
            "Iteration 84, loss = 0.72932068\n",
            "Iteration 85, loss = 0.75193428\n",
            "Iteration 86, loss = 0.72977293\n",
            "Iteration 87, loss = 0.69283849\n",
            "Iteration 88, loss = 0.69141002\n",
            "Iteration 89, loss = 0.67189457\n",
            "Iteration 90, loss = 0.66941655\n",
            "Iteration 91, loss = 0.66543065\n",
            "Iteration 92, loss = 0.65132800\n",
            "Iteration 93, loss = 0.64416342\n",
            "Iteration 94, loss = 0.65478976\n",
            "Iteration 95, loss = 0.65931262\n",
            "Iteration 96, loss = 0.65363358\n",
            "Iteration 97, loss = 0.62539756\n",
            "Iteration 98, loss = 0.63340966\n",
            "Iteration 99, loss = 0.63829896\n",
            "Iteration 100, loss = 0.62335952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40243730\n",
            "Iteration 2, loss = 1.37200930\n",
            "Iteration 3, loss = 1.35902324\n",
            "Iteration 4, loss = 1.33815712\n",
            "Iteration 5, loss = 1.32233823\n",
            "Iteration 6, loss = 1.29551529\n",
            "Iteration 7, loss = 1.29613990\n",
            "Iteration 8, loss = 1.27731251\n",
            "Iteration 9, loss = 1.25823820\n",
            "Iteration 10, loss = 1.23690258\n",
            "Iteration 11, loss = 1.23653360\n",
            "Iteration 12, loss = 1.23608151\n",
            "Iteration 13, loss = 1.22906019\n",
            "Iteration 14, loss = 1.22128239\n",
            "Iteration 15, loss = 1.20604700\n",
            "Iteration 16, loss = 1.21451418\n",
            "Iteration 17, loss = 1.20047282\n",
            "Iteration 18, loss = 1.20935307\n",
            "Iteration 19, loss = 1.21556933\n",
            "Iteration 20, loss = 1.22622660\n",
            "Iteration 21, loss = 1.19648023\n",
            "Iteration 22, loss = 1.18820903\n",
            "Iteration 23, loss = 1.17422274\n",
            "Iteration 24, loss = 1.16750911\n",
            "Iteration 25, loss = 1.15594053\n",
            "Iteration 26, loss = 1.14991131\n",
            "Iteration 27, loss = 1.14670117\n",
            "Iteration 28, loss = 1.15141993\n",
            "Iteration 29, loss = 1.13961393\n",
            "Iteration 30, loss = 1.13068221\n",
            "Iteration 31, loss = 1.11279024\n",
            "Iteration 32, loss = 1.11815234\n",
            "Iteration 33, loss = 1.12023964\n",
            "Iteration 34, loss = 1.10697075\n",
            "Iteration 35, loss = 1.10493245\n",
            "Iteration 36, loss = 1.09346150\n",
            "Iteration 37, loss = 1.08810367\n",
            "Iteration 38, loss = 1.08112999\n",
            "Iteration 39, loss = 1.06919993\n",
            "Iteration 40, loss = 1.06512846\n",
            "Iteration 41, loss = 1.06978881\n",
            "Iteration 42, loss = 1.06715306\n",
            "Iteration 43, loss = 1.06336257\n",
            "Iteration 44, loss = 1.05736155\n",
            "Iteration 45, loss = 1.03972140\n",
            "Iteration 46, loss = 1.03431512\n",
            "Iteration 47, loss = 1.01083572\n",
            "Iteration 48, loss = 0.99693278\n",
            "Iteration 49, loss = 0.99597083\n",
            "Iteration 50, loss = 0.98516974\n",
            "Iteration 51, loss = 0.97942104\n",
            "Iteration 52, loss = 0.96559922\n",
            "Iteration 53, loss = 0.96173212\n",
            "Iteration 54, loss = 0.95479690\n",
            "Iteration 55, loss = 0.95402317\n",
            "Iteration 56, loss = 0.94089245\n",
            "Iteration 57, loss = 0.95138022\n",
            "Iteration 58, loss = 0.91674129\n",
            "Iteration 59, loss = 0.92018306\n",
            "Iteration 60, loss = 0.91219789\n",
            "Iteration 61, loss = 0.89471567\n",
            "Iteration 62, loss = 0.89190734\n",
            "Iteration 63, loss = 0.91257761\n",
            "Iteration 64, loss = 0.89357218\n",
            "Iteration 65, loss = 0.89652287\n",
            "Iteration 66, loss = 0.89448889\n",
            "Iteration 67, loss = 0.89373138\n",
            "Iteration 68, loss = 0.88556589\n",
            "Iteration 69, loss = 0.86263315\n",
            "Iteration 70, loss = 0.87692204\n",
            "Iteration 71, loss = 0.86814894\n",
            "Iteration 72, loss = 0.83479927\n",
            "Iteration 73, loss = 0.79754553\n",
            "Iteration 74, loss = 0.79757042\n",
            "Iteration 75, loss = 0.78709198\n",
            "Iteration 76, loss = 0.79463597\n",
            "Iteration 77, loss = 0.79826374\n",
            "Iteration 78, loss = 0.78426473\n",
            "Iteration 79, loss = 0.76651094\n",
            "Iteration 80, loss = 0.76396372\n",
            "Iteration 81, loss = 0.76402308\n",
            "Iteration 82, loss = 0.73171699\n",
            "Iteration 83, loss = 0.72913842\n",
            "Iteration 84, loss = 0.75269057\n",
            "Iteration 85, loss = 0.76234658\n",
            "Iteration 86, loss = 0.73476754\n",
            "Iteration 87, loss = 0.71022523\n",
            "Iteration 88, loss = 0.69145401\n",
            "Iteration 89, loss = 0.69094219\n",
            "Iteration 90, loss = 0.67097510\n",
            "Iteration 91, loss = 0.69259723\n",
            "Iteration 92, loss = 0.68159290\n",
            "Iteration 93, loss = 0.65787631\n",
            "Iteration 94, loss = 0.65897544\n",
            "Iteration 95, loss = 0.67738139\n",
            "Iteration 96, loss = 0.68666511\n",
            "Iteration 97, loss = 0.66450217\n",
            "Iteration 98, loss = 0.64509650\n",
            "Iteration 99, loss = 0.63834213\n",
            "Iteration 100, loss = 0.61991806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39844946\n",
            "Iteration 2, loss = 1.37003384\n",
            "Iteration 3, loss = 1.36070673\n",
            "Iteration 4, loss = 1.34253198\n",
            "Iteration 5, loss = 1.32871606\n",
            "Iteration 6, loss = 1.30239726\n",
            "Iteration 7, loss = 1.30275226\n",
            "Iteration 8, loss = 1.28539609\n",
            "Iteration 9, loss = 1.26905740\n",
            "Iteration 10, loss = 1.25599090\n",
            "Iteration 11, loss = 1.25436804\n",
            "Iteration 12, loss = 1.24476644\n",
            "Iteration 13, loss = 1.24174247\n",
            "Iteration 14, loss = 1.23020372\n",
            "Iteration 15, loss = 1.22638365\n",
            "Iteration 16, loss = 1.23081018\n",
            "Iteration 17, loss = 1.22176422\n",
            "Iteration 18, loss = 1.21715614\n",
            "Iteration 19, loss = 1.21690015\n",
            "Iteration 20, loss = 1.22005747\n",
            "Iteration 21, loss = 1.20844251\n",
            "Iteration 22, loss = 1.19774298\n",
            "Iteration 23, loss = 1.19400238\n",
            "Iteration 24, loss = 1.20312427\n",
            "Iteration 25, loss = 1.19106732\n",
            "Iteration 26, loss = 1.17890630\n",
            "Iteration 27, loss = 1.17707922\n",
            "Iteration 28, loss = 1.17527059\n",
            "Iteration 29, loss = 1.17191800\n",
            "Iteration 30, loss = 1.16256496\n",
            "Iteration 31, loss = 1.14563148\n",
            "Iteration 32, loss = 1.14519163\n",
            "Iteration 33, loss = 1.15104316\n",
            "Iteration 34, loss = 1.15033578\n",
            "Iteration 35, loss = 1.16137136\n",
            "Iteration 36, loss = 1.14190345\n",
            "Iteration 37, loss = 1.14070720\n",
            "Iteration 38, loss = 1.12744536\n",
            "Iteration 39, loss = 1.11471833\n",
            "Iteration 40, loss = 1.12171008\n",
            "Iteration 41, loss = 1.11231401\n",
            "Iteration 42, loss = 1.08593771\n",
            "Iteration 43, loss = 1.07124877\n",
            "Iteration 44, loss = 1.06984588\n",
            "Iteration 45, loss = 1.07564253\n",
            "Iteration 46, loss = 1.06838961\n",
            "Iteration 47, loss = 1.05894367\n",
            "Iteration 48, loss = 1.04472213\n",
            "Iteration 49, loss = 1.02490985\n",
            "Iteration 50, loss = 1.01283570\n",
            "Iteration 51, loss = 0.99940314\n",
            "Iteration 52, loss = 1.00315307\n",
            "Iteration 53, loss = 0.99541488\n",
            "Iteration 54, loss = 0.98466824\n",
            "Iteration 55, loss = 0.98310379\n",
            "Iteration 56, loss = 0.98150544\n",
            "Iteration 57, loss = 0.97524883\n",
            "Iteration 58, loss = 0.97639855\n",
            "Iteration 59, loss = 0.95185776\n",
            "Iteration 60, loss = 0.93830836\n",
            "Iteration 61, loss = 0.93348778\n",
            "Iteration 62, loss = 0.93300422\n",
            "Iteration 63, loss = 0.93000264\n",
            "Iteration 64, loss = 0.92798170\n",
            "Iteration 65, loss = 0.91613893\n",
            "Iteration 66, loss = 0.93440981\n",
            "Iteration 67, loss = 0.89730763\n",
            "Iteration 68, loss = 0.87754282\n",
            "Iteration 69, loss = 0.89839168\n",
            "Iteration 70, loss = 0.90244924\n",
            "Iteration 71, loss = 0.88203903\n",
            "Iteration 72, loss = 0.85785520\n",
            "Iteration 73, loss = 0.83670277\n",
            "Iteration 74, loss = 0.83030132\n",
            "Iteration 75, loss = 0.82929620\n",
            "Iteration 76, loss = 0.86272949\n",
            "Iteration 77, loss = 0.84231603\n",
            "Iteration 78, loss = 0.81930806\n",
            "Iteration 79, loss = 0.81534094\n",
            "Iteration 80, loss = 0.81759994\n",
            "Iteration 81, loss = 0.81791778\n",
            "Iteration 82, loss = 0.79640244\n",
            "Iteration 83, loss = 0.79421976\n",
            "Iteration 84, loss = 0.80298361\n",
            "Iteration 85, loss = 0.83838786\n",
            "Iteration 86, loss = 0.81543793\n",
            "Iteration 87, loss = 0.77008864\n",
            "Iteration 88, loss = 0.74746115\n",
            "Iteration 89, loss = 0.74071262\n",
            "Iteration 90, loss = 0.73099740\n",
            "Iteration 91, loss = 0.73052909\n",
            "Iteration 92, loss = 0.73153004\n",
            "Iteration 93, loss = 0.70378800\n",
            "Iteration 94, loss = 0.70950492\n",
            "Iteration 95, loss = 0.69614616\n",
            "Iteration 96, loss = 0.70379806\n",
            "Iteration 97, loss = 0.71001040\n",
            "Iteration 98, loss = 0.72621750\n",
            "Iteration 99, loss = 0.71485122\n",
            "Iteration 100, loss = 0.69187162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39181903\n",
            "Iteration 2, loss = 1.36014908\n",
            "Iteration 3, loss = 1.34355985\n",
            "Iteration 4, loss = 1.32375634\n",
            "Iteration 5, loss = 1.30874138\n",
            "Iteration 6, loss = 1.29773652\n",
            "Iteration 7, loss = 1.29625054\n",
            "Iteration 8, loss = 1.28073689\n",
            "Iteration 9, loss = 1.26783139\n",
            "Iteration 10, loss = 1.24431649\n",
            "Iteration 11, loss = 1.24244197\n",
            "Iteration 12, loss = 1.23857863\n",
            "Iteration 13, loss = 1.24765268\n",
            "Iteration 14, loss = 1.23402203\n",
            "Iteration 15, loss = 1.23058710\n",
            "Iteration 16, loss = 1.23802436\n",
            "Iteration 17, loss = 1.21603011\n",
            "Iteration 18, loss = 1.20834087\n",
            "Iteration 19, loss = 1.20887829\n",
            "Iteration 20, loss = 1.21429899\n",
            "Iteration 21, loss = 1.20911035\n",
            "Iteration 22, loss = 1.18841309\n",
            "Iteration 23, loss = 1.18235212\n",
            "Iteration 24, loss = 1.18308880\n",
            "Iteration 25, loss = 1.17426279\n",
            "Iteration 26, loss = 1.16007241\n",
            "Iteration 27, loss = 1.15689401\n",
            "Iteration 28, loss = 1.15643073\n",
            "Iteration 29, loss = 1.15759545\n",
            "Iteration 30, loss = 1.15307428\n",
            "Iteration 31, loss = 1.14588973\n",
            "Iteration 32, loss = 1.13703557\n",
            "Iteration 33, loss = 1.12323763\n",
            "Iteration 34, loss = 1.12756546\n",
            "Iteration 35, loss = 1.13157067\n",
            "Iteration 36, loss = 1.11511886\n",
            "Iteration 37, loss = 1.10432652\n",
            "Iteration 38, loss = 1.09731533\n",
            "Iteration 39, loss = 1.09029123\n",
            "Iteration 40, loss = 1.10155485\n",
            "Iteration 41, loss = 1.10844942\n",
            "Iteration 42, loss = 1.09086481\n",
            "Iteration 43, loss = 1.07149161\n",
            "Iteration 44, loss = 1.05924263\n",
            "Iteration 45, loss = 1.06080928\n",
            "Iteration 46, loss = 1.06170542\n",
            "Iteration 47, loss = 1.04113378\n",
            "Iteration 48, loss = 1.03293965\n",
            "Iteration 49, loss = 1.03766950\n",
            "Iteration 50, loss = 1.03299341\n",
            "Iteration 51, loss = 1.02586582\n",
            "Iteration 52, loss = 1.03733862\n",
            "Iteration 53, loss = 1.04001066\n",
            "Iteration 54, loss = 1.03698171\n",
            "Iteration 55, loss = 1.03925106\n",
            "Iteration 56, loss = 1.00831370\n",
            "Iteration 57, loss = 0.99097365\n",
            "Iteration 58, loss = 0.97672749\n",
            "Iteration 59, loss = 0.96954736\n",
            "Iteration 60, loss = 0.96193176\n",
            "Iteration 61, loss = 0.96990305\n",
            "Iteration 62, loss = 0.97663776\n",
            "Iteration 63, loss = 0.97213392\n",
            "Iteration 64, loss = 0.95558838\n",
            "Iteration 65, loss = 0.94998033\n",
            "Iteration 66, loss = 0.93124416\n",
            "Iteration 67, loss = 0.92149933\n",
            "Iteration 68, loss = 0.91501449\n",
            "Iteration 69, loss = 0.91072725\n",
            "Iteration 70, loss = 0.91418646\n",
            "Iteration 71, loss = 0.92702349\n",
            "Iteration 72, loss = 0.89986053\n",
            "Iteration 73, loss = 0.87295496\n",
            "Iteration 74, loss = 0.87372178\n",
            "Iteration 75, loss = 0.86973341\n",
            "Iteration 76, loss = 0.87803223\n",
            "Iteration 77, loss = 0.87963905\n",
            "Iteration 78, loss = 0.86774963\n",
            "Iteration 79, loss = 0.85442908\n",
            "Iteration 80, loss = 0.85850964\n",
            "Iteration 81, loss = 0.85520243\n",
            "Iteration 82, loss = 0.85863201\n",
            "Iteration 83, loss = 0.83646828\n",
            "Iteration 84, loss = 0.83741066\n",
            "Iteration 85, loss = 0.82642531\n",
            "Iteration 86, loss = 0.80089950\n",
            "Iteration 87, loss = 0.77862494\n",
            "Iteration 88, loss = 0.78881997\n",
            "Iteration 89, loss = 0.77128366\n",
            "Iteration 90, loss = 0.77373961\n",
            "Iteration 91, loss = 0.77560349\n",
            "Iteration 92, loss = 0.75150218\n",
            "Iteration 93, loss = 0.73886084\n",
            "Iteration 94, loss = 0.77710701\n",
            "Iteration 95, loss = 0.79612048\n",
            "Iteration 96, loss = 0.75601242\n",
            "Iteration 97, loss = 0.74152954\n",
            "Iteration 98, loss = 0.73020322\n",
            "Iteration 99, loss = 0.71688732\n",
            "Iteration 100, loss = 0.71933520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38182728\n",
            "Iteration 2, loss = 1.35150250\n",
            "Iteration 3, loss = 1.32801246\n",
            "Iteration 4, loss = 1.30816898\n",
            "Iteration 5, loss = 1.29570051\n",
            "Iteration 6, loss = 1.28625968\n",
            "Iteration 7, loss = 1.28616542\n",
            "Iteration 8, loss = 1.27289960\n",
            "Iteration 9, loss = 1.26650814\n",
            "Iteration 10, loss = 1.24931302\n",
            "Iteration 11, loss = 1.24687468\n",
            "Iteration 12, loss = 1.24944902\n",
            "Iteration 13, loss = 1.26666493\n",
            "Iteration 14, loss = 1.24997279\n",
            "Iteration 15, loss = 1.23615259\n",
            "Iteration 16, loss = 1.23271561\n",
            "Iteration 17, loss = 1.22928661\n",
            "Iteration 18, loss = 1.22763869\n",
            "Iteration 19, loss = 1.22670417\n",
            "Iteration 20, loss = 1.23048931\n",
            "Iteration 21, loss = 1.22487313\n",
            "Iteration 22, loss = 1.23179572\n",
            "Iteration 23, loss = 1.21834656\n",
            "Iteration 24, loss = 1.21782530\n",
            "Iteration 25, loss = 1.20879843\n",
            "Iteration 26, loss = 1.19576928\n",
            "Iteration 27, loss = 1.19095459\n",
            "Iteration 28, loss = 1.19245762\n",
            "Iteration 29, loss = 1.18971342\n",
            "Iteration 30, loss = 1.17806103\n",
            "Iteration 31, loss = 1.17374690\n",
            "Iteration 32, loss = 1.17007652\n",
            "Iteration 33, loss = 1.16350876\n",
            "Iteration 34, loss = 1.16460502\n",
            "Iteration 35, loss = 1.17125262\n",
            "Iteration 36, loss = 1.16628544\n",
            "Iteration 37, loss = 1.16744285\n",
            "Iteration 38, loss = 1.14863350\n",
            "Iteration 39, loss = 1.14175146\n",
            "Iteration 40, loss = 1.13500458\n",
            "Iteration 41, loss = 1.13917573\n",
            "Iteration 42, loss = 1.13214480\n",
            "Iteration 43, loss = 1.11503442\n",
            "Iteration 44, loss = 1.11772417\n",
            "Iteration 45, loss = 1.12117369\n",
            "Iteration 46, loss = 1.09090578\n",
            "Iteration 47, loss = 1.08663923\n",
            "Iteration 48, loss = 1.07478636\n",
            "Iteration 49, loss = 1.07588852\n",
            "Iteration 50, loss = 1.06473751\n",
            "Iteration 51, loss = 1.05337133\n",
            "Iteration 52, loss = 1.05189091\n",
            "Iteration 53, loss = 1.04694474\n",
            "Iteration 54, loss = 1.03033295\n",
            "Iteration 55, loss = 1.03480410\n",
            "Iteration 56, loss = 1.01813362\n",
            "Iteration 57, loss = 1.01335941\n",
            "Iteration 58, loss = 1.00921949\n",
            "Iteration 59, loss = 1.01370213\n",
            "Iteration 60, loss = 1.00874781\n",
            "Iteration 61, loss = 0.98062994\n",
            "Iteration 62, loss = 0.97157357\n",
            "Iteration 63, loss = 0.98951899\n",
            "Iteration 64, loss = 0.97410344\n",
            "Iteration 65, loss = 0.95864596\n",
            "Iteration 66, loss = 0.93361433\n",
            "Iteration 67, loss = 0.93739718\n",
            "Iteration 68, loss = 0.91099571\n",
            "Iteration 69, loss = 0.91180603\n",
            "Iteration 70, loss = 0.94325214\n",
            "Iteration 71, loss = 0.93876371\n",
            "Iteration 72, loss = 0.92526746\n",
            "Iteration 73, loss = 0.88827438\n",
            "Iteration 74, loss = 0.87172914\n",
            "Iteration 75, loss = 0.87717866\n",
            "Iteration 76, loss = 0.86420589\n",
            "Iteration 77, loss = 0.85131287\n",
            "Iteration 78, loss = 0.83799434\n",
            "Iteration 79, loss = 0.84290645\n",
            "Iteration 80, loss = 0.85575929\n",
            "Iteration 81, loss = 0.82388529\n",
            "Iteration 82, loss = 0.79912605\n",
            "Iteration 83, loss = 0.79479874\n",
            "Iteration 84, loss = 0.79791185\n",
            "Iteration 85, loss = 0.81213761\n",
            "Iteration 86, loss = 0.80002897\n",
            "Iteration 87, loss = 0.78196087\n",
            "Iteration 88, loss = 0.77949503\n",
            "Iteration 89, loss = 0.75809422\n",
            "Iteration 90, loss = 0.74505541\n",
            "Iteration 91, loss = 0.73821974\n",
            "Iteration 92, loss = 0.71461186\n",
            "Iteration 93, loss = 0.71375200\n",
            "Iteration 94, loss = 0.72930957\n",
            "Iteration 95, loss = 0.69449623\n",
            "Iteration 96, loss = 0.69930605\n",
            "Iteration 97, loss = 0.70489745\n",
            "Iteration 98, loss = 0.69388887\n",
            "Iteration 99, loss = 0.67144400\n",
            "Iteration 100, loss = 0.68684826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40277887\n",
            "Iteration 2, loss = 1.36102680\n",
            "Iteration 3, loss = 1.34795536\n",
            "Iteration 4, loss = 1.32550658\n",
            "Iteration 5, loss = 1.30951722\n",
            "Iteration 6, loss = 1.31657479\n",
            "Iteration 7, loss = 1.30297635\n",
            "Iteration 8, loss = 1.28359923\n",
            "Iteration 9, loss = 1.26942667\n",
            "Iteration 10, loss = 1.26022849\n",
            "Iteration 11, loss = 1.25793280\n",
            "Iteration 12, loss = 1.25532012\n",
            "Iteration 13, loss = 1.25424680\n",
            "Iteration 14, loss = 1.24532659\n",
            "Iteration 15, loss = 1.23439969\n",
            "Iteration 16, loss = 1.22736684\n",
            "Iteration 17, loss = 1.22995133\n",
            "Iteration 18, loss = 1.22518472\n",
            "Iteration 19, loss = 1.21910374\n",
            "Iteration 20, loss = 1.22504466\n",
            "Iteration 21, loss = 1.22508096\n",
            "Iteration 22, loss = 1.21792294\n",
            "Iteration 23, loss = 1.20241061\n",
            "Iteration 24, loss = 1.20771206\n",
            "Iteration 25, loss = 1.19326901\n",
            "Iteration 26, loss = 1.18051172\n",
            "Iteration 27, loss = 1.17739921\n",
            "Iteration 28, loss = 1.18446501\n",
            "Iteration 29, loss = 1.17705218\n",
            "Iteration 30, loss = 1.16640379\n",
            "Iteration 31, loss = 1.16219399\n",
            "Iteration 32, loss = 1.15290850\n",
            "Iteration 33, loss = 1.14267898\n",
            "Iteration 34, loss = 1.14112482\n",
            "Iteration 35, loss = 1.15779775\n",
            "Iteration 36, loss = 1.13313196\n",
            "Iteration 37, loss = 1.13388124\n",
            "Iteration 38, loss = 1.12394974\n",
            "Iteration 39, loss = 1.12308360\n",
            "Iteration 40, loss = 1.12782056\n",
            "Iteration 41, loss = 1.12288067\n",
            "Iteration 42, loss = 1.11701331\n",
            "Iteration 43, loss = 1.10073919\n",
            "Iteration 44, loss = 1.10355203\n",
            "Iteration 45, loss = 1.08568209\n",
            "Iteration 46, loss = 1.07458678\n",
            "Iteration 47, loss = 1.08884927\n",
            "Iteration 48, loss = 1.07061082\n",
            "Iteration 49, loss = 1.05701207\n",
            "Iteration 50, loss = 1.05268013\n",
            "Iteration 51, loss = 1.04253534\n",
            "Iteration 52, loss = 1.04041730\n",
            "Iteration 53, loss = 1.03434121\n",
            "Iteration 54, loss = 1.03200272\n",
            "Iteration 55, loss = 1.03387090\n",
            "Iteration 56, loss = 1.04650066\n",
            "Iteration 57, loss = 1.02373076\n",
            "Iteration 58, loss = 1.02823936\n",
            "Iteration 59, loss = 0.99448982\n",
            "Iteration 60, loss = 0.98799202\n",
            "Iteration 61, loss = 0.98584104\n",
            "Iteration 62, loss = 0.99147170\n",
            "Iteration 63, loss = 1.00330294\n",
            "Iteration 64, loss = 0.98149836\n",
            "Iteration 65, loss = 0.97356622\n",
            "Iteration 66, loss = 0.98436594\n",
            "Iteration 67, loss = 0.94980149\n",
            "Iteration 68, loss = 0.94102645\n",
            "Iteration 69, loss = 0.93426483\n",
            "Iteration 70, loss = 0.94762539\n",
            "Iteration 71, loss = 0.95040406\n",
            "Iteration 72, loss = 0.93813188\n",
            "Iteration 73, loss = 0.91864035\n",
            "Iteration 74, loss = 0.90997612\n",
            "Iteration 75, loss = 0.94327946\n",
            "Iteration 76, loss = 0.90711015\n",
            "Iteration 77, loss = 0.89573459\n",
            "Iteration 78, loss = 0.89642039\n",
            "Iteration 79, loss = 0.88366498\n",
            "Iteration 80, loss = 0.90629290\n",
            "Iteration 81, loss = 0.88330669\n",
            "Iteration 82, loss = 0.86037586\n",
            "Iteration 83, loss = 0.84461851\n",
            "Iteration 84, loss = 0.84447822\n",
            "Iteration 85, loss = 0.83724335\n",
            "Iteration 86, loss = 0.84444316\n",
            "Iteration 87, loss = 0.82552587\n",
            "Iteration 88, loss = 0.82120119\n",
            "Iteration 89, loss = 0.80492266\n",
            "Iteration 90, loss = 0.80286599\n",
            "Iteration 91, loss = 0.79204322\n",
            "Iteration 92, loss = 0.77483592\n",
            "Iteration 93, loss = 0.78314268\n",
            "Iteration 94, loss = 0.79075936\n",
            "Iteration 95, loss = 0.81413415\n",
            "Iteration 96, loss = 0.78938104\n",
            "Iteration 97, loss = 0.79426214\n",
            "Iteration 98, loss = 0.78178423\n",
            "Iteration 99, loss = 0.74261220\n",
            "Iteration 100, loss = 0.73370979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39883065\n",
            "Iteration 2, loss = 1.36651491\n",
            "Iteration 3, loss = 1.35261210\n",
            "Iteration 4, loss = 1.32641469\n",
            "Iteration 5, loss = 1.30641000\n",
            "Iteration 6, loss = 1.31754636\n",
            "Iteration 7, loss = 1.29664402\n",
            "Iteration 8, loss = 1.27833357\n",
            "Iteration 9, loss = 1.26812484\n",
            "Iteration 10, loss = 1.25465247\n",
            "Iteration 11, loss = 1.24399173\n",
            "Iteration 12, loss = 1.25033010\n",
            "Iteration 13, loss = 1.24728638\n",
            "Iteration 14, loss = 1.24122684\n",
            "Iteration 15, loss = 1.23650755\n",
            "Iteration 16, loss = 1.22651989\n",
            "Iteration 17, loss = 1.22063795\n",
            "Iteration 18, loss = 1.21217511\n",
            "Iteration 19, loss = 1.20979471\n",
            "Iteration 20, loss = 1.20151567\n",
            "Iteration 21, loss = 1.20325239\n",
            "Iteration 22, loss = 1.19874095\n",
            "Iteration 23, loss = 1.18835332\n",
            "Iteration 24, loss = 1.19420420\n",
            "Iteration 25, loss = 1.17864111\n",
            "Iteration 26, loss = 1.16827773\n",
            "Iteration 27, loss = 1.16408530\n",
            "Iteration 28, loss = 1.16663892\n",
            "Iteration 29, loss = 1.16138424\n",
            "Iteration 30, loss = 1.14826857\n",
            "Iteration 31, loss = 1.13039395\n",
            "Iteration 32, loss = 1.12785136\n",
            "Iteration 33, loss = 1.11627977\n",
            "Iteration 34, loss = 1.12643370\n",
            "Iteration 35, loss = 1.14547401\n",
            "Iteration 36, loss = 1.11007154\n",
            "Iteration 37, loss = 1.09943183\n",
            "Iteration 38, loss = 1.08771511\n",
            "Iteration 39, loss = 1.09727573\n",
            "Iteration 40, loss = 1.11354958\n",
            "Iteration 41, loss = 1.08918772\n",
            "Iteration 42, loss = 1.06377337\n",
            "Iteration 43, loss = 1.05094916\n",
            "Iteration 44, loss = 1.05888725\n",
            "Iteration 45, loss = 1.04929970\n",
            "Iteration 46, loss = 1.02224752\n",
            "Iteration 47, loss = 1.04627351\n",
            "Iteration 48, loss = 1.03111566\n",
            "Iteration 49, loss = 1.02027269\n",
            "Iteration 50, loss = 1.00394928\n",
            "Iteration 51, loss = 1.00106404\n",
            "Iteration 52, loss = 0.98487298\n",
            "Iteration 53, loss = 0.98553344\n",
            "Iteration 54, loss = 0.99273371\n",
            "Iteration 55, loss = 0.97122219\n",
            "Iteration 56, loss = 0.96271689\n",
            "Iteration 57, loss = 0.95156218\n",
            "Iteration 58, loss = 0.94663372\n",
            "Iteration 59, loss = 0.94450803\n",
            "Iteration 60, loss = 0.93631700\n",
            "Iteration 61, loss = 0.92443366\n",
            "Iteration 62, loss = 0.91640828\n",
            "Iteration 63, loss = 0.92826608\n",
            "Iteration 64, loss = 0.92194112\n",
            "Iteration 65, loss = 0.93323729\n",
            "Iteration 66, loss = 0.91194135\n",
            "Iteration 67, loss = 0.89553452\n",
            "Iteration 68, loss = 0.86644111\n",
            "Iteration 69, loss = 0.85274654\n",
            "Iteration 70, loss = 0.85668947\n",
            "Iteration 71, loss = 0.86058065\n",
            "Iteration 72, loss = 0.86490195\n",
            "Iteration 73, loss = 0.86330392\n",
            "Iteration 74, loss = 0.84403796\n",
            "Iteration 75, loss = 0.83973985\n",
            "Iteration 76, loss = 0.81638531\n",
            "Iteration 77, loss = 0.79276798\n",
            "Iteration 78, loss = 0.79241339\n",
            "Iteration 79, loss = 0.79041311\n",
            "Iteration 80, loss = 0.78718848\n",
            "Iteration 81, loss = 0.79134088\n",
            "Iteration 82, loss = 0.78272054\n",
            "Iteration 83, loss = 0.76382240\n",
            "Iteration 84, loss = 0.75473061\n",
            "Iteration 85, loss = 0.75199730\n",
            "Iteration 86, loss = 0.76701275\n",
            "Iteration 87, loss = 0.79795500\n",
            "Iteration 88, loss = 0.76437128\n",
            "Iteration 89, loss = 0.76814730\n",
            "Iteration 90, loss = 0.73183954\n",
            "Iteration 91, loss = 0.73220917\n",
            "Iteration 92, loss = 0.71377627\n",
            "Iteration 93, loss = 0.72623150\n",
            "Iteration 94, loss = 0.73276310\n",
            "Iteration 95, loss = 0.72019524\n",
            "Iteration 96, loss = 0.67516944\n",
            "Iteration 97, loss = 0.67168537\n",
            "Iteration 98, loss = 0.66744735\n",
            "Iteration 99, loss = 0.65492766\n",
            "Iteration 100, loss = 0.65988664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36768100\n",
            "Iteration 2, loss = 1.34566500\n",
            "Iteration 3, loss = 1.32721228\n",
            "Iteration 4, loss = 1.29962361\n",
            "Iteration 5, loss = 1.27481619\n",
            "Iteration 6, loss = 1.28320056\n",
            "Iteration 7, loss = 1.27066139\n",
            "Iteration 8, loss = 1.25423495\n",
            "Iteration 9, loss = 1.23472555\n",
            "Iteration 10, loss = 1.22477827\n",
            "Iteration 11, loss = 1.21698799\n",
            "Iteration 12, loss = 1.22021306\n",
            "Iteration 13, loss = 1.21205013\n",
            "Iteration 14, loss = 1.20382348\n",
            "Iteration 15, loss = 1.19509781\n",
            "Iteration 16, loss = 1.19048216\n",
            "Iteration 17, loss = 1.18827437\n",
            "Iteration 18, loss = 1.17540424\n",
            "Iteration 19, loss = 1.17285096\n",
            "Iteration 20, loss = 1.16481393\n",
            "Iteration 21, loss = 1.16721666\n",
            "Iteration 22, loss = 1.17420613\n",
            "Iteration 23, loss = 1.16768997\n",
            "Iteration 24, loss = 1.17870930\n",
            "Iteration 25, loss = 1.15734046\n",
            "Iteration 26, loss = 1.13688702\n",
            "Iteration 27, loss = 1.12605712\n",
            "Iteration 28, loss = 1.13836838\n",
            "Iteration 29, loss = 1.12016761\n",
            "Iteration 30, loss = 1.10726004\n",
            "Iteration 31, loss = 1.10112531\n",
            "Iteration 32, loss = 1.10300510\n",
            "Iteration 33, loss = 1.09150339\n",
            "Iteration 34, loss = 1.09689953\n",
            "Iteration 35, loss = 1.10773430\n",
            "Iteration 36, loss = 1.07765860\n",
            "Iteration 37, loss = 1.06555323\n",
            "Iteration 38, loss = 1.06013993\n",
            "Iteration 39, loss = 1.06705511\n",
            "Iteration 40, loss = 1.05895218\n",
            "Iteration 41, loss = 1.05215712\n",
            "Iteration 42, loss = 1.03626260\n",
            "Iteration 43, loss = 1.02306514\n",
            "Iteration 44, loss = 1.02225456\n",
            "Iteration 45, loss = 1.02031990\n",
            "Iteration 46, loss = 0.99891313\n",
            "Iteration 47, loss = 1.00809403\n",
            "Iteration 48, loss = 1.00087328\n",
            "Iteration 49, loss = 0.97053641\n",
            "Iteration 50, loss = 0.96483627\n",
            "Iteration 51, loss = 0.95774201\n",
            "Iteration 52, loss = 0.95797568\n",
            "Iteration 53, loss = 0.94861861\n",
            "Iteration 54, loss = 0.96883564\n",
            "Iteration 55, loss = 0.98444884\n",
            "Iteration 56, loss = 0.94406213\n",
            "Iteration 57, loss = 0.93671264\n",
            "Iteration 58, loss = 0.94204759\n",
            "Iteration 59, loss = 0.91064927\n",
            "Iteration 60, loss = 0.91086228\n",
            "Iteration 61, loss = 0.90038497\n",
            "Iteration 62, loss = 0.89245126\n",
            "Iteration 63, loss = 0.90904301\n",
            "Iteration 64, loss = 0.88391813\n",
            "Iteration 65, loss = 0.86497674\n",
            "Iteration 66, loss = 0.86397180\n",
            "Iteration 67, loss = 0.86523764\n",
            "Iteration 68, loss = 0.85012245\n",
            "Iteration 69, loss = 0.84091973\n",
            "Iteration 70, loss = 0.83200234\n",
            "Iteration 71, loss = 0.84492729\n",
            "Iteration 72, loss = 0.84503252\n",
            "Iteration 73, loss = 0.84094302\n",
            "Iteration 74, loss = 0.82462465\n",
            "Iteration 75, loss = 0.80550029\n",
            "Iteration 76, loss = 0.80426725\n",
            "Iteration 77, loss = 0.79040916\n",
            "Iteration 78, loss = 0.77855558\n",
            "Iteration 79, loss = 0.79437109\n",
            "Iteration 80, loss = 0.78730534\n",
            "Iteration 81, loss = 0.77629108\n",
            "Iteration 82, loss = 0.77736198\n",
            "Iteration 83, loss = 0.74513424\n",
            "Iteration 84, loss = 0.73866199\n",
            "Iteration 85, loss = 0.74514214\n",
            "Iteration 86, loss = 0.75743517\n",
            "Iteration 87, loss = 0.79736049\n",
            "Iteration 88, loss = 0.77561886\n",
            "Iteration 89, loss = 0.78263679\n",
            "Iteration 90, loss = 0.75815719\n",
            "Iteration 91, loss = 0.74502976\n",
            "Iteration 92, loss = 0.69572990\n",
            "Iteration 93, loss = 0.72118519\n",
            "Iteration 94, loss = 0.72717643\n",
            "Iteration 95, loss = 0.68098777\n",
            "Iteration 96, loss = 0.69059390\n",
            "Iteration 97, loss = 0.67883049\n",
            "Iteration 98, loss = 0.65864007\n",
            "Iteration 99, loss = 0.64633241\n",
            "Iteration 100, loss = 0.64369402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n",
            "Iteration 1, loss = 1.35894406\n",
            "Iteration 2, loss = 1.29019845\n",
            "Iteration 3, loss = 1.21837408\n",
            "Iteration 4, loss = 1.13856243\n",
            "Iteration 5, loss = 1.08546389\n",
            "Iteration 6, loss = 1.00954114\n",
            "Iteration 7, loss = 0.94344616\n",
            "Iteration 8, loss = 0.89734136\n",
            "Iteration 9, loss = 0.85130008\n",
            "Iteration 10, loss = 0.80343353\n",
            "Iteration 11, loss = 0.78232786\n",
            "Iteration 12, loss = 0.74546965\n",
            "Iteration 13, loss = 0.73460162\n",
            "Iteration 14, loss = 0.72681325\n",
            "Iteration 15, loss = 0.70705191\n",
            "Iteration 16, loss = 0.68405121\n",
            "Iteration 17, loss = 0.67738593\n",
            "Iteration 18, loss = 0.66476253\n",
            "Iteration 19, loss = 0.64627867\n",
            "Iteration 20, loss = 0.63223891\n",
            "Iteration 21, loss = 0.63570435\n",
            "Iteration 22, loss = 0.60602203\n",
            "Iteration 23, loss = 0.61536261\n",
            "Iteration 24, loss = 0.63085833\n",
            "Iteration 25, loss = 0.59787238\n",
            "Iteration 26, loss = 0.57658575\n",
            "Iteration 27, loss = 0.56603375\n",
            "Iteration 28, loss = 0.54897082\n",
            "Iteration 29, loss = 0.55693693\n",
            "Iteration 30, loss = 0.56182868\n",
            "Iteration 31, loss = 0.55189614\n",
            "Iteration 32, loss = 0.54624076\n",
            "Iteration 33, loss = 0.52391739\n",
            "Iteration 34, loss = 0.53456796\n",
            "Iteration 35, loss = 0.52548331\n",
            "Iteration 36, loss = 0.52535940\n",
            "Iteration 37, loss = 0.51299510\n",
            "Iteration 38, loss = 0.50921193\n",
            "Iteration 39, loss = 0.54188038\n",
            "Iteration 40, loss = 0.51976006\n",
            "Iteration 41, loss = 0.51671497\n",
            "Iteration 42, loss = 0.50901276\n",
            "Iteration 43, loss = 0.48773818\n",
            "Iteration 44, loss = 0.46632793\n",
            "Iteration 45, loss = 0.46991951\n",
            "Iteration 46, loss = 0.47539802\n",
            "Iteration 47, loss = 0.50034270\n",
            "Iteration 48, loss = 0.45454790\n",
            "Iteration 49, loss = 0.43969231\n",
            "Iteration 50, loss = 0.43545791\n",
            "Iteration 51, loss = 0.44066764\n",
            "Iteration 52, loss = 0.44641262\n",
            "Iteration 53, loss = 0.47544315\n",
            "Iteration 54, loss = 0.46148005\n",
            "Iteration 55, loss = 0.42898947\n",
            "Iteration 56, loss = 0.43329091\n",
            "Iteration 57, loss = 0.41726980\n",
            "Iteration 58, loss = 0.42026649\n",
            "Iteration 59, loss = 0.44467834\n",
            "Iteration 60, loss = 0.43292562\n",
            "Iteration 61, loss = 0.41987398\n",
            "Iteration 62, loss = 0.40016253\n",
            "Iteration 63, loss = 0.39761068\n",
            "Iteration 64, loss = 0.39634883\n",
            "Iteration 65, loss = 0.41535579\n",
            "Iteration 66, loss = 0.42735292\n",
            "Iteration 67, loss = 0.43958767\n",
            "Iteration 68, loss = 0.37955750\n",
            "Iteration 69, loss = 0.38604126\n",
            "Iteration 70, loss = 0.38038161\n",
            "Iteration 71, loss = 0.37811573\n",
            "Iteration 72, loss = 0.42003699\n",
            "Iteration 73, loss = 0.41813604\n",
            "Iteration 74, loss = 0.37854186\n",
            "Iteration 75, loss = 0.35620755\n",
            "Iteration 76, loss = 0.34806109\n",
            "Iteration 77, loss = 0.35800247\n",
            "Iteration 78, loss = 0.37435157\n",
            "Iteration 79, loss = 0.35081174\n",
            "Iteration 80, loss = 0.33363927\n",
            "Iteration 81, loss = 0.33143895\n",
            "Iteration 82, loss = 0.33961903\n",
            "Iteration 83, loss = 0.31135161\n",
            "Iteration 84, loss = 0.31914770\n",
            "Iteration 85, loss = 0.33371327\n",
            "Iteration 86, loss = 0.31542138\n",
            "Iteration 87, loss = 0.30980746\n",
            "Iteration 88, loss = 0.29369762\n",
            "Iteration 89, loss = 0.30435147\n",
            "Iteration 90, loss = 0.34286825\n",
            "Iteration 91, loss = 0.30597227\n",
            "Iteration 92, loss = 0.31938146\n",
            "Iteration 93, loss = 0.34588665\n",
            "Iteration 94, loss = 0.32652985\n",
            "Iteration 95, loss = 0.28866165\n",
            "Iteration 96, loss = 0.27006888\n",
            "Iteration 97, loss = 0.26732326\n",
            "Iteration 98, loss = 0.27163935\n",
            "Iteration 99, loss = 0.26751174\n",
            "Iteration 100, loss = 0.28159040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35049430\n",
            "Iteration 2, loss = 1.26781811\n",
            "Iteration 3, loss = 1.17529683\n",
            "Iteration 4, loss = 1.08366640\n",
            "Iteration 5, loss = 0.99230318\n",
            "Iteration 6, loss = 0.93704166\n",
            "Iteration 7, loss = 0.86917898\n",
            "Iteration 8, loss = 0.82934223\n",
            "Iteration 9, loss = 0.79168047\n",
            "Iteration 10, loss = 0.75925375\n",
            "Iteration 11, loss = 0.74482631\n",
            "Iteration 12, loss = 0.72069054\n",
            "Iteration 13, loss = 0.71529559\n",
            "Iteration 14, loss = 0.72423654\n",
            "Iteration 15, loss = 0.70449528\n",
            "Iteration 16, loss = 0.66431961\n",
            "Iteration 17, loss = 0.67180211\n",
            "Iteration 18, loss = 0.66206987\n",
            "Iteration 19, loss = 0.63492218\n",
            "Iteration 20, loss = 0.61789119\n",
            "Iteration 21, loss = 0.62218501\n",
            "Iteration 22, loss = 0.59772887\n",
            "Iteration 23, loss = 0.59814943\n",
            "Iteration 24, loss = 0.58947069\n",
            "Iteration 25, loss = 0.57590615\n",
            "Iteration 26, loss = 0.56475766\n",
            "Iteration 27, loss = 0.55820236\n",
            "Iteration 28, loss = 0.54092346\n",
            "Iteration 29, loss = 0.54702328\n",
            "Iteration 30, loss = 0.53259147\n",
            "Iteration 31, loss = 0.54224385\n",
            "Iteration 32, loss = 0.53410346\n",
            "Iteration 33, loss = 0.52596103\n",
            "Iteration 34, loss = 0.53676273\n",
            "Iteration 35, loss = 0.51487699\n",
            "Iteration 36, loss = 0.54111298\n",
            "Iteration 37, loss = 0.53246566\n",
            "Iteration 38, loss = 0.50934729\n",
            "Iteration 39, loss = 0.53723164\n",
            "Iteration 40, loss = 0.50496560\n",
            "Iteration 41, loss = 0.50853011\n",
            "Iteration 42, loss = 0.49555451\n",
            "Iteration 43, loss = 0.51588686\n",
            "Iteration 44, loss = 0.49854004\n",
            "Iteration 45, loss = 0.48019952\n",
            "Iteration 46, loss = 0.47449268\n",
            "Iteration 47, loss = 0.46982246\n",
            "Iteration 48, loss = 0.46229904\n",
            "Iteration 49, loss = 0.45249598\n",
            "Iteration 50, loss = 0.46052128\n",
            "Iteration 51, loss = 0.46620272\n",
            "Iteration 52, loss = 0.47173477\n",
            "Iteration 53, loss = 0.47678695\n",
            "Iteration 54, loss = 0.44698628\n",
            "Iteration 55, loss = 0.42214507\n",
            "Iteration 56, loss = 0.42353830\n",
            "Iteration 57, loss = 0.42741061\n",
            "Iteration 58, loss = 0.43626628\n",
            "Iteration 59, loss = 0.46764450\n",
            "Iteration 60, loss = 0.43730801\n",
            "Iteration 61, loss = 0.42008207\n",
            "Iteration 62, loss = 0.39929798\n",
            "Iteration 63, loss = 0.41222954\n",
            "Iteration 64, loss = 0.42079473\n",
            "Iteration 65, loss = 0.43068027\n",
            "Iteration 66, loss = 0.42641514\n",
            "Iteration 67, loss = 0.42626491\n",
            "Iteration 68, loss = 0.37655890\n",
            "Iteration 69, loss = 0.39582244\n",
            "Iteration 70, loss = 0.39884352\n",
            "Iteration 71, loss = 0.40169655\n",
            "Iteration 72, loss = 0.42548586\n",
            "Iteration 73, loss = 0.39545245\n",
            "Iteration 74, loss = 0.36741410\n",
            "Iteration 75, loss = 0.35719141\n",
            "Iteration 76, loss = 0.36182072\n",
            "Iteration 77, loss = 0.36468519\n",
            "Iteration 78, loss = 0.38754412\n",
            "Iteration 79, loss = 0.35199841\n",
            "Iteration 80, loss = 0.33151254\n",
            "Iteration 81, loss = 0.34064287\n",
            "Iteration 82, loss = 0.34602021\n",
            "Iteration 83, loss = 0.33541544\n",
            "Iteration 84, loss = 0.32172497\n",
            "Iteration 85, loss = 0.31458299\n",
            "Iteration 86, loss = 0.32842018\n",
            "Iteration 87, loss = 0.33731672\n",
            "Iteration 88, loss = 0.32198367\n",
            "Iteration 89, loss = 0.30527635\n",
            "Iteration 90, loss = 0.31484977\n",
            "Iteration 91, loss = 0.29606113\n",
            "Iteration 92, loss = 0.30592789\n",
            "Iteration 93, loss = 0.33958956\n",
            "Iteration 94, loss = 0.30331865\n",
            "Iteration 95, loss = 0.27995995\n",
            "Iteration 96, loss = 0.27956657\n",
            "Iteration 97, loss = 0.28401524\n",
            "Iteration 98, loss = 0.29725369\n",
            "Iteration 99, loss = 0.28511392\n",
            "Iteration 100, loss = 0.31435755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35135805\n",
            "Iteration 2, loss = 1.26760595\n",
            "Iteration 3, loss = 1.17544637\n",
            "Iteration 4, loss = 1.08438913\n",
            "Iteration 5, loss = 1.00338216\n",
            "Iteration 6, loss = 0.95202270\n",
            "Iteration 7, loss = 0.89206140\n",
            "Iteration 8, loss = 0.84874028\n",
            "Iteration 9, loss = 0.80933291\n",
            "Iteration 10, loss = 0.77851314\n",
            "Iteration 11, loss = 0.75718224\n",
            "Iteration 12, loss = 0.75110318\n",
            "Iteration 13, loss = 0.75502321\n",
            "Iteration 14, loss = 0.71404653\n",
            "Iteration 15, loss = 0.70731503\n",
            "Iteration 16, loss = 0.67210046\n",
            "Iteration 17, loss = 0.66433155\n",
            "Iteration 18, loss = 0.66792792\n",
            "Iteration 19, loss = 0.63852469\n",
            "Iteration 20, loss = 0.62248171\n",
            "Iteration 21, loss = 0.61711877\n",
            "Iteration 22, loss = 0.59486135\n",
            "Iteration 23, loss = 0.61317598\n",
            "Iteration 24, loss = 0.61027076\n",
            "Iteration 25, loss = 0.60332821\n",
            "Iteration 26, loss = 0.57241173\n",
            "Iteration 27, loss = 0.58247745\n",
            "Iteration 28, loss = 0.56290824\n",
            "Iteration 29, loss = 0.54925833\n",
            "Iteration 30, loss = 0.57086869\n",
            "Iteration 31, loss = 0.57300541\n",
            "Iteration 32, loss = 0.55761361\n",
            "Iteration 33, loss = 0.55787035\n",
            "Iteration 34, loss = 0.53763394\n",
            "Iteration 35, loss = 0.52181986\n",
            "Iteration 36, loss = 0.51770620\n",
            "Iteration 37, loss = 0.51381162\n",
            "Iteration 38, loss = 0.51366950\n",
            "Iteration 39, loss = 0.51987437\n",
            "Iteration 40, loss = 0.49358742\n",
            "Iteration 41, loss = 0.48765303\n",
            "Iteration 42, loss = 0.49751319\n",
            "Iteration 43, loss = 0.55056934\n",
            "Iteration 44, loss = 0.52416142\n",
            "Iteration 45, loss = 0.51302242\n",
            "Iteration 46, loss = 0.48593120\n",
            "Iteration 47, loss = 0.49080357\n",
            "Iteration 48, loss = 0.49035527\n",
            "Iteration 49, loss = 0.49237112\n",
            "Iteration 50, loss = 0.49460579\n",
            "Iteration 51, loss = 0.46690548\n",
            "Iteration 52, loss = 0.48444341\n",
            "Iteration 53, loss = 0.47775910\n",
            "Iteration 54, loss = 0.44273619\n",
            "Iteration 55, loss = 0.42889584\n",
            "Iteration 56, loss = 0.44754167\n",
            "Iteration 57, loss = 0.43472219\n",
            "Iteration 58, loss = 0.43105345\n",
            "Iteration 59, loss = 0.44291514\n",
            "Iteration 60, loss = 0.43887311\n",
            "Iteration 61, loss = 0.44033151\n",
            "Iteration 62, loss = 0.44173829\n",
            "Iteration 63, loss = 0.43142073\n",
            "Iteration 64, loss = 0.44661898\n",
            "Iteration 65, loss = 0.42830932\n",
            "Iteration 66, loss = 0.47159730\n",
            "Iteration 67, loss = 0.50129263\n",
            "Iteration 68, loss = 0.48662611\n",
            "Iteration 69, loss = 0.46481682\n",
            "Iteration 70, loss = 0.42534551\n",
            "Iteration 71, loss = 0.48881281\n",
            "Iteration 72, loss = 0.45364539\n",
            "Iteration 73, loss = 0.43710475\n",
            "Iteration 74, loss = 0.39830536\n",
            "Iteration 75, loss = 0.38506302\n",
            "Iteration 76, loss = 0.39223617\n",
            "Iteration 77, loss = 0.39163708\n",
            "Iteration 78, loss = 0.42759425\n",
            "Iteration 79, loss = 0.38174372\n",
            "Iteration 80, loss = 0.35586060\n",
            "Iteration 81, loss = 0.36362363\n",
            "Iteration 82, loss = 0.36043825\n",
            "Iteration 83, loss = 0.34434767\n",
            "Iteration 84, loss = 0.34211445\n",
            "Iteration 85, loss = 0.34020769\n",
            "Iteration 86, loss = 0.33562255\n",
            "Iteration 87, loss = 0.35233421\n",
            "Iteration 88, loss = 0.35403068\n",
            "Iteration 89, loss = 0.33155615\n",
            "Iteration 90, loss = 0.33099091\n",
            "Iteration 91, loss = 0.34321134\n",
            "Iteration 92, loss = 0.36026249\n",
            "Iteration 93, loss = 0.34380828\n",
            "Iteration 94, loss = 0.33819456\n",
            "Iteration 95, loss = 0.32545556\n",
            "Iteration 96, loss = 0.33276232\n",
            "Iteration 97, loss = 0.31356681\n",
            "Iteration 98, loss = 0.31261329\n",
            "Iteration 99, loss = 0.32855263\n",
            "Iteration 100, loss = 0.33577788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35270657\n",
            "Iteration 2, loss = 1.26476778\n",
            "Iteration 3, loss = 1.17092683\n",
            "Iteration 4, loss = 1.07962846\n",
            "Iteration 5, loss = 0.99564264\n",
            "Iteration 6, loss = 0.95121570\n",
            "Iteration 7, loss = 0.89403599\n",
            "Iteration 8, loss = 0.85630551\n",
            "Iteration 9, loss = 0.82316105\n",
            "Iteration 10, loss = 0.77329827\n",
            "Iteration 11, loss = 0.76927516\n",
            "Iteration 12, loss = 0.75433920\n",
            "Iteration 13, loss = 0.76591950\n",
            "Iteration 14, loss = 0.72152282\n",
            "Iteration 15, loss = 0.68055956\n",
            "Iteration 16, loss = 0.67002048\n",
            "Iteration 17, loss = 0.66076764\n",
            "Iteration 18, loss = 0.65444014\n",
            "Iteration 19, loss = 0.65101354\n",
            "Iteration 20, loss = 0.64418015\n",
            "Iteration 21, loss = 0.63471402\n",
            "Iteration 22, loss = 0.61626220\n",
            "Iteration 23, loss = 0.63106853\n",
            "Iteration 24, loss = 0.61819643\n",
            "Iteration 25, loss = 0.63357825\n",
            "Iteration 26, loss = 0.59075639\n",
            "Iteration 27, loss = 0.58820260\n",
            "Iteration 28, loss = 0.56143136\n",
            "Iteration 29, loss = 0.56625548\n",
            "Iteration 30, loss = 0.57490875\n",
            "Iteration 31, loss = 0.55845231\n",
            "Iteration 32, loss = 0.55697758\n",
            "Iteration 33, loss = 0.58819526\n",
            "Iteration 34, loss = 0.55981396\n",
            "Iteration 35, loss = 0.54053613\n",
            "Iteration 36, loss = 0.54698608\n",
            "Iteration 37, loss = 0.53769072\n",
            "Iteration 38, loss = 0.52933192\n",
            "Iteration 39, loss = 0.52289119\n",
            "Iteration 40, loss = 0.51515051\n",
            "Iteration 41, loss = 0.50526589\n",
            "Iteration 42, loss = 0.49659395\n",
            "Iteration 43, loss = 0.57020457\n",
            "Iteration 44, loss = 0.56278426\n",
            "Iteration 45, loss = 0.51857453\n",
            "Iteration 46, loss = 0.50932952\n",
            "Iteration 47, loss = 0.49718558\n",
            "Iteration 48, loss = 0.49168028\n",
            "Iteration 49, loss = 0.47024676\n",
            "Iteration 50, loss = 0.46944891\n",
            "Iteration 51, loss = 0.45704022\n",
            "Iteration 52, loss = 0.46745093\n",
            "Iteration 53, loss = 0.47646062\n",
            "Iteration 54, loss = 0.44804814\n",
            "Iteration 55, loss = 0.43332859\n",
            "Iteration 56, loss = 0.42778076\n",
            "Iteration 57, loss = 0.43901477\n",
            "Iteration 58, loss = 0.42783700\n",
            "Iteration 59, loss = 0.44109599\n",
            "Iteration 60, loss = 0.45891679\n",
            "Iteration 61, loss = 0.45976812\n",
            "Iteration 62, loss = 0.41816788\n",
            "Iteration 63, loss = 0.41286480\n",
            "Iteration 64, loss = 0.40205129\n",
            "Iteration 65, loss = 0.41068019\n",
            "Iteration 66, loss = 0.42360270\n",
            "Iteration 67, loss = 0.45611198\n",
            "Iteration 68, loss = 0.45481901\n",
            "Iteration 69, loss = 0.44823410\n",
            "Iteration 70, loss = 0.41956188\n",
            "Iteration 71, loss = 0.43008115\n",
            "Iteration 72, loss = 0.41830038\n",
            "Iteration 73, loss = 0.39029871\n",
            "Iteration 74, loss = 0.39219665\n",
            "Iteration 75, loss = 0.37585952\n",
            "Iteration 76, loss = 0.37095183\n",
            "Iteration 77, loss = 0.37138301\n",
            "Iteration 78, loss = 0.38641665\n",
            "Iteration 79, loss = 0.38035404\n",
            "Iteration 80, loss = 0.38823821\n",
            "Iteration 81, loss = 0.38274017\n",
            "Iteration 82, loss = 0.35111477\n",
            "Iteration 83, loss = 0.33275259\n",
            "Iteration 84, loss = 0.33288682\n",
            "Iteration 85, loss = 0.32002735\n",
            "Iteration 86, loss = 0.32850914\n",
            "Iteration 87, loss = 0.32097328\n",
            "Iteration 88, loss = 0.32331789\n",
            "Iteration 89, loss = 0.30408161\n",
            "Iteration 90, loss = 0.31881697\n",
            "Iteration 91, loss = 0.32895439\n",
            "Iteration 92, loss = 0.32967139\n",
            "Iteration 93, loss = 0.33205575\n",
            "Iteration 94, loss = 0.32785327\n",
            "Iteration 95, loss = 0.30074409\n",
            "Iteration 96, loss = 0.28626694\n",
            "Iteration 97, loss = 0.29342889\n",
            "Iteration 98, loss = 0.28543432\n",
            "Iteration 99, loss = 0.30711342\n",
            "Iteration 100, loss = 0.36812417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35949319\n",
            "Iteration 2, loss = 1.27001970\n",
            "Iteration 3, loss = 1.18081555\n",
            "Iteration 4, loss = 1.08010288\n",
            "Iteration 5, loss = 1.00502239\n",
            "Iteration 6, loss = 0.93638451\n",
            "Iteration 7, loss = 0.86780335\n",
            "Iteration 8, loss = 0.82706709\n",
            "Iteration 9, loss = 0.79742750\n",
            "Iteration 10, loss = 0.77066333\n",
            "Iteration 11, loss = 0.76485160\n",
            "Iteration 12, loss = 0.73876062\n",
            "Iteration 13, loss = 0.72042053\n",
            "Iteration 14, loss = 0.70160146\n",
            "Iteration 15, loss = 0.68032140\n",
            "Iteration 16, loss = 0.65215288\n",
            "Iteration 17, loss = 0.65110399\n",
            "Iteration 18, loss = 0.63161098\n",
            "Iteration 19, loss = 0.62742023\n",
            "Iteration 20, loss = 0.63327050\n",
            "Iteration 21, loss = 0.61936191\n",
            "Iteration 22, loss = 0.58650073\n",
            "Iteration 23, loss = 0.58946513\n",
            "Iteration 24, loss = 0.58808200\n",
            "Iteration 25, loss = 0.60149597\n",
            "Iteration 26, loss = 0.56564734\n",
            "Iteration 27, loss = 0.55948588\n",
            "Iteration 28, loss = 0.56499796\n",
            "Iteration 29, loss = 0.55860931\n",
            "Iteration 30, loss = 0.54922419\n",
            "Iteration 31, loss = 0.54605367\n",
            "Iteration 32, loss = 0.53842913\n",
            "Iteration 33, loss = 0.54029120\n",
            "Iteration 34, loss = 0.52604959\n",
            "Iteration 35, loss = 0.55679425\n",
            "Iteration 36, loss = 0.57103574\n",
            "Iteration 37, loss = 0.52144992\n",
            "Iteration 38, loss = 0.50377606\n",
            "Iteration 39, loss = 0.50348950\n",
            "Iteration 40, loss = 0.49470853\n",
            "Iteration 41, loss = 0.49585123\n",
            "Iteration 42, loss = 0.49055807\n",
            "Iteration 43, loss = 0.52112194\n",
            "Iteration 44, loss = 0.52518697\n",
            "Iteration 45, loss = 0.50106903\n",
            "Iteration 46, loss = 0.48347807\n",
            "Iteration 47, loss = 0.47465388\n",
            "Iteration 48, loss = 0.45519502\n",
            "Iteration 49, loss = 0.45274063\n",
            "Iteration 50, loss = 0.44205449\n",
            "Iteration 51, loss = 0.44711707\n",
            "Iteration 52, loss = 0.46516656\n",
            "Iteration 53, loss = 0.47303389\n",
            "Iteration 54, loss = 0.44707650\n",
            "Iteration 55, loss = 0.43503045\n",
            "Iteration 56, loss = 0.43645328\n",
            "Iteration 57, loss = 0.44989692\n",
            "Iteration 58, loss = 0.42267396\n",
            "Iteration 59, loss = 0.43727926\n",
            "Iteration 60, loss = 0.45135032\n",
            "Iteration 61, loss = 0.44530902\n",
            "Iteration 62, loss = 0.41782128\n",
            "Iteration 63, loss = 0.41266006\n",
            "Iteration 64, loss = 0.41980485\n",
            "Iteration 65, loss = 0.40700595\n",
            "Iteration 66, loss = 0.44811686\n",
            "Iteration 67, loss = 0.46574588\n",
            "Iteration 68, loss = 0.45492764\n",
            "Iteration 69, loss = 0.42186616\n",
            "Iteration 70, loss = 0.39394989\n",
            "Iteration 71, loss = 0.39353454\n",
            "Iteration 72, loss = 0.38002195\n",
            "Iteration 73, loss = 0.37156982\n",
            "Iteration 74, loss = 0.38549782\n",
            "Iteration 75, loss = 0.38229921\n",
            "Iteration 76, loss = 0.36865623\n",
            "Iteration 77, loss = 0.36244433\n",
            "Iteration 78, loss = 0.37236139\n",
            "Iteration 79, loss = 0.37440428\n",
            "Iteration 80, loss = 0.35716920\n",
            "Iteration 81, loss = 0.35428492\n",
            "Iteration 82, loss = 0.34850429\n",
            "Iteration 83, loss = 0.34349399\n",
            "Iteration 84, loss = 0.32134387\n",
            "Iteration 85, loss = 0.32643090\n",
            "Iteration 86, loss = 0.31649355\n",
            "Iteration 87, loss = 0.31833250\n",
            "Iteration 88, loss = 0.34178577\n",
            "Iteration 89, loss = 0.35757291\n",
            "Iteration 90, loss = 0.35241277\n",
            "Iteration 91, loss = 0.38318301\n",
            "Iteration 92, loss = 0.32854089\n",
            "Iteration 93, loss = 0.30595501\n",
            "Iteration 94, loss = 0.30555664\n",
            "Iteration 95, loss = 0.29517561\n",
            "Iteration 96, loss = 0.29147469\n",
            "Iteration 97, loss = 0.30027418\n",
            "Iteration 98, loss = 0.28670329\n",
            "Iteration 99, loss = 0.31910002\n",
            "Iteration 100, loss = 0.38282762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38045875\n",
            "Iteration 2, loss = 1.31487153\n",
            "Iteration 3, loss = 1.25178336\n",
            "Iteration 4, loss = 1.17735392\n",
            "Iteration 5, loss = 1.12336431\n",
            "Iteration 6, loss = 1.06230298\n",
            "Iteration 7, loss = 1.00046504\n",
            "Iteration 8, loss = 0.94203642\n",
            "Iteration 9, loss = 0.89113093\n",
            "Iteration 10, loss = 0.85319899\n",
            "Iteration 11, loss = 0.82624930\n",
            "Iteration 12, loss = 0.81270333\n",
            "Iteration 13, loss = 0.78923559\n",
            "Iteration 14, loss = 0.77772361\n",
            "Iteration 15, loss = 0.74142741\n",
            "Iteration 16, loss = 0.72919980\n",
            "Iteration 17, loss = 0.71988803\n",
            "Iteration 18, loss = 0.69234829\n",
            "Iteration 19, loss = 0.68674336\n",
            "Iteration 20, loss = 0.69931621\n",
            "Iteration 21, loss = 0.69585438\n",
            "Iteration 22, loss = 0.65663585\n",
            "Iteration 23, loss = 0.66262227\n",
            "Iteration 24, loss = 0.66194483\n",
            "Iteration 25, loss = 0.65973748\n",
            "Iteration 26, loss = 0.63047865\n",
            "Iteration 27, loss = 0.63808385\n",
            "Iteration 28, loss = 0.60522246\n",
            "Iteration 29, loss = 0.59017124\n",
            "Iteration 30, loss = 0.61060759\n",
            "Iteration 31, loss = 0.59086906\n",
            "Iteration 32, loss = 0.59591531\n",
            "Iteration 33, loss = 0.58188883\n",
            "Iteration 34, loss = 0.56886891\n",
            "Iteration 35, loss = 0.56431981\n",
            "Iteration 36, loss = 0.60064525\n",
            "Iteration 37, loss = 0.57610994\n",
            "Iteration 38, loss = 0.55230600\n",
            "Iteration 39, loss = 0.55051358\n",
            "Iteration 40, loss = 0.53910641\n",
            "Iteration 41, loss = 0.53509852\n",
            "Iteration 42, loss = 0.51862390\n",
            "Iteration 43, loss = 0.53961489\n",
            "Iteration 44, loss = 0.53846972\n",
            "Iteration 45, loss = 0.51590476\n",
            "Iteration 46, loss = 0.51407343\n",
            "Iteration 47, loss = 0.51355409\n",
            "Iteration 48, loss = 0.49519426\n",
            "Iteration 49, loss = 0.50713092\n",
            "Iteration 50, loss = 0.49834571\n",
            "Iteration 51, loss = 0.50680462\n",
            "Iteration 52, loss = 0.52880617\n",
            "Iteration 53, loss = 0.51380868\n",
            "Iteration 54, loss = 0.47548984\n",
            "Iteration 55, loss = 0.46717330\n",
            "Iteration 56, loss = 0.47474673\n",
            "Iteration 57, loss = 0.46112338\n",
            "Iteration 58, loss = 0.44349192\n",
            "Iteration 59, loss = 0.44448771\n",
            "Iteration 60, loss = 0.47108278\n",
            "Iteration 61, loss = 0.48682698\n",
            "Iteration 62, loss = 0.45403805\n",
            "Iteration 63, loss = 0.45441963\n",
            "Iteration 64, loss = 0.43241662\n",
            "Iteration 65, loss = 0.44528241\n",
            "Iteration 66, loss = 0.43246038\n",
            "Iteration 67, loss = 0.51889555\n",
            "Iteration 68, loss = 0.50843883\n",
            "Iteration 69, loss = 0.45348610\n",
            "Iteration 70, loss = 0.42888296\n",
            "Iteration 71, loss = 0.41437355\n",
            "Iteration 72, loss = 0.41652923\n",
            "Iteration 73, loss = 0.39742386\n",
            "Iteration 74, loss = 0.39241148\n",
            "Iteration 75, loss = 0.40151733\n",
            "Iteration 76, loss = 0.37964882\n",
            "Iteration 77, loss = 0.37317091\n",
            "Iteration 78, loss = 0.40724035\n",
            "Iteration 79, loss = 0.41068089\n",
            "Iteration 80, loss = 0.38494360\n",
            "Iteration 81, loss = 0.37038499\n",
            "Iteration 82, loss = 0.36454928\n",
            "Iteration 83, loss = 0.36059342\n",
            "Iteration 84, loss = 0.35671343\n",
            "Iteration 85, loss = 0.35988073\n",
            "Iteration 86, loss = 0.33118030\n",
            "Iteration 87, loss = 0.36079920\n",
            "Iteration 88, loss = 0.40350350\n",
            "Iteration 89, loss = 0.37004644\n",
            "Iteration 90, loss = 0.35980547\n",
            "Iteration 91, loss = 0.38506762\n",
            "Iteration 92, loss = 0.36073543\n",
            "Iteration 93, loss = 0.32366652\n",
            "Iteration 94, loss = 0.31413040\n",
            "Iteration 95, loss = 0.31010245\n",
            "Iteration 96, loss = 0.30788882\n",
            "Iteration 97, loss = 0.35386742\n",
            "Iteration 98, loss = 0.30617392\n",
            "Iteration 99, loss = 0.33569653\n",
            "Iteration 100, loss = 0.38784942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38031337\n",
            "Iteration 2, loss = 1.32296326\n",
            "Iteration 3, loss = 1.27056751\n",
            "Iteration 4, loss = 1.20922987\n",
            "Iteration 5, loss = 1.15458073\n",
            "Iteration 6, loss = 1.10844276\n",
            "Iteration 7, loss = 1.04573065\n",
            "Iteration 8, loss = 1.00505575\n",
            "Iteration 9, loss = 0.95533975\n",
            "Iteration 10, loss = 0.90723272\n",
            "Iteration 11, loss = 0.88421889\n",
            "Iteration 12, loss = 0.87322920\n",
            "Iteration 13, loss = 0.83962384\n",
            "Iteration 14, loss = 0.82325994\n",
            "Iteration 15, loss = 0.78826448\n",
            "Iteration 16, loss = 0.76314385\n",
            "Iteration 17, loss = 0.74892265\n",
            "Iteration 18, loss = 0.73915989\n",
            "Iteration 19, loss = 0.73386710\n",
            "Iteration 20, loss = 0.73884060\n",
            "Iteration 21, loss = 0.72388270\n",
            "Iteration 22, loss = 0.70842280\n",
            "Iteration 23, loss = 0.71027260\n",
            "Iteration 24, loss = 0.69342758\n",
            "Iteration 25, loss = 0.68588382\n",
            "Iteration 26, loss = 0.68202505\n",
            "Iteration 27, loss = 0.69893565\n",
            "Iteration 28, loss = 0.64503073\n",
            "Iteration 29, loss = 0.63096162\n",
            "Iteration 30, loss = 0.65006210\n",
            "Iteration 31, loss = 0.63318505\n",
            "Iteration 32, loss = 0.62945206\n",
            "Iteration 33, loss = 0.62403931\n",
            "Iteration 34, loss = 0.62185231\n",
            "Iteration 35, loss = 0.60526121\n",
            "Iteration 36, loss = 0.63546638\n",
            "Iteration 37, loss = 0.60936368\n",
            "Iteration 38, loss = 0.60675488\n",
            "Iteration 39, loss = 0.58909769\n",
            "Iteration 40, loss = 0.58969069\n",
            "Iteration 41, loss = 0.58265346\n",
            "Iteration 42, loss = 0.56203870\n",
            "Iteration 43, loss = 0.57995492\n",
            "Iteration 44, loss = 0.57041825\n",
            "Iteration 45, loss = 0.54941831\n",
            "Iteration 46, loss = 0.54809817\n",
            "Iteration 47, loss = 0.54881494\n",
            "Iteration 48, loss = 0.53890979\n",
            "Iteration 49, loss = 0.53908678\n",
            "Iteration 50, loss = 0.53256762\n",
            "Iteration 51, loss = 0.54755872\n",
            "Iteration 52, loss = 0.55895597\n",
            "Iteration 53, loss = 0.53957985\n",
            "Iteration 54, loss = 0.51318315\n",
            "Iteration 55, loss = 0.53319681\n",
            "Iteration 56, loss = 0.54436274\n",
            "Iteration 57, loss = 0.52939421\n",
            "Iteration 58, loss = 0.50502490\n",
            "Iteration 59, loss = 0.49304511\n",
            "Iteration 60, loss = 0.50302906\n",
            "Iteration 61, loss = 0.51579602\n",
            "Iteration 62, loss = 0.46994674\n",
            "Iteration 63, loss = 0.46114364\n",
            "Iteration 64, loss = 0.46025188\n",
            "Iteration 65, loss = 0.47981047\n",
            "Iteration 66, loss = 0.46711440\n",
            "Iteration 67, loss = 0.52043078\n",
            "Iteration 68, loss = 0.51450340\n",
            "Iteration 69, loss = 0.47471667\n",
            "Iteration 70, loss = 0.46444441\n",
            "Iteration 71, loss = 0.46263977\n",
            "Iteration 72, loss = 0.48498673\n",
            "Iteration 73, loss = 0.48477991\n",
            "Iteration 74, loss = 0.43681617\n",
            "Iteration 75, loss = 0.44408501\n",
            "Iteration 76, loss = 0.41849129\n",
            "Iteration 77, loss = 0.41659495\n",
            "Iteration 78, loss = 0.41276462\n",
            "Iteration 79, loss = 0.42674788\n",
            "Iteration 80, loss = 0.43415606\n",
            "Iteration 81, loss = 0.42270891\n",
            "Iteration 82, loss = 0.40277358\n",
            "Iteration 83, loss = 0.40820352\n",
            "Iteration 84, loss = 0.39868029\n",
            "Iteration 85, loss = 0.38917722\n",
            "Iteration 86, loss = 0.38470861\n",
            "Iteration 87, loss = 0.41694244\n",
            "Iteration 88, loss = 0.40842837\n",
            "Iteration 89, loss = 0.38010670\n",
            "Iteration 90, loss = 0.38125139\n",
            "Iteration 91, loss = 0.41066079\n",
            "Iteration 92, loss = 0.36584789\n",
            "Iteration 93, loss = 0.35696986\n",
            "Iteration 94, loss = 0.37296790\n",
            "Iteration 95, loss = 0.35496458\n",
            "Iteration 96, loss = 0.34960797\n",
            "Iteration 97, loss = 0.34418096\n",
            "Iteration 98, loss = 0.32449849\n",
            "Iteration 99, loss = 0.38082072\n",
            "Iteration 100, loss = 0.42851437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37758078\n",
            "Iteration 2, loss = 1.31336491\n",
            "Iteration 3, loss = 1.25214108\n",
            "Iteration 4, loss = 1.19022469\n",
            "Iteration 5, loss = 1.13158269\n",
            "Iteration 6, loss = 1.07852507\n",
            "Iteration 7, loss = 1.00975716\n",
            "Iteration 8, loss = 0.95655065\n",
            "Iteration 9, loss = 0.91330944\n",
            "Iteration 10, loss = 0.86633741\n",
            "Iteration 11, loss = 0.83522162\n",
            "Iteration 12, loss = 0.83430955\n",
            "Iteration 13, loss = 0.80555333\n",
            "Iteration 14, loss = 0.79287522\n",
            "Iteration 15, loss = 0.75726790\n",
            "Iteration 16, loss = 0.73560135\n",
            "Iteration 17, loss = 0.73221606\n",
            "Iteration 18, loss = 0.72611363\n",
            "Iteration 19, loss = 0.71499704\n",
            "Iteration 20, loss = 0.70979142\n",
            "Iteration 21, loss = 0.67745359\n",
            "Iteration 22, loss = 0.65665903\n",
            "Iteration 23, loss = 0.66790101\n",
            "Iteration 24, loss = 0.66441030\n",
            "Iteration 25, loss = 0.64672824\n",
            "Iteration 26, loss = 0.65644530\n",
            "Iteration 27, loss = 0.65491906\n",
            "Iteration 28, loss = 0.62036968\n",
            "Iteration 29, loss = 0.59997839\n",
            "Iteration 30, loss = 0.62199949\n",
            "Iteration 31, loss = 0.62138411\n",
            "Iteration 32, loss = 0.61468312\n",
            "Iteration 33, loss = 0.63218123\n",
            "Iteration 34, loss = 0.60447244\n",
            "Iteration 35, loss = 0.59594607\n",
            "Iteration 36, loss = 0.59739846\n",
            "Iteration 37, loss = 0.59444996\n",
            "Iteration 38, loss = 0.57798195\n",
            "Iteration 39, loss = 0.54995793\n",
            "Iteration 40, loss = 0.56110861\n",
            "Iteration 41, loss = 0.56111731\n",
            "Iteration 42, loss = 0.55172407\n",
            "Iteration 43, loss = 0.61851045\n",
            "Iteration 44, loss = 0.61409954\n",
            "Iteration 45, loss = 0.57808299\n",
            "Iteration 46, loss = 0.55218866\n",
            "Iteration 47, loss = 0.54733002\n",
            "Iteration 48, loss = 0.54221664\n",
            "Iteration 49, loss = 0.52792578\n",
            "Iteration 50, loss = 0.51763288\n",
            "Iteration 51, loss = 0.50466251\n",
            "Iteration 52, loss = 0.49635888\n",
            "Iteration 53, loss = 0.50130896\n",
            "Iteration 54, loss = 0.48600963\n",
            "Iteration 55, loss = 0.50201678\n",
            "Iteration 56, loss = 0.50602429\n",
            "Iteration 57, loss = 0.48969528\n",
            "Iteration 58, loss = 0.46750867\n",
            "Iteration 59, loss = 0.47057009\n",
            "Iteration 60, loss = 0.48174029\n",
            "Iteration 61, loss = 0.51197187\n",
            "Iteration 62, loss = 0.47444772\n",
            "Iteration 63, loss = 0.46505020\n",
            "Iteration 64, loss = 0.44949345\n",
            "Iteration 65, loss = 0.45695441\n",
            "Iteration 66, loss = 0.45654661\n",
            "Iteration 67, loss = 0.44502219\n",
            "Iteration 68, loss = 0.45273623\n",
            "Iteration 69, loss = 0.46740637\n",
            "Iteration 70, loss = 0.45296077\n",
            "Iteration 71, loss = 0.45766894\n",
            "Iteration 72, loss = 0.45211497\n",
            "Iteration 73, loss = 0.45877340\n",
            "Iteration 74, loss = 0.43085938\n",
            "Iteration 75, loss = 0.40730774\n",
            "Iteration 76, loss = 0.41829344\n",
            "Iteration 77, loss = 0.41249191\n",
            "Iteration 78, loss = 0.41404442\n",
            "Iteration 79, loss = 0.41157608\n",
            "Iteration 80, loss = 0.42465170\n",
            "Iteration 81, loss = 0.44750211\n",
            "Iteration 82, loss = 0.42959099\n",
            "Iteration 83, loss = 0.40647046\n",
            "Iteration 84, loss = 0.38760970\n",
            "Iteration 85, loss = 0.37658709\n",
            "Iteration 86, loss = 0.37133998\n",
            "Iteration 87, loss = 0.37994726\n",
            "Iteration 88, loss = 0.42271380\n",
            "Iteration 89, loss = 0.39951331\n",
            "Iteration 90, loss = 0.38491399\n",
            "Iteration 91, loss = 0.39116749\n",
            "Iteration 92, loss = 0.35851377\n",
            "Iteration 93, loss = 0.35630253\n",
            "Iteration 94, loss = 0.34370975\n",
            "Iteration 95, loss = 0.34297944\n",
            "Iteration 96, loss = 0.34226801\n",
            "Iteration 97, loss = 0.34410596\n",
            "Iteration 98, loss = 0.33070778\n",
            "Iteration 99, loss = 0.36112582\n",
            "Iteration 100, loss = 0.35445151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35789932\n",
            "Iteration 2, loss = 1.30346795\n",
            "Iteration 3, loss = 1.24260643\n",
            "Iteration 4, loss = 1.16708719\n",
            "Iteration 5, loss = 1.07951093\n",
            "Iteration 6, loss = 0.99487437\n",
            "Iteration 7, loss = 0.91982655\n",
            "Iteration 8, loss = 0.85418235\n",
            "Iteration 9, loss = 0.80546618\n",
            "Iteration 10, loss = 0.78545809\n",
            "Iteration 11, loss = 0.76864968\n",
            "Iteration 12, loss = 0.77539753\n",
            "Iteration 13, loss = 0.74831169\n",
            "Iteration 14, loss = 0.72881966\n",
            "Iteration 15, loss = 0.69648648\n",
            "Iteration 16, loss = 0.68105863\n",
            "Iteration 17, loss = 0.66052164\n",
            "Iteration 18, loss = 0.65916000\n",
            "Iteration 19, loss = 0.66381734\n",
            "Iteration 20, loss = 0.65076824\n",
            "Iteration 21, loss = 0.63851029\n",
            "Iteration 22, loss = 0.63015202\n",
            "Iteration 23, loss = 0.60263999\n",
            "Iteration 24, loss = 0.59843961\n",
            "Iteration 25, loss = 0.59187732\n",
            "Iteration 26, loss = 0.59903854\n",
            "Iteration 27, loss = 0.59268646\n",
            "Iteration 28, loss = 0.56652813\n",
            "Iteration 29, loss = 0.55077992\n",
            "Iteration 30, loss = 0.55986088\n",
            "Iteration 31, loss = 0.55789880\n",
            "Iteration 32, loss = 0.56519793\n",
            "Iteration 33, loss = 0.56811193\n",
            "Iteration 34, loss = 0.56811122\n",
            "Iteration 35, loss = 0.57070913\n",
            "Iteration 36, loss = 0.55111257\n",
            "Iteration 37, loss = 0.54951556\n",
            "Iteration 38, loss = 0.53630257\n",
            "Iteration 39, loss = 0.51632056\n",
            "Iteration 40, loss = 0.53902920\n",
            "Iteration 41, loss = 0.52264260\n",
            "Iteration 42, loss = 0.49642405\n",
            "Iteration 43, loss = 0.50754895\n",
            "Iteration 44, loss = 0.50447494\n",
            "Iteration 45, loss = 0.49185488\n",
            "Iteration 46, loss = 0.48810213\n",
            "Iteration 47, loss = 0.49325500\n",
            "Iteration 48, loss = 0.47454866\n",
            "Iteration 49, loss = 0.48187538\n",
            "Iteration 50, loss = 0.47313598\n",
            "Iteration 51, loss = 0.48211379\n",
            "Iteration 52, loss = 0.46113112\n",
            "Iteration 53, loss = 0.46218163\n",
            "Iteration 54, loss = 0.44911712\n",
            "Iteration 55, loss = 0.45617708\n",
            "Iteration 56, loss = 0.45726932\n",
            "Iteration 57, loss = 0.48338259\n",
            "Iteration 58, loss = 0.45860807\n",
            "Iteration 59, loss = 0.45075876\n",
            "Iteration 60, loss = 0.47658655\n",
            "Iteration 61, loss = 0.51430237\n",
            "Iteration 62, loss = 0.47863395\n",
            "Iteration 63, loss = 0.46782751\n",
            "Iteration 64, loss = 0.46472767\n",
            "Iteration 65, loss = 0.44057715\n",
            "Iteration 66, loss = 0.44583827\n",
            "Iteration 67, loss = 0.40892784\n",
            "Iteration 68, loss = 0.40923927\n",
            "Iteration 69, loss = 0.42718217\n",
            "Iteration 70, loss = 0.41000170\n",
            "Iteration 71, loss = 0.41316168\n",
            "Iteration 72, loss = 0.44022391\n",
            "Iteration 73, loss = 0.43181535\n",
            "Iteration 74, loss = 0.40749302\n",
            "Iteration 75, loss = 0.40605310\n",
            "Iteration 76, loss = 0.38275231\n",
            "Iteration 77, loss = 0.38069857\n",
            "Iteration 78, loss = 0.37498774\n",
            "Iteration 79, loss = 0.38683338\n",
            "Iteration 80, loss = 0.37106560\n",
            "Iteration 81, loss = 0.36852475\n",
            "Iteration 82, loss = 0.36830037\n",
            "Iteration 83, loss = 0.36208804\n",
            "Iteration 84, loss = 0.35599464\n",
            "Iteration 85, loss = 0.36355762\n",
            "Iteration 86, loss = 0.35350978\n",
            "Iteration 87, loss = 0.34185638\n",
            "Iteration 88, loss = 0.37838637\n",
            "Iteration 89, loss = 0.37611121\n",
            "Iteration 90, loss = 0.36491489\n",
            "Iteration 91, loss = 0.34594330\n",
            "Iteration 92, loss = 0.31988368\n",
            "Iteration 93, loss = 0.31529288\n",
            "Iteration 94, loss = 0.31972139\n",
            "Iteration 95, loss = 0.30712873\n",
            "Iteration 96, loss = 0.30845398\n",
            "Iteration 97, loss = 0.33117337\n",
            "Iteration 98, loss = 0.31812199\n",
            "Iteration 99, loss = 0.31940027\n",
            "Iteration 100, loss = 0.29217341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33133486\n",
            "Iteration 2, loss = 1.25292888\n",
            "Iteration 3, loss = 1.19899012\n",
            "Iteration 4, loss = 1.10890835\n",
            "Iteration 5, loss = 1.04143055\n",
            "Iteration 6, loss = 0.96990889\n",
            "Iteration 7, loss = 0.90481853\n",
            "Iteration 8, loss = 0.85200358\n",
            "Iteration 9, loss = 0.82129294\n",
            "Iteration 10, loss = 0.79197596\n",
            "Iteration 11, loss = 0.76579134\n",
            "Iteration 12, loss = 0.76749863\n",
            "Iteration 13, loss = 0.74917208\n",
            "Iteration 14, loss = 0.71835239\n",
            "Iteration 15, loss = 0.71477537\n",
            "Iteration 16, loss = 0.68516055\n",
            "Iteration 17, loss = 0.67298989\n",
            "Iteration 18, loss = 0.67877122\n",
            "Iteration 19, loss = 0.66619415\n",
            "Iteration 20, loss = 0.67089226\n",
            "Iteration 21, loss = 0.64025263\n",
            "Iteration 22, loss = 0.66442846\n",
            "Iteration 23, loss = 0.63839651\n",
            "Iteration 24, loss = 0.61188559\n",
            "Iteration 25, loss = 0.60511202\n",
            "Iteration 26, loss = 0.61032475\n",
            "Iteration 27, loss = 0.61896003\n",
            "Iteration 28, loss = 0.58707757\n",
            "Iteration 29, loss = 0.57880595\n",
            "Iteration 30, loss = 0.56876475\n",
            "Iteration 31, loss = 0.58899280\n",
            "Iteration 32, loss = 0.58724551\n",
            "Iteration 33, loss = 0.56961037\n",
            "Iteration 34, loss = 0.56191160\n",
            "Iteration 35, loss = 0.53848569\n",
            "Iteration 36, loss = 0.52587210\n",
            "Iteration 37, loss = 0.54203599\n",
            "Iteration 38, loss = 0.54158560\n",
            "Iteration 39, loss = 0.51804254\n",
            "Iteration 40, loss = 0.51561524\n",
            "Iteration 41, loss = 0.53936126\n",
            "Iteration 42, loss = 0.51558507\n",
            "Iteration 43, loss = 0.50394434\n",
            "Iteration 44, loss = 0.50059795\n",
            "Iteration 45, loss = 0.49193893\n",
            "Iteration 46, loss = 0.49754404\n",
            "Iteration 47, loss = 0.50973618\n",
            "Iteration 48, loss = 0.48269693\n",
            "Iteration 49, loss = 0.48853450\n",
            "Iteration 50, loss = 0.47772223\n",
            "Iteration 51, loss = 0.48582578\n",
            "Iteration 52, loss = 0.46824837\n",
            "Iteration 53, loss = 0.46082488\n",
            "Iteration 54, loss = 0.44557983\n",
            "Iteration 55, loss = 0.44135808\n",
            "Iteration 56, loss = 0.45185133\n",
            "Iteration 57, loss = 0.45518526\n",
            "Iteration 58, loss = 0.43733048\n",
            "Iteration 59, loss = 0.43673670\n",
            "Iteration 60, loss = 0.47278493\n",
            "Iteration 61, loss = 0.49942443\n",
            "Iteration 62, loss = 0.47698246\n",
            "Iteration 63, loss = 0.45896586\n",
            "Iteration 64, loss = 0.43724960\n",
            "Iteration 65, loss = 0.42837524\n",
            "Iteration 66, loss = 0.42326082\n",
            "Iteration 67, loss = 0.47136685\n",
            "Iteration 68, loss = 0.41639981\n",
            "Iteration 69, loss = 0.41683081\n",
            "Iteration 70, loss = 0.40834291\n",
            "Iteration 71, loss = 0.40369705\n",
            "Iteration 72, loss = 0.41992213\n",
            "Iteration 73, loss = 0.43200555\n",
            "Iteration 74, loss = 0.39778838\n",
            "Iteration 75, loss = 0.38459559\n",
            "Iteration 76, loss = 0.36813869\n",
            "Iteration 77, loss = 0.37048565\n",
            "Iteration 78, loss = 0.37311427\n",
            "Iteration 79, loss = 0.40606678\n",
            "Iteration 80, loss = 0.37934086\n",
            "Iteration 81, loss = 0.36192854\n",
            "Iteration 82, loss = 0.34615283\n",
            "Iteration 83, loss = 0.34524034\n",
            "Iteration 84, loss = 0.33978792\n",
            "Iteration 85, loss = 0.32967239\n",
            "Iteration 86, loss = 0.33071141\n",
            "Iteration 87, loss = 0.33040965\n",
            "Iteration 88, loss = 0.34753264\n",
            "Iteration 89, loss = 0.33885712\n",
            "Iteration 90, loss = 0.32320075\n",
            "Iteration 91, loss = 0.33621349\n",
            "Iteration 92, loss = 0.34084912\n",
            "Iteration 93, loss = 0.32563437\n",
            "Iteration 94, loss = 0.30947060\n",
            "Iteration 95, loss = 0.29283850\n",
            "Iteration 96, loss = 0.30143611\n",
            "Iteration 97, loss = 0.29573458\n",
            "Iteration 98, loss = 0.29445890\n",
            "Iteration 99, loss = 0.33509490\n",
            "Iteration 100, loss = 0.32292348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "Iteration 1, loss = 1.38520787\n",
            "Iteration 2, loss = 1.35936051\n",
            "Iteration 3, loss = 1.34019546\n",
            "Iteration 4, loss = 1.33682326\n",
            "Iteration 5, loss = 1.32643307\n",
            "Iteration 6, loss = 1.31050474\n",
            "Iteration 7, loss = 1.30557835\n",
            "Iteration 8, loss = 1.28247678\n",
            "Iteration 9, loss = 1.27671837\n",
            "Iteration 10, loss = 1.25394191\n",
            "Iteration 11, loss = 1.23647754\n",
            "Iteration 12, loss = 1.21454796\n",
            "Iteration 13, loss = 1.21856240\n",
            "Iteration 14, loss = 1.18050512\n",
            "Iteration 15, loss = 1.19401488\n",
            "Iteration 16, loss = 1.20957962\n",
            "Iteration 17, loss = 1.18104332\n",
            "Iteration 18, loss = 1.15333579\n",
            "Iteration 19, loss = 1.16175208\n",
            "Iteration 20, loss = 1.14133040\n",
            "Iteration 21, loss = 1.12477963\n",
            "Iteration 22, loss = 1.10365409\n",
            "Iteration 23, loss = 1.09132105\n",
            "Iteration 24, loss = 1.09687546\n",
            "Iteration 25, loss = 1.09247568\n",
            "Iteration 26, loss = 1.05939322\n",
            "Iteration 27, loss = 1.05752660\n",
            "Iteration 28, loss = 1.04672087\n",
            "Iteration 29, loss = 1.02919648\n",
            "Iteration 30, loss = 1.02296556\n",
            "Iteration 31, loss = 1.02402310\n",
            "Iteration 32, loss = 1.05508106\n",
            "Iteration 33, loss = 1.08678808\n",
            "Iteration 34, loss = 1.04531599\n",
            "Iteration 35, loss = 1.00273374\n",
            "Iteration 36, loss = 0.97866689\n",
            "Iteration 37, loss = 0.99636595\n",
            "Iteration 38, loss = 0.96846143\n",
            "Iteration 39, loss = 0.96644236\n",
            "Iteration 40, loss = 0.96319960\n",
            "Iteration 41, loss = 0.94849626\n",
            "Iteration 42, loss = 0.93127133\n",
            "Iteration 43, loss = 0.92124962\n",
            "Iteration 44, loss = 0.94393982\n",
            "Iteration 45, loss = 0.93675303\n",
            "Iteration 46, loss = 0.91094693\n",
            "Iteration 47, loss = 0.89389253\n",
            "Iteration 48, loss = 0.90526372\n",
            "Iteration 49, loss = 0.87705174\n",
            "Iteration 50, loss = 0.89629992\n",
            "Iteration 51, loss = 0.91068223\n",
            "Iteration 52, loss = 0.86144378\n",
            "Iteration 53, loss = 0.86685580\n",
            "Iteration 54, loss = 0.92083475\n",
            "Iteration 55, loss = 0.98590034\n",
            "Iteration 56, loss = 0.91162761\n",
            "Iteration 57, loss = 0.85397375\n",
            "Iteration 58, loss = 0.82169283\n",
            "Iteration 59, loss = 0.81633010\n",
            "Iteration 60, loss = 0.78718084\n",
            "Iteration 61, loss = 0.78160787\n",
            "Iteration 62, loss = 0.78117239\n",
            "Iteration 63, loss = 0.77072586\n",
            "Iteration 64, loss = 0.75922284\n",
            "Iteration 65, loss = 0.78221616\n",
            "Iteration 66, loss = 0.78006852\n",
            "Iteration 67, loss = 0.79020385\n",
            "Iteration 68, loss = 0.73720498\n",
            "Iteration 69, loss = 0.74312226\n",
            "Iteration 70, loss = 0.78285291\n",
            "Iteration 71, loss = 0.80102388\n",
            "Iteration 72, loss = 0.75373515\n",
            "Iteration 73, loss = 0.72337767\n",
            "Iteration 74, loss = 0.71280805\n",
            "Iteration 75, loss = 0.71127492\n",
            "Iteration 76, loss = 0.71966158\n",
            "Iteration 77, loss = 0.72385444\n",
            "Iteration 78, loss = 0.70710902\n",
            "Iteration 79, loss = 0.67937004\n",
            "Iteration 80, loss = 0.67047759\n",
            "Iteration 81, loss = 0.66458841\n",
            "Iteration 82, loss = 0.67633188\n",
            "Iteration 83, loss = 0.70332205\n",
            "Iteration 84, loss = 0.70715136\n",
            "Iteration 85, loss = 0.70926478\n",
            "Iteration 86, loss = 0.69325475\n",
            "Iteration 87, loss = 0.75065742\n",
            "Iteration 88, loss = 0.69460929\n",
            "Iteration 89, loss = 0.65336776\n",
            "Iteration 90, loss = 0.64369186\n",
            "Iteration 91, loss = 0.69142388\n",
            "Iteration 92, loss = 0.72449284\n",
            "Iteration 93, loss = 0.74317140\n",
            "Iteration 94, loss = 0.71322694\n",
            "Iteration 95, loss = 0.65112618\n",
            "Iteration 96, loss = 0.64219387\n",
            "Iteration 97, loss = 0.62518341\n",
            "Iteration 98, loss = 0.62277362\n",
            "Iteration 99, loss = 0.60048852\n",
            "Iteration 100, loss = 0.63916892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38553819\n",
            "Iteration 2, loss = 1.36441934\n",
            "Iteration 3, loss = 1.34960101\n",
            "Iteration 4, loss = 1.34530682\n",
            "Iteration 5, loss = 1.32629933\n",
            "Iteration 6, loss = 1.31410728\n",
            "Iteration 7, loss = 1.30642036\n",
            "Iteration 8, loss = 1.28977324\n",
            "Iteration 9, loss = 1.28069683\n",
            "Iteration 10, loss = 1.25392778\n",
            "Iteration 11, loss = 1.23152057\n",
            "Iteration 12, loss = 1.21054071\n",
            "Iteration 13, loss = 1.21404762\n",
            "Iteration 14, loss = 1.17997632\n",
            "Iteration 15, loss = 1.19665832\n",
            "Iteration 16, loss = 1.20416193\n",
            "Iteration 17, loss = 1.16444616\n",
            "Iteration 18, loss = 1.15438788\n",
            "Iteration 19, loss = 1.12653809\n",
            "Iteration 20, loss = 1.12983077\n",
            "Iteration 21, loss = 1.10191797\n",
            "Iteration 22, loss = 1.07656071\n",
            "Iteration 23, loss = 1.07360393\n",
            "Iteration 24, loss = 1.07724383\n",
            "Iteration 25, loss = 1.07444966\n",
            "Iteration 26, loss = 1.10407902\n",
            "Iteration 27, loss = 1.05881105\n",
            "Iteration 28, loss = 1.03081228\n",
            "Iteration 29, loss = 1.01240472\n",
            "Iteration 30, loss = 1.02050021\n",
            "Iteration 31, loss = 1.02877918\n",
            "Iteration 32, loss = 1.03260425\n",
            "Iteration 33, loss = 1.02546598\n",
            "Iteration 34, loss = 1.00003888\n",
            "Iteration 35, loss = 0.98591595\n",
            "Iteration 36, loss = 0.96520911\n",
            "Iteration 37, loss = 0.95889053\n",
            "Iteration 38, loss = 0.93653923\n",
            "Iteration 39, loss = 0.95082185\n",
            "Iteration 40, loss = 0.94662254\n",
            "Iteration 41, loss = 0.95422719\n",
            "Iteration 42, loss = 0.91892352\n",
            "Iteration 43, loss = 0.90714514\n",
            "Iteration 44, loss = 0.89187706\n",
            "Iteration 45, loss = 0.89263624\n",
            "Iteration 46, loss = 0.87564651\n",
            "Iteration 47, loss = 0.87328025\n",
            "Iteration 48, loss = 0.87357284\n",
            "Iteration 49, loss = 0.84315226\n",
            "Iteration 50, loss = 0.84227887\n",
            "Iteration 51, loss = 0.85825191\n",
            "Iteration 52, loss = 0.84587358\n",
            "Iteration 53, loss = 0.88059706\n",
            "Iteration 54, loss = 0.88118397\n",
            "Iteration 55, loss = 0.90360733\n",
            "Iteration 56, loss = 0.87140095\n",
            "Iteration 57, loss = 0.81314052\n",
            "Iteration 58, loss = 0.80646072\n",
            "Iteration 59, loss = 0.79247254\n",
            "Iteration 60, loss = 0.78412018\n",
            "Iteration 61, loss = 0.78380762\n",
            "Iteration 62, loss = 0.77504913\n",
            "Iteration 63, loss = 0.78222553\n",
            "Iteration 64, loss = 0.76218394\n",
            "Iteration 65, loss = 0.80115435\n",
            "Iteration 66, loss = 0.78946170\n",
            "Iteration 67, loss = 0.81958742\n",
            "Iteration 68, loss = 0.79537013\n",
            "Iteration 69, loss = 0.76622825\n",
            "Iteration 70, loss = 0.74852691\n",
            "Iteration 71, loss = 0.75794109\n",
            "Iteration 72, loss = 0.75452739\n",
            "Iteration 73, loss = 0.73569051\n",
            "Iteration 74, loss = 0.72022865\n",
            "Iteration 75, loss = 0.71059301\n",
            "Iteration 76, loss = 0.69673898\n",
            "Iteration 77, loss = 0.69525925\n",
            "Iteration 78, loss = 0.68208492\n",
            "Iteration 79, loss = 0.68741014\n",
            "Iteration 80, loss = 0.68589512\n",
            "Iteration 81, loss = 0.67246309\n",
            "Iteration 82, loss = 0.66242737\n",
            "Iteration 83, loss = 0.70329122\n",
            "Iteration 84, loss = 0.68794981\n",
            "Iteration 85, loss = 0.72569063\n",
            "Iteration 86, loss = 0.74589981\n",
            "Iteration 87, loss = 0.71324746\n",
            "Iteration 88, loss = 0.73429691\n",
            "Iteration 89, loss = 0.67590646\n",
            "Iteration 90, loss = 0.63102544\n",
            "Iteration 91, loss = 0.67075515\n",
            "Iteration 92, loss = 0.62600739\n",
            "Iteration 93, loss = 0.63207578\n",
            "Iteration 94, loss = 0.64756593\n",
            "Iteration 95, loss = 0.66691299\n",
            "Iteration 96, loss = 0.75485544\n",
            "Iteration 97, loss = 0.64410616\n",
            "Iteration 98, loss = 0.61946353\n",
            "Iteration 99, loss = 0.61163150\n",
            "Iteration 100, loss = 0.59709122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38331735\n",
            "Iteration 2, loss = 1.34863551\n",
            "Iteration 3, loss = 1.33957225\n",
            "Iteration 4, loss = 1.33591474\n",
            "Iteration 5, loss = 1.32797189\n",
            "Iteration 6, loss = 1.31392179\n",
            "Iteration 7, loss = 1.30858484\n",
            "Iteration 8, loss = 1.30982817\n",
            "Iteration 9, loss = 1.29300695\n",
            "Iteration 10, loss = 1.28695194\n",
            "Iteration 11, loss = 1.27011193\n",
            "Iteration 12, loss = 1.25371169\n",
            "Iteration 13, loss = 1.24828862\n",
            "Iteration 14, loss = 1.24007410\n",
            "Iteration 15, loss = 1.24698871\n",
            "Iteration 16, loss = 1.21768384\n",
            "Iteration 17, loss = 1.20884256\n",
            "Iteration 18, loss = 1.20106380\n",
            "Iteration 19, loss = 1.19953134\n",
            "Iteration 20, loss = 1.17787962\n",
            "Iteration 21, loss = 1.14041919\n",
            "Iteration 22, loss = 1.12070230\n",
            "Iteration 23, loss = 1.10478609\n",
            "Iteration 24, loss = 1.10134085\n",
            "Iteration 25, loss = 1.10360731\n",
            "Iteration 26, loss = 1.10368813\n",
            "Iteration 27, loss = 1.07656968\n",
            "Iteration 28, loss = 1.04570862\n",
            "Iteration 29, loss = 1.02496732\n",
            "Iteration 30, loss = 1.02154351\n",
            "Iteration 31, loss = 1.02522128\n",
            "Iteration 32, loss = 1.02311405\n",
            "Iteration 33, loss = 1.03290418\n",
            "Iteration 34, loss = 1.00472141\n",
            "Iteration 35, loss = 0.99051733\n",
            "Iteration 36, loss = 1.00203174\n",
            "Iteration 37, loss = 0.98477948\n",
            "Iteration 38, loss = 0.96557400\n",
            "Iteration 39, loss = 0.97781322\n",
            "Iteration 40, loss = 0.97165373\n",
            "Iteration 41, loss = 0.96162286\n",
            "Iteration 42, loss = 0.93171880\n",
            "Iteration 43, loss = 0.91354029\n",
            "Iteration 44, loss = 0.93373062\n",
            "Iteration 45, loss = 0.92324150\n",
            "Iteration 46, loss = 0.94329818\n",
            "Iteration 47, loss = 0.93727119\n",
            "Iteration 48, loss = 0.91381001\n",
            "Iteration 49, loss = 0.86764740\n",
            "Iteration 50, loss = 0.89181615\n",
            "Iteration 51, loss = 0.89904807\n",
            "Iteration 52, loss = 0.86357584\n",
            "Iteration 53, loss = 0.89907813\n",
            "Iteration 54, loss = 0.94802434\n",
            "Iteration 55, loss = 0.87933942\n",
            "Iteration 56, loss = 0.85088640\n",
            "Iteration 57, loss = 0.84264642\n",
            "Iteration 58, loss = 0.85685094\n",
            "Iteration 59, loss = 0.84276185\n",
            "Iteration 60, loss = 0.82002293\n",
            "Iteration 61, loss = 0.84025545\n",
            "Iteration 62, loss = 0.88354877\n",
            "Iteration 63, loss = 0.82287924\n",
            "Iteration 64, loss = 0.80392057\n",
            "Iteration 65, loss = 0.82527736\n",
            "Iteration 66, loss = 0.80290391\n",
            "Iteration 67, loss = 0.83915078\n",
            "Iteration 68, loss = 0.80220623\n",
            "Iteration 69, loss = 0.77346768\n",
            "Iteration 70, loss = 0.77651961\n",
            "Iteration 71, loss = 0.76048700\n",
            "Iteration 72, loss = 0.76068587\n",
            "Iteration 73, loss = 0.76311629\n",
            "Iteration 74, loss = 0.73416334\n",
            "Iteration 75, loss = 0.74372828\n",
            "Iteration 76, loss = 0.75797079\n",
            "Iteration 77, loss = 0.74347569\n",
            "Iteration 78, loss = 0.71733258\n",
            "Iteration 79, loss = 0.75007215\n",
            "Iteration 80, loss = 0.72501191\n",
            "Iteration 81, loss = 0.73365421\n",
            "Iteration 82, loss = 0.75438288\n",
            "Iteration 83, loss = 0.73048887\n",
            "Iteration 84, loss = 0.72230398\n",
            "Iteration 85, loss = 0.74811005\n",
            "Iteration 86, loss = 0.70983312\n",
            "Iteration 87, loss = 0.75369370\n",
            "Iteration 88, loss = 0.72206403\n",
            "Iteration 89, loss = 0.68131648\n",
            "Iteration 90, loss = 0.66380227\n",
            "Iteration 91, loss = 0.72549335\n",
            "Iteration 92, loss = 0.68436672\n",
            "Iteration 93, loss = 0.73097804\n",
            "Iteration 94, loss = 0.69837333\n",
            "Iteration 95, loss = 0.71106684\n",
            "Iteration 96, loss = 0.76113233\n",
            "Iteration 97, loss = 0.68494567\n",
            "Iteration 98, loss = 0.66957557\n",
            "Iteration 99, loss = 0.65700296\n",
            "Iteration 100, loss = 0.66447188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39171552\n",
            "Iteration 2, loss = 1.35965898\n",
            "Iteration 3, loss = 1.35046131\n",
            "Iteration 4, loss = 1.34330769\n",
            "Iteration 5, loss = 1.33465256\n",
            "Iteration 6, loss = 1.31305475\n",
            "Iteration 7, loss = 1.31282223\n",
            "Iteration 8, loss = 1.31171147\n",
            "Iteration 9, loss = 1.28710998\n",
            "Iteration 10, loss = 1.27765831\n",
            "Iteration 11, loss = 1.26312941\n",
            "Iteration 12, loss = 1.25320642\n",
            "Iteration 13, loss = 1.22802078\n",
            "Iteration 14, loss = 1.21066920\n",
            "Iteration 15, loss = 1.20950024\n",
            "Iteration 16, loss = 1.21507818\n",
            "Iteration 17, loss = 1.18141807\n",
            "Iteration 18, loss = 1.17530134\n",
            "Iteration 19, loss = 1.17412514\n",
            "Iteration 20, loss = 1.13860791\n",
            "Iteration 21, loss = 1.12404596\n",
            "Iteration 22, loss = 1.11023818\n",
            "Iteration 23, loss = 1.10421374\n",
            "Iteration 24, loss = 1.11332518\n",
            "Iteration 25, loss = 1.08550076\n",
            "Iteration 26, loss = 1.06142512\n",
            "Iteration 27, loss = 1.04864397\n",
            "Iteration 28, loss = 1.04745920\n",
            "Iteration 29, loss = 1.01873341\n",
            "Iteration 30, loss = 1.02691719\n",
            "Iteration 31, loss = 1.02685659\n",
            "Iteration 32, loss = 1.03804318\n",
            "Iteration 33, loss = 1.03156710\n",
            "Iteration 34, loss = 1.00957097\n",
            "Iteration 35, loss = 0.98361791\n",
            "Iteration 36, loss = 0.98270274\n",
            "Iteration 37, loss = 0.96819096\n",
            "Iteration 38, loss = 0.95483427\n",
            "Iteration 39, loss = 0.95703472\n",
            "Iteration 40, loss = 0.95323112\n",
            "Iteration 41, loss = 0.98851233\n",
            "Iteration 42, loss = 0.94960520\n",
            "Iteration 43, loss = 0.92213664\n",
            "Iteration 44, loss = 0.92406394\n",
            "Iteration 45, loss = 0.90246089\n",
            "Iteration 46, loss = 0.88903137\n",
            "Iteration 47, loss = 0.91870180\n",
            "Iteration 48, loss = 0.88090566\n",
            "Iteration 49, loss = 0.89756855\n",
            "Iteration 50, loss = 0.87506936\n",
            "Iteration 51, loss = 0.88960136\n",
            "Iteration 52, loss = 0.83410846\n",
            "Iteration 53, loss = 0.84235391\n",
            "Iteration 54, loss = 0.87044655\n",
            "Iteration 55, loss = 0.92701381\n",
            "Iteration 56, loss = 0.90229407\n",
            "Iteration 57, loss = 0.83377959\n",
            "Iteration 58, loss = 0.81370227\n",
            "Iteration 59, loss = 0.80631319\n",
            "Iteration 60, loss = 0.79664385\n",
            "Iteration 61, loss = 0.80154125\n",
            "Iteration 62, loss = 0.78362656\n",
            "Iteration 63, loss = 0.78996640\n",
            "Iteration 64, loss = 0.76874831\n",
            "Iteration 65, loss = 0.79149624\n",
            "Iteration 66, loss = 0.76601099\n",
            "Iteration 67, loss = 0.77501983\n",
            "Iteration 68, loss = 0.77666012\n",
            "Iteration 69, loss = 0.76500873\n",
            "Iteration 70, loss = 0.75645800\n",
            "Iteration 71, loss = 0.77292499\n",
            "Iteration 72, loss = 0.79176244\n",
            "Iteration 73, loss = 0.74337846\n",
            "Iteration 74, loss = 0.71399131\n",
            "Iteration 75, loss = 0.71589442\n",
            "Iteration 76, loss = 0.70223380\n",
            "Iteration 77, loss = 0.72371532\n",
            "Iteration 78, loss = 0.72417515\n",
            "Iteration 79, loss = 0.75705687\n",
            "Iteration 80, loss = 0.70471670\n",
            "Iteration 81, loss = 0.68892446\n",
            "Iteration 82, loss = 0.73832884\n",
            "Iteration 83, loss = 0.72005971\n",
            "Iteration 84, loss = 0.69864242\n",
            "Iteration 85, loss = 0.71110661\n",
            "Iteration 86, loss = 0.71224054\n",
            "Iteration 87, loss = 0.67192904\n",
            "Iteration 88, loss = 0.65535162\n",
            "Iteration 89, loss = 0.63901410\n",
            "Iteration 90, loss = 0.64824574\n",
            "Iteration 91, loss = 0.67126500\n",
            "Iteration 92, loss = 0.65740558\n",
            "Iteration 93, loss = 0.61893808\n",
            "Iteration 94, loss = 0.64743273\n",
            "Iteration 95, loss = 0.71669463\n",
            "Iteration 96, loss = 0.77975492\n",
            "Iteration 97, loss = 0.71206488\n",
            "Iteration 98, loss = 0.68416614\n",
            "Iteration 99, loss = 0.64364179\n",
            "Iteration 100, loss = 0.61209289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38437718\n",
            "Iteration 2, loss = 1.35717357\n",
            "Iteration 3, loss = 1.34685486\n",
            "Iteration 4, loss = 1.34218630\n",
            "Iteration 5, loss = 1.33506346\n",
            "Iteration 6, loss = 1.32735565\n",
            "Iteration 7, loss = 1.32510661\n",
            "Iteration 8, loss = 1.31459329\n",
            "Iteration 9, loss = 1.30185403\n",
            "Iteration 10, loss = 1.27864697\n",
            "Iteration 11, loss = 1.27143362\n",
            "Iteration 12, loss = 1.26457728\n",
            "Iteration 13, loss = 1.25472900\n",
            "Iteration 14, loss = 1.23000176\n",
            "Iteration 15, loss = 1.22465512\n",
            "Iteration 16, loss = 1.20413278\n",
            "Iteration 17, loss = 1.19210804\n",
            "Iteration 18, loss = 1.20361136\n",
            "Iteration 19, loss = 1.20449546\n",
            "Iteration 20, loss = 1.16681515\n",
            "Iteration 21, loss = 1.14503016\n",
            "Iteration 22, loss = 1.14122943\n",
            "Iteration 23, loss = 1.13911860\n",
            "Iteration 24, loss = 1.11123143\n",
            "Iteration 25, loss = 1.09744746\n",
            "Iteration 26, loss = 1.10459259\n",
            "Iteration 27, loss = 1.10723289\n",
            "Iteration 28, loss = 1.09123961\n",
            "Iteration 29, loss = 1.06253292\n",
            "Iteration 30, loss = 1.05069086\n",
            "Iteration 31, loss = 1.05243999\n",
            "Iteration 32, loss = 1.04787354\n",
            "Iteration 33, loss = 1.04024558\n",
            "Iteration 34, loss = 1.02948577\n",
            "Iteration 35, loss = 1.04413106\n",
            "Iteration 36, loss = 1.02511927\n",
            "Iteration 37, loss = 0.99084320\n",
            "Iteration 38, loss = 0.97891509\n",
            "Iteration 39, loss = 0.98389348\n",
            "Iteration 40, loss = 0.99835332\n",
            "Iteration 41, loss = 0.98082678\n",
            "Iteration 42, loss = 0.97624882\n",
            "Iteration 43, loss = 0.95189842\n",
            "Iteration 44, loss = 0.95271137\n",
            "Iteration 45, loss = 0.92809776\n",
            "Iteration 46, loss = 0.90693789\n",
            "Iteration 47, loss = 0.93317872\n",
            "Iteration 48, loss = 0.92759275\n",
            "Iteration 49, loss = 0.92455616\n",
            "Iteration 50, loss = 0.89844841\n",
            "Iteration 51, loss = 0.87736596\n",
            "Iteration 52, loss = 0.85132603\n",
            "Iteration 53, loss = 0.87577855\n",
            "Iteration 54, loss = 0.86088690\n",
            "Iteration 55, loss = 0.89855969\n",
            "Iteration 56, loss = 0.88105677\n",
            "Iteration 57, loss = 0.85063820\n",
            "Iteration 58, loss = 0.83737013\n",
            "Iteration 59, loss = 0.81941568\n",
            "Iteration 60, loss = 0.80739143\n",
            "Iteration 61, loss = 0.82796239\n",
            "Iteration 62, loss = 0.84320731\n",
            "Iteration 63, loss = 0.80944263\n",
            "Iteration 64, loss = 0.81695593\n",
            "Iteration 65, loss = 0.87953447\n",
            "Iteration 66, loss = 0.85342174\n",
            "Iteration 67, loss = 0.94336198\n",
            "Iteration 68, loss = 0.87648848\n",
            "Iteration 69, loss = 0.80243269\n",
            "Iteration 70, loss = 0.78424625\n",
            "Iteration 71, loss = 0.76536461\n",
            "Iteration 72, loss = 0.76962421\n",
            "Iteration 73, loss = 0.75594748\n",
            "Iteration 74, loss = 0.75270920\n",
            "Iteration 75, loss = 0.77955493\n",
            "Iteration 76, loss = 0.78116353\n",
            "Iteration 77, loss = 0.81206224\n",
            "Iteration 78, loss = 0.78570561\n",
            "Iteration 79, loss = 0.71992249\n",
            "Iteration 80, loss = 0.72739087\n",
            "Iteration 81, loss = 0.72069844\n",
            "Iteration 82, loss = 0.75464396\n",
            "Iteration 83, loss = 0.78006600\n",
            "Iteration 84, loss = 0.73925394\n",
            "Iteration 85, loss = 0.72482363\n",
            "Iteration 86, loss = 0.72165988\n",
            "Iteration 87, loss = 0.74882030\n",
            "Iteration 88, loss = 0.72076210\n",
            "Iteration 89, loss = 0.72996755\n",
            "Iteration 90, loss = 0.70128750\n",
            "Iteration 91, loss = 0.68992042\n",
            "Iteration 92, loss = 0.68452928\n",
            "Iteration 93, loss = 0.68312309\n",
            "Iteration 94, loss = 0.70072483\n",
            "Iteration 95, loss = 0.71275056\n",
            "Iteration 96, loss = 0.73607304\n",
            "Iteration 97, loss = 0.67679531\n",
            "Iteration 98, loss = 0.66348469\n",
            "Iteration 99, loss = 0.66313561\n",
            "Iteration 100, loss = 0.67319908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38418514\n",
            "Iteration 2, loss = 1.35230186\n",
            "Iteration 3, loss = 1.34877202\n",
            "Iteration 4, loss = 1.33795137\n",
            "Iteration 5, loss = 1.32684682\n",
            "Iteration 6, loss = 1.31522103\n",
            "Iteration 7, loss = 1.31311253\n",
            "Iteration 8, loss = 1.30608445\n",
            "Iteration 9, loss = 1.29593873\n",
            "Iteration 10, loss = 1.28107032\n",
            "Iteration 11, loss = 1.27350482\n",
            "Iteration 12, loss = 1.26141317\n",
            "Iteration 13, loss = 1.25107769\n",
            "Iteration 14, loss = 1.25910184\n",
            "Iteration 15, loss = 1.23790800\n",
            "Iteration 16, loss = 1.22603631\n",
            "Iteration 17, loss = 1.19741783\n",
            "Iteration 18, loss = 1.20660334\n",
            "Iteration 19, loss = 1.19763964\n",
            "Iteration 20, loss = 1.18048433\n",
            "Iteration 21, loss = 1.16137480\n",
            "Iteration 22, loss = 1.16943508\n",
            "Iteration 23, loss = 1.15727552\n",
            "Iteration 24, loss = 1.13706267\n",
            "Iteration 25, loss = 1.10921167\n",
            "Iteration 26, loss = 1.11597622\n",
            "Iteration 27, loss = 1.09119770\n",
            "Iteration 28, loss = 1.07573416\n",
            "Iteration 29, loss = 1.08266911\n",
            "Iteration 30, loss = 1.07987907\n",
            "Iteration 31, loss = 1.06352880\n",
            "Iteration 32, loss = 1.06964578\n",
            "Iteration 33, loss = 1.04695302\n",
            "Iteration 34, loss = 1.04223788\n",
            "Iteration 35, loss = 1.01482456\n",
            "Iteration 36, loss = 1.01163559\n",
            "Iteration 37, loss = 1.00149483\n",
            "Iteration 38, loss = 0.99107864\n",
            "Iteration 39, loss = 0.99856054\n",
            "Iteration 40, loss = 0.95669371\n",
            "Iteration 41, loss = 1.02942706\n",
            "Iteration 42, loss = 0.97564345\n",
            "Iteration 43, loss = 0.95590103\n",
            "Iteration 44, loss = 0.95459418\n",
            "Iteration 45, loss = 0.93523215\n",
            "Iteration 46, loss = 0.91678813\n",
            "Iteration 47, loss = 0.90768389\n",
            "Iteration 48, loss = 0.90888575\n",
            "Iteration 49, loss = 0.92833250\n",
            "Iteration 50, loss = 0.90934591\n",
            "Iteration 51, loss = 0.90786227\n",
            "Iteration 52, loss = 0.89738875\n",
            "Iteration 53, loss = 0.89111642\n",
            "Iteration 54, loss = 0.87795475\n",
            "Iteration 55, loss = 0.90379616\n",
            "Iteration 56, loss = 0.92584295\n",
            "Iteration 57, loss = 0.88999804\n",
            "Iteration 58, loss = 0.83767950\n",
            "Iteration 59, loss = 0.83402308\n",
            "Iteration 60, loss = 0.81866973\n",
            "Iteration 61, loss = 0.83031685\n",
            "Iteration 62, loss = 0.82934438\n",
            "Iteration 63, loss = 0.78948444\n",
            "Iteration 64, loss = 0.80891706\n",
            "Iteration 65, loss = 0.80808242\n",
            "Iteration 66, loss = 0.79981613\n",
            "Iteration 67, loss = 0.80030416\n",
            "Iteration 68, loss = 0.89510488\n",
            "Iteration 69, loss = 0.83281242\n",
            "Iteration 70, loss = 0.77984163\n",
            "Iteration 71, loss = 0.75182992\n",
            "Iteration 72, loss = 0.75496218\n",
            "Iteration 73, loss = 0.76423955\n",
            "Iteration 74, loss = 0.76903899\n",
            "Iteration 75, loss = 0.72535231\n",
            "Iteration 76, loss = 0.71772994\n",
            "Iteration 77, loss = 0.74467843\n",
            "Iteration 78, loss = 0.73532382\n",
            "Iteration 79, loss = 0.73601833\n",
            "Iteration 80, loss = 0.72205218\n",
            "Iteration 81, loss = 0.72070398\n",
            "Iteration 82, loss = 0.71207055\n",
            "Iteration 83, loss = 0.70096636\n",
            "Iteration 84, loss = 0.71176429\n",
            "Iteration 85, loss = 0.70752968\n",
            "Iteration 86, loss = 0.70201605\n",
            "Iteration 87, loss = 0.71981267\n",
            "Iteration 88, loss = 0.74288786\n",
            "Iteration 89, loss = 0.73028070\n",
            "Iteration 90, loss = 0.72683105\n",
            "Iteration 91, loss = 0.68952700\n",
            "Iteration 92, loss = 0.66800973\n",
            "Iteration 93, loss = 0.66365339\n",
            "Iteration 94, loss = 0.67850668\n",
            "Iteration 95, loss = 0.69346724\n",
            "Iteration 96, loss = 0.68469903\n",
            "Iteration 97, loss = 0.64583081\n",
            "Iteration 98, loss = 0.69983845\n",
            "Iteration 99, loss = 0.71532952\n",
            "Iteration 100, loss = 0.67225318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36707871\n",
            "Iteration 2, loss = 1.33852030\n",
            "Iteration 3, loss = 1.33000618\n",
            "Iteration 4, loss = 1.32150404\n",
            "Iteration 5, loss = 1.31137053\n",
            "Iteration 6, loss = 1.30240377\n",
            "Iteration 7, loss = 1.29506823\n",
            "Iteration 8, loss = 1.28925168\n",
            "Iteration 9, loss = 1.29416494\n",
            "Iteration 10, loss = 1.27457653\n",
            "Iteration 11, loss = 1.25776152\n",
            "Iteration 12, loss = 1.25584358\n",
            "Iteration 13, loss = 1.25328304\n",
            "Iteration 14, loss = 1.23204763\n",
            "Iteration 15, loss = 1.21060774\n",
            "Iteration 16, loss = 1.19687437\n",
            "Iteration 17, loss = 1.18258068\n",
            "Iteration 18, loss = 1.18944600\n",
            "Iteration 19, loss = 1.17949569\n",
            "Iteration 20, loss = 1.14796422\n",
            "Iteration 21, loss = 1.14253754\n",
            "Iteration 22, loss = 1.15306923\n",
            "Iteration 23, loss = 1.16850303\n",
            "Iteration 24, loss = 1.13649704\n",
            "Iteration 25, loss = 1.12617772\n",
            "Iteration 26, loss = 1.12674481\n",
            "Iteration 27, loss = 1.11882643\n",
            "Iteration 28, loss = 1.10991694\n",
            "Iteration 29, loss = 1.10426761\n",
            "Iteration 30, loss = 1.07459611\n",
            "Iteration 31, loss = 1.07488055\n",
            "Iteration 32, loss = 1.09514448\n",
            "Iteration 33, loss = 1.10326376\n",
            "Iteration 34, loss = 1.08077155\n",
            "Iteration 35, loss = 1.08596675\n",
            "Iteration 36, loss = 1.05742888\n",
            "Iteration 37, loss = 1.02632421\n",
            "Iteration 38, loss = 1.02630533\n",
            "Iteration 39, loss = 1.02613144\n",
            "Iteration 40, loss = 1.02770973\n",
            "Iteration 41, loss = 1.02330659\n",
            "Iteration 42, loss = 0.99671185\n",
            "Iteration 43, loss = 0.98487607\n",
            "Iteration 44, loss = 1.04343890\n",
            "Iteration 45, loss = 1.01431361\n",
            "Iteration 46, loss = 0.99935841\n",
            "Iteration 47, loss = 0.98538594\n",
            "Iteration 48, loss = 0.98287886\n",
            "Iteration 49, loss = 0.95722996\n",
            "Iteration 50, loss = 0.95436217\n",
            "Iteration 51, loss = 0.95204465\n",
            "Iteration 52, loss = 0.94247478\n",
            "Iteration 53, loss = 0.93116430\n",
            "Iteration 54, loss = 0.91580924\n",
            "Iteration 55, loss = 0.93901134\n",
            "Iteration 56, loss = 0.93944097\n",
            "Iteration 57, loss = 0.91428066\n",
            "Iteration 58, loss = 0.90370940\n",
            "Iteration 59, loss = 0.91746029\n",
            "Iteration 60, loss = 0.88683602\n",
            "Iteration 61, loss = 0.87760446\n",
            "Iteration 62, loss = 0.89065134\n",
            "Iteration 63, loss = 0.86167664\n",
            "Iteration 64, loss = 0.85091562\n",
            "Iteration 65, loss = 0.88074312\n",
            "Iteration 66, loss = 0.87675525\n",
            "Iteration 67, loss = 0.85420365\n",
            "Iteration 68, loss = 0.87748434\n",
            "Iteration 69, loss = 0.82717881\n",
            "Iteration 70, loss = 0.82842539\n",
            "Iteration 71, loss = 0.81825721\n",
            "Iteration 72, loss = 0.82889277\n",
            "Iteration 73, loss = 0.82093255\n",
            "Iteration 74, loss = 0.79217820\n",
            "Iteration 75, loss = 0.77352542\n",
            "Iteration 76, loss = 0.79192841\n",
            "Iteration 77, loss = 0.84550634\n",
            "Iteration 78, loss = 0.80742476\n",
            "Iteration 79, loss = 0.78144852\n",
            "Iteration 80, loss = 0.79022627\n",
            "Iteration 81, loss = 0.79921156\n",
            "Iteration 82, loss = 0.83172400\n",
            "Iteration 83, loss = 0.78838476\n",
            "Iteration 84, loss = 0.78732857\n",
            "Iteration 85, loss = 0.78370557\n",
            "Iteration 86, loss = 0.76840034\n",
            "Iteration 87, loss = 0.76561715\n",
            "Iteration 88, loss = 0.76368087\n",
            "Iteration 89, loss = 0.72849933\n",
            "Iteration 90, loss = 0.72400254\n",
            "Iteration 91, loss = 0.72124286\n",
            "Iteration 92, loss = 0.72542717\n",
            "Iteration 93, loss = 0.78077040\n",
            "Iteration 94, loss = 0.74477913\n",
            "Iteration 95, loss = 0.80135356\n",
            "Iteration 96, loss = 0.82798725\n",
            "Iteration 97, loss = 0.81075630\n",
            "Iteration 98, loss = 0.76694782\n",
            "Iteration 99, loss = 0.72871653\n",
            "Iteration 100, loss = 0.68434716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37693397\n",
            "Iteration 2, loss = 1.34651748\n",
            "Iteration 3, loss = 1.33930595\n",
            "Iteration 4, loss = 1.33098005\n",
            "Iteration 5, loss = 1.32222163\n",
            "Iteration 6, loss = 1.31176583\n",
            "Iteration 7, loss = 1.30577123\n",
            "Iteration 8, loss = 1.28967809\n",
            "Iteration 9, loss = 1.28621322\n",
            "Iteration 10, loss = 1.26824095\n",
            "Iteration 11, loss = 1.25272588\n",
            "Iteration 12, loss = 1.24119054\n",
            "Iteration 13, loss = 1.24772260\n",
            "Iteration 14, loss = 1.26724550\n",
            "Iteration 15, loss = 1.23707402\n",
            "Iteration 16, loss = 1.22337143\n",
            "Iteration 17, loss = 1.20353006\n",
            "Iteration 18, loss = 1.18574189\n",
            "Iteration 19, loss = 1.18377639\n",
            "Iteration 20, loss = 1.16461457\n",
            "Iteration 21, loss = 1.15565115\n",
            "Iteration 22, loss = 1.17801960\n",
            "Iteration 23, loss = 1.17306896\n",
            "Iteration 24, loss = 1.13728287\n",
            "Iteration 25, loss = 1.11437498\n",
            "Iteration 26, loss = 1.11705892\n",
            "Iteration 27, loss = 1.10701998\n",
            "Iteration 28, loss = 1.11596590\n",
            "Iteration 29, loss = 1.08897947\n",
            "Iteration 30, loss = 1.06787973\n",
            "Iteration 31, loss = 1.04738241\n",
            "Iteration 32, loss = 1.06666326\n",
            "Iteration 33, loss = 1.08868412\n",
            "Iteration 34, loss = 1.05685440\n",
            "Iteration 35, loss = 1.02589582\n",
            "Iteration 36, loss = 1.05531435\n",
            "Iteration 37, loss = 1.01055351\n",
            "Iteration 38, loss = 1.01390663\n",
            "Iteration 39, loss = 1.00429947\n",
            "Iteration 40, loss = 0.98797222\n",
            "Iteration 41, loss = 0.99243945\n",
            "Iteration 42, loss = 0.97352905\n",
            "Iteration 43, loss = 0.99717520\n",
            "Iteration 44, loss = 1.01983197\n",
            "Iteration 45, loss = 1.01315426\n",
            "Iteration 46, loss = 0.97574323\n",
            "Iteration 47, loss = 0.94029018\n",
            "Iteration 48, loss = 0.91427800\n",
            "Iteration 49, loss = 0.93168084\n",
            "Iteration 50, loss = 0.91586573\n",
            "Iteration 51, loss = 0.90847388\n",
            "Iteration 52, loss = 0.91045024\n",
            "Iteration 53, loss = 0.89433179\n",
            "Iteration 54, loss = 0.87874187\n",
            "Iteration 55, loss = 0.88421984\n",
            "Iteration 56, loss = 0.92844835\n",
            "Iteration 57, loss = 0.95297100\n",
            "Iteration 58, loss = 0.90709793\n",
            "Iteration 59, loss = 0.89445603\n",
            "Iteration 60, loss = 0.85587491\n",
            "Iteration 61, loss = 0.86767002\n",
            "Iteration 62, loss = 0.87374683\n",
            "Iteration 63, loss = 0.83249981\n",
            "Iteration 64, loss = 0.82307743\n",
            "Iteration 65, loss = 0.84898708\n",
            "Iteration 66, loss = 0.83110824\n",
            "Iteration 67, loss = 0.85705096\n",
            "Iteration 68, loss = 0.87313135\n",
            "Iteration 69, loss = 0.82410996\n",
            "Iteration 70, loss = 0.78750296\n",
            "Iteration 71, loss = 0.76548427\n",
            "Iteration 72, loss = 0.77153645\n",
            "Iteration 73, loss = 0.78731620\n",
            "Iteration 74, loss = 0.78893671\n",
            "Iteration 75, loss = 0.78566584\n",
            "Iteration 76, loss = 0.78996844\n",
            "Iteration 77, loss = 0.81580262\n",
            "Iteration 78, loss = 0.80001916\n",
            "Iteration 79, loss = 0.76161910\n",
            "Iteration 80, loss = 0.74930040\n",
            "Iteration 81, loss = 0.71876822\n",
            "Iteration 82, loss = 0.76259498\n",
            "Iteration 83, loss = 0.74997842\n",
            "Iteration 84, loss = 0.71026482\n",
            "Iteration 85, loss = 0.73497824\n",
            "Iteration 86, loss = 0.75123287\n",
            "Iteration 87, loss = 0.73609878\n",
            "Iteration 88, loss = 0.69939263\n",
            "Iteration 89, loss = 0.69914682\n",
            "Iteration 90, loss = 0.67679252\n",
            "Iteration 91, loss = 0.69363024\n",
            "Iteration 92, loss = 0.68932408\n",
            "Iteration 93, loss = 0.66813577\n",
            "Iteration 94, loss = 0.68471195\n",
            "Iteration 95, loss = 0.71612545\n",
            "Iteration 96, loss = 0.76711896\n",
            "Iteration 97, loss = 0.75622231\n",
            "Iteration 98, loss = 0.74969898\n",
            "Iteration 99, loss = 0.70521169\n",
            "Iteration 100, loss = 0.67152027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37417419\n",
            "Iteration 2, loss = 1.34880192\n",
            "Iteration 3, loss = 1.34024987\n",
            "Iteration 4, loss = 1.33093115\n",
            "Iteration 5, loss = 1.31889223\n",
            "Iteration 6, loss = 1.30335789\n",
            "Iteration 7, loss = 1.30315394\n",
            "Iteration 8, loss = 1.28793712\n",
            "Iteration 9, loss = 1.27621017\n",
            "Iteration 10, loss = 1.25635737\n",
            "Iteration 11, loss = 1.23539146\n",
            "Iteration 12, loss = 1.22027017\n",
            "Iteration 13, loss = 1.23690369\n",
            "Iteration 14, loss = 1.25163064\n",
            "Iteration 15, loss = 1.21837736\n",
            "Iteration 16, loss = 1.19947507\n",
            "Iteration 17, loss = 1.17725989\n",
            "Iteration 18, loss = 1.17283033\n",
            "Iteration 19, loss = 1.16268962\n",
            "Iteration 20, loss = 1.17989190\n",
            "Iteration 21, loss = 1.15311573\n",
            "Iteration 22, loss = 1.15707178\n",
            "Iteration 23, loss = 1.12589978\n",
            "Iteration 24, loss = 1.11197344\n",
            "Iteration 25, loss = 1.09885164\n",
            "Iteration 26, loss = 1.10851696\n",
            "Iteration 27, loss = 1.09225486\n",
            "Iteration 28, loss = 1.09615197\n",
            "Iteration 29, loss = 1.08765540\n",
            "Iteration 30, loss = 1.05753061\n",
            "Iteration 31, loss = 1.04974493\n",
            "Iteration 32, loss = 1.05895076\n",
            "Iteration 33, loss = 1.01887282\n",
            "Iteration 34, loss = 1.01056266\n",
            "Iteration 35, loss = 1.01211277\n",
            "Iteration 36, loss = 1.02477726\n",
            "Iteration 37, loss = 1.00249895\n",
            "Iteration 38, loss = 0.99921822\n",
            "Iteration 39, loss = 0.98973816\n",
            "Iteration 40, loss = 0.98256386\n",
            "Iteration 41, loss = 0.97752034\n",
            "Iteration 42, loss = 0.95367074\n",
            "Iteration 43, loss = 0.96398884\n",
            "Iteration 44, loss = 0.97264173\n",
            "Iteration 45, loss = 0.96651079\n",
            "Iteration 46, loss = 0.91008837\n",
            "Iteration 47, loss = 0.90897516\n",
            "Iteration 48, loss = 0.89735046\n",
            "Iteration 49, loss = 0.90202978\n",
            "Iteration 50, loss = 0.91668886\n",
            "Iteration 51, loss = 0.90737024\n",
            "Iteration 52, loss = 0.91426387\n",
            "Iteration 53, loss = 0.86801247\n",
            "Iteration 54, loss = 0.86051967\n",
            "Iteration 55, loss = 0.86533684\n",
            "Iteration 56, loss = 0.90666489\n",
            "Iteration 57, loss = 0.91790822\n",
            "Iteration 58, loss = 0.85714097\n",
            "Iteration 59, loss = 0.84478991\n",
            "Iteration 60, loss = 0.81304516\n",
            "Iteration 61, loss = 0.81593042\n",
            "Iteration 62, loss = 0.83876887\n",
            "Iteration 63, loss = 0.81245047\n",
            "Iteration 64, loss = 0.79847740\n",
            "Iteration 65, loss = 0.78475099\n",
            "Iteration 66, loss = 0.80274369\n",
            "Iteration 67, loss = 0.81586816\n",
            "Iteration 68, loss = 0.83994800\n",
            "Iteration 69, loss = 0.82022821\n",
            "Iteration 70, loss = 0.80542194\n",
            "Iteration 71, loss = 0.75590526\n",
            "Iteration 72, loss = 0.74604916\n",
            "Iteration 73, loss = 0.72664277\n",
            "Iteration 74, loss = 0.72374027\n",
            "Iteration 75, loss = 0.70561707\n",
            "Iteration 76, loss = 0.73047832\n",
            "Iteration 77, loss = 0.72560870\n",
            "Iteration 78, loss = 0.70532873\n",
            "Iteration 79, loss = 0.71344501\n",
            "Iteration 80, loss = 0.72058214\n",
            "Iteration 81, loss = 0.71381997\n",
            "Iteration 82, loss = 0.68641394\n",
            "Iteration 83, loss = 0.71276961\n",
            "Iteration 84, loss = 0.67653610\n",
            "Iteration 85, loss = 0.70304860\n",
            "Iteration 86, loss = 0.74255350\n",
            "Iteration 87, loss = 0.74960306\n",
            "Iteration 88, loss = 0.68165737\n",
            "Iteration 89, loss = 0.69351303\n",
            "Iteration 90, loss = 0.69322875\n",
            "Iteration 91, loss = 0.67528382\n",
            "Iteration 92, loss = 0.66304102\n",
            "Iteration 93, loss = 0.66133682\n",
            "Iteration 94, loss = 0.65088878\n",
            "Iteration 95, loss = 0.67856751\n",
            "Iteration 96, loss = 0.76366299\n",
            "Iteration 97, loss = 0.73722862\n",
            "Iteration 98, loss = 0.69536412\n",
            "Iteration 99, loss = 0.64519909\n",
            "Iteration 100, loss = 0.60960071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36000169\n",
            "Iteration 2, loss = 1.34226701\n",
            "Iteration 3, loss = 1.33106672\n",
            "Iteration 4, loss = 1.32241895\n",
            "Iteration 5, loss = 1.31107243\n",
            "Iteration 6, loss = 1.29977210\n",
            "Iteration 7, loss = 1.29480701\n",
            "Iteration 8, loss = 1.26988648\n",
            "Iteration 9, loss = 1.25405306\n",
            "Iteration 10, loss = 1.23151735\n",
            "Iteration 11, loss = 1.23507382\n",
            "Iteration 12, loss = 1.21280344\n",
            "Iteration 13, loss = 1.22247645\n",
            "Iteration 14, loss = 1.22682189\n",
            "Iteration 15, loss = 1.18736018\n",
            "Iteration 16, loss = 1.16794630\n",
            "Iteration 17, loss = 1.15032817\n",
            "Iteration 18, loss = 1.14037899\n",
            "Iteration 19, loss = 1.13684301\n",
            "Iteration 20, loss = 1.13911809\n",
            "Iteration 21, loss = 1.12461683\n",
            "Iteration 22, loss = 1.15098951\n",
            "Iteration 23, loss = 1.12321825\n",
            "Iteration 24, loss = 1.10212741\n",
            "Iteration 25, loss = 1.08261391\n",
            "Iteration 26, loss = 1.08039806\n",
            "Iteration 27, loss = 1.06713677\n",
            "Iteration 28, loss = 1.06689322\n",
            "Iteration 29, loss = 1.03033121\n",
            "Iteration 30, loss = 1.01972423\n",
            "Iteration 31, loss = 0.99957990\n",
            "Iteration 32, loss = 1.00088165\n",
            "Iteration 33, loss = 0.99661335\n",
            "Iteration 34, loss = 0.99174582\n",
            "Iteration 35, loss = 0.96938125\n",
            "Iteration 36, loss = 0.96608109\n",
            "Iteration 37, loss = 0.96979504\n",
            "Iteration 38, loss = 0.96325880\n",
            "Iteration 39, loss = 0.95972326\n",
            "Iteration 40, loss = 0.96129838\n",
            "Iteration 41, loss = 0.94270994\n",
            "Iteration 42, loss = 0.94692732\n",
            "Iteration 43, loss = 0.97050713\n",
            "Iteration 44, loss = 1.00011251\n",
            "Iteration 45, loss = 0.96487087\n",
            "Iteration 46, loss = 0.92646818\n",
            "Iteration 47, loss = 0.90477056\n",
            "Iteration 48, loss = 0.89557648\n",
            "Iteration 49, loss = 0.87806938\n",
            "Iteration 50, loss = 0.88046156\n",
            "Iteration 51, loss = 0.88136988\n",
            "Iteration 52, loss = 0.85858062\n",
            "Iteration 53, loss = 0.86181370\n",
            "Iteration 54, loss = 0.82496700\n",
            "Iteration 55, loss = 0.85951219\n",
            "Iteration 56, loss = 0.96725262\n",
            "Iteration 57, loss = 0.92793522\n",
            "Iteration 58, loss = 0.83328795\n",
            "Iteration 59, loss = 0.81570353\n",
            "Iteration 60, loss = 0.80269164\n",
            "Iteration 61, loss = 0.80765457\n",
            "Iteration 62, loss = 0.80397091\n",
            "Iteration 63, loss = 0.80334754\n",
            "Iteration 64, loss = 0.78353991\n",
            "Iteration 65, loss = 0.79384183\n",
            "Iteration 66, loss = 0.79953749\n",
            "Iteration 67, loss = 0.80617426\n",
            "Iteration 68, loss = 0.81391670\n",
            "Iteration 69, loss = 0.79061465\n",
            "Iteration 70, loss = 0.79937397\n",
            "Iteration 71, loss = 0.74273462\n",
            "Iteration 72, loss = 0.75616632\n",
            "Iteration 73, loss = 0.73891374\n",
            "Iteration 74, loss = 0.71983567\n",
            "Iteration 75, loss = 0.71997658\n",
            "Iteration 76, loss = 0.76361924\n",
            "Iteration 77, loss = 0.75913357\n",
            "Iteration 78, loss = 0.75751351\n",
            "Iteration 79, loss = 0.73027001\n",
            "Iteration 80, loss = 0.73090624\n",
            "Iteration 81, loss = 0.71203806\n",
            "Iteration 82, loss = 0.74872681\n",
            "Iteration 83, loss = 0.74170271\n",
            "Iteration 84, loss = 0.70783265\n",
            "Iteration 85, loss = 0.68850532\n",
            "Iteration 86, loss = 0.72891056\n",
            "Iteration 87, loss = 0.73276855\n",
            "Iteration 88, loss = 0.68868241\n",
            "Iteration 89, loss = 0.66490649\n",
            "Iteration 90, loss = 0.65010265\n",
            "Iteration 91, loss = 0.64265606\n",
            "Iteration 92, loss = 0.69004668\n",
            "Iteration 93, loss = 0.66481178\n",
            "Iteration 94, loss = 0.72241880\n",
            "Iteration 95, loss = 0.79153125\n",
            "Iteration 96, loss = 0.74186341\n",
            "Iteration 97, loss = 0.67995945\n",
            "Iteration 98, loss = 0.67077365\n",
            "Iteration 99, loss = 0.68171606\n",
            "Iteration 100, loss = 0.65619520\n",
            "43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39116579\n",
            "Iteration 2, loss = 1.38148539\n",
            "Iteration 3, loss = 1.37646028\n",
            "Iteration 4, loss = 1.37269183\n",
            "Iteration 5, loss = 1.37138911\n",
            "Iteration 6, loss = 1.37084637\n",
            "Iteration 7, loss = 1.36868121\n",
            "Iteration 8, loss = 1.36764320\n",
            "Iteration 9, loss = 1.36707524\n",
            "Iteration 10, loss = 1.36632114\n",
            "Iteration 11, loss = 1.36584559\n",
            "Iteration 12, loss = 1.36551765\n",
            "Iteration 13, loss = 1.36489046\n",
            "Iteration 14, loss = 1.36458061\n",
            "Iteration 15, loss = 1.36392569\n",
            "Iteration 16, loss = 1.36340288\n",
            "Iteration 17, loss = 1.36347177\n",
            "Iteration 18, loss = 1.36272971\n",
            "Iteration 19, loss = 1.36240429\n",
            "Iteration 20, loss = 1.36211500\n",
            "Iteration 21, loss = 1.36160570\n",
            "Iteration 22, loss = 1.36151102\n",
            "Iteration 23, loss = 1.36102166\n",
            "Iteration 24, loss = 1.36069003\n",
            "Iteration 25, loss = 1.36057572\n",
            "Iteration 26, loss = 1.36088023\n",
            "Iteration 27, loss = 1.35936908\n",
            "Iteration 28, loss = 1.35906491\n",
            "Iteration 29, loss = 1.35878112\n",
            "Iteration 30, loss = 1.35881114\n",
            "Iteration 31, loss = 1.35921513\n",
            "Iteration 32, loss = 1.35981852\n",
            "Iteration 33, loss = 1.35922172\n",
            "Iteration 34, loss = 1.35784664\n",
            "Iteration 35, loss = 1.35717382\n",
            "Iteration 36, loss = 1.35748085\n",
            "Iteration 37, loss = 1.35596765\n",
            "Iteration 38, loss = 1.35622226\n",
            "Iteration 39, loss = 1.35603627\n",
            "Iteration 40, loss = 1.35416409\n",
            "Iteration 41, loss = 1.35582750\n",
            "Iteration 42, loss = 1.35532788\n",
            "Iteration 43, loss = 1.35398748\n",
            "Iteration 44, loss = 1.35469997\n",
            "Iteration 45, loss = 1.35422375\n",
            "Iteration 46, loss = 1.35327958\n",
            "Iteration 47, loss = 1.35392788\n",
            "Iteration 48, loss = 1.35189072\n",
            "Iteration 49, loss = 1.35266026\n",
            "Iteration 50, loss = 1.35215638\n",
            "Iteration 51, loss = 1.35276578\n",
            "Iteration 52, loss = 1.35057506\n",
            "Iteration 53, loss = 1.35107355\n",
            "Iteration 54, loss = 1.35125605\n",
            "Iteration 55, loss = 1.34967535\n",
            "Iteration 56, loss = 1.35051651\n",
            "Iteration 57, loss = 1.34923492\n",
            "Iteration 58, loss = 1.35146092\n",
            "Iteration 59, loss = 1.35024738\n",
            "Iteration 60, loss = 1.34932655\n",
            "Iteration 61, loss = 1.34878031\n",
            "Iteration 62, loss = 1.34890203\n",
            "Iteration 63, loss = 1.34957896\n",
            "Iteration 64, loss = 1.34995950\n",
            "Iteration 65, loss = 1.34914533\n",
            "Iteration 66, loss = 1.34881294\n",
            "Iteration 67, loss = 1.34720719\n",
            "Iteration 68, loss = 1.34726462\n",
            "Iteration 69, loss = 1.34692432\n",
            "Iteration 70, loss = 1.34786811\n",
            "Iteration 71, loss = 1.34774689\n",
            "Iteration 72, loss = 1.34709255\n",
            "Iteration 73, loss = 1.34716886\n",
            "Iteration 74, loss = 1.34668554\n",
            "Iteration 75, loss = 1.34718463\n",
            "Iteration 76, loss = 1.34733472\n",
            "Iteration 77, loss = 1.34617789\n",
            "Iteration 78, loss = 1.34590992\n",
            "Iteration 79, loss = 1.34642723\n",
            "Iteration 80, loss = 1.34543654\n",
            "Iteration 81, loss = 1.34553162\n",
            "Iteration 82, loss = 1.34592694\n",
            "Iteration 83, loss = 1.34587676\n",
            "Iteration 84, loss = 1.34578946\n",
            "Iteration 85, loss = 1.34573825\n",
            "Iteration 86, loss = 1.34570781\n",
            "Iteration 87, loss = 1.34512375\n",
            "Iteration 88, loss = 1.34563400\n",
            "Iteration 89, loss = 1.34732556\n",
            "Iteration 90, loss = 1.34528917\n",
            "Iteration 91, loss = 1.34548391\n",
            "Iteration 92, loss = 1.34466518\n",
            "Iteration 93, loss = 1.34430916\n",
            "Iteration 94, loss = 1.34408502\n",
            "Iteration 95, loss = 1.34417742\n",
            "Iteration 96, loss = 1.34455924\n",
            "Iteration 97, loss = 1.34692609\n",
            "Iteration 98, loss = 1.34625497\n",
            "Iteration 99, loss = 1.34431065\n",
            "Iteration 100, loss = 1.34416891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39266885\n",
            "Iteration 2, loss = 1.38301614\n",
            "Iteration 3, loss = 1.37928002\n",
            "Iteration 4, loss = 1.37570762\n",
            "Iteration 5, loss = 1.37457505\n",
            "Iteration 6, loss = 1.37386984\n",
            "Iteration 7, loss = 1.37211328\n",
            "Iteration 8, loss = 1.37117010\n",
            "Iteration 9, loss = 1.37039045\n",
            "Iteration 10, loss = 1.36996046\n",
            "Iteration 11, loss = 1.36893277\n",
            "Iteration 12, loss = 1.36917419\n",
            "Iteration 13, loss = 1.36830375\n",
            "Iteration 14, loss = 1.36845157\n",
            "Iteration 15, loss = 1.36812775\n",
            "Iteration 16, loss = 1.36696039\n",
            "Iteration 17, loss = 1.36715500\n",
            "Iteration 18, loss = 1.36645172\n",
            "Iteration 19, loss = 1.36604683\n",
            "Iteration 20, loss = 1.36613484\n",
            "Iteration 21, loss = 1.36559915\n",
            "Iteration 22, loss = 1.36641949\n",
            "Iteration 23, loss = 1.36551806\n",
            "Iteration 24, loss = 1.36516852\n",
            "Iteration 25, loss = 1.36487411\n",
            "Iteration 26, loss = 1.36564640\n",
            "Iteration 27, loss = 1.36437773\n",
            "Iteration 28, loss = 1.36394753\n",
            "Iteration 29, loss = 1.36407793\n",
            "Iteration 30, loss = 1.36382385\n",
            "Iteration 31, loss = 1.36439815\n",
            "Iteration 32, loss = 1.36573518\n",
            "Iteration 33, loss = 1.36437538\n",
            "Iteration 34, loss = 1.36295932\n",
            "Iteration 35, loss = 1.36270900\n",
            "Iteration 36, loss = 1.36315245\n",
            "Iteration 37, loss = 1.36196945\n",
            "Iteration 38, loss = 1.36188183\n",
            "Iteration 39, loss = 1.36157166\n",
            "Iteration 40, loss = 1.36096621\n",
            "Iteration 41, loss = 1.36194207\n",
            "Iteration 42, loss = 1.36213439\n",
            "Iteration 43, loss = 1.36030542\n",
            "Iteration 44, loss = 1.36129530\n",
            "Iteration 45, loss = 1.36091238\n",
            "Iteration 46, loss = 1.36032650\n",
            "Iteration 47, loss = 1.36092811\n",
            "Iteration 48, loss = 1.35892504\n",
            "Iteration 49, loss = 1.35981548\n",
            "Iteration 50, loss = 1.35849242\n",
            "Iteration 51, loss = 1.35962163\n",
            "Iteration 52, loss = 1.35726937\n",
            "Iteration 53, loss = 1.35775206\n",
            "Iteration 54, loss = 1.35754996\n",
            "Iteration 55, loss = 1.35646390\n",
            "Iteration 56, loss = 1.35696951\n",
            "Iteration 57, loss = 1.35585817\n",
            "Iteration 58, loss = 1.35775042\n",
            "Iteration 59, loss = 1.35650059\n",
            "Iteration 60, loss = 1.35574479\n",
            "Iteration 61, loss = 1.35521957\n",
            "Iteration 62, loss = 1.35538457\n",
            "Iteration 63, loss = 1.35671225\n",
            "Iteration 64, loss = 1.35617700\n",
            "Iteration 65, loss = 1.35539793\n",
            "Iteration 66, loss = 1.35572044\n",
            "Iteration 67, loss = 1.35422430\n",
            "Iteration 68, loss = 1.35347061\n",
            "Iteration 69, loss = 1.35377875\n",
            "Iteration 70, loss = 1.35395784\n",
            "Iteration 71, loss = 1.35408255\n",
            "Iteration 72, loss = 1.35334342\n",
            "Iteration 73, loss = 1.35373929\n",
            "Iteration 74, loss = 1.35324325\n",
            "Iteration 75, loss = 1.35346581\n",
            "Iteration 76, loss = 1.35369773\n",
            "Iteration 77, loss = 1.35259296\n",
            "Iteration 78, loss = 1.35286871\n",
            "Iteration 79, loss = 1.35278423\n",
            "Iteration 80, loss = 1.35259939\n",
            "Iteration 81, loss = 1.35269276\n",
            "Iteration 82, loss = 1.35228677\n",
            "Iteration 83, loss = 1.35213090\n",
            "Iteration 84, loss = 1.35146935\n",
            "Iteration 85, loss = 1.35292221\n",
            "Iteration 86, loss = 1.35251915\n",
            "Iteration 87, loss = 1.35201363\n",
            "Iteration 88, loss = 1.35198643\n",
            "Iteration 89, loss = 1.35302319\n",
            "Iteration 90, loss = 1.35071229\n",
            "Iteration 91, loss = 1.35119541\n",
            "Iteration 92, loss = 1.35031350\n",
            "Iteration 93, loss = 1.35097928\n",
            "Iteration 94, loss = 1.35082795\n",
            "Iteration 95, loss = 1.35066899\n",
            "Iteration 96, loss = 1.35046177\n",
            "Iteration 97, loss = 1.35338964\n",
            "Iteration 98, loss = 1.35255929\n",
            "Iteration 99, loss = 1.34960630\n",
            "Iteration 100, loss = 1.35033784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39540501\n",
            "Iteration 2, loss = 1.38363708\n",
            "Iteration 3, loss = 1.38029132\n",
            "Iteration 4, loss = 1.37672187\n",
            "Iteration 5, loss = 1.37496635\n",
            "Iteration 6, loss = 1.37375631\n",
            "Iteration 7, loss = 1.37219364\n",
            "Iteration 8, loss = 1.37086121\n",
            "Iteration 9, loss = 1.37079530\n",
            "Iteration 10, loss = 1.37069076\n",
            "Iteration 11, loss = 1.36920500\n",
            "Iteration 12, loss = 1.36870665\n",
            "Iteration 13, loss = 1.36836343\n",
            "Iteration 14, loss = 1.36814049\n",
            "Iteration 15, loss = 1.36788100\n",
            "Iteration 16, loss = 1.36642699\n",
            "Iteration 17, loss = 1.36648178\n",
            "Iteration 18, loss = 1.36600369\n",
            "Iteration 19, loss = 1.36540989\n",
            "Iteration 20, loss = 1.36553394\n",
            "Iteration 21, loss = 1.36488249\n",
            "Iteration 22, loss = 1.36528440\n",
            "Iteration 23, loss = 1.36420748\n",
            "Iteration 24, loss = 1.36358105\n",
            "Iteration 25, loss = 1.36236432\n",
            "Iteration 26, loss = 1.36354013\n",
            "Iteration 27, loss = 1.36181724\n",
            "Iteration 28, loss = 1.36175019\n",
            "Iteration 29, loss = 1.36182723\n",
            "Iteration 30, loss = 1.36096737\n",
            "Iteration 31, loss = 1.36083748\n",
            "Iteration 32, loss = 1.36188390\n",
            "Iteration 33, loss = 1.36119433\n",
            "Iteration 34, loss = 1.35937680\n",
            "Iteration 35, loss = 1.35861600\n",
            "Iteration 36, loss = 1.35908842\n",
            "Iteration 37, loss = 1.35830329\n",
            "Iteration 38, loss = 1.35823692\n",
            "Iteration 39, loss = 1.35726035\n",
            "Iteration 40, loss = 1.35649777\n",
            "Iteration 41, loss = 1.35667473\n",
            "Iteration 42, loss = 1.35707065\n",
            "Iteration 43, loss = 1.35502116\n",
            "Iteration 44, loss = 1.35538611\n",
            "Iteration 45, loss = 1.35550995\n",
            "Iteration 46, loss = 1.35421743\n",
            "Iteration 47, loss = 1.35443020\n",
            "Iteration 48, loss = 1.35358992\n",
            "Iteration 49, loss = 1.35403571\n",
            "Iteration 50, loss = 1.35335823\n",
            "Iteration 51, loss = 1.35461205\n",
            "Iteration 52, loss = 1.35172320\n",
            "Iteration 53, loss = 1.35159351\n",
            "Iteration 54, loss = 1.35188778\n",
            "Iteration 55, loss = 1.34975919\n",
            "Iteration 56, loss = 1.35026601\n",
            "Iteration 57, loss = 1.35032069\n",
            "Iteration 58, loss = 1.35171377\n",
            "Iteration 59, loss = 1.35007371\n",
            "Iteration 60, loss = 1.34935353\n",
            "Iteration 61, loss = 1.34816403\n",
            "Iteration 62, loss = 1.34836510\n",
            "Iteration 63, loss = 1.34883783\n",
            "Iteration 64, loss = 1.34942324\n",
            "Iteration 65, loss = 1.34958686\n",
            "Iteration 66, loss = 1.34854726\n",
            "Iteration 67, loss = 1.34678047\n",
            "Iteration 68, loss = 1.34642328\n",
            "Iteration 69, loss = 1.34625400\n",
            "Iteration 70, loss = 1.34649239\n",
            "Iteration 71, loss = 1.34638684\n",
            "Iteration 72, loss = 1.34687785\n",
            "Iteration 73, loss = 1.34674295\n",
            "Iteration 74, loss = 1.34600963\n",
            "Iteration 75, loss = 1.34704682\n",
            "Iteration 76, loss = 1.34701032\n",
            "Iteration 77, loss = 1.34446053\n",
            "Iteration 78, loss = 1.34526698\n",
            "Iteration 79, loss = 1.34468661\n",
            "Iteration 80, loss = 1.34670285\n",
            "Iteration 81, loss = 1.34726804\n",
            "Iteration 82, loss = 1.34529678\n",
            "Iteration 83, loss = 1.34473577\n",
            "Iteration 84, loss = 1.34388367\n",
            "Iteration 85, loss = 1.34478872\n",
            "Iteration 86, loss = 1.34439960\n",
            "Iteration 87, loss = 1.34407783\n",
            "Iteration 88, loss = 1.34370694\n",
            "Iteration 89, loss = 1.34496972\n",
            "Iteration 90, loss = 1.34440078\n",
            "Iteration 91, loss = 1.34313615\n",
            "Iteration 92, loss = 1.34227895\n",
            "Iteration 93, loss = 1.34240064\n",
            "Iteration 94, loss = 1.34445188\n",
            "Iteration 95, loss = 1.34300889\n",
            "Iteration 96, loss = 1.34355700\n",
            "Iteration 97, loss = 1.34646058\n",
            "Iteration 98, loss = 1.34391510\n",
            "Iteration 99, loss = 1.34241270\n",
            "Iteration 100, loss = 1.34290751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39592983\n",
            "Iteration 2, loss = 1.38278264\n",
            "Iteration 3, loss = 1.37964330\n",
            "Iteration 4, loss = 1.37723523\n",
            "Iteration 5, loss = 1.37445725\n",
            "Iteration 6, loss = 1.37324428\n",
            "Iteration 7, loss = 1.37148882\n",
            "Iteration 8, loss = 1.37031198\n",
            "Iteration 9, loss = 1.36966259\n",
            "Iteration 10, loss = 1.36883003\n",
            "Iteration 11, loss = 1.36750759\n",
            "Iteration 12, loss = 1.36747447\n",
            "Iteration 13, loss = 1.36678798\n",
            "Iteration 14, loss = 1.36725107\n",
            "Iteration 15, loss = 1.36702218\n",
            "Iteration 16, loss = 1.36392263\n",
            "Iteration 17, loss = 1.36380762\n",
            "Iteration 18, loss = 1.36303751\n",
            "Iteration 19, loss = 1.36280902\n",
            "Iteration 20, loss = 1.36255335\n",
            "Iteration 21, loss = 1.36167999\n",
            "Iteration 22, loss = 1.36211193\n",
            "Iteration 23, loss = 1.36138841\n",
            "Iteration 24, loss = 1.36004354\n",
            "Iteration 25, loss = 1.35934812\n",
            "Iteration 26, loss = 1.35990049\n",
            "Iteration 27, loss = 1.35852252\n",
            "Iteration 28, loss = 1.35849668\n",
            "Iteration 29, loss = 1.35812118\n",
            "Iteration 30, loss = 1.35762348\n",
            "Iteration 31, loss = 1.35795758\n",
            "Iteration 32, loss = 1.35862299\n",
            "Iteration 33, loss = 1.35783718\n",
            "Iteration 34, loss = 1.35714578\n",
            "Iteration 35, loss = 1.35583215\n",
            "Iteration 36, loss = 1.35641260\n",
            "Iteration 37, loss = 1.35502523\n",
            "Iteration 38, loss = 1.35660215\n",
            "Iteration 39, loss = 1.35479531\n",
            "Iteration 40, loss = 1.35401436\n",
            "Iteration 41, loss = 1.35508278\n",
            "Iteration 42, loss = 1.35407870\n",
            "Iteration 43, loss = 1.35350267\n",
            "Iteration 44, loss = 1.35371679\n",
            "Iteration 45, loss = 1.35392717\n",
            "Iteration 46, loss = 1.35379710\n",
            "Iteration 47, loss = 1.35282217\n",
            "Iteration 48, loss = 1.35262983\n",
            "Iteration 49, loss = 1.35260728\n",
            "Iteration 50, loss = 1.35197981\n",
            "Iteration 51, loss = 1.35371969\n",
            "Iteration 52, loss = 1.34949495\n",
            "Iteration 53, loss = 1.35071017\n",
            "Iteration 54, loss = 1.35128455\n",
            "Iteration 55, loss = 1.34957944\n",
            "Iteration 56, loss = 1.35000240\n",
            "Iteration 57, loss = 1.34987179\n",
            "Iteration 58, loss = 1.35184880\n",
            "Iteration 59, loss = 1.35053702\n",
            "Iteration 60, loss = 1.34955452\n",
            "Iteration 61, loss = 1.34955145\n",
            "Iteration 62, loss = 1.34851580\n",
            "Iteration 63, loss = 1.34969042\n",
            "Iteration 64, loss = 1.34885453\n",
            "Iteration 65, loss = 1.34918240\n",
            "Iteration 66, loss = 1.34842826\n",
            "Iteration 67, loss = 1.34707284\n",
            "Iteration 68, loss = 1.34723686\n",
            "Iteration 69, loss = 1.34711353\n",
            "Iteration 70, loss = 1.34676727\n",
            "Iteration 71, loss = 1.34683098\n",
            "Iteration 72, loss = 1.34720246\n",
            "Iteration 73, loss = 1.34683442\n",
            "Iteration 74, loss = 1.34583262\n",
            "Iteration 75, loss = 1.34650944\n",
            "Iteration 76, loss = 1.34615341\n",
            "Iteration 77, loss = 1.34524333\n",
            "Iteration 78, loss = 1.34570974\n",
            "Iteration 79, loss = 1.34462919\n",
            "Iteration 80, loss = 1.34730617\n",
            "Iteration 81, loss = 1.34832265\n",
            "Iteration 82, loss = 1.34691551\n",
            "Iteration 83, loss = 1.34457923\n",
            "Iteration 84, loss = 1.34429760\n",
            "Iteration 85, loss = 1.34552026\n",
            "Iteration 86, loss = 1.34423159\n",
            "Iteration 87, loss = 1.34463787\n",
            "Iteration 88, loss = 1.34435657\n",
            "Iteration 89, loss = 1.34435076\n",
            "Iteration 90, loss = 1.34369487\n",
            "Iteration 91, loss = 1.34377110\n",
            "Iteration 92, loss = 1.34220407\n",
            "Iteration 93, loss = 1.34274342\n",
            "Iteration 94, loss = 1.34345864\n",
            "Iteration 95, loss = 1.34332720\n",
            "Iteration 96, loss = 1.34239784\n",
            "Iteration 97, loss = 1.34600051\n",
            "Iteration 98, loss = 1.34308291\n",
            "Iteration 99, loss = 1.34261628\n",
            "Iteration 100, loss = 1.34361658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39451365\n",
            "Iteration 2, loss = 1.38287549\n",
            "Iteration 3, loss = 1.37865980\n",
            "Iteration 4, loss = 1.37736919\n",
            "Iteration 5, loss = 1.37446520\n",
            "Iteration 6, loss = 1.37331102\n",
            "Iteration 7, loss = 1.37176449\n",
            "Iteration 8, loss = 1.37012590\n",
            "Iteration 9, loss = 1.36986051\n",
            "Iteration 10, loss = 1.36882452\n",
            "Iteration 11, loss = 1.36811319\n",
            "Iteration 12, loss = 1.36774357\n",
            "Iteration 13, loss = 1.36746065\n",
            "Iteration 14, loss = 1.36761603\n",
            "Iteration 15, loss = 1.36671801\n",
            "Iteration 16, loss = 1.36525423\n",
            "Iteration 17, loss = 1.36437894\n",
            "Iteration 18, loss = 1.36397408\n",
            "Iteration 19, loss = 1.36385737\n",
            "Iteration 20, loss = 1.36327229\n",
            "Iteration 21, loss = 1.36355508\n",
            "Iteration 22, loss = 1.36310523\n",
            "Iteration 23, loss = 1.36217501\n",
            "Iteration 24, loss = 1.36126720\n",
            "Iteration 25, loss = 1.36050893\n",
            "Iteration 26, loss = 1.36036552\n",
            "Iteration 27, loss = 1.36001708\n",
            "Iteration 28, loss = 1.35907856\n",
            "Iteration 29, loss = 1.35944397\n",
            "Iteration 30, loss = 1.35881507\n",
            "Iteration 31, loss = 1.35890964\n",
            "Iteration 32, loss = 1.35877685\n",
            "Iteration 33, loss = 1.35802019\n",
            "Iteration 34, loss = 1.35826829\n",
            "Iteration 35, loss = 1.35654420\n",
            "Iteration 36, loss = 1.35736511\n",
            "Iteration 37, loss = 1.35624916\n",
            "Iteration 38, loss = 1.35689715\n",
            "Iteration 39, loss = 1.35506507\n",
            "Iteration 40, loss = 1.35581756\n",
            "Iteration 41, loss = 1.35474677\n",
            "Iteration 42, loss = 1.35438879\n",
            "Iteration 43, loss = 1.35398714\n",
            "Iteration 44, loss = 1.35335094\n",
            "Iteration 45, loss = 1.35339416\n",
            "Iteration 46, loss = 1.35381652\n",
            "Iteration 47, loss = 1.35225136\n",
            "Iteration 48, loss = 1.35333977\n",
            "Iteration 49, loss = 1.35231329\n",
            "Iteration 50, loss = 1.35314337\n",
            "Iteration 51, loss = 1.35323880\n",
            "Iteration 52, loss = 1.34998868\n",
            "Iteration 53, loss = 1.35071721\n",
            "Iteration 54, loss = 1.35071670\n",
            "Iteration 55, loss = 1.34928026\n",
            "Iteration 56, loss = 1.34949677\n",
            "Iteration 57, loss = 1.34938866\n",
            "Iteration 58, loss = 1.35124231\n",
            "Iteration 59, loss = 1.34930194\n",
            "Iteration 60, loss = 1.34976894\n",
            "Iteration 61, loss = 1.34967601\n",
            "Iteration 62, loss = 1.35029920\n",
            "Iteration 63, loss = 1.35031743\n",
            "Iteration 64, loss = 1.34939691\n",
            "Iteration 65, loss = 1.34740109\n",
            "Iteration 66, loss = 1.34749022\n",
            "Iteration 67, loss = 1.34607903\n",
            "Iteration 68, loss = 1.34590205\n",
            "Iteration 69, loss = 1.34644086\n",
            "Iteration 70, loss = 1.34601717\n",
            "Iteration 71, loss = 1.34581218\n",
            "Iteration 72, loss = 1.34564647\n",
            "Iteration 73, loss = 1.34550076\n",
            "Iteration 74, loss = 1.34488315\n",
            "Iteration 75, loss = 1.34409677\n",
            "Iteration 76, loss = 1.34448241\n",
            "Iteration 77, loss = 1.34434098\n",
            "Iteration 78, loss = 1.34469005\n",
            "Iteration 79, loss = 1.34344714\n",
            "Iteration 80, loss = 1.34576282\n",
            "Iteration 81, loss = 1.34656428\n",
            "Iteration 82, loss = 1.34466004\n",
            "Iteration 83, loss = 1.34322207\n",
            "Iteration 84, loss = 1.34341029\n",
            "Iteration 85, loss = 1.34280848\n",
            "Iteration 86, loss = 1.34238428\n",
            "Iteration 87, loss = 1.34277709\n",
            "Iteration 88, loss = 1.34275034\n",
            "Iteration 89, loss = 1.34219558\n",
            "Iteration 90, loss = 1.34154701\n",
            "Iteration 91, loss = 1.34215386\n",
            "Iteration 92, loss = 1.34044405\n",
            "Iteration 93, loss = 1.34082005\n",
            "Iteration 94, loss = 1.34146104\n",
            "Iteration 95, loss = 1.34094351\n",
            "Iteration 96, loss = 1.34102084\n",
            "Iteration 97, loss = 1.34638959\n",
            "Iteration 98, loss = 1.34471607\n",
            "Iteration 99, loss = 1.34127060\n",
            "Iteration 100, loss = 1.34210625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39391136\n",
            "Iteration 2, loss = 1.38068799\n",
            "Iteration 3, loss = 1.37740964\n",
            "Iteration 4, loss = 1.37535369\n",
            "Iteration 5, loss = 1.37390704\n",
            "Iteration 6, loss = 1.37216896\n",
            "Iteration 7, loss = 1.37111054\n",
            "Iteration 8, loss = 1.36966790\n",
            "Iteration 9, loss = 1.36872684\n",
            "Iteration 10, loss = 1.36871685\n",
            "Iteration 11, loss = 1.36774514\n",
            "Iteration 12, loss = 1.36719016\n",
            "Iteration 13, loss = 1.36674615\n",
            "Iteration 14, loss = 1.36673946\n",
            "Iteration 15, loss = 1.36559483\n",
            "Iteration 16, loss = 1.36485139\n",
            "Iteration 17, loss = 1.36429696\n",
            "Iteration 18, loss = 1.36383725\n",
            "Iteration 19, loss = 1.36377790\n",
            "Iteration 20, loss = 1.36338420\n",
            "Iteration 21, loss = 1.36318181\n",
            "Iteration 22, loss = 1.36335038\n",
            "Iteration 23, loss = 1.36222155\n",
            "Iteration 24, loss = 1.36205450\n",
            "Iteration 25, loss = 1.36171223\n",
            "Iteration 26, loss = 1.36234988\n",
            "Iteration 27, loss = 1.36126873\n",
            "Iteration 28, loss = 1.36056719\n",
            "Iteration 29, loss = 1.36140464\n",
            "Iteration 30, loss = 1.36034318\n",
            "Iteration 31, loss = 1.36010644\n",
            "Iteration 32, loss = 1.36158578\n",
            "Iteration 33, loss = 1.36082377\n",
            "Iteration 34, loss = 1.35992713\n",
            "Iteration 35, loss = 1.35863482\n",
            "Iteration 36, loss = 1.35900729\n",
            "Iteration 37, loss = 1.35780914\n",
            "Iteration 38, loss = 1.35855795\n",
            "Iteration 39, loss = 1.35734078\n",
            "Iteration 40, loss = 1.35789642\n",
            "Iteration 41, loss = 1.35690397\n",
            "Iteration 42, loss = 1.35661533\n",
            "Iteration 43, loss = 1.35659301\n",
            "Iteration 44, loss = 1.35559310\n",
            "Iteration 45, loss = 1.35511745\n",
            "Iteration 46, loss = 1.35522032\n",
            "Iteration 47, loss = 1.35503497\n",
            "Iteration 48, loss = 1.35617730\n",
            "Iteration 49, loss = 1.35585754\n",
            "Iteration 50, loss = 1.35556976\n",
            "Iteration 51, loss = 1.35416651\n",
            "Iteration 52, loss = 1.35409743\n",
            "Iteration 53, loss = 1.35316709\n",
            "Iteration 54, loss = 1.35336092\n",
            "Iteration 55, loss = 1.35250091\n",
            "Iteration 56, loss = 1.35234709\n",
            "Iteration 57, loss = 1.35146378\n",
            "Iteration 58, loss = 1.35291099\n",
            "Iteration 59, loss = 1.35175164\n",
            "Iteration 60, loss = 1.35353862\n",
            "Iteration 61, loss = 1.35203598\n",
            "Iteration 62, loss = 1.35190477\n",
            "Iteration 63, loss = 1.35254847\n",
            "Iteration 64, loss = 1.35193516\n",
            "Iteration 65, loss = 1.35063674\n",
            "Iteration 66, loss = 1.34988673\n",
            "Iteration 67, loss = 1.34898524\n",
            "Iteration 68, loss = 1.34938685\n",
            "Iteration 69, loss = 1.35010918\n",
            "Iteration 70, loss = 1.34962442\n",
            "Iteration 71, loss = 1.34903388\n",
            "Iteration 72, loss = 1.34909409\n",
            "Iteration 73, loss = 1.34858779\n",
            "Iteration 74, loss = 1.34877405\n",
            "Iteration 75, loss = 1.34922188\n",
            "Iteration 76, loss = 1.34791230\n",
            "Iteration 77, loss = 1.34780544\n",
            "Iteration 78, loss = 1.34803777\n",
            "Iteration 79, loss = 1.34672380\n",
            "Iteration 80, loss = 1.34878913\n",
            "Iteration 81, loss = 1.34939959\n",
            "Iteration 82, loss = 1.34670055\n",
            "Iteration 83, loss = 1.34663009\n",
            "Iteration 84, loss = 1.34622907\n",
            "Iteration 85, loss = 1.34715904\n",
            "Iteration 86, loss = 1.34688509\n",
            "Iteration 87, loss = 1.34710118\n",
            "Iteration 88, loss = 1.34611608\n",
            "Iteration 89, loss = 1.34566620\n",
            "Iteration 90, loss = 1.34470935\n",
            "Iteration 91, loss = 1.34578638\n",
            "Iteration 92, loss = 1.34467604\n",
            "Iteration 93, loss = 1.34463151\n",
            "Iteration 94, loss = 1.34469439\n",
            "Iteration 95, loss = 1.34532390\n",
            "Iteration 96, loss = 1.34462585\n",
            "Iteration 97, loss = 1.34790144\n",
            "Iteration 98, loss = 1.34752338\n",
            "Iteration 99, loss = 1.34455469\n",
            "Iteration 100, loss = 1.34404731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38701528\n",
            "Iteration 2, loss = 1.36774174\n",
            "Iteration 3, loss = 1.36339161\n",
            "Iteration 4, loss = 1.36007182\n",
            "Iteration 5, loss = 1.35802571\n",
            "Iteration 6, loss = 1.35530668\n",
            "Iteration 7, loss = 1.35434280\n",
            "Iteration 8, loss = 1.35191928\n",
            "Iteration 9, loss = 1.35064967\n",
            "Iteration 10, loss = 1.35011911\n",
            "Iteration 11, loss = 1.34919140\n",
            "Iteration 12, loss = 1.34827495\n",
            "Iteration 13, loss = 1.34792725\n",
            "Iteration 14, loss = 1.34736799\n",
            "Iteration 15, loss = 1.34651860\n",
            "Iteration 16, loss = 1.34615295\n",
            "Iteration 17, loss = 1.34585196\n",
            "Iteration 18, loss = 1.34491226\n",
            "Iteration 19, loss = 1.34472555\n",
            "Iteration 20, loss = 1.34425564\n",
            "Iteration 21, loss = 1.34477200\n",
            "Iteration 22, loss = 1.34462104\n",
            "Iteration 23, loss = 1.34380438\n",
            "Iteration 24, loss = 1.34363495\n",
            "Iteration 25, loss = 1.34417801\n",
            "Iteration 26, loss = 1.34484776\n",
            "Iteration 27, loss = 1.34323558\n",
            "Iteration 28, loss = 1.34361020\n",
            "Iteration 29, loss = 1.34480929\n",
            "Iteration 30, loss = 1.34244180\n",
            "Iteration 31, loss = 1.34304437\n",
            "Iteration 32, loss = 1.34340525\n",
            "Iteration 33, loss = 1.34307687\n",
            "Iteration 34, loss = 1.34316341\n",
            "Iteration 35, loss = 1.34247330\n",
            "Iteration 36, loss = 1.34288131\n",
            "Iteration 37, loss = 1.34139016\n",
            "Iteration 38, loss = 1.34196234\n",
            "Iteration 39, loss = 1.34040394\n",
            "Iteration 40, loss = 1.34167795\n",
            "Iteration 41, loss = 1.34082436\n",
            "Iteration 42, loss = 1.34119733\n",
            "Iteration 43, loss = 1.34007262\n",
            "Iteration 44, loss = 1.33971300\n",
            "Iteration 45, loss = 1.34009612\n",
            "Iteration 46, loss = 1.33943982\n",
            "Iteration 47, loss = 1.33953796\n",
            "Iteration 48, loss = 1.34009288\n",
            "Iteration 49, loss = 1.33989740\n",
            "Iteration 50, loss = 1.34084376\n",
            "Iteration 51, loss = 1.33913789\n",
            "Iteration 52, loss = 1.33907999\n",
            "Iteration 53, loss = 1.33862832\n",
            "Iteration 54, loss = 1.33869903\n",
            "Iteration 55, loss = 1.33816260\n",
            "Iteration 56, loss = 1.33818478\n",
            "Iteration 57, loss = 1.33773031\n",
            "Iteration 58, loss = 1.33767988\n",
            "Iteration 59, loss = 1.33735464\n",
            "Iteration 60, loss = 1.33801047\n",
            "Iteration 61, loss = 1.33748051\n",
            "Iteration 62, loss = 1.33719152\n",
            "Iteration 63, loss = 1.33677250\n",
            "Iteration 64, loss = 1.33712284\n",
            "Iteration 65, loss = 1.33701475\n",
            "Iteration 66, loss = 1.33588536\n",
            "Iteration 67, loss = 1.33577140\n",
            "Iteration 68, loss = 1.33588770\n",
            "Iteration 69, loss = 1.33586049\n",
            "Iteration 70, loss = 1.33579885\n",
            "Iteration 71, loss = 1.33537190\n",
            "Iteration 72, loss = 1.33613972\n",
            "Iteration 73, loss = 1.33596584\n",
            "Iteration 74, loss = 1.33476921\n",
            "Iteration 75, loss = 1.33483835\n",
            "Iteration 76, loss = 1.33461868\n",
            "Iteration 77, loss = 1.33476283\n",
            "Iteration 78, loss = 1.33543855\n",
            "Iteration 79, loss = 1.33385299\n",
            "Iteration 80, loss = 1.33675415\n",
            "Iteration 81, loss = 1.33702151\n",
            "Iteration 82, loss = 1.33417727\n",
            "Iteration 83, loss = 1.33539115\n",
            "Iteration 84, loss = 1.33411064\n",
            "Iteration 85, loss = 1.33533329\n",
            "Iteration 86, loss = 1.33368865\n",
            "Iteration 87, loss = 1.33428538\n",
            "Iteration 88, loss = 1.33364296\n",
            "Iteration 89, loss = 1.33343755\n",
            "Iteration 90, loss = 1.33260171\n",
            "Iteration 91, loss = 1.33443325\n",
            "Iteration 92, loss = 1.33233799\n",
            "Iteration 93, loss = 1.33284657\n",
            "Iteration 94, loss = 1.33251876\n",
            "Iteration 95, loss = 1.33277672\n",
            "Iteration 96, loss = 1.33195504\n",
            "Iteration 97, loss = 1.33377580\n",
            "Iteration 98, loss = 1.33524245\n",
            "Iteration 99, loss = 1.33272356\n",
            "Iteration 100, loss = 1.33159731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38174177\n",
            "Iteration 2, loss = 1.37626768\n",
            "Iteration 3, loss = 1.37423987\n",
            "Iteration 4, loss = 1.37169811\n",
            "Iteration 5, loss = 1.37092675\n",
            "Iteration 6, loss = 1.36855903\n",
            "Iteration 7, loss = 1.36797810\n",
            "Iteration 8, loss = 1.36624652\n",
            "Iteration 9, loss = 1.36563555\n",
            "Iteration 10, loss = 1.36508991\n",
            "Iteration 11, loss = 1.36467381\n",
            "Iteration 12, loss = 1.36381663\n",
            "Iteration 13, loss = 1.36310361\n",
            "Iteration 14, loss = 1.36295280\n",
            "Iteration 15, loss = 1.36173087\n",
            "Iteration 16, loss = 1.36153178\n",
            "Iteration 17, loss = 1.36097492\n",
            "Iteration 18, loss = 1.36026621\n",
            "Iteration 19, loss = 1.36013263\n",
            "Iteration 20, loss = 1.35911686\n",
            "Iteration 21, loss = 1.35921512\n",
            "Iteration 22, loss = 1.35934696\n",
            "Iteration 23, loss = 1.35833709\n",
            "Iteration 24, loss = 1.35777023\n",
            "Iteration 25, loss = 1.35955076\n",
            "Iteration 26, loss = 1.35986283\n",
            "Iteration 27, loss = 1.35694367\n",
            "Iteration 28, loss = 1.35613370\n",
            "Iteration 29, loss = 1.35907707\n",
            "Iteration 30, loss = 1.35614408\n",
            "Iteration 31, loss = 1.35483337\n",
            "Iteration 32, loss = 1.35562828\n",
            "Iteration 33, loss = 1.35464027\n",
            "Iteration 34, loss = 1.35508200\n",
            "Iteration 35, loss = 1.35423793\n",
            "Iteration 36, loss = 1.35402169\n",
            "Iteration 37, loss = 1.35329938\n",
            "Iteration 38, loss = 1.35308232\n",
            "Iteration 39, loss = 1.35213154\n",
            "Iteration 40, loss = 1.35295403\n",
            "Iteration 41, loss = 1.35256059\n",
            "Iteration 42, loss = 1.35085720\n",
            "Iteration 43, loss = 1.35127981\n",
            "Iteration 44, loss = 1.35053376\n",
            "Iteration 45, loss = 1.34999007\n",
            "Iteration 46, loss = 1.35008359\n",
            "Iteration 47, loss = 1.35007545\n",
            "Iteration 48, loss = 1.35262567\n",
            "Iteration 49, loss = 1.35034584\n",
            "Iteration 50, loss = 1.35270151\n",
            "Iteration 51, loss = 1.35222985\n",
            "Iteration 52, loss = 1.34938760\n",
            "Iteration 53, loss = 1.34961289\n",
            "Iteration 54, loss = 1.34916361\n",
            "Iteration 55, loss = 1.34800819\n",
            "Iteration 56, loss = 1.34773298\n",
            "Iteration 57, loss = 1.34779092\n",
            "Iteration 58, loss = 1.34765312\n",
            "Iteration 59, loss = 1.34806197\n",
            "Iteration 60, loss = 1.34827071\n",
            "Iteration 61, loss = 1.34761262\n",
            "Iteration 62, loss = 1.34672224\n",
            "Iteration 63, loss = 1.34640469\n",
            "Iteration 64, loss = 1.34649838\n",
            "Iteration 65, loss = 1.34601109\n",
            "Iteration 66, loss = 1.34599108\n",
            "Iteration 67, loss = 1.34605107\n",
            "Iteration 68, loss = 1.34598397\n",
            "Iteration 69, loss = 1.34635609\n",
            "Iteration 70, loss = 1.34568389\n",
            "Iteration 71, loss = 1.34638838\n",
            "Iteration 72, loss = 1.34788328\n",
            "Iteration 73, loss = 1.34665555\n",
            "Iteration 74, loss = 1.34472648\n",
            "Iteration 75, loss = 1.34539902\n",
            "Iteration 76, loss = 1.34570219\n",
            "Iteration 77, loss = 1.34519509\n",
            "Iteration 78, loss = 1.34569974\n",
            "Iteration 79, loss = 1.34490612\n",
            "Iteration 80, loss = 1.34629572\n",
            "Iteration 81, loss = 1.34733800\n",
            "Iteration 82, loss = 1.34469013\n",
            "Iteration 83, loss = 1.34418860\n",
            "Iteration 84, loss = 1.34466154\n",
            "Iteration 85, loss = 1.34518570\n",
            "Iteration 86, loss = 1.34525848\n",
            "Iteration 87, loss = 1.34404749\n",
            "Iteration 88, loss = 1.34379294\n",
            "Iteration 89, loss = 1.34429268\n",
            "Iteration 90, loss = 1.34346217\n",
            "Iteration 91, loss = 1.34378927\n",
            "Iteration 92, loss = 1.34311403\n",
            "Iteration 93, loss = 1.34328289\n",
            "Iteration 94, loss = 1.34320194\n",
            "Iteration 95, loss = 1.34350443\n",
            "Iteration 96, loss = 1.34295700\n",
            "Iteration 97, loss = 1.34395323\n",
            "Iteration 98, loss = 1.34487956\n",
            "Iteration 99, loss = 1.34520802\n",
            "Iteration 100, loss = 1.34437402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37766012\n",
            "Iteration 2, loss = 1.37269177\n",
            "Iteration 3, loss = 1.37116282\n",
            "Iteration 4, loss = 1.36869575\n",
            "Iteration 5, loss = 1.36743521\n",
            "Iteration 6, loss = 1.36547193\n",
            "Iteration 7, loss = 1.36475836\n",
            "Iteration 8, loss = 1.36384689\n",
            "Iteration 9, loss = 1.36304218\n",
            "Iteration 10, loss = 1.36273319\n",
            "Iteration 11, loss = 1.36205282\n",
            "Iteration 12, loss = 1.36171019\n",
            "Iteration 13, loss = 1.36109376\n",
            "Iteration 14, loss = 1.36038525\n",
            "Iteration 15, loss = 1.35947706\n",
            "Iteration 16, loss = 1.35936938\n",
            "Iteration 17, loss = 1.35905235\n",
            "Iteration 18, loss = 1.35861185\n",
            "Iteration 19, loss = 1.35889691\n",
            "Iteration 20, loss = 1.35787212\n",
            "Iteration 21, loss = 1.35821839\n",
            "Iteration 22, loss = 1.35736674\n",
            "Iteration 23, loss = 1.35693331\n",
            "Iteration 24, loss = 1.35666217\n",
            "Iteration 25, loss = 1.35776040\n",
            "Iteration 26, loss = 1.35828870\n",
            "Iteration 27, loss = 1.35625453\n",
            "Iteration 28, loss = 1.35562902\n",
            "Iteration 29, loss = 1.35704780\n",
            "Iteration 30, loss = 1.35530198\n",
            "Iteration 31, loss = 1.35377698\n",
            "Iteration 32, loss = 1.35483734\n",
            "Iteration 33, loss = 1.35379712\n",
            "Iteration 34, loss = 1.35400057\n",
            "Iteration 35, loss = 1.35340717\n",
            "Iteration 36, loss = 1.35475315\n",
            "Iteration 37, loss = 1.35166126\n",
            "Iteration 38, loss = 1.35245296\n",
            "Iteration 39, loss = 1.35168514\n",
            "Iteration 40, loss = 1.35256632\n",
            "Iteration 41, loss = 1.35153943\n",
            "Iteration 42, loss = 1.35019149\n",
            "Iteration 43, loss = 1.35084715\n",
            "Iteration 44, loss = 1.34997588\n",
            "Iteration 45, loss = 1.35049443\n",
            "Iteration 46, loss = 1.35014675\n",
            "Iteration 47, loss = 1.35024084\n",
            "Iteration 48, loss = 1.34961805\n",
            "Iteration 49, loss = 1.34859447\n",
            "Iteration 50, loss = 1.34884878\n",
            "Iteration 51, loss = 1.34878835\n",
            "Iteration 52, loss = 1.34702683\n",
            "Iteration 53, loss = 1.34758799\n",
            "Iteration 54, loss = 1.34832724\n",
            "Iteration 55, loss = 1.34720825\n",
            "Iteration 56, loss = 1.34649119\n",
            "Iteration 57, loss = 1.34667166\n",
            "Iteration 58, loss = 1.34545217\n",
            "Iteration 59, loss = 1.34615800\n",
            "Iteration 60, loss = 1.34603541\n",
            "Iteration 61, loss = 1.34611138\n",
            "Iteration 62, loss = 1.34479753\n",
            "Iteration 63, loss = 1.34410608\n",
            "Iteration 64, loss = 1.34345529\n",
            "Iteration 65, loss = 1.34402125\n",
            "Iteration 66, loss = 1.34365368\n",
            "Iteration 67, loss = 1.34414709\n",
            "Iteration 68, loss = 1.34400098\n",
            "Iteration 69, loss = 1.34337777\n",
            "Iteration 70, loss = 1.34311065\n",
            "Iteration 71, loss = 1.34362355\n",
            "Iteration 72, loss = 1.34437962\n",
            "Iteration 73, loss = 1.34385706\n",
            "Iteration 74, loss = 1.34247644\n",
            "Iteration 75, loss = 1.34286771\n",
            "Iteration 76, loss = 1.34299093\n",
            "Iteration 77, loss = 1.34234650\n",
            "Iteration 78, loss = 1.34261524\n",
            "Iteration 79, loss = 1.34286074\n",
            "Iteration 80, loss = 1.34443297\n",
            "Iteration 81, loss = 1.34336114\n",
            "Iteration 82, loss = 1.34339235\n",
            "Iteration 83, loss = 1.34138165\n",
            "Iteration 84, loss = 1.34273644\n",
            "Iteration 85, loss = 1.34366763\n",
            "Iteration 86, loss = 1.34247585\n",
            "Iteration 87, loss = 1.34229529\n",
            "Iteration 88, loss = 1.34132872\n",
            "Iteration 89, loss = 1.34150203\n",
            "Iteration 90, loss = 1.34047800\n",
            "Iteration 91, loss = 1.34114839\n",
            "Iteration 92, loss = 1.34055904\n",
            "Iteration 93, loss = 1.34074511\n",
            "Iteration 94, loss = 1.34120718\n",
            "Iteration 95, loss = 1.34145249\n",
            "Iteration 96, loss = 1.34180198\n",
            "Iteration 97, loss = 1.34292373\n",
            "Iteration 98, loss = 1.34308047\n",
            "Iteration 99, loss = 1.34312633\n",
            "Iteration 100, loss = 1.34199952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37694513\n",
            "Iteration 2, loss = 1.36380339\n",
            "Iteration 3, loss = 1.35993501\n",
            "Iteration 4, loss = 1.35858219\n",
            "Iteration 5, loss = 1.35677859\n",
            "Iteration 6, loss = 1.35551727\n",
            "Iteration 7, loss = 1.35483483\n",
            "Iteration 8, loss = 1.35347181\n",
            "Iteration 9, loss = 1.35280338\n",
            "Iteration 10, loss = 1.35201134\n",
            "Iteration 11, loss = 1.35118689\n",
            "Iteration 12, loss = 1.35079888\n",
            "Iteration 13, loss = 1.35039405\n",
            "Iteration 14, loss = 1.35045102\n",
            "Iteration 15, loss = 1.34883659\n",
            "Iteration 16, loss = 1.34859214\n",
            "Iteration 17, loss = 1.34808871\n",
            "Iteration 18, loss = 1.34766330\n",
            "Iteration 19, loss = 1.34744672\n",
            "Iteration 20, loss = 1.34678494\n",
            "Iteration 21, loss = 1.34670288\n",
            "Iteration 22, loss = 1.34642477\n",
            "Iteration 23, loss = 1.34609839\n",
            "Iteration 24, loss = 1.34552044\n",
            "Iteration 25, loss = 1.34736903\n",
            "Iteration 26, loss = 1.34785143\n",
            "Iteration 27, loss = 1.34488690\n",
            "Iteration 28, loss = 1.34376693\n",
            "Iteration 29, loss = 1.34508178\n",
            "Iteration 30, loss = 1.34396344\n",
            "Iteration 31, loss = 1.34281396\n",
            "Iteration 32, loss = 1.34376022\n",
            "Iteration 33, loss = 1.34208795\n",
            "Iteration 34, loss = 1.34288651\n",
            "Iteration 35, loss = 1.34201492\n",
            "Iteration 36, loss = 1.34211004\n",
            "Iteration 37, loss = 1.34001273\n",
            "Iteration 38, loss = 1.34030823\n",
            "Iteration 39, loss = 1.34011125\n",
            "Iteration 40, loss = 1.33988812\n",
            "Iteration 41, loss = 1.33973511\n",
            "Iteration 42, loss = 1.33813684\n",
            "Iteration 43, loss = 1.33911625\n",
            "Iteration 44, loss = 1.33779943\n",
            "Iteration 45, loss = 1.33841310\n",
            "Iteration 46, loss = 1.33883299\n",
            "Iteration 47, loss = 1.33930155\n",
            "Iteration 48, loss = 1.33806382\n",
            "Iteration 49, loss = 1.33728200\n",
            "Iteration 50, loss = 1.33793055\n",
            "Iteration 51, loss = 1.33879093\n",
            "Iteration 52, loss = 1.33545933\n",
            "Iteration 53, loss = 1.33568131\n",
            "Iteration 54, loss = 1.33522890\n",
            "Iteration 55, loss = 1.33679605\n",
            "Iteration 56, loss = 1.33458630\n",
            "Iteration 57, loss = 1.33508139\n",
            "Iteration 58, loss = 1.33410478\n",
            "Iteration 59, loss = 1.33440428\n",
            "Iteration 60, loss = 1.33502971\n",
            "Iteration 61, loss = 1.33490112\n",
            "Iteration 62, loss = 1.33357660\n",
            "Iteration 63, loss = 1.33284342\n",
            "Iteration 64, loss = 1.33241596\n",
            "Iteration 65, loss = 1.33301230\n",
            "Iteration 66, loss = 1.33191810\n",
            "Iteration 67, loss = 1.33233468\n",
            "Iteration 68, loss = 1.33238890\n",
            "Iteration 69, loss = 1.33220493\n",
            "Iteration 70, loss = 1.33195635\n",
            "Iteration 71, loss = 1.33305068\n",
            "Iteration 72, loss = 1.33344908\n",
            "Iteration 73, loss = 1.33211305\n",
            "Iteration 74, loss = 1.33109338\n",
            "Iteration 75, loss = 1.33093767\n",
            "Iteration 76, loss = 1.33225501\n",
            "Iteration 77, loss = 1.33073412\n",
            "Iteration 78, loss = 1.33183583\n",
            "Iteration 79, loss = 1.33193315\n",
            "Iteration 80, loss = 1.33232044\n",
            "Iteration 81, loss = 1.33322926\n",
            "Iteration 82, loss = 1.33341588\n",
            "Iteration 83, loss = 1.33074213\n",
            "Iteration 84, loss = 1.33194403\n",
            "Iteration 85, loss = 1.33184598\n",
            "Iteration 86, loss = 1.33048471\n",
            "Iteration 87, loss = 1.33065418\n",
            "Iteration 88, loss = 1.32968282\n",
            "Iteration 89, loss = 1.32977704\n",
            "Iteration 90, loss = 1.32918128\n",
            "Iteration 91, loss = 1.32995601\n",
            "Iteration 92, loss = 1.32961229\n",
            "Iteration 93, loss = 1.32932510\n",
            "Iteration 94, loss = 1.32946232\n",
            "Iteration 95, loss = 1.33042260\n",
            "Iteration 96, loss = 1.32985705\n",
            "Iteration 97, loss = 1.33029688\n",
            "Iteration 98, loss = 1.33092693\n",
            "Iteration 99, loss = 1.33121165\n",
            "Iteration 100, loss = 1.33303090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n",
            "Iteration 1, loss = 1.39129299\n",
            "Iteration 2, loss = 1.38183898\n",
            "Iteration 3, loss = 1.37725523\n",
            "Iteration 4, loss = 1.37323687\n",
            "Iteration 5, loss = 1.37237933\n",
            "Iteration 6, loss = 1.37187642\n",
            "Iteration 7, loss = 1.36968200\n",
            "Iteration 8, loss = 1.36859661\n",
            "Iteration 9, loss = 1.36808786\n",
            "Iteration 10, loss = 1.36734820\n",
            "Iteration 11, loss = 1.36699698\n",
            "Iteration 12, loss = 1.36677845\n",
            "Iteration 13, loss = 1.36632773\n",
            "Iteration 14, loss = 1.36624613\n",
            "Iteration 15, loss = 1.36531168\n",
            "Iteration 16, loss = 1.36506716\n",
            "Iteration 17, loss = 1.36518787\n",
            "Iteration 18, loss = 1.36436715\n",
            "Iteration 19, loss = 1.36408594\n",
            "Iteration 20, loss = 1.36384464\n",
            "Iteration 21, loss = 1.36325167\n",
            "Iteration 22, loss = 1.36337037\n",
            "Iteration 23, loss = 1.36268914\n",
            "Iteration 24, loss = 1.36246870\n",
            "Iteration 25, loss = 1.36218090\n",
            "Iteration 26, loss = 1.36304691\n",
            "Iteration 27, loss = 1.36122090\n",
            "Iteration 28, loss = 1.36090667\n",
            "Iteration 29, loss = 1.36052966\n",
            "Iteration 30, loss = 1.36028175\n",
            "Iteration 31, loss = 1.36071963\n",
            "Iteration 32, loss = 1.36135398\n",
            "Iteration 33, loss = 1.36073188\n",
            "Iteration 34, loss = 1.35925486\n",
            "Iteration 35, loss = 1.35796569\n",
            "Iteration 36, loss = 1.35865857\n",
            "Iteration 37, loss = 1.35695042\n",
            "Iteration 38, loss = 1.35713826\n",
            "Iteration 39, loss = 1.35661066\n",
            "Iteration 40, loss = 1.35495501\n",
            "Iteration 41, loss = 1.35622421\n",
            "Iteration 42, loss = 1.35578788\n",
            "Iteration 43, loss = 1.35427808\n",
            "Iteration 44, loss = 1.35516781\n",
            "Iteration 45, loss = 1.35480943\n",
            "Iteration 46, loss = 1.35322008\n",
            "Iteration 47, loss = 1.35377700\n",
            "Iteration 48, loss = 1.35220959\n",
            "Iteration 49, loss = 1.35269656\n",
            "Iteration 50, loss = 1.35222574\n",
            "Iteration 51, loss = 1.35271826\n",
            "Iteration 52, loss = 1.35046124\n",
            "Iteration 53, loss = 1.35021453\n",
            "Iteration 54, loss = 1.35065289\n",
            "Iteration 55, loss = 1.34918002\n",
            "Iteration 56, loss = 1.34990667\n",
            "Iteration 57, loss = 1.34839037\n",
            "Iteration 58, loss = 1.35072462\n",
            "Iteration 59, loss = 1.34959801\n",
            "Iteration 60, loss = 1.34879556\n",
            "Iteration 61, loss = 1.34806004\n",
            "Iteration 62, loss = 1.34818796\n",
            "Iteration 63, loss = 1.34872596\n",
            "Iteration 64, loss = 1.34919747\n",
            "Iteration 65, loss = 1.34833370\n",
            "Iteration 66, loss = 1.34806589\n",
            "Iteration 67, loss = 1.34629099\n",
            "Iteration 68, loss = 1.34634342\n",
            "Iteration 69, loss = 1.34610072\n",
            "Iteration 70, loss = 1.34694582\n",
            "Iteration 71, loss = 1.34678436\n",
            "Iteration 72, loss = 1.34610632\n",
            "Iteration 73, loss = 1.34641623\n",
            "Iteration 74, loss = 1.34546183\n",
            "Iteration 75, loss = 1.34604972\n",
            "Iteration 76, loss = 1.34641751\n",
            "Iteration 77, loss = 1.34514737\n",
            "Iteration 78, loss = 1.34497741\n",
            "Iteration 79, loss = 1.34565249\n",
            "Iteration 80, loss = 1.34460927\n",
            "Iteration 81, loss = 1.34471044\n",
            "Iteration 82, loss = 1.34527503\n",
            "Iteration 83, loss = 1.34528714\n",
            "Iteration 84, loss = 1.34481477\n",
            "Iteration 85, loss = 1.34527072\n",
            "Iteration 86, loss = 1.34512891\n",
            "Iteration 87, loss = 1.34454529\n",
            "Iteration 88, loss = 1.34535135\n",
            "Iteration 89, loss = 1.34740798\n",
            "Iteration 90, loss = 1.34445833\n",
            "Iteration 91, loss = 1.34483567\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39278924\n",
            "Iteration 2, loss = 1.38312244\n",
            "Iteration 3, loss = 1.37951744\n",
            "Iteration 4, loss = 1.37628848\n",
            "Iteration 5, loss = 1.37529151\n",
            "Iteration 6, loss = 1.37415320\n",
            "Iteration 7, loss = 1.37284407\n",
            "Iteration 8, loss = 1.37226730\n",
            "Iteration 9, loss = 1.37173109\n",
            "Iteration 10, loss = 1.37137579\n",
            "Iteration 11, loss = 1.37031605\n",
            "Iteration 12, loss = 1.37076453\n",
            "Iteration 13, loss = 1.36963416\n",
            "Iteration 14, loss = 1.36998420\n",
            "Iteration 15, loss = 1.36954970\n",
            "Iteration 16, loss = 1.36835042\n",
            "Iteration 17, loss = 1.36861990\n",
            "Iteration 18, loss = 1.36789864\n",
            "Iteration 19, loss = 1.36722417\n",
            "Iteration 20, loss = 1.36737895\n",
            "Iteration 21, loss = 1.36681634\n",
            "Iteration 22, loss = 1.36742384\n",
            "Iteration 23, loss = 1.36662656\n",
            "Iteration 24, loss = 1.36641354\n",
            "Iteration 25, loss = 1.36563580\n",
            "Iteration 26, loss = 1.36667431\n",
            "Iteration 27, loss = 1.36551167\n",
            "Iteration 28, loss = 1.36481823\n",
            "Iteration 29, loss = 1.36503881\n",
            "Iteration 30, loss = 1.36465719\n",
            "Iteration 31, loss = 1.36531516\n",
            "Iteration 32, loss = 1.36606681\n",
            "Iteration 33, loss = 1.36508520\n",
            "Iteration 34, loss = 1.36354275\n",
            "Iteration 35, loss = 1.36316200\n",
            "Iteration 36, loss = 1.36345241\n",
            "Iteration 37, loss = 1.36238364\n",
            "Iteration 38, loss = 1.36256506\n",
            "Iteration 39, loss = 1.36226639\n",
            "Iteration 40, loss = 1.36134545\n",
            "Iteration 41, loss = 1.36210225\n",
            "Iteration 42, loss = 1.36206392\n",
            "Iteration 43, loss = 1.36030949\n",
            "Iteration 44, loss = 1.36092609\n",
            "Iteration 45, loss = 1.36082568\n",
            "Iteration 46, loss = 1.36037303\n",
            "Iteration 47, loss = 1.36035753\n",
            "Iteration 48, loss = 1.35906656\n",
            "Iteration 49, loss = 1.35991642\n",
            "Iteration 50, loss = 1.35847068\n",
            "Iteration 51, loss = 1.35900177\n",
            "Iteration 52, loss = 1.35757628\n",
            "Iteration 53, loss = 1.35748124\n",
            "Iteration 54, loss = 1.35739361\n",
            "Iteration 55, loss = 1.35645188\n",
            "Iteration 56, loss = 1.35700706\n",
            "Iteration 57, loss = 1.35592069\n",
            "Iteration 58, loss = 1.35876991\n",
            "Iteration 59, loss = 1.35677261\n",
            "Iteration 60, loss = 1.35601587\n",
            "Iteration 61, loss = 1.35525107\n",
            "Iteration 62, loss = 1.35536099\n",
            "Iteration 63, loss = 1.35659235\n",
            "Iteration 64, loss = 1.35627605\n",
            "Iteration 65, loss = 1.35526027\n",
            "Iteration 66, loss = 1.35557048\n",
            "Iteration 67, loss = 1.35415793\n",
            "Iteration 68, loss = 1.35366987\n",
            "Iteration 69, loss = 1.35355055\n",
            "Iteration 70, loss = 1.35398890\n",
            "Iteration 71, loss = 1.35390436\n",
            "Iteration 72, loss = 1.35329105\n",
            "Iteration 73, loss = 1.35374054\n",
            "Iteration 74, loss = 1.35285539\n",
            "Iteration 75, loss = 1.35309653\n",
            "Iteration 76, loss = 1.35347328\n",
            "Iteration 77, loss = 1.35209768\n",
            "Iteration 78, loss = 1.35243202\n",
            "Iteration 79, loss = 1.35260867\n",
            "Iteration 80, loss = 1.35235734\n",
            "Iteration 81, loss = 1.35231325\n",
            "Iteration 82, loss = 1.35198305\n",
            "Iteration 83, loss = 1.35188617\n",
            "Iteration 84, loss = 1.35140456\n",
            "Iteration 85, loss = 1.35250952\n",
            "Iteration 86, loss = 1.35244901\n",
            "Iteration 87, loss = 1.35206556\n",
            "Iteration 88, loss = 1.35183531\n",
            "Iteration 89, loss = 1.35309784\n",
            "Iteration 90, loss = 1.35071286\n",
            "Iteration 91, loss = 1.35096780\n",
            "Iteration 92, loss = 1.34987674\n",
            "Iteration 93, loss = 1.35049420\n",
            "Iteration 94, loss = 1.35050666\n",
            "Iteration 95, loss = 1.35027705\n",
            "Iteration 96, loss = 1.35037313\n",
            "Iteration 97, loss = 1.35346429\n",
            "Iteration 98, loss = 1.35208021\n",
            "Iteration 99, loss = 1.34937118\n",
            "Iteration 100, loss = 1.34998154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39551656\n",
            "Iteration 2, loss = 1.38378049\n",
            "Iteration 3, loss = 1.38052538\n",
            "Iteration 4, loss = 1.37689695\n",
            "Iteration 5, loss = 1.37530268\n",
            "Iteration 6, loss = 1.37410766\n",
            "Iteration 7, loss = 1.37259948\n",
            "Iteration 8, loss = 1.37132019\n",
            "Iteration 9, loss = 1.37110492\n",
            "Iteration 10, loss = 1.37091650\n",
            "Iteration 11, loss = 1.36955516\n",
            "Iteration 12, loss = 1.36896016\n",
            "Iteration 13, loss = 1.36853262\n",
            "Iteration 14, loss = 1.36866520\n",
            "Iteration 15, loss = 1.36812101\n",
            "Iteration 16, loss = 1.36701096\n",
            "Iteration 17, loss = 1.36694875\n",
            "Iteration 18, loss = 1.36655917\n",
            "Iteration 19, loss = 1.36595295\n",
            "Iteration 20, loss = 1.36611457\n",
            "Iteration 21, loss = 1.36541772\n",
            "Iteration 22, loss = 1.36585918\n",
            "Iteration 23, loss = 1.36470995\n",
            "Iteration 24, loss = 1.36421745\n",
            "Iteration 25, loss = 1.36311571\n",
            "Iteration 26, loss = 1.36411978\n",
            "Iteration 27, loss = 1.36263199\n",
            "Iteration 28, loss = 1.36237316\n",
            "Iteration 29, loss = 1.36248422\n",
            "Iteration 30, loss = 1.36167300\n",
            "Iteration 31, loss = 1.36145923\n",
            "Iteration 32, loss = 1.36277138\n",
            "Iteration 33, loss = 1.36221124\n",
            "Iteration 34, loss = 1.36019725\n",
            "Iteration 35, loss = 1.35950840\n",
            "Iteration 36, loss = 1.36000645\n",
            "Iteration 37, loss = 1.35922937\n",
            "Iteration 38, loss = 1.35922040\n",
            "Iteration 39, loss = 1.35820282\n",
            "Iteration 40, loss = 1.35713850\n",
            "Iteration 41, loss = 1.35767363\n",
            "Iteration 42, loss = 1.35771408\n",
            "Iteration 43, loss = 1.35585488\n",
            "Iteration 44, loss = 1.35606711\n",
            "Iteration 45, loss = 1.35594639\n",
            "Iteration 46, loss = 1.35490355\n",
            "Iteration 47, loss = 1.35500639\n",
            "Iteration 48, loss = 1.35428592\n",
            "Iteration 49, loss = 1.35450261\n",
            "Iteration 50, loss = 1.35413856\n",
            "Iteration 51, loss = 1.35541027\n",
            "Iteration 52, loss = 1.35169482\n",
            "Iteration 53, loss = 1.35182378\n",
            "Iteration 54, loss = 1.35228144\n",
            "Iteration 55, loss = 1.35010016\n",
            "Iteration 56, loss = 1.35009525\n",
            "Iteration 57, loss = 1.35052102\n",
            "Iteration 58, loss = 1.35205742\n",
            "Iteration 59, loss = 1.35015086\n",
            "Iteration 60, loss = 1.34964727\n",
            "Iteration 61, loss = 1.34845577\n",
            "Iteration 62, loss = 1.34844509\n",
            "Iteration 63, loss = 1.34890713\n",
            "Iteration 64, loss = 1.35015802\n",
            "Iteration 65, loss = 1.34987455\n",
            "Iteration 66, loss = 1.34852503\n",
            "Iteration 67, loss = 1.34671954\n",
            "Iteration 68, loss = 1.34661032\n",
            "Iteration 69, loss = 1.34603473\n",
            "Iteration 70, loss = 1.34677651\n",
            "Iteration 71, loss = 1.34637828\n",
            "Iteration 72, loss = 1.34669654\n",
            "Iteration 73, loss = 1.34643179\n",
            "Iteration 74, loss = 1.34614969\n",
            "Iteration 75, loss = 1.34663083\n",
            "Iteration 76, loss = 1.34665417\n",
            "Iteration 77, loss = 1.34443822\n",
            "Iteration 78, loss = 1.34519228\n",
            "Iteration 79, loss = 1.34455409\n",
            "Iteration 80, loss = 1.34688431\n",
            "Iteration 81, loss = 1.34719208\n",
            "Iteration 82, loss = 1.34521742\n",
            "Iteration 83, loss = 1.34417006\n",
            "Iteration 84, loss = 1.34395127\n",
            "Iteration 85, loss = 1.34522634\n",
            "Iteration 86, loss = 1.34479237\n",
            "Iteration 87, loss = 1.34481257\n",
            "Iteration 88, loss = 1.34395460\n",
            "Iteration 89, loss = 1.34575870\n",
            "Iteration 90, loss = 1.34366272\n",
            "Iteration 91, loss = 1.34305210\n",
            "Iteration 92, loss = 1.34206631\n",
            "Iteration 93, loss = 1.34230641\n",
            "Iteration 94, loss = 1.34488106\n",
            "Iteration 95, loss = 1.34329499\n",
            "Iteration 96, loss = 1.34312218\n",
            "Iteration 97, loss = 1.34675504\n",
            "Iteration 98, loss = 1.34429649\n",
            "Iteration 99, loss = 1.34258884\n",
            "Iteration 100, loss = 1.34312134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39603432\n",
            "Iteration 2, loss = 1.38305866\n",
            "Iteration 3, loss = 1.38013955\n",
            "Iteration 4, loss = 1.37779293\n",
            "Iteration 5, loss = 1.37507969\n",
            "Iteration 6, loss = 1.37375285\n",
            "Iteration 7, loss = 1.37208328\n",
            "Iteration 8, loss = 1.37110280\n",
            "Iteration 9, loss = 1.37063234\n",
            "Iteration 10, loss = 1.36974653\n",
            "Iteration 11, loss = 1.36873089\n",
            "Iteration 12, loss = 1.36867717\n",
            "Iteration 13, loss = 1.36771220\n",
            "Iteration 14, loss = 1.36886750\n",
            "Iteration 15, loss = 1.36798964\n",
            "Iteration 16, loss = 1.36529367\n",
            "Iteration 17, loss = 1.36499734\n",
            "Iteration 18, loss = 1.36443652\n",
            "Iteration 19, loss = 1.36393627\n",
            "Iteration 20, loss = 1.36380158\n",
            "Iteration 21, loss = 1.36316609\n",
            "Iteration 22, loss = 1.36327174\n",
            "Iteration 23, loss = 1.36258522\n",
            "Iteration 24, loss = 1.36153878\n",
            "Iteration 25, loss = 1.36078194\n",
            "Iteration 26, loss = 1.36130582\n",
            "Iteration 27, loss = 1.36002648\n",
            "Iteration 28, loss = 1.36000169\n",
            "Iteration 29, loss = 1.35960099\n",
            "Iteration 30, loss = 1.35920359\n",
            "Iteration 31, loss = 1.35939823\n",
            "Iteration 32, loss = 1.36006943\n",
            "Iteration 33, loss = 1.35930236\n",
            "Iteration 34, loss = 1.35855807\n",
            "Iteration 35, loss = 1.35709906\n",
            "Iteration 36, loss = 1.35787926\n",
            "Iteration 37, loss = 1.35626832\n",
            "Iteration 38, loss = 1.35785400\n",
            "Iteration 39, loss = 1.35605326\n",
            "Iteration 40, loss = 1.35564909\n",
            "Iteration 41, loss = 1.35605558\n",
            "Iteration 42, loss = 1.35533796\n",
            "Iteration 43, loss = 1.35480902\n",
            "Iteration 44, loss = 1.35459213\n",
            "Iteration 45, loss = 1.35551168\n",
            "Iteration 46, loss = 1.35459611\n",
            "Iteration 47, loss = 1.35392720\n",
            "Iteration 48, loss = 1.35383070\n",
            "Iteration 49, loss = 1.35350610\n",
            "Iteration 50, loss = 1.35337173\n",
            "Iteration 51, loss = 1.35453164\n",
            "Iteration 52, loss = 1.35073898\n",
            "Iteration 53, loss = 1.35160564\n",
            "Iteration 54, loss = 1.35209583\n",
            "Iteration 55, loss = 1.35042390\n",
            "Iteration 56, loss = 1.35078424\n",
            "Iteration 57, loss = 1.35045681\n",
            "Iteration 58, loss = 1.35332028\n",
            "Iteration 59, loss = 1.35143449\n",
            "Iteration 60, loss = 1.35071629\n",
            "Iteration 61, loss = 1.35057095\n",
            "Iteration 62, loss = 1.34933442\n",
            "Iteration 63, loss = 1.35080367\n",
            "Iteration 64, loss = 1.35008828\n",
            "Iteration 65, loss = 1.34984845\n",
            "Iteration 66, loss = 1.34924606\n",
            "Iteration 67, loss = 1.34779118\n",
            "Iteration 68, loss = 1.34762180\n",
            "Iteration 69, loss = 1.34755954\n",
            "Iteration 70, loss = 1.34743534\n",
            "Iteration 71, loss = 1.34700823\n",
            "Iteration 72, loss = 1.34736889\n",
            "Iteration 73, loss = 1.34695658\n",
            "Iteration 74, loss = 1.34611353\n",
            "Iteration 75, loss = 1.34654231\n",
            "Iteration 76, loss = 1.34611415\n",
            "Iteration 77, loss = 1.34547130\n",
            "Iteration 78, loss = 1.34611846\n",
            "Iteration 79, loss = 1.34439687\n",
            "Iteration 80, loss = 1.34804360\n",
            "Iteration 81, loss = 1.34855245\n",
            "Iteration 82, loss = 1.34669068\n",
            "Iteration 83, loss = 1.34427341\n",
            "Iteration 84, loss = 1.34424271\n",
            "Iteration 85, loss = 1.34515232\n",
            "Iteration 86, loss = 1.34411468\n",
            "Iteration 87, loss = 1.34485023\n",
            "Iteration 88, loss = 1.34451299\n",
            "Iteration 89, loss = 1.34447432\n",
            "Iteration 90, loss = 1.34385099\n",
            "Iteration 91, loss = 1.34368981\n",
            "Iteration 92, loss = 1.34207023\n",
            "Iteration 93, loss = 1.34235762\n",
            "Iteration 94, loss = 1.34332489\n",
            "Iteration 95, loss = 1.34285403\n",
            "Iteration 96, loss = 1.34200093\n",
            "Iteration 97, loss = 1.34564845\n",
            "Iteration 98, loss = 1.34363450\n",
            "Iteration 99, loss = 1.34213454\n",
            "Iteration 100, loss = 1.34283772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39460223\n",
            "Iteration 2, loss = 1.38310925\n",
            "Iteration 3, loss = 1.37902829\n",
            "Iteration 4, loss = 1.37786229\n",
            "Iteration 5, loss = 1.37496661\n",
            "Iteration 6, loss = 1.37381339\n",
            "Iteration 7, loss = 1.37234488\n",
            "Iteration 8, loss = 1.37084922\n",
            "Iteration 9, loss = 1.37062685\n",
            "Iteration 10, loss = 1.36960217\n",
            "Iteration 11, loss = 1.36878227\n",
            "Iteration 12, loss = 1.36876663\n",
            "Iteration 13, loss = 1.36814362\n",
            "Iteration 14, loss = 1.36856586\n",
            "Iteration 15, loss = 1.36775530\n",
            "Iteration 16, loss = 1.36622645\n",
            "Iteration 17, loss = 1.36554126\n",
            "Iteration 18, loss = 1.36516072\n",
            "Iteration 19, loss = 1.36493848\n",
            "Iteration 20, loss = 1.36466010\n",
            "Iteration 21, loss = 1.36495050\n",
            "Iteration 22, loss = 1.36452252\n",
            "Iteration 23, loss = 1.36347605\n",
            "Iteration 24, loss = 1.36280546\n",
            "Iteration 25, loss = 1.36202159\n",
            "Iteration 26, loss = 1.36200739\n",
            "Iteration 27, loss = 1.36150230\n",
            "Iteration 28, loss = 1.36076960\n",
            "Iteration 29, loss = 1.36123361\n",
            "Iteration 30, loss = 1.36051826\n",
            "Iteration 31, loss = 1.36043294\n",
            "Iteration 32, loss = 1.36037920\n",
            "Iteration 33, loss = 1.35954759\n",
            "Iteration 34, loss = 1.35983436\n",
            "Iteration 35, loss = 1.35805439\n",
            "Iteration 36, loss = 1.35887453\n",
            "Iteration 37, loss = 1.35760660\n",
            "Iteration 38, loss = 1.35803903\n",
            "Iteration 39, loss = 1.35638901\n",
            "Iteration 40, loss = 1.35700532\n",
            "Iteration 41, loss = 1.35595210\n",
            "Iteration 42, loss = 1.35543596\n",
            "Iteration 43, loss = 1.35515531\n",
            "Iteration 44, loss = 1.35469236\n",
            "Iteration 45, loss = 1.35464680\n",
            "Iteration 46, loss = 1.35465599\n",
            "Iteration 47, loss = 1.35315894\n",
            "Iteration 48, loss = 1.35426361\n",
            "Iteration 49, loss = 1.35302420\n",
            "Iteration 50, loss = 1.35398767\n",
            "Iteration 51, loss = 1.35394135\n",
            "Iteration 52, loss = 1.35067821\n",
            "Iteration 53, loss = 1.35074031\n",
            "Iteration 54, loss = 1.35075054\n",
            "Iteration 55, loss = 1.35016584\n",
            "Iteration 56, loss = 1.34986490\n",
            "Iteration 57, loss = 1.34974069\n",
            "Iteration 58, loss = 1.35169316\n",
            "Iteration 59, loss = 1.34967762\n",
            "Iteration 60, loss = 1.35014217\n",
            "Iteration 61, loss = 1.34975036\n",
            "Iteration 62, loss = 1.35037214\n",
            "Iteration 63, loss = 1.35060764\n",
            "Iteration 64, loss = 1.34969321\n",
            "Iteration 65, loss = 1.34716605\n",
            "Iteration 66, loss = 1.34715114\n",
            "Iteration 67, loss = 1.34578645\n",
            "Iteration 68, loss = 1.34541495\n",
            "Iteration 69, loss = 1.34666746\n",
            "Iteration 70, loss = 1.34636875\n",
            "Iteration 71, loss = 1.34530191\n",
            "Iteration 72, loss = 1.34537941\n",
            "Iteration 73, loss = 1.34514889\n",
            "Iteration 74, loss = 1.34433352\n",
            "Iteration 75, loss = 1.34408984\n",
            "Iteration 76, loss = 1.34400782\n",
            "Iteration 77, loss = 1.34403918\n",
            "Iteration 78, loss = 1.34437815\n",
            "Iteration 79, loss = 1.34293925\n",
            "Iteration 80, loss = 1.34551837\n",
            "Iteration 81, loss = 1.34682415\n",
            "Iteration 82, loss = 1.34420336\n",
            "Iteration 83, loss = 1.34268106\n",
            "Iteration 84, loss = 1.34268895\n",
            "Iteration 85, loss = 1.34264209\n",
            "Iteration 86, loss = 1.34234790\n",
            "Iteration 87, loss = 1.34270250\n",
            "Iteration 88, loss = 1.34255929\n",
            "Iteration 89, loss = 1.34165966\n",
            "Iteration 90, loss = 1.34140951\n",
            "Iteration 91, loss = 1.34151180\n",
            "Iteration 92, loss = 1.33984898\n",
            "Iteration 93, loss = 1.34015867\n",
            "Iteration 94, loss = 1.34157523\n",
            "Iteration 95, loss = 1.34112927\n",
            "Iteration 96, loss = 1.34110178\n",
            "Iteration 97, loss = 1.34646184\n",
            "Iteration 98, loss = 1.34497358\n",
            "Iteration 99, loss = 1.34122463\n",
            "Iteration 100, loss = 1.34103127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39392988\n",
            "Iteration 2, loss = 1.38077604\n",
            "Iteration 3, loss = 1.37754876\n",
            "Iteration 4, loss = 1.37524320\n",
            "Iteration 5, loss = 1.37390177\n",
            "Iteration 6, loss = 1.37177469\n",
            "Iteration 7, loss = 1.37112713\n",
            "Iteration 8, loss = 1.36979752\n",
            "Iteration 9, loss = 1.36899959\n",
            "Iteration 10, loss = 1.36924374\n",
            "Iteration 11, loss = 1.36821553\n",
            "Iteration 12, loss = 1.36760820\n",
            "Iteration 13, loss = 1.36712365\n",
            "Iteration 14, loss = 1.36720822\n",
            "Iteration 15, loss = 1.36622846\n",
            "Iteration 16, loss = 1.36542262\n",
            "Iteration 17, loss = 1.36493141\n",
            "Iteration 18, loss = 1.36448146\n",
            "Iteration 19, loss = 1.36446627\n",
            "Iteration 20, loss = 1.36410475\n",
            "Iteration 21, loss = 1.36403507\n",
            "Iteration 22, loss = 1.36413218\n",
            "Iteration 23, loss = 1.36322602\n",
            "Iteration 24, loss = 1.36308694\n",
            "Iteration 25, loss = 1.36289698\n",
            "Iteration 26, loss = 1.36291419\n",
            "Iteration 27, loss = 1.36240042\n",
            "Iteration 28, loss = 1.36151615\n",
            "Iteration 29, loss = 1.36235649\n",
            "Iteration 30, loss = 1.36164234\n",
            "Iteration 31, loss = 1.36137935\n",
            "Iteration 32, loss = 1.36258498\n",
            "Iteration 33, loss = 1.36166841\n",
            "Iteration 34, loss = 1.36113838\n",
            "Iteration 35, loss = 1.35998672\n",
            "Iteration 36, loss = 1.36040076\n",
            "Iteration 37, loss = 1.35946941\n",
            "Iteration 38, loss = 1.36054472\n",
            "Iteration 39, loss = 1.35872415\n",
            "Iteration 40, loss = 1.35977652\n",
            "Iteration 41, loss = 1.35832975\n",
            "Iteration 42, loss = 1.35834238\n",
            "Iteration 43, loss = 1.35752939\n",
            "Iteration 44, loss = 1.35676100\n",
            "Iteration 45, loss = 1.35595141\n",
            "Iteration 46, loss = 1.35629588\n",
            "Iteration 47, loss = 1.35576549\n",
            "Iteration 48, loss = 1.35671366\n",
            "Iteration 49, loss = 1.35614267\n",
            "Iteration 50, loss = 1.35610665\n",
            "Iteration 51, loss = 1.35486833\n",
            "Iteration 52, loss = 1.35466078\n",
            "Iteration 53, loss = 1.35372043\n",
            "Iteration 54, loss = 1.35372706\n",
            "Iteration 55, loss = 1.35276552\n",
            "Iteration 56, loss = 1.35253576\n",
            "Iteration 57, loss = 1.35167057\n",
            "Iteration 58, loss = 1.35328093\n",
            "Iteration 59, loss = 1.35187889\n",
            "Iteration 60, loss = 1.35395925\n",
            "Iteration 61, loss = 1.35295693\n",
            "Iteration 62, loss = 1.35210852\n",
            "Iteration 63, loss = 1.35235764\n",
            "Iteration 64, loss = 1.35175018\n",
            "Iteration 65, loss = 1.35101221\n",
            "Iteration 66, loss = 1.34998783\n",
            "Iteration 67, loss = 1.34879903\n",
            "Iteration 68, loss = 1.34915939\n",
            "Iteration 69, loss = 1.34984246\n",
            "Iteration 70, loss = 1.34909653\n",
            "Iteration 71, loss = 1.34844186\n",
            "Iteration 72, loss = 1.34852878\n",
            "Iteration 73, loss = 1.34831106\n",
            "Iteration 74, loss = 1.34845279\n",
            "Iteration 75, loss = 1.34853309\n",
            "Iteration 76, loss = 1.34715780\n",
            "Iteration 77, loss = 1.34734446\n",
            "Iteration 78, loss = 1.34778870\n",
            "Iteration 79, loss = 1.34589020\n",
            "Iteration 80, loss = 1.34782603\n",
            "Iteration 81, loss = 1.34863434\n",
            "Iteration 82, loss = 1.34589632\n",
            "Iteration 83, loss = 1.34608407\n",
            "Iteration 84, loss = 1.34590270\n",
            "Iteration 85, loss = 1.34594180\n",
            "Iteration 86, loss = 1.34587496\n",
            "Iteration 87, loss = 1.34719726\n",
            "Iteration 88, loss = 1.34531581\n",
            "Iteration 89, loss = 1.34514927\n",
            "Iteration 90, loss = 1.34414188\n",
            "Iteration 91, loss = 1.34508519\n",
            "Iteration 92, loss = 1.34374120\n",
            "Iteration 93, loss = 1.34411743\n",
            "Iteration 94, loss = 1.34381187\n",
            "Iteration 95, loss = 1.34456487\n",
            "Iteration 96, loss = 1.34352375\n",
            "Iteration 97, loss = 1.34618984\n",
            "Iteration 98, loss = 1.34586294\n",
            "Iteration 99, loss = 1.34273184\n",
            "Iteration 100, loss = 1.34360461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38701223\n",
            "Iteration 2, loss = 1.36780026\n",
            "Iteration 3, loss = 1.36345188\n",
            "Iteration 4, loss = 1.36049450\n",
            "Iteration 5, loss = 1.35863960\n",
            "Iteration 6, loss = 1.35618326\n",
            "Iteration 7, loss = 1.35517800\n",
            "Iteration 8, loss = 1.35282457\n",
            "Iteration 9, loss = 1.35155769\n",
            "Iteration 10, loss = 1.35104872\n",
            "Iteration 11, loss = 1.35031792\n",
            "Iteration 12, loss = 1.34940133\n",
            "Iteration 13, loss = 1.34930082\n",
            "Iteration 14, loss = 1.34885618\n",
            "Iteration 15, loss = 1.34807660\n",
            "Iteration 16, loss = 1.34773452\n",
            "Iteration 17, loss = 1.34746574\n",
            "Iteration 18, loss = 1.34661098\n",
            "Iteration 19, loss = 1.34648954\n",
            "Iteration 20, loss = 1.34595956\n",
            "Iteration 21, loss = 1.34660655\n",
            "Iteration 22, loss = 1.34635130\n",
            "Iteration 23, loss = 1.34558816\n",
            "Iteration 24, loss = 1.34562194\n",
            "Iteration 25, loss = 1.34581244\n",
            "Iteration 26, loss = 1.34622708\n",
            "Iteration 27, loss = 1.34511835\n",
            "Iteration 28, loss = 1.34515878\n",
            "Iteration 29, loss = 1.34671112\n",
            "Iteration 30, loss = 1.34435084\n",
            "Iteration 31, loss = 1.34486188\n",
            "Iteration 32, loss = 1.34540695\n",
            "Iteration 33, loss = 1.34494848\n",
            "Iteration 34, loss = 1.34503383\n",
            "Iteration 35, loss = 1.34440192\n",
            "Iteration 36, loss = 1.34489015\n",
            "Iteration 37, loss = 1.34333717\n",
            "Iteration 38, loss = 1.34395792\n",
            "Iteration 39, loss = 1.34258267\n",
            "Iteration 40, loss = 1.34380460\n",
            "Iteration 41, loss = 1.34302055\n",
            "Iteration 42, loss = 1.34324218\n",
            "Iteration 43, loss = 1.34207944\n",
            "Iteration 44, loss = 1.34193804\n",
            "Iteration 45, loss = 1.34187033\n",
            "Iteration 46, loss = 1.34148526\n",
            "Iteration 47, loss = 1.34163665\n",
            "Iteration 48, loss = 1.34208448\n",
            "Iteration 49, loss = 1.34181025\n",
            "Iteration 50, loss = 1.34243971\n",
            "Iteration 51, loss = 1.34120022\n",
            "Iteration 52, loss = 1.34124736\n",
            "Iteration 53, loss = 1.34058089\n",
            "Iteration 54, loss = 1.34093769\n",
            "Iteration 55, loss = 1.34019165\n",
            "Iteration 56, loss = 1.34009879\n",
            "Iteration 57, loss = 1.33996265\n",
            "Iteration 58, loss = 1.33991294\n",
            "Iteration 59, loss = 1.33921981\n",
            "Iteration 60, loss = 1.33977409\n",
            "Iteration 61, loss = 1.33946307\n",
            "Iteration 62, loss = 1.33946768\n",
            "Iteration 63, loss = 1.33941185\n",
            "Iteration 64, loss = 1.33965491\n",
            "Iteration 65, loss = 1.33885581\n",
            "Iteration 66, loss = 1.33821884\n",
            "Iteration 67, loss = 1.33788733\n",
            "Iteration 68, loss = 1.33812348\n",
            "Iteration 69, loss = 1.33797344\n",
            "Iteration 70, loss = 1.33782998\n",
            "Iteration 71, loss = 1.33766588\n",
            "Iteration 72, loss = 1.33858698\n",
            "Iteration 73, loss = 1.33765174\n",
            "Iteration 74, loss = 1.33696345\n",
            "Iteration 75, loss = 1.33755908\n",
            "Iteration 76, loss = 1.33650826\n",
            "Iteration 77, loss = 1.33744065\n",
            "Iteration 78, loss = 1.33747240\n",
            "Iteration 79, loss = 1.33653421\n",
            "Iteration 80, loss = 1.33854035\n",
            "Iteration 81, loss = 1.33916283\n",
            "Iteration 82, loss = 1.33631611\n",
            "Iteration 83, loss = 1.33709455\n",
            "Iteration 84, loss = 1.33634657\n",
            "Iteration 85, loss = 1.33668701\n",
            "Iteration 86, loss = 1.33590609\n",
            "Iteration 87, loss = 1.33594577\n",
            "Iteration 88, loss = 1.33542135\n",
            "Iteration 89, loss = 1.33531239\n",
            "Iteration 90, loss = 1.33420561\n",
            "Iteration 91, loss = 1.33585780\n",
            "Iteration 92, loss = 1.33382715\n",
            "Iteration 93, loss = 1.33397852\n",
            "Iteration 94, loss = 1.33408393\n",
            "Iteration 95, loss = 1.33430371\n",
            "Iteration 96, loss = 1.33358883\n",
            "Iteration 97, loss = 1.33579562\n",
            "Iteration 98, loss = 1.33702417\n",
            "Iteration 99, loss = 1.33468546\n",
            "Iteration 100, loss = 1.33389162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38182731\n",
            "Iteration 2, loss = 1.37647398\n",
            "Iteration 3, loss = 1.37481438\n",
            "Iteration 4, loss = 1.37191240\n",
            "Iteration 5, loss = 1.37118328\n",
            "Iteration 6, loss = 1.36894215\n",
            "Iteration 7, loss = 1.36824043\n",
            "Iteration 8, loss = 1.36657234\n",
            "Iteration 9, loss = 1.36586141\n",
            "Iteration 10, loss = 1.36552974\n",
            "Iteration 11, loss = 1.36491406\n",
            "Iteration 12, loss = 1.36414706\n",
            "Iteration 13, loss = 1.36358810\n",
            "Iteration 14, loss = 1.36333043\n",
            "Iteration 15, loss = 1.36215418\n",
            "Iteration 16, loss = 1.36201165\n",
            "Iteration 17, loss = 1.36153354\n",
            "Iteration 18, loss = 1.36079863\n",
            "Iteration 19, loss = 1.36047265\n",
            "Iteration 20, loss = 1.35953244\n",
            "Iteration 21, loss = 1.35974516\n",
            "Iteration 22, loss = 1.35977773\n",
            "Iteration 23, loss = 1.35875895\n",
            "Iteration 24, loss = 1.35804124\n",
            "Iteration 25, loss = 1.35925044\n",
            "Iteration 26, loss = 1.36014555\n",
            "Iteration 27, loss = 1.35735656\n",
            "Iteration 28, loss = 1.35626734\n",
            "Iteration 29, loss = 1.35925739\n",
            "Iteration 30, loss = 1.35610151\n",
            "Iteration 31, loss = 1.35496671\n",
            "Iteration 32, loss = 1.35586344\n",
            "Iteration 33, loss = 1.35467982\n",
            "Iteration 34, loss = 1.35548793\n",
            "Iteration 35, loss = 1.35428501\n",
            "Iteration 36, loss = 1.35404152\n",
            "Iteration 37, loss = 1.35304858\n",
            "Iteration 38, loss = 1.35311844\n",
            "Iteration 39, loss = 1.35205882\n",
            "Iteration 40, loss = 1.35254722\n",
            "Iteration 41, loss = 1.35226185\n",
            "Iteration 42, loss = 1.35042557\n",
            "Iteration 43, loss = 1.35098575\n",
            "Iteration 44, loss = 1.34989284\n",
            "Iteration 45, loss = 1.34925926\n",
            "Iteration 46, loss = 1.34947086\n",
            "Iteration 47, loss = 1.34920622\n",
            "Iteration 48, loss = 1.35193884\n",
            "Iteration 49, loss = 1.34963392\n",
            "Iteration 50, loss = 1.35188556\n",
            "Iteration 51, loss = 1.35190857\n",
            "Iteration 52, loss = 1.34821766\n",
            "Iteration 53, loss = 1.34845243\n",
            "Iteration 54, loss = 1.34813946\n",
            "Iteration 55, loss = 1.34700548\n",
            "Iteration 56, loss = 1.34681525\n",
            "Iteration 57, loss = 1.34690222\n",
            "Iteration 58, loss = 1.34667182\n",
            "Iteration 59, loss = 1.34675308\n",
            "Iteration 60, loss = 1.34750567\n",
            "Iteration 61, loss = 1.34646243\n",
            "Iteration 62, loss = 1.34569075\n",
            "Iteration 63, loss = 1.34509370\n",
            "Iteration 64, loss = 1.34543704\n",
            "Iteration 65, loss = 1.34470696\n",
            "Iteration 66, loss = 1.34469925\n",
            "Iteration 67, loss = 1.34480920\n",
            "Iteration 68, loss = 1.34448209\n",
            "Iteration 69, loss = 1.34468994\n",
            "Iteration 70, loss = 1.34425271\n",
            "Iteration 71, loss = 1.34528437\n",
            "Iteration 72, loss = 1.34649449\n",
            "Iteration 73, loss = 1.34633815\n",
            "Iteration 74, loss = 1.34307170\n",
            "Iteration 75, loss = 1.34421484\n",
            "Iteration 76, loss = 1.34448331\n",
            "Iteration 77, loss = 1.34371699\n",
            "Iteration 78, loss = 1.34452144\n",
            "Iteration 79, loss = 1.34398370\n",
            "Iteration 80, loss = 1.34518931\n",
            "Iteration 81, loss = 1.34645860\n",
            "Iteration 82, loss = 1.34350298\n",
            "Iteration 83, loss = 1.34281371\n",
            "Iteration 84, loss = 1.34350028\n",
            "Iteration 85, loss = 1.34416710\n",
            "Iteration 86, loss = 1.34384467\n",
            "Iteration 87, loss = 1.34241194\n",
            "Iteration 88, loss = 1.34234187\n",
            "Iteration 89, loss = 1.34283848\n",
            "Iteration 90, loss = 1.34187396\n",
            "Iteration 91, loss = 1.34257036\n",
            "Iteration 92, loss = 1.34176480\n",
            "Iteration 93, loss = 1.34160348\n",
            "Iteration 94, loss = 1.34187234\n",
            "Iteration 95, loss = 1.34191289\n",
            "Iteration 96, loss = 1.34174633\n",
            "Iteration 97, loss = 1.34233829\n",
            "Iteration 98, loss = 1.34332895\n",
            "Iteration 99, loss = 1.34370460\n",
            "Iteration 100, loss = 1.34301909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37781476\n",
            "Iteration 2, loss = 1.37302948\n",
            "Iteration 3, loss = 1.37178198\n",
            "Iteration 4, loss = 1.36952088\n",
            "Iteration 5, loss = 1.36827182\n",
            "Iteration 6, loss = 1.36647328\n",
            "Iteration 7, loss = 1.36565741\n",
            "Iteration 8, loss = 1.36474725\n",
            "Iteration 9, loss = 1.36389213\n",
            "Iteration 10, loss = 1.36361476\n",
            "Iteration 11, loss = 1.36298869\n",
            "Iteration 12, loss = 1.36281208\n",
            "Iteration 13, loss = 1.36206776\n",
            "Iteration 14, loss = 1.36126718\n",
            "Iteration 15, loss = 1.36053441\n",
            "Iteration 16, loss = 1.36031856\n",
            "Iteration 17, loss = 1.36015350\n",
            "Iteration 18, loss = 1.35956139\n",
            "Iteration 19, loss = 1.35988666\n",
            "Iteration 20, loss = 1.35906573\n",
            "Iteration 21, loss = 1.35963853\n",
            "Iteration 22, loss = 1.35871273\n",
            "Iteration 23, loss = 1.35789459\n",
            "Iteration 24, loss = 1.35800907\n",
            "Iteration 25, loss = 1.35903733\n",
            "Iteration 26, loss = 1.35940828\n",
            "Iteration 27, loss = 1.35734508\n",
            "Iteration 28, loss = 1.35705366\n",
            "Iteration 29, loss = 1.35883454\n",
            "Iteration 30, loss = 1.35610589\n",
            "Iteration 31, loss = 1.35501429\n",
            "Iteration 32, loss = 1.35581532\n",
            "Iteration 33, loss = 1.35457248\n",
            "Iteration 34, loss = 1.35528536\n",
            "Iteration 35, loss = 1.35445355\n",
            "Iteration 36, loss = 1.35602176\n",
            "Iteration 37, loss = 1.35256537\n",
            "Iteration 38, loss = 1.35373217\n",
            "Iteration 39, loss = 1.35322997\n",
            "Iteration 40, loss = 1.35351761\n",
            "Iteration 41, loss = 1.35275039\n",
            "Iteration 42, loss = 1.35143937\n",
            "Iteration 43, loss = 1.35211459\n",
            "Iteration 44, loss = 1.35138500\n",
            "Iteration 45, loss = 1.35199216\n",
            "Iteration 46, loss = 1.35147039\n",
            "Iteration 47, loss = 1.35141756\n",
            "Iteration 48, loss = 1.35072666\n",
            "Iteration 49, loss = 1.35017844\n",
            "Iteration 50, loss = 1.35043186\n",
            "Iteration 51, loss = 1.35100559\n",
            "Iteration 52, loss = 1.34821105\n",
            "Iteration 53, loss = 1.34918217\n",
            "Iteration 54, loss = 1.34949194\n",
            "Iteration 55, loss = 1.34829256\n",
            "Iteration 56, loss = 1.34764286\n",
            "Iteration 57, loss = 1.34748508\n",
            "Iteration 58, loss = 1.34673058\n",
            "Iteration 59, loss = 1.34739684\n",
            "Iteration 60, loss = 1.34700487\n",
            "Iteration 61, loss = 1.34698903\n",
            "Iteration 62, loss = 1.34573284\n",
            "Iteration 63, loss = 1.34504591\n",
            "Iteration 64, loss = 1.34445852\n",
            "Iteration 65, loss = 1.34493594\n",
            "Iteration 66, loss = 1.34430787\n",
            "Iteration 67, loss = 1.34459339\n",
            "Iteration 68, loss = 1.34424439\n",
            "Iteration 69, loss = 1.34420045\n",
            "Iteration 70, loss = 1.34367667\n",
            "Iteration 71, loss = 1.34443755\n",
            "Iteration 72, loss = 1.34495362\n",
            "Iteration 73, loss = 1.34358317\n",
            "Iteration 74, loss = 1.34273028\n",
            "Iteration 75, loss = 1.34326619\n",
            "Iteration 76, loss = 1.34302577\n",
            "Iteration 77, loss = 1.34277120\n",
            "Iteration 78, loss = 1.34275129\n",
            "Iteration 79, loss = 1.34259642\n",
            "Iteration 80, loss = 1.34484977\n",
            "Iteration 81, loss = 1.34386817\n",
            "Iteration 82, loss = 1.34443578\n",
            "Iteration 83, loss = 1.34185544\n",
            "Iteration 84, loss = 1.34379239\n",
            "Iteration 85, loss = 1.34476138\n",
            "Iteration 86, loss = 1.34227408\n",
            "Iteration 87, loss = 1.34193333\n",
            "Iteration 88, loss = 1.34104551\n",
            "Iteration 89, loss = 1.34165604\n",
            "Iteration 90, loss = 1.34055332\n",
            "Iteration 91, loss = 1.34076343\n",
            "Iteration 92, loss = 1.34046998\n",
            "Iteration 93, loss = 1.34031654\n",
            "Iteration 94, loss = 1.34143595\n",
            "Iteration 95, loss = 1.34166718\n",
            "Iteration 96, loss = 1.34124436\n",
            "Iteration 97, loss = 1.34253532\n",
            "Iteration 98, loss = 1.34349665\n",
            "Iteration 99, loss = 1.34395175\n",
            "Iteration 100, loss = 1.34392729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37706443\n",
            "Iteration 2, loss = 1.36408631\n",
            "Iteration 3, loss = 1.36030392\n",
            "Iteration 4, loss = 1.35902853\n",
            "Iteration 5, loss = 1.35734672\n",
            "Iteration 6, loss = 1.35620413\n",
            "Iteration 7, loss = 1.35552453\n",
            "Iteration 8, loss = 1.35433825\n",
            "Iteration 9, loss = 1.35355844\n",
            "Iteration 10, loss = 1.35294823\n",
            "Iteration 11, loss = 1.35222482\n",
            "Iteration 12, loss = 1.35185170\n",
            "Iteration 13, loss = 1.35156356\n",
            "Iteration 14, loss = 1.35128503\n",
            "Iteration 15, loss = 1.34994180\n",
            "Iteration 16, loss = 1.34960924\n",
            "Iteration 17, loss = 1.34900443\n",
            "Iteration 18, loss = 1.34856165\n",
            "Iteration 19, loss = 1.34838473\n",
            "Iteration 20, loss = 1.34763041\n",
            "Iteration 21, loss = 1.34759177\n",
            "Iteration 22, loss = 1.34719667\n",
            "Iteration 23, loss = 1.34661647\n",
            "Iteration 24, loss = 1.34605997\n",
            "Iteration 25, loss = 1.34789006\n",
            "Iteration 26, loss = 1.34878117\n",
            "Iteration 27, loss = 1.34531742\n",
            "Iteration 28, loss = 1.34464354\n",
            "Iteration 29, loss = 1.34557018\n",
            "Iteration 30, loss = 1.34420199\n",
            "Iteration 31, loss = 1.34308314\n",
            "Iteration 32, loss = 1.34349920\n",
            "Iteration 33, loss = 1.34170039\n",
            "Iteration 34, loss = 1.34197381\n",
            "Iteration 35, loss = 1.34116342\n",
            "Iteration 36, loss = 1.34140287\n",
            "Iteration 37, loss = 1.33911990\n",
            "Iteration 38, loss = 1.33896518\n",
            "Iteration 39, loss = 1.33891802\n",
            "Iteration 40, loss = 1.33866582\n",
            "Iteration 41, loss = 1.33884899\n",
            "Iteration 42, loss = 1.33669247\n",
            "Iteration 43, loss = 1.33781991\n",
            "Iteration 44, loss = 1.33612349\n",
            "Iteration 45, loss = 1.33645452\n",
            "Iteration 46, loss = 1.33674062\n",
            "Iteration 47, loss = 1.33736296\n",
            "Iteration 48, loss = 1.33582488\n",
            "Iteration 49, loss = 1.33545995\n",
            "Iteration 50, loss = 1.33551087\n",
            "Iteration 51, loss = 1.33646297\n",
            "Iteration 52, loss = 1.33322285\n",
            "Iteration 53, loss = 1.33342448\n",
            "Iteration 54, loss = 1.33328830\n",
            "Iteration 55, loss = 1.33444184\n",
            "Iteration 56, loss = 1.33246817\n",
            "Iteration 57, loss = 1.33326274\n",
            "Iteration 58, loss = 1.33191841\n",
            "Iteration 59, loss = 1.33247303\n",
            "Iteration 60, loss = 1.33282266\n",
            "Iteration 61, loss = 1.33265244\n",
            "Iteration 62, loss = 1.33131043\n",
            "Iteration 63, loss = 1.33097463\n",
            "Iteration 64, loss = 1.33050679\n",
            "Iteration 65, loss = 1.33121873\n",
            "Iteration 66, loss = 1.33030817\n",
            "Iteration 67, loss = 1.33058319\n",
            "Iteration 68, loss = 1.33096482\n",
            "Iteration 69, loss = 1.33073184\n",
            "Iteration 70, loss = 1.33050744\n",
            "Iteration 71, loss = 1.33110588\n",
            "Iteration 72, loss = 1.33134022\n",
            "Iteration 73, loss = 1.33056160\n",
            "Iteration 74, loss = 1.32954896\n",
            "Iteration 75, loss = 1.32965037\n",
            "Iteration 76, loss = 1.33060606\n",
            "Iteration 77, loss = 1.32964639\n",
            "Iteration 78, loss = 1.33049606\n",
            "Iteration 79, loss = 1.33037895\n",
            "Iteration 80, loss = 1.33092209\n",
            "Iteration 81, loss = 1.33223796\n",
            "Iteration 82, loss = 1.33180067\n",
            "Iteration 83, loss = 1.32930675\n",
            "Iteration 84, loss = 1.33039897\n",
            "Iteration 85, loss = 1.33044035\n",
            "Iteration 86, loss = 1.32911539\n",
            "Iteration 87, loss = 1.32905366\n",
            "Iteration 88, loss = 1.32861060\n",
            "Iteration 89, loss = 1.32820461\n",
            "Iteration 90, loss = 1.32748578\n",
            "Iteration 91, loss = 1.32849998\n",
            "Iteration 92, loss = 1.32761525\n",
            "Iteration 93, loss = 1.32800912\n",
            "Iteration 94, loss = 1.32818742\n",
            "Iteration 95, loss = 1.32873445\n",
            "Iteration 96, loss = 1.32859519\n",
            "Iteration 97, loss = 1.32879978\n",
            "Iteration 98, loss = 1.32953158\n",
            "Iteration 99, loss = 1.32991772\n",
            "Iteration 100, loss = 1.33137659\n",
            "45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39125600\n",
            "Iteration 2, loss = 1.38181007\n",
            "Iteration 3, loss = 1.37721547\n",
            "Iteration 4, loss = 1.37320285\n",
            "Iteration 5, loss = 1.37221195\n",
            "Iteration 6, loss = 1.37172134\n",
            "Iteration 7, loss = 1.36948996\n",
            "Iteration 8, loss = 1.36841584\n",
            "Iteration 9, loss = 1.36780223\n",
            "Iteration 10, loss = 1.36686647\n",
            "Iteration 11, loss = 1.36632653\n",
            "Iteration 12, loss = 1.36609868\n",
            "Iteration 13, loss = 1.36565612\n",
            "Iteration 14, loss = 1.36553368\n",
            "Iteration 15, loss = 1.36475152\n",
            "Iteration 16, loss = 1.36437656\n",
            "Iteration 17, loss = 1.36445084\n",
            "Iteration 18, loss = 1.36396336\n",
            "Iteration 19, loss = 1.36347792\n",
            "Iteration 20, loss = 1.36316078\n",
            "Iteration 21, loss = 1.36274483\n",
            "Iteration 22, loss = 1.36273938\n",
            "Iteration 23, loss = 1.36216841\n",
            "Iteration 24, loss = 1.36202255\n",
            "Iteration 25, loss = 1.36152485\n",
            "Iteration 26, loss = 1.36231751\n",
            "Iteration 27, loss = 1.36064452\n",
            "Iteration 28, loss = 1.36021948\n",
            "Iteration 29, loss = 1.35984283\n",
            "Iteration 30, loss = 1.35984706\n",
            "Iteration 31, loss = 1.36008387\n",
            "Iteration 32, loss = 1.36072906\n",
            "Iteration 33, loss = 1.36031305\n",
            "Iteration 34, loss = 1.35906134\n",
            "Iteration 35, loss = 1.35762861\n",
            "Iteration 36, loss = 1.35816715\n",
            "Iteration 37, loss = 1.35650524\n",
            "Iteration 38, loss = 1.35653123\n",
            "Iteration 39, loss = 1.35651071\n",
            "Iteration 40, loss = 1.35452393\n",
            "Iteration 41, loss = 1.35578131\n",
            "Iteration 42, loss = 1.35531849\n",
            "Iteration 43, loss = 1.35387438\n",
            "Iteration 44, loss = 1.35486430\n",
            "Iteration 45, loss = 1.35462168\n",
            "Iteration 46, loss = 1.35291833\n",
            "Iteration 47, loss = 1.35354775\n",
            "Iteration 48, loss = 1.35193659\n",
            "Iteration 49, loss = 1.35253270\n",
            "Iteration 50, loss = 1.35195870\n",
            "Iteration 51, loss = 1.35236508\n",
            "Iteration 52, loss = 1.35019147\n",
            "Iteration 53, loss = 1.35016433\n",
            "Iteration 54, loss = 1.35029029\n",
            "Iteration 55, loss = 1.34889578\n",
            "Iteration 56, loss = 1.34993709\n",
            "Iteration 57, loss = 1.34829859\n",
            "Iteration 58, loss = 1.35042116\n",
            "Iteration 59, loss = 1.34967575\n",
            "Iteration 60, loss = 1.34866562\n",
            "Iteration 61, loss = 1.34792395\n",
            "Iteration 62, loss = 1.34783581\n",
            "Iteration 63, loss = 1.34882098\n",
            "Iteration 64, loss = 1.34940035\n",
            "Iteration 65, loss = 1.34813028\n",
            "Iteration 66, loss = 1.34806109\n",
            "Iteration 67, loss = 1.34659982\n",
            "Iteration 68, loss = 1.34620615\n",
            "Iteration 69, loss = 1.34584006\n",
            "Iteration 70, loss = 1.34669928\n",
            "Iteration 71, loss = 1.34640783\n",
            "Iteration 72, loss = 1.34573069\n",
            "Iteration 73, loss = 1.34557298\n",
            "Iteration 74, loss = 1.34538236\n",
            "Iteration 75, loss = 1.34551756\n",
            "Iteration 76, loss = 1.34589857\n",
            "Iteration 77, loss = 1.34479465\n",
            "Iteration 78, loss = 1.34457759\n",
            "Iteration 79, loss = 1.34497818\n",
            "Iteration 80, loss = 1.34456111\n",
            "Iteration 81, loss = 1.34426726\n",
            "Iteration 82, loss = 1.34476257\n",
            "Iteration 83, loss = 1.34493880\n",
            "Iteration 84, loss = 1.34413063\n",
            "Iteration 85, loss = 1.34447282\n",
            "Iteration 86, loss = 1.34464025\n",
            "Iteration 87, loss = 1.34406111\n",
            "Iteration 88, loss = 1.34446651\n",
            "Iteration 89, loss = 1.34720418\n",
            "Iteration 90, loss = 1.34446104\n",
            "Iteration 91, loss = 1.34423284\n",
            "Iteration 92, loss = 1.34299093\n",
            "Iteration 93, loss = 1.34265903\n",
            "Iteration 94, loss = 1.34286892\n",
            "Iteration 95, loss = 1.34269162\n",
            "Iteration 96, loss = 1.34315767\n",
            "Iteration 97, loss = 1.34618483\n",
            "Iteration 98, loss = 1.34525474\n",
            "Iteration 99, loss = 1.34249316\n",
            "Iteration 100, loss = 1.34242000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39275464\n",
            "Iteration 2, loss = 1.38318916\n",
            "Iteration 3, loss = 1.37952333\n",
            "Iteration 4, loss = 1.37625801\n",
            "Iteration 5, loss = 1.37507014\n",
            "Iteration 6, loss = 1.37413408\n",
            "Iteration 7, loss = 1.37265377\n",
            "Iteration 8, loss = 1.37188409\n",
            "Iteration 9, loss = 1.37123750\n",
            "Iteration 10, loss = 1.37080073\n",
            "Iteration 11, loss = 1.37002085\n",
            "Iteration 12, loss = 1.37028192\n",
            "Iteration 13, loss = 1.36923616\n",
            "Iteration 14, loss = 1.36956807\n",
            "Iteration 15, loss = 1.36912047\n",
            "Iteration 16, loss = 1.36781750\n",
            "Iteration 17, loss = 1.36798248\n",
            "Iteration 18, loss = 1.36757159\n",
            "Iteration 19, loss = 1.36674736\n",
            "Iteration 20, loss = 1.36681423\n",
            "Iteration 21, loss = 1.36632112\n",
            "Iteration 22, loss = 1.36688264\n",
            "Iteration 23, loss = 1.36615734\n",
            "Iteration 24, loss = 1.36596591\n",
            "Iteration 25, loss = 1.36535379\n",
            "Iteration 26, loss = 1.36615671\n",
            "Iteration 27, loss = 1.36505357\n",
            "Iteration 28, loss = 1.36422394\n",
            "Iteration 29, loss = 1.36437321\n",
            "Iteration 30, loss = 1.36405455\n",
            "Iteration 31, loss = 1.36468426\n",
            "Iteration 32, loss = 1.36539773\n",
            "Iteration 33, loss = 1.36421039\n",
            "Iteration 34, loss = 1.36271502\n",
            "Iteration 35, loss = 1.36226695\n",
            "Iteration 36, loss = 1.36270984\n",
            "Iteration 37, loss = 1.36153171\n",
            "Iteration 38, loss = 1.36155864\n",
            "Iteration 39, loss = 1.36126481\n",
            "Iteration 40, loss = 1.36035807\n",
            "Iteration 41, loss = 1.36102281\n",
            "Iteration 42, loss = 1.36108564\n",
            "Iteration 43, loss = 1.35929210\n",
            "Iteration 44, loss = 1.36035370\n",
            "Iteration 45, loss = 1.36020491\n",
            "Iteration 46, loss = 1.35929231\n",
            "Iteration 47, loss = 1.35950340\n",
            "Iteration 48, loss = 1.35776002\n",
            "Iteration 49, loss = 1.35860162\n",
            "Iteration 50, loss = 1.35734817\n",
            "Iteration 51, loss = 1.35781669\n",
            "Iteration 52, loss = 1.35608341\n",
            "Iteration 53, loss = 1.35618626\n",
            "Iteration 54, loss = 1.35628599\n",
            "Iteration 55, loss = 1.35506446\n",
            "Iteration 56, loss = 1.35555807\n",
            "Iteration 57, loss = 1.35451936\n",
            "Iteration 58, loss = 1.35682598\n",
            "Iteration 59, loss = 1.35519361\n",
            "Iteration 60, loss = 1.35424075\n",
            "Iteration 61, loss = 1.35386840\n",
            "Iteration 62, loss = 1.35390028\n",
            "Iteration 63, loss = 1.35494356\n",
            "Iteration 64, loss = 1.35479659\n",
            "Iteration 65, loss = 1.35356021\n",
            "Iteration 66, loss = 1.35441993\n",
            "Iteration 67, loss = 1.35269873\n",
            "Iteration 68, loss = 1.35217297\n",
            "Iteration 69, loss = 1.35209118\n",
            "Iteration 70, loss = 1.35246875\n",
            "Iteration 71, loss = 1.35236790\n",
            "Iteration 72, loss = 1.35180078\n",
            "Iteration 73, loss = 1.35212883\n",
            "Iteration 74, loss = 1.35177618\n",
            "Iteration 75, loss = 1.35168892\n",
            "Iteration 76, loss = 1.35183800\n",
            "Iteration 77, loss = 1.35063126\n",
            "Iteration 78, loss = 1.35102266\n",
            "Iteration 79, loss = 1.35123714\n",
            "Iteration 80, loss = 1.35091021\n",
            "Iteration 81, loss = 1.35106071\n",
            "Iteration 82, loss = 1.35062510\n",
            "Iteration 83, loss = 1.35053485\n",
            "Iteration 84, loss = 1.35009063\n",
            "Iteration 85, loss = 1.35114947\n",
            "Iteration 86, loss = 1.35104493\n",
            "Iteration 87, loss = 1.35056570\n",
            "Iteration 88, loss = 1.35049569\n",
            "Iteration 89, loss = 1.35164830\n",
            "Iteration 90, loss = 1.34896801\n",
            "Iteration 91, loss = 1.34969214\n",
            "Iteration 92, loss = 1.34851475\n",
            "Iteration 93, loss = 1.34917057\n",
            "Iteration 94, loss = 1.34906189\n",
            "Iteration 95, loss = 1.34884139\n",
            "Iteration 96, loss = 1.34871215\n",
            "Iteration 97, loss = 1.35203511\n",
            "Iteration 98, loss = 1.35063200\n",
            "Iteration 99, loss = 1.34785942\n",
            "Iteration 100, loss = 1.34879346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39551742\n",
            "Iteration 2, loss = 1.38376043\n",
            "Iteration 3, loss = 1.38037776\n",
            "Iteration 4, loss = 1.37685043\n",
            "Iteration 5, loss = 1.37516015\n",
            "Iteration 6, loss = 1.37403561\n",
            "Iteration 7, loss = 1.37254023\n",
            "Iteration 8, loss = 1.37125006\n",
            "Iteration 9, loss = 1.37098046\n",
            "Iteration 10, loss = 1.37081478\n",
            "Iteration 11, loss = 1.36943487\n",
            "Iteration 12, loss = 1.36879052\n",
            "Iteration 13, loss = 1.36830252\n",
            "Iteration 14, loss = 1.36847649\n",
            "Iteration 15, loss = 1.36768023\n",
            "Iteration 16, loss = 1.36659124\n",
            "Iteration 17, loss = 1.36644046\n",
            "Iteration 18, loss = 1.36604010\n",
            "Iteration 19, loss = 1.36536314\n",
            "Iteration 20, loss = 1.36555049\n",
            "Iteration 21, loss = 1.36478790\n",
            "Iteration 22, loss = 1.36509102\n",
            "Iteration 23, loss = 1.36394644\n",
            "Iteration 24, loss = 1.36361090\n",
            "Iteration 25, loss = 1.36217031\n",
            "Iteration 26, loss = 1.36313251\n",
            "Iteration 27, loss = 1.36170746\n",
            "Iteration 28, loss = 1.36149538\n",
            "Iteration 29, loss = 1.36149871\n",
            "Iteration 30, loss = 1.36065895\n",
            "Iteration 31, loss = 1.36036977\n",
            "Iteration 32, loss = 1.36143633\n",
            "Iteration 33, loss = 1.36106682\n",
            "Iteration 34, loss = 1.35919297\n",
            "Iteration 35, loss = 1.35831441\n",
            "Iteration 36, loss = 1.35842469\n",
            "Iteration 37, loss = 1.35753662\n",
            "Iteration 38, loss = 1.35752081\n",
            "Iteration 39, loss = 1.35659488\n",
            "Iteration 40, loss = 1.35566333\n",
            "Iteration 41, loss = 1.35607778\n",
            "Iteration 42, loss = 1.35607007\n",
            "Iteration 43, loss = 1.35411905\n",
            "Iteration 44, loss = 1.35443238\n",
            "Iteration 45, loss = 1.35454886\n",
            "Iteration 46, loss = 1.35339990\n",
            "Iteration 47, loss = 1.35314248\n",
            "Iteration 48, loss = 1.35251717\n",
            "Iteration 49, loss = 1.35275597\n",
            "Iteration 50, loss = 1.35242693\n",
            "Iteration 51, loss = 1.35390018\n",
            "Iteration 52, loss = 1.35033152\n",
            "Iteration 53, loss = 1.35006292\n",
            "Iteration 54, loss = 1.35061234\n",
            "Iteration 55, loss = 1.34856599\n",
            "Iteration 56, loss = 1.34895955\n",
            "Iteration 57, loss = 1.34884263\n",
            "Iteration 58, loss = 1.35071256\n",
            "Iteration 59, loss = 1.34905467\n",
            "Iteration 60, loss = 1.34820131\n",
            "Iteration 61, loss = 1.34697459\n",
            "Iteration 62, loss = 1.34730207\n",
            "Iteration 63, loss = 1.34764986\n",
            "Iteration 64, loss = 1.34846557\n",
            "Iteration 65, loss = 1.34849908\n",
            "Iteration 66, loss = 1.34707257\n",
            "Iteration 67, loss = 1.34529565\n",
            "Iteration 68, loss = 1.34523416\n",
            "Iteration 69, loss = 1.34517706\n",
            "Iteration 70, loss = 1.34538093\n",
            "Iteration 71, loss = 1.34521025\n",
            "Iteration 72, loss = 1.34561829\n",
            "Iteration 73, loss = 1.34569849\n",
            "Iteration 74, loss = 1.34494353\n",
            "Iteration 75, loss = 1.34570322\n",
            "Iteration 76, loss = 1.34577382\n",
            "Iteration 77, loss = 1.34393652\n",
            "Iteration 78, loss = 1.34411312\n",
            "Iteration 79, loss = 1.34404169\n",
            "Iteration 80, loss = 1.34690461\n",
            "Iteration 81, loss = 1.34706066\n",
            "Iteration 82, loss = 1.34475962\n",
            "Iteration 83, loss = 1.34385707\n",
            "Iteration 84, loss = 1.34298257\n",
            "Iteration 85, loss = 1.34407842\n",
            "Iteration 86, loss = 1.34385568\n",
            "Iteration 87, loss = 1.34352191\n",
            "Iteration 88, loss = 1.34301466\n",
            "Iteration 89, loss = 1.34450160\n",
            "Iteration 90, loss = 1.34317735\n",
            "Iteration 91, loss = 1.34265715\n",
            "Iteration 92, loss = 1.34143588\n",
            "Iteration 93, loss = 1.34172525\n",
            "Iteration 94, loss = 1.34374862\n",
            "Iteration 95, loss = 1.34197403\n",
            "Iteration 96, loss = 1.34226461\n",
            "Iteration 97, loss = 1.34512623\n",
            "Iteration 98, loss = 1.34336953\n",
            "Iteration 99, loss = 1.34168706\n",
            "Iteration 100, loss = 1.34242064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39603250\n",
            "Iteration 2, loss = 1.38299546\n",
            "Iteration 3, loss = 1.37994514\n",
            "Iteration 4, loss = 1.37768886\n",
            "Iteration 5, loss = 1.37485363\n",
            "Iteration 6, loss = 1.37348525\n",
            "Iteration 7, loss = 1.37188600\n",
            "Iteration 8, loss = 1.37071880\n",
            "Iteration 9, loss = 1.37013405\n",
            "Iteration 10, loss = 1.36912801\n",
            "Iteration 11, loss = 1.36797412\n",
            "Iteration 12, loss = 1.36817192\n",
            "Iteration 13, loss = 1.36694491\n",
            "Iteration 14, loss = 1.36778674\n",
            "Iteration 15, loss = 1.36718867\n",
            "Iteration 16, loss = 1.36482429\n",
            "Iteration 17, loss = 1.36448641\n",
            "Iteration 18, loss = 1.36371928\n",
            "Iteration 19, loss = 1.36354898\n",
            "Iteration 20, loss = 1.36326100\n",
            "Iteration 21, loss = 1.36238721\n",
            "Iteration 22, loss = 1.36298535\n",
            "Iteration 23, loss = 1.36242781\n",
            "Iteration 24, loss = 1.36091199\n",
            "Iteration 25, loss = 1.36018158\n",
            "Iteration 26, loss = 1.36095434\n",
            "Iteration 27, loss = 1.35925229\n",
            "Iteration 28, loss = 1.35924054\n",
            "Iteration 29, loss = 1.35874061\n",
            "Iteration 30, loss = 1.35838341\n",
            "Iteration 31, loss = 1.35850525\n",
            "Iteration 32, loss = 1.35945619\n",
            "Iteration 33, loss = 1.35841836\n",
            "Iteration 34, loss = 1.35775973\n",
            "Iteration 35, loss = 1.35661327\n",
            "Iteration 36, loss = 1.35732288\n",
            "Iteration 37, loss = 1.35543279\n",
            "Iteration 38, loss = 1.35737181\n",
            "Iteration 39, loss = 1.35517888\n",
            "Iteration 40, loss = 1.35479482\n",
            "Iteration 41, loss = 1.35531454\n",
            "Iteration 42, loss = 1.35461362\n",
            "Iteration 43, loss = 1.35406727\n",
            "Iteration 44, loss = 1.35416134\n",
            "Iteration 45, loss = 1.35454953\n",
            "Iteration 46, loss = 1.35366079\n",
            "Iteration 47, loss = 1.35330832\n",
            "Iteration 48, loss = 1.35285357\n",
            "Iteration 49, loss = 1.35273242\n",
            "Iteration 50, loss = 1.35217334\n",
            "Iteration 51, loss = 1.35383200\n",
            "Iteration 52, loss = 1.34964960\n",
            "Iteration 53, loss = 1.35070113\n",
            "Iteration 54, loss = 1.35133990\n",
            "Iteration 55, loss = 1.34984559\n",
            "Iteration 56, loss = 1.35016307\n",
            "Iteration 57, loss = 1.35011667\n",
            "Iteration 58, loss = 1.35190882\n",
            "Iteration 59, loss = 1.35060643\n",
            "Iteration 60, loss = 1.34976092\n",
            "Iteration 61, loss = 1.34954365\n",
            "Iteration 62, loss = 1.34834525\n",
            "Iteration 63, loss = 1.34933897\n",
            "Iteration 64, loss = 1.34885644\n",
            "Iteration 65, loss = 1.34939289\n",
            "Iteration 66, loss = 1.34820655\n",
            "Iteration 67, loss = 1.34675136\n",
            "Iteration 68, loss = 1.34642389\n",
            "Iteration 69, loss = 1.34669062\n",
            "Iteration 70, loss = 1.34612842\n",
            "Iteration 71, loss = 1.34622506\n",
            "Iteration 72, loss = 1.34660949\n",
            "Iteration 73, loss = 1.34610817\n",
            "Iteration 74, loss = 1.34544658\n",
            "Iteration 75, loss = 1.34585986\n",
            "Iteration 76, loss = 1.34516491\n",
            "Iteration 77, loss = 1.34463294\n",
            "Iteration 78, loss = 1.34526736\n",
            "Iteration 79, loss = 1.34387783\n",
            "Iteration 80, loss = 1.34689733\n",
            "Iteration 81, loss = 1.34760191\n",
            "Iteration 82, loss = 1.34588076\n",
            "Iteration 83, loss = 1.34390565\n",
            "Iteration 84, loss = 1.34345264\n",
            "Iteration 85, loss = 1.34488415\n",
            "Iteration 86, loss = 1.34392179\n",
            "Iteration 87, loss = 1.34428299\n",
            "Iteration 88, loss = 1.34365084\n",
            "Iteration 89, loss = 1.34383056\n",
            "Iteration 90, loss = 1.34309075\n",
            "Iteration 91, loss = 1.34332598\n",
            "Iteration 92, loss = 1.34149627\n",
            "Iteration 93, loss = 1.34201412\n",
            "Iteration 94, loss = 1.34277664\n",
            "Iteration 95, loss = 1.34251077\n",
            "Iteration 96, loss = 1.34147427\n",
            "Iteration 97, loss = 1.34585537\n",
            "Iteration 98, loss = 1.34256557\n",
            "Iteration 99, loss = 1.34154329\n",
            "Iteration 100, loss = 1.34207978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39460364\n",
            "Iteration 2, loss = 1.38308381\n",
            "Iteration 3, loss = 1.37897096\n",
            "Iteration 4, loss = 1.37772565\n",
            "Iteration 5, loss = 1.37479877\n",
            "Iteration 6, loss = 1.37373247\n",
            "Iteration 7, loss = 1.37221940\n",
            "Iteration 8, loss = 1.37097424\n",
            "Iteration 9, loss = 1.37063456\n",
            "Iteration 10, loss = 1.36949165\n",
            "Iteration 11, loss = 1.36877023\n",
            "Iteration 12, loss = 1.36852573\n",
            "Iteration 13, loss = 1.36820892\n",
            "Iteration 14, loss = 1.36878612\n",
            "Iteration 15, loss = 1.36775901\n",
            "Iteration 16, loss = 1.36631435\n",
            "Iteration 17, loss = 1.36553741\n",
            "Iteration 18, loss = 1.36513467\n",
            "Iteration 19, loss = 1.36511885\n",
            "Iteration 20, loss = 1.36484893\n",
            "Iteration 21, loss = 1.36505601\n",
            "Iteration 22, loss = 1.36460108\n",
            "Iteration 23, loss = 1.36339496\n",
            "Iteration 24, loss = 1.36280950\n",
            "Iteration 25, loss = 1.36201229\n",
            "Iteration 26, loss = 1.36195277\n",
            "Iteration 27, loss = 1.36169032\n",
            "Iteration 28, loss = 1.36075547\n",
            "Iteration 29, loss = 1.36128709\n",
            "Iteration 30, loss = 1.36050636\n",
            "Iteration 31, loss = 1.36049507\n",
            "Iteration 32, loss = 1.36053954\n",
            "Iteration 33, loss = 1.35973452\n",
            "Iteration 34, loss = 1.35983881\n",
            "Iteration 35, loss = 1.35815987\n",
            "Iteration 36, loss = 1.35875616\n",
            "Iteration 37, loss = 1.35776528\n",
            "Iteration 38, loss = 1.35852597\n",
            "Iteration 39, loss = 1.35666092\n",
            "Iteration 40, loss = 1.35706180\n",
            "Iteration 41, loss = 1.35638656\n",
            "Iteration 42, loss = 1.35565145\n",
            "Iteration 43, loss = 1.35530236\n",
            "Iteration 44, loss = 1.35463997\n",
            "Iteration 45, loss = 1.35479204\n",
            "Iteration 46, loss = 1.35458011\n",
            "Iteration 47, loss = 1.35303937\n",
            "Iteration 48, loss = 1.35406436\n",
            "Iteration 49, loss = 1.35337404\n",
            "Iteration 50, loss = 1.35398144\n",
            "Iteration 51, loss = 1.35431677\n",
            "Iteration 52, loss = 1.35026056\n",
            "Iteration 53, loss = 1.35057401\n",
            "Iteration 54, loss = 1.35052876\n",
            "Iteration 55, loss = 1.35008562\n",
            "Iteration 56, loss = 1.34917875\n",
            "Iteration 57, loss = 1.34897818\n",
            "Iteration 58, loss = 1.35109747\n",
            "Iteration 59, loss = 1.34902807\n",
            "Iteration 60, loss = 1.35027046\n",
            "Iteration 61, loss = 1.34964799\n",
            "Iteration 62, loss = 1.35099434\n",
            "Iteration 63, loss = 1.35160965\n",
            "Iteration 64, loss = 1.34998013\n",
            "Iteration 65, loss = 1.34671872\n",
            "Iteration 66, loss = 1.34705281\n",
            "Iteration 67, loss = 1.34598273\n",
            "Iteration 68, loss = 1.34526966\n",
            "Iteration 69, loss = 1.34614438\n",
            "Iteration 70, loss = 1.34611840\n",
            "Iteration 71, loss = 1.34503824\n",
            "Iteration 72, loss = 1.34496339\n",
            "Iteration 73, loss = 1.34453943\n",
            "Iteration 74, loss = 1.34400271\n",
            "Iteration 75, loss = 1.34401030\n",
            "Iteration 76, loss = 1.34372397\n",
            "Iteration 77, loss = 1.34389134\n",
            "Iteration 78, loss = 1.34448649\n",
            "Iteration 79, loss = 1.34316829\n",
            "Iteration 80, loss = 1.34513452\n",
            "Iteration 81, loss = 1.34630312\n",
            "Iteration 82, loss = 1.34352312\n",
            "Iteration 83, loss = 1.34250480\n",
            "Iteration 84, loss = 1.34342187\n",
            "Iteration 85, loss = 1.34324454\n",
            "Iteration 86, loss = 1.34260545\n",
            "Iteration 87, loss = 1.34304118\n",
            "Iteration 88, loss = 1.34272692\n",
            "Iteration 89, loss = 1.34224527\n",
            "Iteration 90, loss = 1.34176646\n",
            "Iteration 91, loss = 1.34210211\n",
            "Iteration 92, loss = 1.34042552\n",
            "Iteration 93, loss = 1.34078374\n",
            "Iteration 94, loss = 1.34191974\n",
            "Iteration 95, loss = 1.34121241\n",
            "Iteration 96, loss = 1.34138917\n",
            "Iteration 97, loss = 1.34709029\n",
            "Iteration 98, loss = 1.34648731\n",
            "Iteration 99, loss = 1.34256644\n",
            "Iteration 100, loss = 1.34197990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39398698\n",
            "Iteration 2, loss = 1.38073661\n",
            "Iteration 3, loss = 1.37742873\n",
            "Iteration 4, loss = 1.37527579\n",
            "Iteration 5, loss = 1.37400701\n",
            "Iteration 6, loss = 1.37199559\n",
            "Iteration 7, loss = 1.37094633\n",
            "Iteration 8, loss = 1.36960292\n",
            "Iteration 9, loss = 1.36877858\n",
            "Iteration 10, loss = 1.36878285\n",
            "Iteration 11, loss = 1.36780300\n",
            "Iteration 12, loss = 1.36724998\n",
            "Iteration 13, loss = 1.36687506\n",
            "Iteration 14, loss = 1.36685221\n",
            "Iteration 15, loss = 1.36579276\n",
            "Iteration 16, loss = 1.36487158\n",
            "Iteration 17, loss = 1.36439966\n",
            "Iteration 18, loss = 1.36415105\n",
            "Iteration 19, loss = 1.36410629\n",
            "Iteration 20, loss = 1.36361107\n",
            "Iteration 21, loss = 1.36342408\n",
            "Iteration 22, loss = 1.36348639\n",
            "Iteration 23, loss = 1.36259167\n",
            "Iteration 24, loss = 1.36254920\n",
            "Iteration 25, loss = 1.36229445\n",
            "Iteration 26, loss = 1.36260670\n",
            "Iteration 27, loss = 1.36185599\n",
            "Iteration 28, loss = 1.36091193\n",
            "Iteration 29, loss = 1.36176428\n",
            "Iteration 30, loss = 1.36106627\n",
            "Iteration 31, loss = 1.36067403\n",
            "Iteration 32, loss = 1.36185673\n",
            "Iteration 33, loss = 1.36082257\n",
            "Iteration 34, loss = 1.36047909\n",
            "Iteration 35, loss = 1.35927756\n",
            "Iteration 36, loss = 1.35981618\n",
            "Iteration 37, loss = 1.35831939\n",
            "Iteration 38, loss = 1.35915413\n",
            "Iteration 39, loss = 1.35792301\n",
            "Iteration 40, loss = 1.35873688\n",
            "Iteration 41, loss = 1.35765046\n",
            "Iteration 42, loss = 1.35730260\n",
            "Iteration 43, loss = 1.35698008\n",
            "Iteration 44, loss = 1.35611970\n",
            "Iteration 45, loss = 1.35529952\n",
            "Iteration 46, loss = 1.35551213\n",
            "Iteration 47, loss = 1.35520017\n",
            "Iteration 48, loss = 1.35625735\n",
            "Iteration 49, loss = 1.35582927\n",
            "Iteration 50, loss = 1.35551086\n",
            "Iteration 51, loss = 1.35428481\n",
            "Iteration 52, loss = 1.35406418\n",
            "Iteration 53, loss = 1.35321848\n",
            "Iteration 54, loss = 1.35338473\n",
            "Iteration 55, loss = 1.35212873\n",
            "Iteration 56, loss = 1.35206780\n",
            "Iteration 57, loss = 1.35121317\n",
            "Iteration 58, loss = 1.35239671\n",
            "Iteration 59, loss = 1.35132978\n",
            "Iteration 60, loss = 1.35329635\n",
            "Iteration 61, loss = 1.35165509\n",
            "Iteration 62, loss = 1.35121654\n",
            "Iteration 63, loss = 1.35188424\n",
            "Iteration 64, loss = 1.35109446\n",
            "Iteration 65, loss = 1.35010867\n",
            "Iteration 66, loss = 1.34936821\n",
            "Iteration 67, loss = 1.34828878\n",
            "Iteration 68, loss = 1.34858658\n",
            "Iteration 69, loss = 1.34971047\n",
            "Iteration 70, loss = 1.34873693\n",
            "Iteration 71, loss = 1.34816927\n",
            "Iteration 72, loss = 1.34795348\n",
            "Iteration 73, loss = 1.34801461\n",
            "Iteration 74, loss = 1.34794791\n",
            "Iteration 75, loss = 1.34766454\n",
            "Iteration 76, loss = 1.34645148\n",
            "Iteration 77, loss = 1.34693561\n",
            "Iteration 78, loss = 1.34741498\n",
            "Iteration 79, loss = 1.34571332\n",
            "Iteration 80, loss = 1.34743627\n",
            "Iteration 81, loss = 1.34835783\n",
            "Iteration 82, loss = 1.34534235\n",
            "Iteration 83, loss = 1.34549165\n",
            "Iteration 84, loss = 1.34518268\n",
            "Iteration 85, loss = 1.34566522\n",
            "Iteration 86, loss = 1.34554278\n",
            "Iteration 87, loss = 1.34622050\n",
            "Iteration 88, loss = 1.34470393\n",
            "Iteration 89, loss = 1.34471600\n",
            "Iteration 90, loss = 1.34381344\n",
            "Iteration 91, loss = 1.34480882\n",
            "Iteration 92, loss = 1.34360221\n",
            "Iteration 93, loss = 1.34391965\n",
            "Iteration 94, loss = 1.34363366\n",
            "Iteration 95, loss = 1.34485260\n",
            "Iteration 96, loss = 1.34295190\n",
            "Iteration 97, loss = 1.34560183\n",
            "Iteration 98, loss = 1.34512350\n",
            "Iteration 99, loss = 1.34264676\n",
            "Iteration 100, loss = 1.34313551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38706675\n",
            "Iteration 2, loss = 1.36786890\n",
            "Iteration 3, loss = 1.36362997\n",
            "Iteration 4, loss = 1.36033475\n",
            "Iteration 5, loss = 1.35844120\n",
            "Iteration 6, loss = 1.35586405\n",
            "Iteration 7, loss = 1.35505748\n",
            "Iteration 8, loss = 1.35258166\n",
            "Iteration 9, loss = 1.35134214\n",
            "Iteration 10, loss = 1.35093850\n",
            "Iteration 11, loss = 1.35001720\n",
            "Iteration 12, loss = 1.34942080\n",
            "Iteration 13, loss = 1.34872416\n",
            "Iteration 14, loss = 1.34833506\n",
            "Iteration 15, loss = 1.34768291\n",
            "Iteration 16, loss = 1.34714730\n",
            "Iteration 17, loss = 1.34678135\n",
            "Iteration 18, loss = 1.34592905\n",
            "Iteration 19, loss = 1.34582941\n",
            "Iteration 20, loss = 1.34523253\n",
            "Iteration 21, loss = 1.34575166\n",
            "Iteration 22, loss = 1.34554707\n",
            "Iteration 23, loss = 1.34472313\n",
            "Iteration 24, loss = 1.34453093\n",
            "Iteration 25, loss = 1.34501338\n",
            "Iteration 26, loss = 1.34581801\n",
            "Iteration 27, loss = 1.34410083\n",
            "Iteration 28, loss = 1.34428680\n",
            "Iteration 29, loss = 1.34585816\n",
            "Iteration 30, loss = 1.34320351\n",
            "Iteration 31, loss = 1.34380615\n",
            "Iteration 32, loss = 1.34436159\n",
            "Iteration 33, loss = 1.34388200\n",
            "Iteration 34, loss = 1.34380714\n",
            "Iteration 35, loss = 1.34291299\n",
            "Iteration 36, loss = 1.34385872\n",
            "Iteration 37, loss = 1.34218744\n",
            "Iteration 38, loss = 1.34294343\n",
            "Iteration 39, loss = 1.34140607\n",
            "Iteration 40, loss = 1.34230595\n",
            "Iteration 41, loss = 1.34160667\n",
            "Iteration 42, loss = 1.34169703\n",
            "Iteration 43, loss = 1.34043263\n",
            "Iteration 44, loss = 1.34038023\n",
            "Iteration 45, loss = 1.34046628\n",
            "Iteration 46, loss = 1.34008315\n",
            "Iteration 47, loss = 1.33997388\n",
            "Iteration 48, loss = 1.34064110\n",
            "Iteration 49, loss = 1.34000450\n",
            "Iteration 50, loss = 1.34111387\n",
            "Iteration 51, loss = 1.33948424\n",
            "Iteration 52, loss = 1.33937995\n",
            "Iteration 53, loss = 1.33882550\n",
            "Iteration 54, loss = 1.33886539\n",
            "Iteration 55, loss = 1.33824244\n",
            "Iteration 56, loss = 1.33838738\n",
            "Iteration 57, loss = 1.33788051\n",
            "Iteration 58, loss = 1.33794643\n",
            "Iteration 59, loss = 1.33736613\n",
            "Iteration 60, loss = 1.33769708\n",
            "Iteration 61, loss = 1.33761657\n",
            "Iteration 62, loss = 1.33726737\n",
            "Iteration 63, loss = 1.33718792\n",
            "Iteration 64, loss = 1.33765094\n",
            "Iteration 65, loss = 1.33691137\n",
            "Iteration 66, loss = 1.33603675\n",
            "Iteration 67, loss = 1.33559513\n",
            "Iteration 68, loss = 1.33593227\n",
            "Iteration 69, loss = 1.33606393\n",
            "Iteration 70, loss = 1.33569870\n",
            "Iteration 71, loss = 1.33538148\n",
            "Iteration 72, loss = 1.33638926\n",
            "Iteration 73, loss = 1.33563587\n",
            "Iteration 74, loss = 1.33452140\n",
            "Iteration 75, loss = 1.33547864\n",
            "Iteration 76, loss = 1.33419796\n",
            "Iteration 77, loss = 1.33534886\n",
            "Iteration 78, loss = 1.33571155\n",
            "Iteration 79, loss = 1.33399514\n",
            "Iteration 80, loss = 1.33652545\n",
            "Iteration 81, loss = 1.33731283\n",
            "Iteration 82, loss = 1.33427731\n",
            "Iteration 83, loss = 1.33520602\n",
            "Iteration 84, loss = 1.33418338\n",
            "Iteration 85, loss = 1.33517250\n",
            "Iteration 86, loss = 1.33371254\n",
            "Iteration 87, loss = 1.33435013\n",
            "Iteration 88, loss = 1.33359518\n",
            "Iteration 89, loss = 1.33335034\n",
            "Iteration 90, loss = 1.33267519\n",
            "Iteration 91, loss = 1.33397870\n",
            "Iteration 92, loss = 1.33219328\n",
            "Iteration 93, loss = 1.33251070\n",
            "Iteration 94, loss = 1.33257786\n",
            "Iteration 95, loss = 1.33269196\n",
            "Iteration 96, loss = 1.33176000\n",
            "Iteration 97, loss = 1.33324660\n",
            "Iteration 98, loss = 1.33524193\n",
            "Iteration 99, loss = 1.33336832\n",
            "Iteration 100, loss = 1.33143652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38184914\n",
            "Iteration 2, loss = 1.37648008\n",
            "Iteration 3, loss = 1.37464653\n",
            "Iteration 4, loss = 1.37160293\n",
            "Iteration 5, loss = 1.37085731\n",
            "Iteration 6, loss = 1.36837030\n",
            "Iteration 7, loss = 1.36786262\n",
            "Iteration 8, loss = 1.36614490\n",
            "Iteration 9, loss = 1.36548196\n",
            "Iteration 10, loss = 1.36513224\n",
            "Iteration 11, loss = 1.36468602\n",
            "Iteration 12, loss = 1.36391407\n",
            "Iteration 13, loss = 1.36318873\n",
            "Iteration 14, loss = 1.36299672\n",
            "Iteration 15, loss = 1.36189702\n",
            "Iteration 16, loss = 1.36182602\n",
            "Iteration 17, loss = 1.36132813\n",
            "Iteration 18, loss = 1.36071395\n",
            "Iteration 19, loss = 1.36038338\n",
            "Iteration 20, loss = 1.35940694\n",
            "Iteration 21, loss = 1.35959840\n",
            "Iteration 22, loss = 1.35975625\n",
            "Iteration 23, loss = 1.35868120\n",
            "Iteration 24, loss = 1.35804836\n",
            "Iteration 25, loss = 1.35960010\n",
            "Iteration 26, loss = 1.36038611\n",
            "Iteration 27, loss = 1.35743720\n",
            "Iteration 28, loss = 1.35654637\n",
            "Iteration 29, loss = 1.35948111\n",
            "Iteration 30, loss = 1.35646627\n",
            "Iteration 31, loss = 1.35516610\n",
            "Iteration 32, loss = 1.35578642\n",
            "Iteration 33, loss = 1.35499159\n",
            "Iteration 34, loss = 1.35526466\n",
            "Iteration 35, loss = 1.35434398\n",
            "Iteration 36, loss = 1.35415544\n",
            "Iteration 37, loss = 1.35298917\n",
            "Iteration 38, loss = 1.35305745\n",
            "Iteration 39, loss = 1.35216332\n",
            "Iteration 40, loss = 1.35276667\n",
            "Iteration 41, loss = 1.35227637\n",
            "Iteration 42, loss = 1.35052247\n",
            "Iteration 43, loss = 1.35062122\n",
            "Iteration 44, loss = 1.34999357\n",
            "Iteration 45, loss = 1.34920738\n",
            "Iteration 46, loss = 1.34945793\n",
            "Iteration 47, loss = 1.34940767\n",
            "Iteration 48, loss = 1.35168334\n",
            "Iteration 49, loss = 1.34964073\n",
            "Iteration 50, loss = 1.35135626\n",
            "Iteration 51, loss = 1.35137540\n",
            "Iteration 52, loss = 1.34759512\n",
            "Iteration 53, loss = 1.34799078\n",
            "Iteration 54, loss = 1.34782963\n",
            "Iteration 55, loss = 1.34652710\n",
            "Iteration 56, loss = 1.34616078\n",
            "Iteration 57, loss = 1.34602421\n",
            "Iteration 58, loss = 1.34608288\n",
            "Iteration 59, loss = 1.34624682\n",
            "Iteration 60, loss = 1.34695370\n",
            "Iteration 61, loss = 1.34629754\n",
            "Iteration 62, loss = 1.34499822\n",
            "Iteration 63, loss = 1.34474639\n",
            "Iteration 64, loss = 1.34439343\n",
            "Iteration 65, loss = 1.34422983\n",
            "Iteration 66, loss = 1.34430800\n",
            "Iteration 67, loss = 1.34439110\n",
            "Iteration 68, loss = 1.34416089\n",
            "Iteration 69, loss = 1.34429824\n",
            "Iteration 70, loss = 1.34357798\n",
            "Iteration 71, loss = 1.34472496\n",
            "Iteration 72, loss = 1.34544313\n",
            "Iteration 73, loss = 1.34494875\n",
            "Iteration 74, loss = 1.34230991\n",
            "Iteration 75, loss = 1.34333792\n",
            "Iteration 76, loss = 1.34377795\n",
            "Iteration 77, loss = 1.34296496\n",
            "Iteration 78, loss = 1.34359272\n",
            "Iteration 79, loss = 1.34329701\n",
            "Iteration 80, loss = 1.34472834\n",
            "Iteration 81, loss = 1.34583863\n",
            "Iteration 82, loss = 1.34278765\n",
            "Iteration 83, loss = 1.34233620\n",
            "Iteration 84, loss = 1.34306429\n",
            "Iteration 85, loss = 1.34371883\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37778137\n",
            "Iteration 2, loss = 1.37297252\n",
            "Iteration 3, loss = 1.37179210\n",
            "Iteration 4, loss = 1.36946248\n",
            "Iteration 5, loss = 1.36812521\n",
            "Iteration 6, loss = 1.36634880\n",
            "Iteration 7, loss = 1.36556043\n",
            "Iteration 8, loss = 1.36447021\n",
            "Iteration 9, loss = 1.36365073\n",
            "Iteration 10, loss = 1.36333770\n",
            "Iteration 11, loss = 1.36267250\n",
            "Iteration 12, loss = 1.36259460\n",
            "Iteration 13, loss = 1.36169722\n",
            "Iteration 14, loss = 1.36105243\n",
            "Iteration 15, loss = 1.36036131\n",
            "Iteration 16, loss = 1.36010831\n",
            "Iteration 17, loss = 1.35992446\n",
            "Iteration 18, loss = 1.35937862\n",
            "Iteration 19, loss = 1.35969637\n",
            "Iteration 20, loss = 1.35862062\n",
            "Iteration 21, loss = 1.35907684\n",
            "Iteration 22, loss = 1.35811699\n",
            "Iteration 23, loss = 1.35759579\n",
            "Iteration 24, loss = 1.35749013\n",
            "Iteration 25, loss = 1.35859276\n",
            "Iteration 26, loss = 1.35929584\n",
            "Iteration 27, loss = 1.35691432\n",
            "Iteration 28, loss = 1.35650107\n",
            "Iteration 29, loss = 1.35818396\n",
            "Iteration 30, loss = 1.35578516\n",
            "Iteration 31, loss = 1.35449498\n",
            "Iteration 32, loss = 1.35548264\n",
            "Iteration 33, loss = 1.35420549\n",
            "Iteration 34, loss = 1.35459711\n",
            "Iteration 35, loss = 1.35380101\n",
            "Iteration 36, loss = 1.35501854\n",
            "Iteration 37, loss = 1.35187209\n",
            "Iteration 38, loss = 1.35251978\n",
            "Iteration 39, loss = 1.35213603\n",
            "Iteration 40, loss = 1.35272587\n",
            "Iteration 41, loss = 1.35159217\n",
            "Iteration 42, loss = 1.35024557\n",
            "Iteration 43, loss = 1.35095585\n",
            "Iteration 44, loss = 1.35009680\n",
            "Iteration 45, loss = 1.35086584\n",
            "Iteration 46, loss = 1.35024172\n",
            "Iteration 47, loss = 1.35021811\n",
            "Iteration 48, loss = 1.34914151\n",
            "Iteration 49, loss = 1.34836719\n",
            "Iteration 50, loss = 1.34877722\n",
            "Iteration 51, loss = 1.34915891\n",
            "Iteration 52, loss = 1.34671627\n",
            "Iteration 53, loss = 1.34715330\n",
            "Iteration 54, loss = 1.34777494\n",
            "Iteration 55, loss = 1.34661798\n",
            "Iteration 56, loss = 1.34604323\n",
            "Iteration 57, loss = 1.34584638\n",
            "Iteration 58, loss = 1.34492538\n",
            "Iteration 59, loss = 1.34557729\n",
            "Iteration 60, loss = 1.34536020\n",
            "Iteration 61, loss = 1.34523129\n",
            "Iteration 62, loss = 1.34390012\n",
            "Iteration 63, loss = 1.34339111\n",
            "Iteration 64, loss = 1.34271198\n",
            "Iteration 65, loss = 1.34311920\n",
            "Iteration 66, loss = 1.34276288\n",
            "Iteration 67, loss = 1.34317323\n",
            "Iteration 68, loss = 1.34282161\n",
            "Iteration 69, loss = 1.34274571\n",
            "Iteration 70, loss = 1.34218715\n",
            "Iteration 71, loss = 1.34273745\n",
            "Iteration 72, loss = 1.34305854\n",
            "Iteration 73, loss = 1.34221490\n",
            "Iteration 74, loss = 1.34150540\n",
            "Iteration 75, loss = 1.34196526\n",
            "Iteration 76, loss = 1.34175607\n",
            "Iteration 77, loss = 1.34139015\n",
            "Iteration 78, loss = 1.34138717\n",
            "Iteration 79, loss = 1.34146320\n",
            "Iteration 80, loss = 1.34322876\n",
            "Iteration 81, loss = 1.34226584\n",
            "Iteration 82, loss = 1.34276857\n",
            "Iteration 83, loss = 1.34017270\n",
            "Iteration 84, loss = 1.34201601\n",
            "Iteration 85, loss = 1.34297611\n",
            "Iteration 86, loss = 1.34121339\n",
            "Iteration 87, loss = 1.34053946\n",
            "Iteration 88, loss = 1.33993258\n",
            "Iteration 89, loss = 1.34006507\n",
            "Iteration 90, loss = 1.33944574\n",
            "Iteration 91, loss = 1.33934704\n",
            "Iteration 92, loss = 1.33941374\n",
            "Iteration 93, loss = 1.33934260\n",
            "Iteration 94, loss = 1.34057498\n",
            "Iteration 95, loss = 1.34054130\n",
            "Iteration 96, loss = 1.34035410\n",
            "Iteration 97, loss = 1.34141929\n",
            "Iteration 98, loss = 1.34171737\n",
            "Iteration 99, loss = 1.34265168\n",
            "Iteration 100, loss = 1.34187220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37704114\n",
            "Iteration 2, loss = 1.36423147\n",
            "Iteration 3, loss = 1.36042409\n",
            "Iteration 4, loss = 1.35908631\n",
            "Iteration 5, loss = 1.35744643\n",
            "Iteration 6, loss = 1.35614209\n",
            "Iteration 7, loss = 1.35534904\n",
            "Iteration 8, loss = 1.35418372\n",
            "Iteration 9, loss = 1.35355814\n",
            "Iteration 10, loss = 1.35272402\n",
            "Iteration 11, loss = 1.35191327\n",
            "Iteration 12, loss = 1.35149457\n",
            "Iteration 13, loss = 1.35116364\n",
            "Iteration 14, loss = 1.35094322\n",
            "Iteration 15, loss = 1.34955296\n",
            "Iteration 16, loss = 1.34909229\n",
            "Iteration 17, loss = 1.34866384\n",
            "Iteration 18, loss = 1.34809575\n",
            "Iteration 19, loss = 1.34777508\n",
            "Iteration 20, loss = 1.34691767\n",
            "Iteration 21, loss = 1.34686503\n",
            "Iteration 22, loss = 1.34636819\n",
            "Iteration 23, loss = 1.34584463\n",
            "Iteration 24, loss = 1.34508352\n",
            "Iteration 25, loss = 1.34700344\n",
            "Iteration 26, loss = 1.34786334\n",
            "Iteration 27, loss = 1.34406778\n",
            "Iteration 28, loss = 1.34302516\n",
            "Iteration 29, loss = 1.34408898\n",
            "Iteration 30, loss = 1.34271028\n",
            "Iteration 31, loss = 1.34157690\n",
            "Iteration 32, loss = 1.34197123\n",
            "Iteration 33, loss = 1.34018860\n",
            "Iteration 34, loss = 1.34104260\n",
            "Iteration 35, loss = 1.33968523\n",
            "Iteration 36, loss = 1.34011104\n",
            "Iteration 37, loss = 1.33763422\n",
            "Iteration 38, loss = 1.33749293\n",
            "Iteration 39, loss = 1.33720487\n",
            "Iteration 40, loss = 1.33746589\n",
            "Iteration 41, loss = 1.33668660\n",
            "Iteration 42, loss = 1.33497153\n",
            "Iteration 43, loss = 1.33582398\n",
            "Iteration 44, loss = 1.33465732\n",
            "Iteration 45, loss = 1.33504584\n",
            "Iteration 46, loss = 1.33548696\n",
            "Iteration 47, loss = 1.33600481\n",
            "Iteration 48, loss = 1.33436016\n",
            "Iteration 49, loss = 1.33361408\n",
            "Iteration 50, loss = 1.33422408\n",
            "Iteration 51, loss = 1.33459789\n",
            "Iteration 52, loss = 1.33182451\n",
            "Iteration 53, loss = 1.33188617\n",
            "Iteration 54, loss = 1.33202264\n",
            "Iteration 55, loss = 1.33306017\n",
            "Iteration 56, loss = 1.33123471\n",
            "Iteration 57, loss = 1.33194928\n",
            "Iteration 58, loss = 1.33069000\n",
            "Iteration 59, loss = 1.33133422\n",
            "Iteration 60, loss = 1.33167111\n",
            "Iteration 61, loss = 1.33196930\n",
            "Iteration 62, loss = 1.33025620\n",
            "Iteration 63, loss = 1.32982692\n",
            "Iteration 64, loss = 1.32935816\n",
            "Iteration 65, loss = 1.32988840\n",
            "Iteration 66, loss = 1.32937121\n",
            "Iteration 67, loss = 1.32956305\n",
            "Iteration 68, loss = 1.32973705\n",
            "Iteration 69, loss = 1.32952937\n",
            "Iteration 70, loss = 1.32920613\n",
            "Iteration 71, loss = 1.32978641\n",
            "Iteration 72, loss = 1.33038132\n",
            "Iteration 73, loss = 1.32908868\n",
            "Iteration 74, loss = 1.32843096\n",
            "Iteration 75, loss = 1.32831847\n",
            "Iteration 76, loss = 1.32950570\n",
            "Iteration 77, loss = 1.32817414\n",
            "Iteration 78, loss = 1.32936877\n",
            "Iteration 79, loss = 1.32970246\n",
            "Iteration 80, loss = 1.33002849\n",
            "Iteration 81, loss = 1.33062485\n",
            "Iteration 82, loss = 1.33035056\n",
            "Iteration 83, loss = 1.32802905\n",
            "Iteration 84, loss = 1.32928981\n",
            "Iteration 85, loss = 1.32953135\n",
            "Iteration 86, loss = 1.32837455\n",
            "Iteration 87, loss = 1.32834061\n",
            "Iteration 88, loss = 1.32744712\n",
            "Iteration 89, loss = 1.32709217\n",
            "Iteration 90, loss = 1.32679684\n",
            "Iteration 91, loss = 1.32746773\n",
            "Iteration 92, loss = 1.32679420\n",
            "Iteration 93, loss = 1.32717198\n",
            "Iteration 94, loss = 1.32736940\n",
            "Iteration 95, loss = 1.32803389\n",
            "Iteration 96, loss = 1.32791024\n",
            "Iteration 97, loss = 1.32777622\n",
            "Iteration 98, loss = 1.32846170\n",
            "Iteration 99, loss = 1.32856059\n",
            "Iteration 100, loss = 1.33004149\n",
            "46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39835504\n",
            "Iteration 2, loss = 1.35137806\n",
            "Iteration 3, loss = 1.31935851\n",
            "Iteration 4, loss = 1.27968238\n",
            "Iteration 5, loss = 1.23272987\n",
            "Iteration 6, loss = 1.17956289\n",
            "Iteration 7, loss = 1.12460381\n",
            "Iteration 8, loss = 1.07857364\n",
            "Iteration 9, loss = 1.04560191\n",
            "Iteration 10, loss = 1.02721334\n",
            "Iteration 11, loss = 1.00933587\n",
            "Iteration 12, loss = 1.00185189\n",
            "Iteration 13, loss = 0.99688991\n",
            "Iteration 14, loss = 0.97572588\n",
            "Iteration 15, loss = 0.96608422\n",
            "Iteration 16, loss = 0.95887179\n",
            "Iteration 17, loss = 0.95427416\n",
            "Iteration 18, loss = 0.95784361\n",
            "Iteration 19, loss = 0.94559707\n",
            "Iteration 20, loss = 0.93649106\n",
            "Iteration 21, loss = 0.93680791\n",
            "Iteration 22, loss = 0.93314665\n",
            "Iteration 23, loss = 0.93414727\n",
            "Iteration 24, loss = 0.93742636\n",
            "Iteration 25, loss = 0.93194154\n",
            "Iteration 26, loss = 0.92555695\n",
            "Iteration 27, loss = 0.92132157\n",
            "Iteration 28, loss = 0.92036745\n",
            "Iteration 29, loss = 0.91543048\n",
            "Iteration 30, loss = 0.91876476\n",
            "Iteration 31, loss = 0.91480231\n",
            "Iteration 32, loss = 0.91562148\n",
            "Iteration 33, loss = 0.90971233\n",
            "Iteration 34, loss = 0.90524090\n",
            "Iteration 35, loss = 0.90684744\n",
            "Iteration 36, loss = 0.92436916\n",
            "Iteration 37, loss = 0.91013272\n",
            "Iteration 38, loss = 0.91233291\n",
            "Iteration 39, loss = 0.90379611\n",
            "Iteration 40, loss = 0.89744475\n",
            "Iteration 41, loss = 0.89437065\n",
            "Iteration 42, loss = 0.89830915\n",
            "Iteration 43, loss = 0.89091154\n",
            "Iteration 44, loss = 0.89537988\n",
            "Iteration 45, loss = 0.89440112\n",
            "Iteration 46, loss = 0.89704224\n",
            "Iteration 47, loss = 0.88983869\n",
            "Iteration 48, loss = 0.88339059\n",
            "Iteration 49, loss = 0.89242092\n",
            "Iteration 50, loss = 0.88930446\n",
            "Iteration 51, loss = 0.88930125\n",
            "Iteration 52, loss = 0.88262210\n",
            "Iteration 53, loss = 0.87324434\n",
            "Iteration 54, loss = 0.87345483\n",
            "Iteration 55, loss = 0.87383922\n",
            "Iteration 56, loss = 0.87127663\n",
            "Iteration 57, loss = 0.87259344\n",
            "Iteration 58, loss = 0.86670666\n",
            "Iteration 59, loss = 0.86495684\n",
            "Iteration 60, loss = 0.86344045\n",
            "Iteration 61, loss = 0.86523541\n",
            "Iteration 62, loss = 0.86544388\n",
            "Iteration 63, loss = 0.86710983\n",
            "Iteration 64, loss = 0.86289646\n",
            "Iteration 65, loss = 0.86440026\n",
            "Iteration 66, loss = 0.85957462\n",
            "Iteration 67, loss = 0.85662106\n",
            "Iteration 68, loss = 0.85415461\n",
            "Iteration 69, loss = 0.85382862\n",
            "Iteration 70, loss = 0.84716686\n",
            "Iteration 71, loss = 0.84964836\n",
            "Iteration 72, loss = 0.84859178\n",
            "Iteration 73, loss = 0.85026978\n",
            "Iteration 74, loss = 0.84736484\n",
            "Iteration 75, loss = 0.84845413\n",
            "Iteration 76, loss = 0.84694982\n",
            "Iteration 77, loss = 0.85012935\n",
            "Iteration 78, loss = 0.83987915\n",
            "Iteration 79, loss = 0.84170471\n",
            "Iteration 80, loss = 0.84295088\n",
            "Iteration 81, loss = 0.84541374\n",
            "Iteration 82, loss = 0.84030975\n",
            "Iteration 83, loss = 0.84232199\n",
            "Iteration 84, loss = 0.83499910\n",
            "Iteration 85, loss = 0.83288607\n",
            "Iteration 86, loss = 0.82960726\n",
            "Iteration 87, loss = 0.83346660\n",
            "Iteration 88, loss = 0.83467878\n",
            "Iteration 89, loss = 0.82898657\n",
            "Iteration 90, loss = 0.82478600\n",
            "Iteration 91, loss = 0.82378447\n",
            "Iteration 92, loss = 0.82135964\n",
            "Iteration 93, loss = 0.82625007\n",
            "Iteration 94, loss = 0.82243012\n",
            "Iteration 95, loss = 0.81812486\n",
            "Iteration 96, loss = 0.82390280\n",
            "Iteration 97, loss = 0.82066407\n",
            "Iteration 98, loss = 0.82809422\n",
            "Iteration 99, loss = 0.82167785\n",
            "Iteration 100, loss = 0.81970629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39630171\n",
            "Iteration 2, loss = 1.35115989\n",
            "Iteration 3, loss = 1.31855251\n",
            "Iteration 4, loss = 1.27804932\n",
            "Iteration 5, loss = 1.22811003\n",
            "Iteration 6, loss = 1.17437367\n",
            "Iteration 7, loss = 1.11996575\n",
            "Iteration 8, loss = 1.07468853\n",
            "Iteration 9, loss = 1.04276558\n",
            "Iteration 10, loss = 1.02165582\n",
            "Iteration 11, loss = 1.00226084\n",
            "Iteration 12, loss = 0.98999601\n",
            "Iteration 13, loss = 0.98442406\n",
            "Iteration 14, loss = 0.96508799\n",
            "Iteration 15, loss = 0.95375111\n",
            "Iteration 16, loss = 0.94896672\n",
            "Iteration 17, loss = 0.95131162\n",
            "Iteration 18, loss = 0.94309785\n",
            "Iteration 19, loss = 0.93618466\n",
            "Iteration 20, loss = 0.92858109\n",
            "Iteration 21, loss = 0.93155069\n",
            "Iteration 22, loss = 0.92413353\n",
            "Iteration 23, loss = 0.92397503\n",
            "Iteration 24, loss = 0.92499119\n",
            "Iteration 25, loss = 0.91835166\n",
            "Iteration 26, loss = 0.91413184\n",
            "Iteration 27, loss = 0.91035138\n",
            "Iteration 28, loss = 0.90973026\n",
            "Iteration 29, loss = 0.90437426\n",
            "Iteration 30, loss = 0.91068753\n",
            "Iteration 31, loss = 0.90818908\n",
            "Iteration 32, loss = 0.90584478\n",
            "Iteration 33, loss = 0.89844763\n",
            "Iteration 34, loss = 0.89892558\n",
            "Iteration 35, loss = 0.89692672\n",
            "Iteration 36, loss = 0.90886396\n",
            "Iteration 37, loss = 0.89329861\n",
            "Iteration 38, loss = 0.89286192\n",
            "Iteration 39, loss = 0.88924041\n",
            "Iteration 40, loss = 0.88529937\n",
            "Iteration 41, loss = 0.87957787\n",
            "Iteration 42, loss = 0.88478457\n",
            "Iteration 43, loss = 0.87477183\n",
            "Iteration 44, loss = 0.87709616\n",
            "Iteration 45, loss = 0.87168891\n",
            "Iteration 46, loss = 0.87255771\n",
            "Iteration 47, loss = 0.86956220\n",
            "Iteration 48, loss = 0.87217266\n",
            "Iteration 49, loss = 0.88271857\n",
            "Iteration 50, loss = 0.88157799\n",
            "Iteration 51, loss = 0.87968071\n",
            "Iteration 52, loss = 0.86815043\n",
            "Iteration 53, loss = 0.86057141\n",
            "Iteration 54, loss = 0.85552690\n",
            "Iteration 55, loss = 0.85914681\n",
            "Iteration 56, loss = 0.85752418\n",
            "Iteration 57, loss = 0.85288337\n",
            "Iteration 58, loss = 0.84976969\n",
            "Iteration 59, loss = 0.84822684\n",
            "Iteration 60, loss = 0.84731340\n",
            "Iteration 61, loss = 0.85151532\n",
            "Iteration 62, loss = 0.84994601\n",
            "Iteration 63, loss = 0.85049044\n",
            "Iteration 64, loss = 0.84283372\n",
            "Iteration 65, loss = 0.85615105\n",
            "Iteration 66, loss = 0.84176412\n",
            "Iteration 67, loss = 0.83799123\n",
            "Iteration 68, loss = 0.83318976\n",
            "Iteration 69, loss = 0.83652240\n",
            "Iteration 70, loss = 0.83298032\n",
            "Iteration 71, loss = 0.82861961\n",
            "Iteration 72, loss = 0.82874042\n",
            "Iteration 73, loss = 0.82922989\n",
            "Iteration 74, loss = 0.82539815\n",
            "Iteration 75, loss = 0.82538737\n",
            "Iteration 76, loss = 0.82506462\n",
            "Iteration 77, loss = 0.82583460\n",
            "Iteration 78, loss = 0.82107131\n",
            "Iteration 79, loss = 0.82567424\n",
            "Iteration 80, loss = 0.81848241\n",
            "Iteration 81, loss = 0.83232375\n",
            "Iteration 82, loss = 0.82416158\n",
            "Iteration 83, loss = 0.82781658\n",
            "Iteration 84, loss = 0.81714010\n",
            "Iteration 85, loss = 0.81728587\n",
            "Iteration 86, loss = 0.81133947\n",
            "Iteration 87, loss = 0.81421638\n",
            "Iteration 88, loss = 0.81284041\n",
            "Iteration 89, loss = 0.80621175\n",
            "Iteration 90, loss = 0.80781260\n",
            "Iteration 91, loss = 0.80652165\n",
            "Iteration 92, loss = 0.80188613\n",
            "Iteration 93, loss = 0.80706667\n",
            "Iteration 94, loss = 0.80481072\n",
            "Iteration 95, loss = 0.80473262\n",
            "Iteration 96, loss = 0.80334344\n",
            "Iteration 97, loss = 0.80057003\n",
            "Iteration 98, loss = 0.80543053\n",
            "Iteration 99, loss = 0.79948960\n",
            "Iteration 100, loss = 0.79895452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39363442\n",
            "Iteration 2, loss = 1.35187400\n",
            "Iteration 3, loss = 1.31804355\n",
            "Iteration 4, loss = 1.27771020\n",
            "Iteration 5, loss = 1.22680579\n",
            "Iteration 6, loss = 1.17364510\n",
            "Iteration 7, loss = 1.12051409\n",
            "Iteration 8, loss = 1.07397286\n",
            "Iteration 9, loss = 1.03972214\n",
            "Iteration 10, loss = 1.01470697\n",
            "Iteration 11, loss = 0.99769834\n",
            "Iteration 12, loss = 0.98542695\n",
            "Iteration 13, loss = 0.98381793\n",
            "Iteration 14, loss = 0.95600842\n",
            "Iteration 15, loss = 0.94938440\n",
            "Iteration 16, loss = 0.93994501\n",
            "Iteration 17, loss = 0.94683209\n",
            "Iteration 18, loss = 0.94626906\n",
            "Iteration 19, loss = 0.94388324\n",
            "Iteration 20, loss = 0.93097595\n",
            "Iteration 21, loss = 0.92745181\n",
            "Iteration 22, loss = 0.91900825\n",
            "Iteration 23, loss = 0.92356358\n",
            "Iteration 24, loss = 0.92917696\n",
            "Iteration 25, loss = 0.92014323\n",
            "Iteration 26, loss = 0.91721382\n",
            "Iteration 27, loss = 0.91161070\n",
            "Iteration 28, loss = 0.90836385\n",
            "Iteration 29, loss = 0.90326702\n",
            "Iteration 30, loss = 0.90799231\n",
            "Iteration 31, loss = 0.90339043\n",
            "Iteration 32, loss = 0.90277847\n",
            "Iteration 33, loss = 0.89649796\n",
            "Iteration 34, loss = 0.89367880\n",
            "Iteration 35, loss = 0.89422390\n",
            "Iteration 36, loss = 0.89955092\n",
            "Iteration 37, loss = 0.88974363\n",
            "Iteration 38, loss = 0.89140588\n",
            "Iteration 39, loss = 0.88664955\n",
            "Iteration 40, loss = 0.88343451\n",
            "Iteration 41, loss = 0.87768664\n",
            "Iteration 42, loss = 0.88197453\n",
            "Iteration 43, loss = 0.87430703\n",
            "Iteration 44, loss = 0.88180853\n",
            "Iteration 45, loss = 0.87465272\n",
            "Iteration 46, loss = 0.87596687\n",
            "Iteration 47, loss = 0.87249741\n",
            "Iteration 48, loss = 0.87294154\n",
            "Iteration 49, loss = 0.87649594\n",
            "Iteration 50, loss = 0.87466173\n",
            "Iteration 51, loss = 0.87569998\n",
            "Iteration 52, loss = 0.86251290\n",
            "Iteration 53, loss = 0.85992277\n",
            "Iteration 54, loss = 0.85757424\n",
            "Iteration 55, loss = 0.85651968\n",
            "Iteration 56, loss = 0.85467878\n",
            "Iteration 57, loss = 0.85120963\n",
            "Iteration 58, loss = 0.84942333\n",
            "Iteration 59, loss = 0.84748383\n",
            "Iteration 60, loss = 0.84978384\n",
            "Iteration 61, loss = 0.84855591\n",
            "Iteration 62, loss = 0.84825283\n",
            "Iteration 63, loss = 0.84602843\n",
            "Iteration 64, loss = 0.84131826\n",
            "Iteration 65, loss = 0.85128915\n",
            "Iteration 66, loss = 0.84349690\n",
            "Iteration 67, loss = 0.83972649\n",
            "Iteration 68, loss = 0.83331183\n",
            "Iteration 69, loss = 0.83589929\n",
            "Iteration 70, loss = 0.83237425\n",
            "Iteration 71, loss = 0.82919266\n",
            "Iteration 72, loss = 0.82874740\n",
            "Iteration 73, loss = 0.82830199\n",
            "Iteration 74, loss = 0.82760620\n",
            "Iteration 75, loss = 0.83021676\n",
            "Iteration 76, loss = 0.82750455\n",
            "Iteration 77, loss = 0.82830522\n",
            "Iteration 78, loss = 0.82790523\n",
            "Iteration 79, loss = 0.82746127\n",
            "Iteration 80, loss = 0.82103887\n",
            "Iteration 81, loss = 0.82960612\n",
            "Iteration 82, loss = 0.82693445\n",
            "Iteration 83, loss = 0.82723982\n",
            "Iteration 84, loss = 0.81369722\n",
            "Iteration 85, loss = 0.81414947\n",
            "Iteration 86, loss = 0.81023690\n",
            "Iteration 87, loss = 0.81444382\n",
            "Iteration 88, loss = 0.81015621\n",
            "Iteration 89, loss = 0.80409033\n",
            "Iteration 90, loss = 0.80590390\n",
            "Iteration 91, loss = 0.80301537\n",
            "Iteration 92, loss = 0.80263592\n",
            "Iteration 93, loss = 0.80682873\n",
            "Iteration 94, loss = 0.80233060\n",
            "Iteration 95, loss = 0.79872923\n",
            "Iteration 96, loss = 0.79798826\n",
            "Iteration 97, loss = 0.80138345\n",
            "Iteration 98, loss = 0.80631934\n",
            "Iteration 99, loss = 0.79958490\n",
            "Iteration 100, loss = 0.79381815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39271921\n",
            "Iteration 2, loss = 1.34859236\n",
            "Iteration 3, loss = 1.31560116\n",
            "Iteration 4, loss = 1.27343297\n",
            "Iteration 5, loss = 1.22329785\n",
            "Iteration 6, loss = 1.17145234\n",
            "Iteration 7, loss = 1.11616176\n",
            "Iteration 8, loss = 1.07201935\n",
            "Iteration 9, loss = 1.03684847\n",
            "Iteration 10, loss = 1.01603599\n",
            "Iteration 11, loss = 1.00241120\n",
            "Iteration 12, loss = 0.98964908\n",
            "Iteration 13, loss = 0.98780766\n",
            "Iteration 14, loss = 0.95821593\n",
            "Iteration 15, loss = 0.95181266\n",
            "Iteration 16, loss = 0.93988264\n",
            "Iteration 17, loss = 0.94467043\n",
            "Iteration 18, loss = 0.94415202\n",
            "Iteration 19, loss = 0.94615637\n",
            "Iteration 20, loss = 0.93026973\n",
            "Iteration 21, loss = 0.92344641\n",
            "Iteration 22, loss = 0.91828105\n",
            "Iteration 23, loss = 0.91926147\n",
            "Iteration 24, loss = 0.91951964\n",
            "Iteration 25, loss = 0.91662138\n",
            "Iteration 26, loss = 0.91065531\n",
            "Iteration 27, loss = 0.90897829\n",
            "Iteration 28, loss = 0.90773190\n",
            "Iteration 29, loss = 0.90328021\n",
            "Iteration 30, loss = 0.90478398\n",
            "Iteration 31, loss = 0.90072394\n",
            "Iteration 32, loss = 0.89631186\n",
            "Iteration 33, loss = 0.89510882\n",
            "Iteration 34, loss = 0.89215079\n",
            "Iteration 35, loss = 0.89204990\n",
            "Iteration 36, loss = 0.88993957\n",
            "Iteration 37, loss = 0.88619457\n",
            "Iteration 38, loss = 0.88492291\n",
            "Iteration 39, loss = 0.88269876\n",
            "Iteration 40, loss = 0.88072349\n",
            "Iteration 41, loss = 0.87644547\n",
            "Iteration 42, loss = 0.87971030\n",
            "Iteration 43, loss = 0.87418838\n",
            "Iteration 44, loss = 0.87680146\n",
            "Iteration 45, loss = 0.87209584\n",
            "Iteration 46, loss = 0.87141698\n",
            "Iteration 47, loss = 0.86873996\n",
            "Iteration 48, loss = 0.87431635\n",
            "Iteration 49, loss = 0.87437973\n",
            "Iteration 50, loss = 0.87053592\n",
            "Iteration 51, loss = 0.87538020\n",
            "Iteration 52, loss = 0.86579899\n",
            "Iteration 53, loss = 0.86511839\n",
            "Iteration 54, loss = 0.86144757\n",
            "Iteration 55, loss = 0.85996559\n",
            "Iteration 56, loss = 0.85908999\n",
            "Iteration 57, loss = 0.85126221\n",
            "Iteration 58, loss = 0.85150014\n",
            "Iteration 59, loss = 0.85355055\n",
            "Iteration 60, loss = 0.85346928\n",
            "Iteration 61, loss = 0.85034276\n",
            "Iteration 62, loss = 0.85005043\n",
            "Iteration 63, loss = 0.84619187\n",
            "Iteration 64, loss = 0.84812133\n",
            "Iteration 65, loss = 0.85683658\n",
            "Iteration 66, loss = 0.85016507\n",
            "Iteration 67, loss = 0.84754162\n",
            "Iteration 68, loss = 0.83899179\n",
            "Iteration 69, loss = 0.83865681\n",
            "Iteration 70, loss = 0.83835627\n",
            "Iteration 71, loss = 0.83449636\n",
            "Iteration 72, loss = 0.83401432\n",
            "Iteration 73, loss = 0.83578182\n",
            "Iteration 74, loss = 0.83583427\n",
            "Iteration 75, loss = 0.83324361\n",
            "Iteration 76, loss = 0.82753135\n",
            "Iteration 77, loss = 0.83120670\n",
            "Iteration 78, loss = 0.83032126\n",
            "Iteration 79, loss = 0.83567912\n",
            "Iteration 80, loss = 0.83208907\n",
            "Iteration 81, loss = 0.83447497\n",
            "Iteration 82, loss = 0.83211902\n",
            "Iteration 83, loss = 0.83104509\n",
            "Iteration 84, loss = 0.82066003\n",
            "Iteration 85, loss = 0.81997122\n",
            "Iteration 86, loss = 0.81835515\n",
            "Iteration 87, loss = 0.81837537\n",
            "Iteration 88, loss = 0.81606748\n",
            "Iteration 89, loss = 0.81373535\n",
            "Iteration 90, loss = 0.81439036\n",
            "Iteration 91, loss = 0.81499247\n",
            "Iteration 92, loss = 0.81451536\n",
            "Iteration 93, loss = 0.82654155\n",
            "Iteration 94, loss = 0.81291848\n",
            "Iteration 95, loss = 0.80945252\n",
            "Iteration 96, loss = 0.81029538\n",
            "Iteration 97, loss = 0.80895844\n",
            "Iteration 98, loss = 0.80956723\n",
            "Iteration 99, loss = 0.81377218\n",
            "Iteration 100, loss = 0.80492370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39357207\n",
            "Iteration 2, loss = 1.35094836\n",
            "Iteration 3, loss = 1.31888773\n",
            "Iteration 4, loss = 1.27768451\n",
            "Iteration 5, loss = 1.22821026\n",
            "Iteration 6, loss = 1.17696715\n",
            "Iteration 7, loss = 1.12073265\n",
            "Iteration 8, loss = 1.07585997\n",
            "Iteration 9, loss = 1.03738124\n",
            "Iteration 10, loss = 1.01240986\n",
            "Iteration 11, loss = 0.98993281\n",
            "Iteration 12, loss = 0.97584753\n",
            "Iteration 13, loss = 0.97035654\n",
            "Iteration 14, loss = 0.95160037\n",
            "Iteration 15, loss = 0.94800588\n",
            "Iteration 16, loss = 0.93818575\n",
            "Iteration 17, loss = 0.93322246\n",
            "Iteration 18, loss = 0.94286067\n",
            "Iteration 19, loss = 0.95378983\n",
            "Iteration 20, loss = 0.93773271\n",
            "Iteration 21, loss = 0.92743044\n",
            "Iteration 22, loss = 0.92140760\n",
            "Iteration 23, loss = 0.91654627\n",
            "Iteration 24, loss = 0.91342921\n",
            "Iteration 25, loss = 0.91220434\n",
            "Iteration 26, loss = 0.90632147\n",
            "Iteration 27, loss = 0.90358211\n",
            "Iteration 28, loss = 0.90443379\n",
            "Iteration 29, loss = 0.90143171\n",
            "Iteration 30, loss = 0.90198068\n",
            "Iteration 31, loss = 0.89595978\n",
            "Iteration 32, loss = 0.89121095\n",
            "Iteration 33, loss = 0.89262632\n",
            "Iteration 34, loss = 0.89020751\n",
            "Iteration 35, loss = 0.88712147\n",
            "Iteration 36, loss = 0.88676514\n",
            "Iteration 37, loss = 0.88702809\n",
            "Iteration 38, loss = 0.88449247\n",
            "Iteration 39, loss = 0.88196146\n",
            "Iteration 40, loss = 0.87481348\n",
            "Iteration 41, loss = 0.87349080\n",
            "Iteration 42, loss = 0.87232244\n",
            "Iteration 43, loss = 0.86650101\n",
            "Iteration 44, loss = 0.86749274\n",
            "Iteration 45, loss = 0.86355471\n",
            "Iteration 46, loss = 0.86530730\n",
            "Iteration 47, loss = 0.85983857\n",
            "Iteration 48, loss = 0.85990712\n",
            "Iteration 49, loss = 0.85834987\n",
            "Iteration 50, loss = 0.85430739\n",
            "Iteration 51, loss = 0.85294022\n",
            "Iteration 52, loss = 0.85014868\n",
            "Iteration 53, loss = 0.84931916\n",
            "Iteration 54, loss = 0.85270301\n",
            "Iteration 55, loss = 0.85375472\n",
            "Iteration 56, loss = 0.85043669\n",
            "Iteration 57, loss = 0.84266643\n",
            "Iteration 58, loss = 0.84229732\n",
            "Iteration 59, loss = 0.84429236\n",
            "Iteration 60, loss = 0.84470900\n",
            "Iteration 61, loss = 0.84246620\n",
            "Iteration 62, loss = 0.84260646\n",
            "Iteration 63, loss = 0.83820317\n",
            "Iteration 64, loss = 0.83288638\n",
            "Iteration 65, loss = 0.83931989\n",
            "Iteration 66, loss = 0.83294349\n",
            "Iteration 67, loss = 0.83447250\n",
            "Iteration 68, loss = 0.82781214\n",
            "Iteration 69, loss = 0.82554670\n",
            "Iteration 70, loss = 0.82463041\n",
            "Iteration 71, loss = 0.82070771\n",
            "Iteration 72, loss = 0.81960165\n",
            "Iteration 73, loss = 0.81931490\n",
            "Iteration 74, loss = 0.81956894\n",
            "Iteration 75, loss = 0.81818543\n",
            "Iteration 76, loss = 0.81043049\n",
            "Iteration 77, loss = 0.81625949\n",
            "Iteration 78, loss = 0.81317381\n",
            "Iteration 79, loss = 0.81875824\n",
            "Iteration 80, loss = 0.81499175\n",
            "Iteration 81, loss = 0.81188556\n",
            "Iteration 82, loss = 0.81413437\n",
            "Iteration 83, loss = 0.80789032\n",
            "Iteration 84, loss = 0.80449151\n",
            "Iteration 85, loss = 0.80304555\n",
            "Iteration 86, loss = 0.80097154\n",
            "Iteration 87, loss = 0.79992425\n",
            "Iteration 88, loss = 0.80083372\n",
            "Iteration 89, loss = 0.79918321\n",
            "Iteration 90, loss = 0.79835261\n",
            "Iteration 91, loss = 0.79875680\n",
            "Iteration 92, loss = 0.80543955\n",
            "Iteration 93, loss = 0.81525452\n",
            "Iteration 94, loss = 0.80032132\n",
            "Iteration 95, loss = 0.79314226\n",
            "Iteration 96, loss = 0.79005144\n",
            "Iteration 97, loss = 0.79739396\n",
            "Iteration 98, loss = 0.79281102\n",
            "Iteration 99, loss = 0.79670183\n",
            "Iteration 100, loss = 0.78292206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37565091\n",
            "Iteration 2, loss = 1.34384796\n",
            "Iteration 3, loss = 1.31549574\n",
            "Iteration 4, loss = 1.27750652\n",
            "Iteration 5, loss = 1.23515184\n",
            "Iteration 6, loss = 1.19043468\n",
            "Iteration 7, loss = 1.14004883\n",
            "Iteration 8, loss = 1.10403430\n",
            "Iteration 9, loss = 1.06695600\n",
            "Iteration 10, loss = 1.04706166\n",
            "Iteration 11, loss = 1.02939584\n",
            "Iteration 12, loss = 1.01627447\n",
            "Iteration 13, loss = 1.00720024\n",
            "Iteration 14, loss = 0.98739710\n",
            "Iteration 15, loss = 0.98761616\n",
            "Iteration 16, loss = 0.97518399\n",
            "Iteration 17, loss = 0.97354016\n",
            "Iteration 18, loss = 0.97853541\n",
            "Iteration 19, loss = 0.99308447\n",
            "Iteration 20, loss = 0.96462978\n",
            "Iteration 21, loss = 0.95613392\n",
            "Iteration 22, loss = 0.95454801\n",
            "Iteration 23, loss = 0.95628209\n",
            "Iteration 24, loss = 0.95563007\n",
            "Iteration 25, loss = 0.94967712\n",
            "Iteration 26, loss = 0.94466096\n",
            "Iteration 27, loss = 0.94139769\n",
            "Iteration 28, loss = 0.94085179\n",
            "Iteration 29, loss = 0.93738535\n",
            "Iteration 30, loss = 0.94037900\n",
            "Iteration 31, loss = 0.93680338\n",
            "Iteration 32, loss = 0.92739414\n",
            "Iteration 33, loss = 0.93143565\n",
            "Iteration 34, loss = 0.92278237\n",
            "Iteration 35, loss = 0.92521040\n",
            "Iteration 36, loss = 0.92109097\n",
            "Iteration 37, loss = 0.92341518\n",
            "Iteration 38, loss = 0.91716605\n",
            "Iteration 39, loss = 0.91601524\n",
            "Iteration 40, loss = 0.91085139\n",
            "Iteration 41, loss = 0.90761856\n",
            "Iteration 42, loss = 0.90508439\n",
            "Iteration 43, loss = 0.90342252\n",
            "Iteration 44, loss = 0.90651183\n",
            "Iteration 45, loss = 0.89971335\n",
            "Iteration 46, loss = 0.89984318\n",
            "Iteration 47, loss = 0.89539438\n",
            "Iteration 48, loss = 0.89415241\n",
            "Iteration 49, loss = 0.89453666\n",
            "Iteration 50, loss = 0.89012049\n",
            "Iteration 51, loss = 0.89017561\n",
            "Iteration 52, loss = 0.88687471\n",
            "Iteration 53, loss = 0.88710377\n",
            "Iteration 54, loss = 0.88753390\n",
            "Iteration 55, loss = 0.89302111\n",
            "Iteration 56, loss = 0.89053050\n",
            "Iteration 57, loss = 0.88712288\n",
            "Iteration 58, loss = 0.87927140\n",
            "Iteration 59, loss = 0.88377235\n",
            "Iteration 60, loss = 0.88179838\n",
            "Iteration 61, loss = 0.87989210\n",
            "Iteration 62, loss = 0.87724155\n",
            "Iteration 63, loss = 0.87566111\n",
            "Iteration 64, loss = 0.87062498\n",
            "Iteration 65, loss = 0.87273986\n",
            "Iteration 66, loss = 0.86679598\n",
            "Iteration 67, loss = 0.86801617\n",
            "Iteration 68, loss = 0.86083392\n",
            "Iteration 69, loss = 0.86497849\n",
            "Iteration 70, loss = 0.86717860\n",
            "Iteration 71, loss = 0.86194206\n",
            "Iteration 72, loss = 0.86384826\n",
            "Iteration 73, loss = 0.86058301\n",
            "Iteration 74, loss = 0.85619904\n",
            "Iteration 75, loss = 0.85330731\n",
            "Iteration 76, loss = 0.84853353\n",
            "Iteration 77, loss = 0.84939456\n",
            "Iteration 78, loss = 0.85568565\n",
            "Iteration 79, loss = 0.84998686\n",
            "Iteration 80, loss = 0.85014317\n",
            "Iteration 81, loss = 0.84528692\n",
            "Iteration 82, loss = 0.84920355\n",
            "Iteration 83, loss = 0.84470875\n",
            "Iteration 84, loss = 0.84215229\n",
            "Iteration 85, loss = 0.84163589\n",
            "Iteration 86, loss = 0.84040808\n",
            "Iteration 87, loss = 0.83996363\n",
            "Iteration 88, loss = 0.83491125\n",
            "Iteration 89, loss = 0.83903312\n",
            "Iteration 90, loss = 0.83387448\n",
            "Iteration 91, loss = 0.84419906\n",
            "Iteration 92, loss = 0.83912421\n",
            "Iteration 93, loss = 0.84150001\n",
            "Iteration 94, loss = 0.83074371\n",
            "Iteration 95, loss = 0.82773404\n",
            "Iteration 96, loss = 0.82732960\n",
            "Iteration 97, loss = 0.82793295\n",
            "Iteration 98, loss = 0.82296541\n",
            "Iteration 99, loss = 0.82506035\n",
            "Iteration 100, loss = 0.81851937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36131244\n",
            "Iteration 2, loss = 1.33337453\n",
            "Iteration 3, loss = 1.30767137\n",
            "Iteration 4, loss = 1.27577959\n",
            "Iteration 5, loss = 1.24228261\n",
            "Iteration 6, loss = 1.20714555\n",
            "Iteration 7, loss = 1.16648908\n",
            "Iteration 8, loss = 1.13359642\n",
            "Iteration 9, loss = 1.09683153\n",
            "Iteration 10, loss = 1.07637599\n",
            "Iteration 11, loss = 1.05804999\n",
            "Iteration 12, loss = 1.04330525\n",
            "Iteration 13, loss = 1.02995979\n",
            "Iteration 14, loss = 1.01690395\n",
            "Iteration 15, loss = 1.01823913\n",
            "Iteration 16, loss = 1.00873357\n",
            "Iteration 17, loss = 1.00095336\n",
            "Iteration 18, loss = 1.00786138\n",
            "Iteration 19, loss = 1.01427670\n",
            "Iteration 20, loss = 0.99474673\n",
            "Iteration 21, loss = 0.98765632\n",
            "Iteration 22, loss = 0.98550938\n",
            "Iteration 23, loss = 0.98663297\n",
            "Iteration 24, loss = 0.98621244\n",
            "Iteration 25, loss = 0.98046299\n",
            "Iteration 26, loss = 0.97486327\n",
            "Iteration 27, loss = 0.97196578\n",
            "Iteration 28, loss = 0.97115094\n",
            "Iteration 29, loss = 0.96907509\n",
            "Iteration 30, loss = 0.97027088\n",
            "Iteration 31, loss = 0.96865090\n",
            "Iteration 32, loss = 0.96048051\n",
            "Iteration 33, loss = 0.96209555\n",
            "Iteration 34, loss = 0.96082495\n",
            "Iteration 35, loss = 0.96292714\n",
            "Iteration 36, loss = 0.95572288\n",
            "Iteration 37, loss = 0.95308383\n",
            "Iteration 38, loss = 0.95072602\n",
            "Iteration 39, loss = 0.95213109\n",
            "Iteration 40, loss = 0.94561420\n",
            "Iteration 41, loss = 0.94512859\n",
            "Iteration 42, loss = 0.94056560\n",
            "Iteration 43, loss = 0.94096901\n",
            "Iteration 44, loss = 0.94171791\n",
            "Iteration 45, loss = 0.93878828\n",
            "Iteration 46, loss = 0.94471794\n",
            "Iteration 47, loss = 0.93596165\n",
            "Iteration 48, loss = 0.93333889\n",
            "Iteration 49, loss = 0.93155351\n",
            "Iteration 50, loss = 0.93105506\n",
            "Iteration 51, loss = 0.92728417\n",
            "Iteration 52, loss = 0.92456652\n",
            "Iteration 53, loss = 0.92439930\n",
            "Iteration 54, loss = 0.92624086\n",
            "Iteration 55, loss = 0.92909372\n",
            "Iteration 56, loss = 0.92626665\n",
            "Iteration 57, loss = 0.92688568\n",
            "Iteration 58, loss = 0.91704238\n",
            "Iteration 59, loss = 0.92224578\n",
            "Iteration 60, loss = 0.92149791\n",
            "Iteration 61, loss = 0.91711770\n",
            "Iteration 62, loss = 0.91347965\n",
            "Iteration 63, loss = 0.91103887\n",
            "Iteration 64, loss = 0.90845630\n",
            "Iteration 65, loss = 0.91401038\n",
            "Iteration 66, loss = 0.91099599\n",
            "Iteration 67, loss = 0.91163821\n",
            "Iteration 68, loss = 0.90403991\n",
            "Iteration 69, loss = 0.90385377\n",
            "Iteration 70, loss = 0.90555842\n",
            "Iteration 71, loss = 0.89921002\n",
            "Iteration 72, loss = 0.90000109\n",
            "Iteration 73, loss = 0.89656956\n",
            "Iteration 74, loss = 0.89888975\n",
            "Iteration 75, loss = 0.89167875\n",
            "Iteration 76, loss = 0.88731164\n",
            "Iteration 77, loss = 0.88810089\n",
            "Iteration 78, loss = 0.89451640\n",
            "Iteration 79, loss = 0.89349554\n",
            "Iteration 80, loss = 0.89473689\n",
            "Iteration 81, loss = 0.88627887\n",
            "Iteration 82, loss = 0.89470000\n",
            "Iteration 83, loss = 0.88658995\n",
            "Iteration 84, loss = 0.88390359\n",
            "Iteration 85, loss = 0.88274129\n",
            "Iteration 86, loss = 0.88391737\n",
            "Iteration 87, loss = 0.87974723\n",
            "Iteration 88, loss = 0.88014370\n",
            "Iteration 89, loss = 0.88366557\n",
            "Iteration 90, loss = 0.87627628\n",
            "Iteration 91, loss = 0.88461968\n",
            "Iteration 92, loss = 0.88333914\n",
            "Iteration 93, loss = 0.88556061\n",
            "Iteration 94, loss = 0.87711328\n",
            "Iteration 95, loss = 0.87080896\n",
            "Iteration 96, loss = 0.86970116\n",
            "Iteration 97, loss = 0.87266199\n",
            "Iteration 98, loss = 0.86684012\n",
            "Iteration 99, loss = 0.86854865\n",
            "Iteration 100, loss = 0.86591070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38440044\n",
            "Iteration 2, loss = 1.35114149\n",
            "Iteration 3, loss = 1.32069007\n",
            "Iteration 4, loss = 1.28668518\n",
            "Iteration 5, loss = 1.24486726\n",
            "Iteration 6, loss = 1.20040848\n",
            "Iteration 7, loss = 1.14905626\n",
            "Iteration 8, loss = 1.10690340\n",
            "Iteration 9, loss = 1.06426940\n",
            "Iteration 10, loss = 1.04378491\n",
            "Iteration 11, loss = 1.02540517\n",
            "Iteration 12, loss = 1.00905490\n",
            "Iteration 13, loss = 0.99617477\n",
            "Iteration 14, loss = 0.98684836\n",
            "Iteration 15, loss = 0.98521918\n",
            "Iteration 16, loss = 0.97226668\n",
            "Iteration 17, loss = 0.97186942\n",
            "Iteration 18, loss = 0.97262784\n",
            "Iteration 19, loss = 0.97728585\n",
            "Iteration 20, loss = 0.95888125\n",
            "Iteration 21, loss = 0.95435512\n",
            "Iteration 22, loss = 0.95344982\n",
            "Iteration 23, loss = 0.95399180\n",
            "Iteration 24, loss = 0.95266864\n",
            "Iteration 25, loss = 0.94444977\n",
            "Iteration 26, loss = 0.94289264\n",
            "Iteration 27, loss = 0.93986997\n",
            "Iteration 28, loss = 0.93772619\n",
            "Iteration 29, loss = 0.93541171\n",
            "Iteration 30, loss = 0.93685514\n",
            "Iteration 31, loss = 0.93342691\n",
            "Iteration 32, loss = 0.92454920\n",
            "Iteration 33, loss = 0.93082305\n",
            "Iteration 34, loss = 0.92757793\n",
            "Iteration 35, loss = 0.92622624\n",
            "Iteration 36, loss = 0.91860363\n",
            "Iteration 37, loss = 0.91578660\n",
            "Iteration 38, loss = 0.91362511\n",
            "Iteration 39, loss = 0.91611505\n",
            "Iteration 40, loss = 0.90975383\n",
            "Iteration 41, loss = 0.90895437\n",
            "Iteration 42, loss = 0.90359210\n",
            "Iteration 43, loss = 0.90428160\n",
            "Iteration 44, loss = 0.90275900\n",
            "Iteration 45, loss = 0.90131755\n",
            "Iteration 46, loss = 0.90730119\n",
            "Iteration 47, loss = 0.90008675\n",
            "Iteration 48, loss = 0.89529640\n",
            "Iteration 49, loss = 0.89381849\n",
            "Iteration 50, loss = 0.89430352\n",
            "Iteration 51, loss = 0.89143787\n",
            "Iteration 52, loss = 0.88596415\n",
            "Iteration 53, loss = 0.88472271\n",
            "Iteration 54, loss = 0.88429366\n",
            "Iteration 55, loss = 0.88627259\n",
            "Iteration 56, loss = 0.88332521\n",
            "Iteration 57, loss = 0.88381647\n",
            "Iteration 58, loss = 0.87722926\n",
            "Iteration 59, loss = 0.88372493\n",
            "Iteration 60, loss = 0.88505660\n",
            "Iteration 61, loss = 0.87556669\n",
            "Iteration 62, loss = 0.87510951\n",
            "Iteration 63, loss = 0.87067028\n",
            "Iteration 64, loss = 0.88095363\n",
            "Iteration 65, loss = 0.88132996\n",
            "Iteration 66, loss = 0.87265521\n",
            "Iteration 67, loss = 0.86962207\n",
            "Iteration 68, loss = 0.87112015\n",
            "Iteration 69, loss = 0.86542092\n",
            "Iteration 70, loss = 0.86346930\n",
            "Iteration 71, loss = 0.86286921\n",
            "Iteration 72, loss = 0.86632117\n",
            "Iteration 73, loss = 0.85796561\n",
            "Iteration 74, loss = 0.86101502\n",
            "Iteration 75, loss = 0.85305432\n",
            "Iteration 76, loss = 0.85051825\n",
            "Iteration 77, loss = 0.84847278\n",
            "Iteration 78, loss = 0.85186486\n",
            "Iteration 79, loss = 0.85575444\n",
            "Iteration 80, loss = 0.85552002\n",
            "Iteration 81, loss = 0.84640838\n",
            "Iteration 82, loss = 0.85203472\n",
            "Iteration 83, loss = 0.84864254\n",
            "Iteration 84, loss = 0.85006740\n",
            "Iteration 85, loss = 0.84501446\n",
            "Iteration 86, loss = 0.84262765\n",
            "Iteration 87, loss = 0.84625787\n",
            "Iteration 88, loss = 0.84779420\n",
            "Iteration 89, loss = 0.83956158\n",
            "Iteration 90, loss = 0.83710099\n",
            "Iteration 91, loss = 0.84901038\n",
            "Iteration 92, loss = 0.84833781\n",
            "Iteration 93, loss = 0.84446904\n",
            "Iteration 94, loss = 0.83980087\n",
            "Iteration 95, loss = 0.83523997\n",
            "Iteration 96, loss = 0.82707517\n",
            "Iteration 97, loss = 0.82961494\n",
            "Iteration 98, loss = 0.82663685\n",
            "Iteration 99, loss = 0.82757342\n",
            "Iteration 100, loss = 0.82310223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39411318\n",
            "Iteration 2, loss = 1.34928730\n",
            "Iteration 3, loss = 1.31917633\n",
            "Iteration 4, loss = 1.28215746\n",
            "Iteration 5, loss = 1.23174036\n",
            "Iteration 6, loss = 1.17321840\n",
            "Iteration 7, loss = 1.11328701\n",
            "Iteration 8, loss = 1.06365575\n",
            "Iteration 9, loss = 1.02618872\n",
            "Iteration 10, loss = 1.00128806\n",
            "Iteration 11, loss = 0.98536623\n",
            "Iteration 12, loss = 0.97317344\n",
            "Iteration 13, loss = 0.95848362\n",
            "Iteration 14, loss = 0.95066283\n",
            "Iteration 15, loss = 0.94594006\n",
            "Iteration 16, loss = 0.93697291\n",
            "Iteration 17, loss = 0.93845956\n",
            "Iteration 18, loss = 0.93167582\n",
            "Iteration 19, loss = 0.94683078\n",
            "Iteration 20, loss = 0.92495547\n",
            "Iteration 21, loss = 0.91894648\n",
            "Iteration 22, loss = 0.91481664\n",
            "Iteration 23, loss = 0.91324754\n",
            "Iteration 24, loss = 0.91626428\n",
            "Iteration 25, loss = 0.90809163\n",
            "Iteration 26, loss = 0.90923382\n",
            "Iteration 27, loss = 0.91036353\n",
            "Iteration 28, loss = 0.90405288\n",
            "Iteration 29, loss = 0.90156631\n",
            "Iteration 30, loss = 0.90192304\n",
            "Iteration 31, loss = 0.89744109\n",
            "Iteration 32, loss = 0.89036605\n",
            "Iteration 33, loss = 0.89302958\n",
            "Iteration 34, loss = 0.88765970\n",
            "Iteration 35, loss = 0.89070529\n",
            "Iteration 36, loss = 0.88527178\n",
            "Iteration 37, loss = 0.88074532\n",
            "Iteration 38, loss = 0.87747723\n",
            "Iteration 39, loss = 0.87829650\n",
            "Iteration 40, loss = 0.87389954\n",
            "Iteration 41, loss = 0.87333498\n",
            "Iteration 42, loss = 0.86925549\n",
            "Iteration 43, loss = 0.87008516\n",
            "Iteration 44, loss = 0.86814177\n",
            "Iteration 45, loss = 0.86370113\n",
            "Iteration 46, loss = 0.86874339\n",
            "Iteration 47, loss = 0.86377449\n",
            "Iteration 48, loss = 0.86368109\n",
            "Iteration 49, loss = 0.86106782\n",
            "Iteration 50, loss = 0.85916378\n",
            "Iteration 51, loss = 0.85947492\n",
            "Iteration 52, loss = 0.85292302\n",
            "Iteration 53, loss = 0.85289788\n",
            "Iteration 54, loss = 0.85099618\n",
            "Iteration 55, loss = 0.85367666\n",
            "Iteration 56, loss = 0.85036865\n",
            "Iteration 57, loss = 0.84750538\n",
            "Iteration 58, loss = 0.84425817\n",
            "Iteration 59, loss = 0.84649050\n",
            "Iteration 60, loss = 0.85153560\n",
            "Iteration 61, loss = 0.84600204\n",
            "Iteration 62, loss = 0.83949160\n",
            "Iteration 63, loss = 0.83651395\n",
            "Iteration 64, loss = 0.83726839\n",
            "Iteration 65, loss = 0.84441540\n",
            "Iteration 66, loss = 0.83562191\n",
            "Iteration 67, loss = 0.83131202\n",
            "Iteration 68, loss = 0.83532495\n",
            "Iteration 69, loss = 0.82901570\n",
            "Iteration 70, loss = 0.82598581\n",
            "Iteration 71, loss = 0.82647110\n",
            "Iteration 72, loss = 0.82816742\n",
            "Iteration 73, loss = 0.82885677\n",
            "Iteration 74, loss = 0.83323191\n",
            "Iteration 75, loss = 0.82170366\n",
            "Iteration 76, loss = 0.81818575\n",
            "Iteration 77, loss = 0.81624977\n",
            "Iteration 78, loss = 0.81760312\n",
            "Iteration 79, loss = 0.81663015\n",
            "Iteration 80, loss = 0.82102495\n",
            "Iteration 81, loss = 0.81462951\n",
            "Iteration 82, loss = 0.81923858\n",
            "Iteration 83, loss = 0.81004337\n",
            "Iteration 84, loss = 0.81333044\n",
            "Iteration 85, loss = 0.80985450\n",
            "Iteration 86, loss = 0.80735016\n",
            "Iteration 87, loss = 0.80921854\n",
            "Iteration 88, loss = 0.81070202\n",
            "Iteration 89, loss = 0.81071074\n",
            "Iteration 90, loss = 0.80173824\n",
            "Iteration 91, loss = 0.81308843\n",
            "Iteration 92, loss = 0.81326223\n",
            "Iteration 93, loss = 0.80931800\n",
            "Iteration 94, loss = 0.80389732\n",
            "Iteration 95, loss = 0.79825275\n",
            "Iteration 96, loss = 0.79096257\n",
            "Iteration 97, loss = 0.79360749\n",
            "Iteration 98, loss = 0.78882533\n",
            "Iteration 99, loss = 0.79269007\n",
            "Iteration 100, loss = 0.78961415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37653833\n",
            "Iteration 2, loss = 1.33501792\n",
            "Iteration 3, loss = 1.30695407\n",
            "Iteration 4, loss = 1.26798964\n",
            "Iteration 5, loss = 1.21944649\n",
            "Iteration 6, loss = 1.16457532\n",
            "Iteration 7, loss = 1.10868412\n",
            "Iteration 8, loss = 1.06148697\n",
            "Iteration 9, loss = 1.02541722\n",
            "Iteration 10, loss = 1.00697245\n",
            "Iteration 11, loss = 0.98403651\n",
            "Iteration 12, loss = 0.96782873\n",
            "Iteration 13, loss = 0.95473350\n",
            "Iteration 14, loss = 0.94376139\n",
            "Iteration 15, loss = 0.93799277\n",
            "Iteration 16, loss = 0.93365408\n",
            "Iteration 17, loss = 0.92717037\n",
            "Iteration 18, loss = 0.91976336\n",
            "Iteration 19, loss = 0.93132935\n",
            "Iteration 20, loss = 0.91614082\n",
            "Iteration 21, loss = 0.91040014\n",
            "Iteration 22, loss = 0.90789561\n",
            "Iteration 23, loss = 0.90573354\n",
            "Iteration 24, loss = 0.90525603\n",
            "Iteration 25, loss = 0.89831955\n",
            "Iteration 26, loss = 0.89841395\n",
            "Iteration 27, loss = 0.89743284\n",
            "Iteration 28, loss = 0.89500257\n",
            "Iteration 29, loss = 0.88764335\n",
            "Iteration 30, loss = 0.88535611\n",
            "Iteration 31, loss = 0.88259184\n",
            "Iteration 32, loss = 0.87764976\n",
            "Iteration 33, loss = 0.87732347\n",
            "Iteration 34, loss = 0.87730277\n",
            "Iteration 35, loss = 0.87804386\n",
            "Iteration 36, loss = 0.87471470\n",
            "Iteration 37, loss = 0.86851998\n",
            "Iteration 38, loss = 0.86515963\n",
            "Iteration 39, loss = 0.86725803\n",
            "Iteration 40, loss = 0.86164934\n",
            "Iteration 41, loss = 0.85962642\n",
            "Iteration 42, loss = 0.85589412\n",
            "Iteration 43, loss = 0.85728271\n",
            "Iteration 44, loss = 0.85708863\n",
            "Iteration 45, loss = 0.85325382\n",
            "Iteration 46, loss = 0.85312920\n",
            "Iteration 47, loss = 0.84843932\n",
            "Iteration 48, loss = 0.84861378\n",
            "Iteration 49, loss = 0.84848434\n",
            "Iteration 50, loss = 0.84285912\n",
            "Iteration 51, loss = 0.84152361\n",
            "Iteration 52, loss = 0.83895914\n",
            "Iteration 53, loss = 0.83585379\n",
            "Iteration 54, loss = 0.83718511\n",
            "Iteration 55, loss = 0.83860659\n",
            "Iteration 56, loss = 0.83720842\n",
            "Iteration 57, loss = 0.83266362\n",
            "Iteration 58, loss = 0.82764589\n",
            "Iteration 59, loss = 0.83294292\n",
            "Iteration 60, loss = 0.83403300\n",
            "Iteration 61, loss = 0.83069313\n",
            "Iteration 62, loss = 0.82201906\n",
            "Iteration 63, loss = 0.81966464\n",
            "Iteration 64, loss = 0.82073452\n",
            "Iteration 65, loss = 0.82502452\n",
            "Iteration 66, loss = 0.82216302\n",
            "Iteration 67, loss = 0.81160788\n",
            "Iteration 68, loss = 0.82017857\n",
            "Iteration 69, loss = 0.81434379\n",
            "Iteration 70, loss = 0.80904840\n",
            "Iteration 71, loss = 0.81010388\n",
            "Iteration 72, loss = 0.81572418\n",
            "Iteration 73, loss = 0.81712635\n",
            "Iteration 74, loss = 0.81878314\n",
            "Iteration 75, loss = 0.80604244\n",
            "Iteration 76, loss = 0.80234516\n",
            "Iteration 77, loss = 0.79927067\n",
            "Iteration 78, loss = 0.80290758\n",
            "Iteration 79, loss = 0.80331826\n",
            "Iteration 80, loss = 0.81029806\n",
            "Iteration 81, loss = 0.80211038\n",
            "Iteration 82, loss = 0.79910608\n",
            "Iteration 83, loss = 0.79231780\n",
            "Iteration 84, loss = 0.80046990\n",
            "Iteration 85, loss = 0.79151243\n",
            "Iteration 86, loss = 0.79558126\n",
            "Iteration 87, loss = 0.79955937\n",
            "Iteration 88, loss = 0.79429755\n",
            "Iteration 89, loss = 0.78798063\n",
            "Iteration 90, loss = 0.78903405\n",
            "Iteration 91, loss = 0.80153389\n",
            "Iteration 92, loss = 0.80425626\n",
            "Iteration 93, loss = 0.80379569\n",
            "Iteration 94, loss = 0.79324407\n",
            "Iteration 95, loss = 0.79169652\n",
            "Iteration 96, loss = 0.77622471\n",
            "Iteration 97, loss = 0.77719229\n",
            "Iteration 98, loss = 0.77629699\n",
            "Iteration 99, loss = 0.77813172\n",
            "Iteration 100, loss = 0.77309039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "Iteration 1, loss = 1.39124136\n",
            "Iteration 2, loss = 1.38167567\n",
            "Iteration 3, loss = 1.37687412\n",
            "Iteration 4, loss = 1.37298190\n",
            "Iteration 5, loss = 1.37197313\n",
            "Iteration 6, loss = 1.37161006\n",
            "Iteration 7, loss = 1.36937087\n",
            "Iteration 8, loss = 1.36833803\n",
            "Iteration 9, loss = 1.36778218\n",
            "Iteration 10, loss = 1.36711742\n",
            "Iteration 11, loss = 1.36662506\n",
            "Iteration 12, loss = 1.36628589\n",
            "Iteration 13, loss = 1.36576488\n",
            "Iteration 14, loss = 1.36572172\n",
            "Iteration 15, loss = 1.36501497\n",
            "Iteration 16, loss = 1.36458829\n",
            "Iteration 17, loss = 1.36488923\n",
            "Iteration 18, loss = 1.36383960\n",
            "Iteration 19, loss = 1.36376892\n",
            "Iteration 20, loss = 1.36329679\n",
            "Iteration 21, loss = 1.36287936\n",
            "Iteration 22, loss = 1.36287313\n",
            "Iteration 23, loss = 1.36240654\n",
            "Iteration 24, loss = 1.36218415\n",
            "Iteration 25, loss = 1.36161527\n",
            "Iteration 26, loss = 1.36232034\n",
            "Iteration 27, loss = 1.36077021\n",
            "Iteration 28, loss = 1.36015569\n",
            "Iteration 29, loss = 1.36013337\n",
            "Iteration 30, loss = 1.35999717\n",
            "Iteration 31, loss = 1.36023510\n",
            "Iteration 32, loss = 1.36106692\n",
            "Iteration 33, loss = 1.36045871\n",
            "Iteration 34, loss = 1.35875641\n",
            "Iteration 35, loss = 1.35786923\n",
            "Iteration 36, loss = 1.35817307\n",
            "Iteration 37, loss = 1.35659367\n",
            "Iteration 38, loss = 1.35671696\n",
            "Iteration 39, loss = 1.35646608\n",
            "Iteration 40, loss = 1.35443684\n",
            "Iteration 41, loss = 1.35595002\n",
            "Iteration 42, loss = 1.35526088\n",
            "Iteration 43, loss = 1.35393431\n",
            "Iteration 44, loss = 1.35480000\n",
            "Iteration 45, loss = 1.35451933\n",
            "Iteration 46, loss = 1.35315183\n",
            "Iteration 47, loss = 1.35401941\n",
            "Iteration 48, loss = 1.35186338\n",
            "Iteration 49, loss = 1.35241974\n",
            "Iteration 50, loss = 1.35190161\n",
            "Iteration 51, loss = 1.35274512\n",
            "Iteration 52, loss = 1.35001842\n",
            "Iteration 53, loss = 1.35018418\n",
            "Iteration 54, loss = 1.35084723\n",
            "Iteration 55, loss = 1.34907330\n",
            "Iteration 56, loss = 1.35002515\n",
            "Iteration 57, loss = 1.34845452\n",
            "Iteration 58, loss = 1.35052445\n",
            "Iteration 59, loss = 1.35014027\n",
            "Iteration 60, loss = 1.34898983\n",
            "Iteration 61, loss = 1.34822003\n",
            "Iteration 62, loss = 1.34811305\n",
            "Iteration 63, loss = 1.34856518\n",
            "Iteration 64, loss = 1.34893405\n",
            "Iteration 65, loss = 1.34847560\n",
            "Iteration 66, loss = 1.34829961\n",
            "Iteration 67, loss = 1.34617304\n",
            "Iteration 68, loss = 1.34646184\n",
            "Iteration 69, loss = 1.34612662\n",
            "Iteration 70, loss = 1.34692857\n",
            "Iteration 71, loss = 1.34662934\n",
            "Iteration 72, loss = 1.34630867\n",
            "Iteration 73, loss = 1.34627905\n",
            "Iteration 74, loss = 1.34577041\n",
            "Iteration 75, loss = 1.34650338\n",
            "Iteration 76, loss = 1.34689362\n",
            "Iteration 77, loss = 1.34536620\n",
            "Iteration 78, loss = 1.34523349\n",
            "Iteration 79, loss = 1.34569726\n",
            "Iteration 80, loss = 1.34448601\n",
            "Iteration 81, loss = 1.34474469\n",
            "Iteration 82, loss = 1.34530614\n",
            "Iteration 83, loss = 1.34480569\n",
            "Iteration 84, loss = 1.34469645\n",
            "Iteration 85, loss = 1.34500760\n",
            "Iteration 86, loss = 1.34500956\n",
            "Iteration 87, loss = 1.34444961\n",
            "Iteration 88, loss = 1.34507067\n",
            "Iteration 89, loss = 1.34651096\n",
            "Iteration 90, loss = 1.34452172\n",
            "Iteration 91, loss = 1.34471112\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.39284307\n",
            "Iteration 2, loss = 1.38300940\n",
            "Iteration 3, loss = 1.37930789\n",
            "Iteration 4, loss = 1.37563364\n",
            "Iteration 5, loss = 1.37452536\n",
            "Iteration 6, loss = 1.37377492\n",
            "Iteration 7, loss = 1.37206710\n",
            "Iteration 8, loss = 1.37132422\n",
            "Iteration 9, loss = 1.37076605\n",
            "Iteration 10, loss = 1.37035381\n",
            "Iteration 11, loss = 1.36950998\n",
            "Iteration 12, loss = 1.36959737\n",
            "Iteration 13, loss = 1.36888548\n",
            "Iteration 14, loss = 1.36909549\n",
            "Iteration 15, loss = 1.36884007\n",
            "Iteration 16, loss = 1.36778174\n",
            "Iteration 17, loss = 1.36802540\n",
            "Iteration 18, loss = 1.36730876\n",
            "Iteration 19, loss = 1.36684866\n",
            "Iteration 20, loss = 1.36700280\n",
            "Iteration 21, loss = 1.36632520\n",
            "Iteration 22, loss = 1.36698353\n",
            "Iteration 23, loss = 1.36600192\n",
            "Iteration 24, loss = 1.36609511\n",
            "Iteration 25, loss = 1.36541504\n",
            "Iteration 26, loss = 1.36618858\n",
            "Iteration 27, loss = 1.36500583\n",
            "Iteration 28, loss = 1.36433153\n",
            "Iteration 29, loss = 1.36449351\n",
            "Iteration 30, loss = 1.36434995\n",
            "Iteration 31, loss = 1.36494115\n",
            "Iteration 32, loss = 1.36599787\n",
            "Iteration 33, loss = 1.36480382\n",
            "Iteration 34, loss = 1.36311565\n",
            "Iteration 35, loss = 1.36286870\n",
            "Iteration 36, loss = 1.36325532\n",
            "Iteration 37, loss = 1.36232404\n",
            "Iteration 38, loss = 1.36220249\n",
            "Iteration 39, loss = 1.36170056\n",
            "Iteration 40, loss = 1.36114212\n",
            "Iteration 41, loss = 1.36195759\n",
            "Iteration 42, loss = 1.36186778\n",
            "Iteration 43, loss = 1.36028981\n",
            "Iteration 44, loss = 1.36100801\n",
            "Iteration 45, loss = 1.36099757\n",
            "Iteration 46, loss = 1.36012827\n",
            "Iteration 47, loss = 1.36064303\n",
            "Iteration 48, loss = 1.35936785\n",
            "Iteration 49, loss = 1.36005417\n",
            "Iteration 50, loss = 1.35841516\n",
            "Iteration 51, loss = 1.35963491\n",
            "Iteration 52, loss = 1.35755553\n",
            "Iteration 53, loss = 1.35751781\n",
            "Iteration 54, loss = 1.35798532\n",
            "Iteration 55, loss = 1.35663023\n",
            "Iteration 56, loss = 1.35704836\n",
            "Iteration 57, loss = 1.35592905\n",
            "Iteration 58, loss = 1.35848000\n",
            "Iteration 59, loss = 1.35662373\n",
            "Iteration 60, loss = 1.35637360\n",
            "Iteration 61, loss = 1.35545537\n",
            "Iteration 62, loss = 1.35562029\n",
            "Iteration 63, loss = 1.35658994\n",
            "Iteration 64, loss = 1.35650005\n",
            "Iteration 65, loss = 1.35562780\n",
            "Iteration 66, loss = 1.35567505\n",
            "Iteration 67, loss = 1.35424226\n",
            "Iteration 68, loss = 1.35365736\n",
            "Iteration 69, loss = 1.35396712\n",
            "Iteration 70, loss = 1.35407109\n",
            "Iteration 71, loss = 1.35415310\n",
            "Iteration 72, loss = 1.35351808\n",
            "Iteration 73, loss = 1.35406471\n",
            "Iteration 74, loss = 1.35344119\n",
            "Iteration 75, loss = 1.35384798\n",
            "Iteration 76, loss = 1.35394763\n",
            "Iteration 77, loss = 1.35255888\n",
            "Iteration 78, loss = 1.35265621\n",
            "Iteration 79, loss = 1.35300122\n",
            "Iteration 80, loss = 1.35287630\n",
            "Iteration 81, loss = 1.35288847\n",
            "Iteration 82, loss = 1.35262295\n",
            "Iteration 83, loss = 1.35216484\n",
            "Iteration 84, loss = 1.35183133\n",
            "Iteration 85, loss = 1.35232578\n",
            "Iteration 86, loss = 1.35224366\n",
            "Iteration 87, loss = 1.35191586\n",
            "Iteration 88, loss = 1.35178206\n",
            "Iteration 89, loss = 1.35359488\n",
            "Iteration 90, loss = 1.35131700\n",
            "Iteration 91, loss = 1.35110265\n",
            "Iteration 92, loss = 1.35038989\n",
            "Iteration 93, loss = 1.35075949\n",
            "Iteration 94, loss = 1.35062638\n",
            "Iteration 95, loss = 1.35044412\n",
            "Iteration 96, loss = 1.35026918\n",
            "Iteration 97, loss = 1.35316696\n",
            "Iteration 98, loss = 1.35221380\n",
            "Iteration 99, loss = 1.34985464\n",
            "Iteration 100, loss = 1.34982414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39520906\n",
            "Iteration 2, loss = 1.38380555\n",
            "Iteration 3, loss = 1.38026143\n",
            "Iteration 4, loss = 1.37724235\n",
            "Iteration 5, loss = 1.37508529\n",
            "Iteration 6, loss = 1.37371431\n",
            "Iteration 7, loss = 1.37212854\n",
            "Iteration 8, loss = 1.37070955\n",
            "Iteration 9, loss = 1.37079104\n",
            "Iteration 10, loss = 1.37067590\n",
            "Iteration 11, loss = 1.36927016\n",
            "Iteration 12, loss = 1.36849569\n",
            "Iteration 13, loss = 1.36795347\n",
            "Iteration 14, loss = 1.36814714\n",
            "Iteration 15, loss = 1.36786780\n",
            "Iteration 16, loss = 1.36635835\n",
            "Iteration 17, loss = 1.36623548\n",
            "Iteration 18, loss = 1.36577078\n",
            "Iteration 19, loss = 1.36538712\n",
            "Iteration 20, loss = 1.36544925\n",
            "Iteration 21, loss = 1.36483665\n",
            "Iteration 22, loss = 1.36557359\n",
            "Iteration 23, loss = 1.36434404\n",
            "Iteration 24, loss = 1.36359629\n",
            "Iteration 25, loss = 1.36264390\n",
            "Iteration 26, loss = 1.36326589\n",
            "Iteration 27, loss = 1.36226432\n",
            "Iteration 28, loss = 1.36165744\n",
            "Iteration 29, loss = 1.36183089\n",
            "Iteration 30, loss = 1.36112423\n",
            "Iteration 31, loss = 1.36065213\n",
            "Iteration 32, loss = 1.36253827\n",
            "Iteration 33, loss = 1.36196935\n",
            "Iteration 34, loss = 1.35972647\n",
            "Iteration 35, loss = 1.35933826\n",
            "Iteration 36, loss = 1.35915877\n",
            "Iteration 37, loss = 1.35854826\n",
            "Iteration 38, loss = 1.35915081\n",
            "Iteration 39, loss = 1.35802830\n",
            "Iteration 40, loss = 1.35658496\n",
            "Iteration 41, loss = 1.35748603\n",
            "Iteration 42, loss = 1.35781432\n",
            "Iteration 43, loss = 1.35547440\n",
            "Iteration 44, loss = 1.35601007\n",
            "Iteration 45, loss = 1.35654866\n",
            "Iteration 46, loss = 1.35504982\n",
            "Iteration 47, loss = 1.35508112\n",
            "Iteration 48, loss = 1.35428446\n",
            "Iteration 49, loss = 1.35418135\n",
            "Iteration 50, loss = 1.35361988\n",
            "Iteration 51, loss = 1.35660334\n",
            "Iteration 52, loss = 1.35139227\n",
            "Iteration 53, loss = 1.35225953\n",
            "Iteration 54, loss = 1.35268767\n",
            "Iteration 55, loss = 1.35037708\n",
            "Iteration 56, loss = 1.35053002\n",
            "Iteration 57, loss = 1.35075700\n",
            "Iteration 58, loss = 1.35248332\n",
            "Iteration 59, loss = 1.35056802\n",
            "Iteration 60, loss = 1.34974575\n",
            "Iteration 61, loss = 1.34935777\n",
            "Iteration 62, loss = 1.34890178\n",
            "Iteration 63, loss = 1.34943911\n",
            "Iteration 64, loss = 1.35077297\n",
            "Iteration 65, loss = 1.35032161\n",
            "Iteration 66, loss = 1.34927593\n",
            "Iteration 67, loss = 1.34742009\n",
            "Iteration 68, loss = 1.34717426\n",
            "Iteration 69, loss = 1.34703062\n",
            "Iteration 70, loss = 1.34706870\n",
            "Iteration 71, loss = 1.34713155\n",
            "Iteration 72, loss = 1.34688625\n",
            "Iteration 73, loss = 1.34697711\n",
            "Iteration 74, loss = 1.34701831\n",
            "Iteration 75, loss = 1.34765155\n",
            "Iteration 76, loss = 1.34798730\n",
            "Iteration 77, loss = 1.34507972\n",
            "Iteration 78, loss = 1.34607821\n",
            "Iteration 79, loss = 1.34531928\n",
            "Iteration 80, loss = 1.34729416\n",
            "Iteration 81, loss = 1.34773929\n",
            "Iteration 82, loss = 1.34581806\n",
            "Iteration 83, loss = 1.34522002\n",
            "Iteration 84, loss = 1.34460714\n",
            "Iteration 85, loss = 1.34513240\n",
            "Iteration 86, loss = 1.34513163\n",
            "Iteration 87, loss = 1.34449117\n",
            "Iteration 88, loss = 1.34428472\n",
            "Iteration 89, loss = 1.34569810\n",
            "Iteration 90, loss = 1.34466170\n",
            "Iteration 91, loss = 1.34385771\n",
            "Iteration 92, loss = 1.34297314\n",
            "Iteration 93, loss = 1.34383530\n",
            "Iteration 94, loss = 1.34597301\n",
            "Iteration 95, loss = 1.34404817\n",
            "Iteration 96, loss = 1.34416877\n",
            "Iteration 97, loss = 1.34719810\n",
            "Iteration 98, loss = 1.34469900\n",
            "Iteration 99, loss = 1.34308081\n",
            "Iteration 100, loss = 1.34411985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39586092\n",
            "Iteration 2, loss = 1.38293899\n",
            "Iteration 3, loss = 1.37987724\n",
            "Iteration 4, loss = 1.37754712\n",
            "Iteration 5, loss = 1.37466052\n",
            "Iteration 6, loss = 1.37380432\n",
            "Iteration 7, loss = 1.37203871\n",
            "Iteration 8, loss = 1.37110957\n",
            "Iteration 9, loss = 1.37042176\n",
            "Iteration 10, loss = 1.36958405\n",
            "Iteration 11, loss = 1.36827415\n",
            "Iteration 12, loss = 1.36813595\n",
            "Iteration 13, loss = 1.36762095\n",
            "Iteration 14, loss = 1.36826316\n",
            "Iteration 15, loss = 1.36763977\n",
            "Iteration 16, loss = 1.36519194\n",
            "Iteration 17, loss = 1.36486248\n",
            "Iteration 18, loss = 1.36417863\n",
            "Iteration 19, loss = 1.36401316\n",
            "Iteration 20, loss = 1.36392776\n",
            "Iteration 21, loss = 1.36314119\n",
            "Iteration 22, loss = 1.36340523\n",
            "Iteration 23, loss = 1.36253648\n",
            "Iteration 24, loss = 1.36161837\n",
            "Iteration 25, loss = 1.36083807\n",
            "Iteration 26, loss = 1.36144354\n",
            "Iteration 27, loss = 1.36022476\n",
            "Iteration 28, loss = 1.35987790\n",
            "Iteration 29, loss = 1.35958967\n",
            "Iteration 30, loss = 1.35932622\n",
            "Iteration 31, loss = 1.35941389\n",
            "Iteration 32, loss = 1.36005250\n",
            "Iteration 33, loss = 1.35915064\n",
            "Iteration 34, loss = 1.35883969\n",
            "Iteration 35, loss = 1.35744898\n",
            "Iteration 36, loss = 1.35813338\n",
            "Iteration 37, loss = 1.35679518\n",
            "Iteration 38, loss = 1.35825894\n",
            "Iteration 39, loss = 1.35629184\n",
            "Iteration 40, loss = 1.35594029\n",
            "Iteration 41, loss = 1.35673880\n",
            "Iteration 42, loss = 1.35578361\n",
            "Iteration 43, loss = 1.35530583\n",
            "Iteration 44, loss = 1.35483466\n",
            "Iteration 45, loss = 1.35613289\n",
            "Iteration 46, loss = 1.35533078\n",
            "Iteration 47, loss = 1.35414845\n",
            "Iteration 48, loss = 1.35396862\n",
            "Iteration 49, loss = 1.35458862\n",
            "Iteration 50, loss = 1.35382048\n",
            "Iteration 51, loss = 1.35514191\n",
            "Iteration 52, loss = 1.35111078\n",
            "Iteration 53, loss = 1.35190818\n",
            "Iteration 54, loss = 1.35274971\n",
            "Iteration 55, loss = 1.35086641\n",
            "Iteration 56, loss = 1.35177613\n",
            "Iteration 57, loss = 1.35110717\n",
            "Iteration 58, loss = 1.35322849\n",
            "Iteration 59, loss = 1.35163524\n",
            "Iteration 60, loss = 1.35079426\n",
            "Iteration 61, loss = 1.35085448\n",
            "Iteration 62, loss = 1.34958738\n",
            "Iteration 63, loss = 1.35067399\n",
            "Iteration 64, loss = 1.34999054\n",
            "Iteration 65, loss = 1.35075293\n",
            "Iteration 66, loss = 1.34977177\n",
            "Iteration 67, loss = 1.34804952\n",
            "Iteration 68, loss = 1.34800525\n",
            "Iteration 69, loss = 1.34810352\n",
            "Iteration 70, loss = 1.34761170\n",
            "Iteration 71, loss = 1.34766698\n",
            "Iteration 72, loss = 1.34803444\n",
            "Iteration 73, loss = 1.34771210\n",
            "Iteration 74, loss = 1.34668275\n",
            "Iteration 75, loss = 1.34699399\n",
            "Iteration 76, loss = 1.34706848\n",
            "Iteration 77, loss = 1.34573132\n",
            "Iteration 78, loss = 1.34652556\n",
            "Iteration 79, loss = 1.34527098\n",
            "Iteration 80, loss = 1.34813325\n",
            "Iteration 81, loss = 1.34917693\n",
            "Iteration 82, loss = 1.34786688\n",
            "Iteration 83, loss = 1.34561924\n",
            "Iteration 84, loss = 1.34520363\n",
            "Iteration 85, loss = 1.34640348\n",
            "Iteration 86, loss = 1.34457344\n",
            "Iteration 87, loss = 1.34491303\n",
            "Iteration 88, loss = 1.34523864\n",
            "Iteration 89, loss = 1.34481582\n",
            "Iteration 90, loss = 1.34514909\n",
            "Iteration 91, loss = 1.34485433\n",
            "Iteration 92, loss = 1.34306596\n",
            "Iteration 93, loss = 1.34319213\n",
            "Iteration 94, loss = 1.34371148\n",
            "Iteration 95, loss = 1.34397571\n",
            "Iteration 96, loss = 1.34299983\n",
            "Iteration 97, loss = 1.34619244\n",
            "Iteration 98, loss = 1.34354812\n",
            "Iteration 99, loss = 1.34281839\n",
            "Iteration 100, loss = 1.34389970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39486747\n",
            "Iteration 2, loss = 1.38335211\n",
            "Iteration 3, loss = 1.37953578\n",
            "Iteration 4, loss = 1.37784043\n",
            "Iteration 5, loss = 1.37501385\n",
            "Iteration 6, loss = 1.37377828\n",
            "Iteration 7, loss = 1.37211815\n",
            "Iteration 8, loss = 1.37072650\n",
            "Iteration 9, loss = 1.37023335\n",
            "Iteration 10, loss = 1.36914227\n",
            "Iteration 11, loss = 1.36823156\n",
            "Iteration 12, loss = 1.36818746\n",
            "Iteration 13, loss = 1.36786190\n",
            "Iteration 14, loss = 1.36833723\n",
            "Iteration 15, loss = 1.36705064\n",
            "Iteration 16, loss = 1.36536782\n",
            "Iteration 17, loss = 1.36488176\n",
            "Iteration 18, loss = 1.36450836\n",
            "Iteration 19, loss = 1.36422033\n",
            "Iteration 20, loss = 1.36370211\n",
            "Iteration 21, loss = 1.36402377\n",
            "Iteration 22, loss = 1.36403130\n",
            "Iteration 23, loss = 1.36263602\n",
            "Iteration 24, loss = 1.36198264\n",
            "Iteration 25, loss = 1.36113356\n",
            "Iteration 26, loss = 1.36094399\n",
            "Iteration 27, loss = 1.36035984\n",
            "Iteration 28, loss = 1.35956706\n",
            "Iteration 29, loss = 1.35986977\n",
            "Iteration 30, loss = 1.35908949\n",
            "Iteration 31, loss = 1.35914084\n",
            "Iteration 32, loss = 1.35925670\n",
            "Iteration 33, loss = 1.35826526\n",
            "Iteration 34, loss = 1.35852154\n",
            "Iteration 35, loss = 1.35677232\n",
            "Iteration 36, loss = 1.35760098\n",
            "Iteration 37, loss = 1.35637572\n",
            "Iteration 38, loss = 1.35706264\n",
            "Iteration 39, loss = 1.35537786\n",
            "Iteration 40, loss = 1.35575191\n",
            "Iteration 41, loss = 1.35512761\n",
            "Iteration 42, loss = 1.35447182\n",
            "Iteration 43, loss = 1.35438852\n",
            "Iteration 44, loss = 1.35395800\n",
            "Iteration 45, loss = 1.35419241\n",
            "Iteration 46, loss = 1.35458065\n",
            "Iteration 47, loss = 1.35302965\n",
            "Iteration 48, loss = 1.35381786\n",
            "Iteration 49, loss = 1.35332063\n",
            "Iteration 50, loss = 1.35382323\n",
            "Iteration 51, loss = 1.35338537\n",
            "Iteration 52, loss = 1.35051451\n",
            "Iteration 53, loss = 1.35093332\n",
            "Iteration 54, loss = 1.35070003\n",
            "Iteration 55, loss = 1.34976064\n",
            "Iteration 56, loss = 1.35008245\n",
            "Iteration 57, loss = 1.34988460\n",
            "Iteration 58, loss = 1.35137603\n",
            "Iteration 59, loss = 1.34990007\n",
            "Iteration 60, loss = 1.34979770\n",
            "Iteration 61, loss = 1.34982820\n",
            "Iteration 62, loss = 1.34982286\n",
            "Iteration 63, loss = 1.34955363\n",
            "Iteration 64, loss = 1.34923541\n",
            "Iteration 65, loss = 1.34799112\n",
            "Iteration 66, loss = 1.34785849\n",
            "Iteration 67, loss = 1.34630016\n",
            "Iteration 68, loss = 1.34638521\n",
            "Iteration 69, loss = 1.34693058\n",
            "Iteration 70, loss = 1.34680576\n",
            "Iteration 71, loss = 1.34610195\n",
            "Iteration 72, loss = 1.34625251\n",
            "Iteration 73, loss = 1.34632561\n",
            "Iteration 74, loss = 1.34525824\n",
            "Iteration 75, loss = 1.34461478\n",
            "Iteration 76, loss = 1.34484087\n",
            "Iteration 77, loss = 1.34491392\n",
            "Iteration 78, loss = 1.34471130\n",
            "Iteration 79, loss = 1.34367526\n",
            "Iteration 80, loss = 1.34696654\n",
            "Iteration 81, loss = 1.34783555\n",
            "Iteration 82, loss = 1.34531558\n",
            "Iteration 83, loss = 1.34388622\n",
            "Iteration 84, loss = 1.34352716\n",
            "Iteration 85, loss = 1.34345342\n",
            "Iteration 86, loss = 1.34297149\n",
            "Iteration 87, loss = 1.34322855\n",
            "Iteration 88, loss = 1.34308729\n",
            "Iteration 89, loss = 1.34253538\n",
            "Iteration 90, loss = 1.34218903\n",
            "Iteration 91, loss = 1.34243257\n",
            "Iteration 92, loss = 1.34082690\n",
            "Iteration 93, loss = 1.34124060\n",
            "Iteration 94, loss = 1.34202401\n",
            "Iteration 95, loss = 1.34194324\n",
            "Iteration 96, loss = 1.34155116\n",
            "Iteration 97, loss = 1.34690883\n",
            "Iteration 98, loss = 1.34503340\n",
            "Iteration 99, loss = 1.34175107\n",
            "Iteration 100, loss = 1.34197448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39393314\n",
            "Iteration 2, loss = 1.38063350\n",
            "Iteration 3, loss = 1.37731237\n",
            "Iteration 4, loss = 1.37515370\n",
            "Iteration 5, loss = 1.37387762\n",
            "Iteration 6, loss = 1.37189326\n",
            "Iteration 7, loss = 1.37107228\n",
            "Iteration 8, loss = 1.36982207\n",
            "Iteration 9, loss = 1.36905682\n",
            "Iteration 10, loss = 1.36911642\n",
            "Iteration 11, loss = 1.36813442\n",
            "Iteration 12, loss = 1.36754071\n",
            "Iteration 13, loss = 1.36721175\n",
            "Iteration 14, loss = 1.36721160\n",
            "Iteration 15, loss = 1.36630726\n",
            "Iteration 16, loss = 1.36517270\n",
            "Iteration 17, loss = 1.36489116\n",
            "Iteration 18, loss = 1.36447823\n",
            "Iteration 19, loss = 1.36438559\n",
            "Iteration 20, loss = 1.36387501\n",
            "Iteration 21, loss = 1.36390838\n",
            "Iteration 22, loss = 1.36414386\n",
            "Iteration 23, loss = 1.36296004\n",
            "Iteration 24, loss = 1.36278613\n",
            "Iteration 25, loss = 1.36257485\n",
            "Iteration 26, loss = 1.36306795\n",
            "Iteration 27, loss = 1.36229617\n",
            "Iteration 28, loss = 1.36144880\n",
            "Iteration 29, loss = 1.36243112\n",
            "Iteration 30, loss = 1.36146607\n",
            "Iteration 31, loss = 1.36123924\n",
            "Iteration 32, loss = 1.36254701\n",
            "Iteration 33, loss = 1.36179008\n",
            "Iteration 34, loss = 1.36110787\n",
            "Iteration 35, loss = 1.35995733\n",
            "Iteration 36, loss = 1.36055158\n",
            "Iteration 37, loss = 1.35898952\n",
            "Iteration 38, loss = 1.36017141\n",
            "Iteration 39, loss = 1.35878794\n",
            "Iteration 40, loss = 1.35918360\n",
            "Iteration 41, loss = 1.35851855\n",
            "Iteration 42, loss = 1.35832615\n",
            "Iteration 43, loss = 1.35794988\n",
            "Iteration 44, loss = 1.35729033\n",
            "Iteration 45, loss = 1.35670465\n",
            "Iteration 46, loss = 1.35691901\n",
            "Iteration 47, loss = 1.35679712\n",
            "Iteration 48, loss = 1.35743962\n",
            "Iteration 49, loss = 1.35700769\n",
            "Iteration 50, loss = 1.35669407\n",
            "Iteration 51, loss = 1.35565408\n",
            "Iteration 52, loss = 1.35561961\n",
            "Iteration 53, loss = 1.35485433\n",
            "Iteration 54, loss = 1.35504859\n",
            "Iteration 55, loss = 1.35421326\n",
            "Iteration 56, loss = 1.35408335\n",
            "Iteration 57, loss = 1.35315139\n",
            "Iteration 58, loss = 1.35431607\n",
            "Iteration 59, loss = 1.35327052\n",
            "Iteration 60, loss = 1.35476785\n",
            "Iteration 61, loss = 1.35359706\n",
            "Iteration 62, loss = 1.35325651\n",
            "Iteration 63, loss = 1.35354924\n",
            "Iteration 64, loss = 1.35302625\n",
            "Iteration 65, loss = 1.35227269\n",
            "Iteration 66, loss = 1.35137173\n",
            "Iteration 67, loss = 1.35081104\n",
            "Iteration 68, loss = 1.35086489\n",
            "Iteration 69, loss = 1.35145795\n",
            "Iteration 70, loss = 1.35073739\n",
            "Iteration 71, loss = 1.34994851\n",
            "Iteration 72, loss = 1.34993410\n",
            "Iteration 73, loss = 1.34972828\n",
            "Iteration 74, loss = 1.34997698\n",
            "Iteration 75, loss = 1.35061860\n",
            "Iteration 76, loss = 1.34910109\n",
            "Iteration 77, loss = 1.34838452\n",
            "Iteration 78, loss = 1.34932441\n",
            "Iteration 79, loss = 1.34791951\n",
            "Iteration 80, loss = 1.34989099\n",
            "Iteration 81, loss = 1.35093298\n",
            "Iteration 82, loss = 1.34735265\n",
            "Iteration 83, loss = 1.34800115\n",
            "Iteration 84, loss = 1.34738154\n",
            "Iteration 85, loss = 1.34762620\n",
            "Iteration 86, loss = 1.34746132\n",
            "Iteration 87, loss = 1.34802397\n",
            "Iteration 88, loss = 1.34704292\n",
            "Iteration 89, loss = 1.34676512\n",
            "Iteration 90, loss = 1.34599489\n",
            "Iteration 91, loss = 1.34717120\n",
            "Iteration 92, loss = 1.34557411\n",
            "Iteration 93, loss = 1.34557466\n",
            "Iteration 94, loss = 1.34557297\n",
            "Iteration 95, loss = 1.34643594\n",
            "Iteration 96, loss = 1.34515382\n",
            "Iteration 97, loss = 1.34836200\n",
            "Iteration 98, loss = 1.34799018\n",
            "Iteration 99, loss = 1.34458335\n",
            "Iteration 100, loss = 1.34521423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38699400\n",
            "Iteration 2, loss = 1.36767527\n",
            "Iteration 3, loss = 1.36343951\n",
            "Iteration 4, loss = 1.36009906\n",
            "Iteration 5, loss = 1.35821339\n",
            "Iteration 6, loss = 1.35564080\n",
            "Iteration 7, loss = 1.35462802\n",
            "Iteration 8, loss = 1.35224319\n",
            "Iteration 9, loss = 1.35095207\n",
            "Iteration 10, loss = 1.35043368\n",
            "Iteration 11, loss = 1.34968571\n",
            "Iteration 12, loss = 1.34891772\n",
            "Iteration 13, loss = 1.34836834\n",
            "Iteration 14, loss = 1.34810681\n",
            "Iteration 15, loss = 1.34727473\n",
            "Iteration 16, loss = 1.34670231\n",
            "Iteration 17, loss = 1.34647663\n",
            "Iteration 18, loss = 1.34558359\n",
            "Iteration 19, loss = 1.34546595\n",
            "Iteration 20, loss = 1.34498410\n",
            "Iteration 21, loss = 1.34546531\n",
            "Iteration 22, loss = 1.34540369\n",
            "Iteration 23, loss = 1.34451763\n",
            "Iteration 24, loss = 1.34445531\n",
            "Iteration 25, loss = 1.34469243\n",
            "Iteration 26, loss = 1.34519754\n",
            "Iteration 27, loss = 1.34382991\n",
            "Iteration 28, loss = 1.34397953\n",
            "Iteration 29, loss = 1.34528471\n",
            "Iteration 30, loss = 1.34301704\n",
            "Iteration 31, loss = 1.34357162\n",
            "Iteration 32, loss = 1.34408458\n",
            "Iteration 33, loss = 1.34375662\n",
            "Iteration 34, loss = 1.34357800\n",
            "Iteration 35, loss = 1.34309821\n",
            "Iteration 36, loss = 1.34305297\n",
            "Iteration 37, loss = 1.34188042\n",
            "Iteration 38, loss = 1.34265260\n",
            "Iteration 39, loss = 1.34102315\n",
            "Iteration 40, loss = 1.34220901\n",
            "Iteration 41, loss = 1.34143406\n",
            "Iteration 42, loss = 1.34169215\n",
            "Iteration 43, loss = 1.34062584\n",
            "Iteration 44, loss = 1.34010770\n",
            "Iteration 45, loss = 1.34044583\n",
            "Iteration 46, loss = 1.34002114\n",
            "Iteration 47, loss = 1.33999618\n",
            "Iteration 48, loss = 1.34072835\n",
            "Iteration 49, loss = 1.34055029\n",
            "Iteration 50, loss = 1.34124426\n",
            "Iteration 51, loss = 1.33968266\n",
            "Iteration 52, loss = 1.33972857\n",
            "Iteration 53, loss = 1.33902755\n",
            "Iteration 54, loss = 1.33916046\n",
            "Iteration 55, loss = 1.33859322\n",
            "Iteration 56, loss = 1.33874914\n",
            "Iteration 57, loss = 1.33819778\n",
            "Iteration 58, loss = 1.33812272\n",
            "Iteration 59, loss = 1.33771229\n",
            "Iteration 60, loss = 1.33814760\n",
            "Iteration 61, loss = 1.33814372\n",
            "Iteration 62, loss = 1.33756390\n",
            "Iteration 63, loss = 1.33736510\n",
            "Iteration 64, loss = 1.33771559\n",
            "Iteration 65, loss = 1.33775254\n",
            "Iteration 66, loss = 1.33668160\n",
            "Iteration 67, loss = 1.33657779\n",
            "Iteration 68, loss = 1.33687885\n",
            "Iteration 69, loss = 1.33657336\n",
            "Iteration 70, loss = 1.33649666\n",
            "Iteration 71, loss = 1.33631984\n",
            "Iteration 72, loss = 1.33726205\n",
            "Iteration 73, loss = 1.33641022\n",
            "Iteration 74, loss = 1.33549554\n",
            "Iteration 75, loss = 1.33591248\n",
            "Iteration 76, loss = 1.33502264\n",
            "Iteration 77, loss = 1.33578000\n",
            "Iteration 78, loss = 1.33648150\n",
            "Iteration 79, loss = 1.33493344\n",
            "Iteration 80, loss = 1.33718731\n",
            "Iteration 81, loss = 1.33784138\n",
            "Iteration 82, loss = 1.33465412\n",
            "Iteration 83, loss = 1.33613006\n",
            "Iteration 84, loss = 1.33511374\n",
            "Iteration 85, loss = 1.33598291\n",
            "Iteration 86, loss = 1.33444009\n",
            "Iteration 87, loss = 1.33494906\n",
            "Iteration 88, loss = 1.33466014\n",
            "Iteration 89, loss = 1.33431550\n",
            "Iteration 90, loss = 1.33386894\n",
            "Iteration 91, loss = 1.33554708\n",
            "Iteration 92, loss = 1.33305816\n",
            "Iteration 93, loss = 1.33392499\n",
            "Iteration 94, loss = 1.33358238\n",
            "Iteration 95, loss = 1.33390159\n",
            "Iteration 96, loss = 1.33304071\n",
            "Iteration 97, loss = 1.33490581\n",
            "Iteration 98, loss = 1.33592426\n",
            "Iteration 99, loss = 1.33346471\n",
            "Iteration 100, loss = 1.33257597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38166018\n",
            "Iteration 2, loss = 1.37612120\n",
            "Iteration 3, loss = 1.37398446\n",
            "Iteration 4, loss = 1.37176388\n",
            "Iteration 5, loss = 1.37107102\n",
            "Iteration 6, loss = 1.36879328\n",
            "Iteration 7, loss = 1.36819197\n",
            "Iteration 8, loss = 1.36651139\n",
            "Iteration 9, loss = 1.36598964\n",
            "Iteration 10, loss = 1.36555077\n",
            "Iteration 11, loss = 1.36495403\n",
            "Iteration 12, loss = 1.36426731\n",
            "Iteration 13, loss = 1.36356556\n",
            "Iteration 14, loss = 1.36355500\n",
            "Iteration 15, loss = 1.36239244\n",
            "Iteration 16, loss = 1.36216618\n",
            "Iteration 17, loss = 1.36158003\n",
            "Iteration 18, loss = 1.36096846\n",
            "Iteration 19, loss = 1.36080335\n",
            "Iteration 20, loss = 1.35988252\n",
            "Iteration 21, loss = 1.36004118\n",
            "Iteration 22, loss = 1.36033987\n",
            "Iteration 23, loss = 1.35933256\n",
            "Iteration 24, loss = 1.35873150\n",
            "Iteration 25, loss = 1.36022167\n",
            "Iteration 26, loss = 1.36049893\n",
            "Iteration 27, loss = 1.35822664\n",
            "Iteration 28, loss = 1.35729095\n",
            "Iteration 29, loss = 1.36017826\n",
            "Iteration 30, loss = 1.35752588\n",
            "Iteration 31, loss = 1.35608582\n",
            "Iteration 32, loss = 1.35704100\n",
            "Iteration 33, loss = 1.35603861\n",
            "Iteration 34, loss = 1.35626379\n",
            "Iteration 35, loss = 1.35566715\n",
            "Iteration 36, loss = 1.35517453\n",
            "Iteration 37, loss = 1.35440984\n",
            "Iteration 38, loss = 1.35448604\n",
            "Iteration 39, loss = 1.35342391\n",
            "Iteration 40, loss = 1.35411387\n",
            "Iteration 41, loss = 1.35379716\n",
            "Iteration 42, loss = 1.35232167\n",
            "Iteration 43, loss = 1.35239954\n",
            "Iteration 44, loss = 1.35161961\n",
            "Iteration 45, loss = 1.35103745\n",
            "Iteration 46, loss = 1.35105630\n",
            "Iteration 47, loss = 1.35113207\n",
            "Iteration 48, loss = 1.35360722\n",
            "Iteration 49, loss = 1.35131782\n",
            "Iteration 50, loss = 1.35342821\n",
            "Iteration 51, loss = 1.35262060\n",
            "Iteration 52, loss = 1.34998379\n",
            "Iteration 53, loss = 1.35034543\n",
            "Iteration 54, loss = 1.35005458\n",
            "Iteration 55, loss = 1.34871768\n",
            "Iteration 56, loss = 1.34847114\n",
            "Iteration 57, loss = 1.34852275\n",
            "Iteration 58, loss = 1.34827183\n",
            "Iteration 59, loss = 1.34856917\n",
            "Iteration 60, loss = 1.34871905\n",
            "Iteration 61, loss = 1.34796656\n",
            "Iteration 62, loss = 1.34705329\n",
            "Iteration 63, loss = 1.34698372\n",
            "Iteration 64, loss = 1.34679598\n",
            "Iteration 65, loss = 1.34668907\n",
            "Iteration 66, loss = 1.34630956\n",
            "Iteration 67, loss = 1.34635292\n",
            "Iteration 68, loss = 1.34616772\n",
            "Iteration 69, loss = 1.34648026\n",
            "Iteration 70, loss = 1.34576851\n",
            "Iteration 71, loss = 1.34634989\n",
            "Iteration 72, loss = 1.34798245\n",
            "Iteration 73, loss = 1.34671204\n",
            "Iteration 74, loss = 1.34485387\n",
            "Iteration 75, loss = 1.34562022\n",
            "Iteration 76, loss = 1.34554240\n",
            "Iteration 77, loss = 1.34526960\n",
            "Iteration 78, loss = 1.34558308\n",
            "Iteration 79, loss = 1.34490741\n",
            "Iteration 80, loss = 1.34626859\n",
            "Iteration 81, loss = 1.34736499\n",
            "Iteration 82, loss = 1.34464429\n",
            "Iteration 83, loss = 1.34434488\n",
            "Iteration 84, loss = 1.34473786\n",
            "Iteration 85, loss = 1.34528106\n",
            "Iteration 86, loss = 1.34543476\n",
            "Iteration 87, loss = 1.34422472\n",
            "Iteration 88, loss = 1.34381875\n",
            "Iteration 89, loss = 1.34411948\n",
            "Iteration 90, loss = 1.34349165\n",
            "Iteration 91, loss = 1.34398997\n",
            "Iteration 92, loss = 1.34319468\n",
            "Iteration 93, loss = 1.34329406\n",
            "Iteration 94, loss = 1.34306979\n",
            "Iteration 95, loss = 1.34338383\n",
            "Iteration 96, loss = 1.34298216\n",
            "Iteration 97, loss = 1.34390399\n",
            "Iteration 98, loss = 1.34482929\n",
            "Iteration 99, loss = 1.34500446\n",
            "Iteration 100, loss = 1.34404026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37777167\n",
            "Iteration 2, loss = 1.37294440\n",
            "Iteration 3, loss = 1.37159967\n",
            "Iteration 4, loss = 1.36950899\n",
            "Iteration 5, loss = 1.36811928\n",
            "Iteration 6, loss = 1.36636008\n",
            "Iteration 7, loss = 1.36554143\n",
            "Iteration 8, loss = 1.36470257\n",
            "Iteration 9, loss = 1.36391283\n",
            "Iteration 10, loss = 1.36354616\n",
            "Iteration 11, loss = 1.36319177\n",
            "Iteration 12, loss = 1.36283606\n",
            "Iteration 13, loss = 1.36207391\n",
            "Iteration 14, loss = 1.36132067\n",
            "Iteration 15, loss = 1.36062414\n",
            "Iteration 16, loss = 1.36067226\n",
            "Iteration 17, loss = 1.36017620\n",
            "Iteration 18, loss = 1.35969477\n",
            "Iteration 19, loss = 1.36009355\n",
            "Iteration 20, loss = 1.35932200\n",
            "Iteration 21, loss = 1.35970588\n",
            "Iteration 22, loss = 1.35857527\n",
            "Iteration 23, loss = 1.35796535\n",
            "Iteration 24, loss = 1.35785009\n",
            "Iteration 25, loss = 1.35894117\n",
            "Iteration 26, loss = 1.35942632\n",
            "Iteration 27, loss = 1.35745635\n",
            "Iteration 28, loss = 1.35679746\n",
            "Iteration 29, loss = 1.35853032\n",
            "Iteration 30, loss = 1.35654057\n",
            "Iteration 31, loss = 1.35488959\n",
            "Iteration 32, loss = 1.35601189\n",
            "Iteration 33, loss = 1.35476531\n",
            "Iteration 34, loss = 1.35491363\n",
            "Iteration 35, loss = 1.35442758\n",
            "Iteration 36, loss = 1.35532552\n",
            "Iteration 37, loss = 1.35251839\n",
            "Iteration 38, loss = 1.35340252\n",
            "Iteration 39, loss = 1.35278348\n",
            "Iteration 40, loss = 1.35320042\n",
            "Iteration 41, loss = 1.35216247\n",
            "Iteration 42, loss = 1.35064209\n",
            "Iteration 43, loss = 1.35121313\n",
            "Iteration 44, loss = 1.34996121\n",
            "Iteration 45, loss = 1.35083302\n",
            "Iteration 46, loss = 1.35018327\n",
            "Iteration 47, loss = 1.35030727\n",
            "Iteration 48, loss = 1.34980612\n",
            "Iteration 49, loss = 1.34879066\n",
            "Iteration 50, loss = 1.34919867\n",
            "Iteration 51, loss = 1.34921107\n",
            "Iteration 52, loss = 1.34715987\n",
            "Iteration 53, loss = 1.34786409\n",
            "Iteration 54, loss = 1.34849017\n",
            "Iteration 55, loss = 1.34746192\n",
            "Iteration 56, loss = 1.34608864\n",
            "Iteration 57, loss = 1.34665973\n",
            "Iteration 58, loss = 1.34525064\n",
            "Iteration 59, loss = 1.34617907\n",
            "Iteration 60, loss = 1.34593214\n",
            "Iteration 61, loss = 1.34599092\n",
            "Iteration 62, loss = 1.34447138\n",
            "Iteration 63, loss = 1.34379210\n",
            "Iteration 64, loss = 1.34306894\n",
            "Iteration 65, loss = 1.34403983\n",
            "Iteration 66, loss = 1.34345429\n",
            "Iteration 67, loss = 1.34395128\n",
            "Iteration 68, loss = 1.34380870\n",
            "Iteration 69, loss = 1.34385187\n",
            "Iteration 70, loss = 1.34330075\n",
            "Iteration 71, loss = 1.34346022\n",
            "Iteration 72, loss = 1.34443343\n",
            "Iteration 73, loss = 1.34302981\n",
            "Iteration 74, loss = 1.34243995\n",
            "Iteration 75, loss = 1.34300529\n",
            "Iteration 76, loss = 1.34284552\n",
            "Iteration 77, loss = 1.34232717\n",
            "Iteration 78, loss = 1.34254796\n",
            "Iteration 79, loss = 1.34245233\n",
            "Iteration 80, loss = 1.34398377\n",
            "Iteration 81, loss = 1.34304364\n",
            "Iteration 82, loss = 1.34313142\n",
            "Iteration 83, loss = 1.34136820\n",
            "Iteration 84, loss = 1.34207902\n",
            "Iteration 85, loss = 1.34343568\n",
            "Iteration 86, loss = 1.34222073\n",
            "Iteration 87, loss = 1.34212731\n",
            "Iteration 88, loss = 1.34119097\n",
            "Iteration 89, loss = 1.34105918\n",
            "Iteration 90, loss = 1.33976500\n",
            "Iteration 91, loss = 1.34100446\n",
            "Iteration 92, loss = 1.34023395\n",
            "Iteration 93, loss = 1.33986977\n",
            "Iteration 94, loss = 1.34093669\n",
            "Iteration 95, loss = 1.34149017\n",
            "Iteration 96, loss = 1.34169230\n",
            "Iteration 97, loss = 1.34273966\n",
            "Iteration 98, loss = 1.34329014\n",
            "Iteration 99, loss = 1.34332730\n",
            "Iteration 100, loss = 1.34302188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37712093\n",
            "Iteration 2, loss = 1.36417490\n",
            "Iteration 3, loss = 1.36046487\n",
            "Iteration 4, loss = 1.35908032\n",
            "Iteration 5, loss = 1.35740253\n",
            "Iteration 6, loss = 1.35628510\n",
            "Iteration 7, loss = 1.35556851\n",
            "Iteration 8, loss = 1.35437790\n",
            "Iteration 9, loss = 1.35375010\n",
            "Iteration 10, loss = 1.35310648\n",
            "Iteration 11, loss = 1.35237838\n",
            "Iteration 12, loss = 1.35209619\n",
            "Iteration 13, loss = 1.35154630\n",
            "Iteration 14, loss = 1.35167490\n",
            "Iteration 15, loss = 1.35008789\n",
            "Iteration 16, loss = 1.34971963\n",
            "Iteration 17, loss = 1.34917740\n",
            "Iteration 18, loss = 1.34876870\n",
            "Iteration 19, loss = 1.34854208\n",
            "Iteration 20, loss = 1.34779740\n",
            "Iteration 21, loss = 1.34772006\n",
            "Iteration 22, loss = 1.34729984\n",
            "Iteration 23, loss = 1.34701796\n",
            "Iteration 24, loss = 1.34629635\n",
            "Iteration 25, loss = 1.34795552\n",
            "Iteration 26, loss = 1.34857545\n",
            "Iteration 27, loss = 1.34521519\n",
            "Iteration 28, loss = 1.34457968\n",
            "Iteration 29, loss = 1.34569548\n",
            "Iteration 30, loss = 1.34448654\n",
            "Iteration 31, loss = 1.34342095\n",
            "Iteration 32, loss = 1.34389639\n",
            "Iteration 33, loss = 1.34226256\n",
            "Iteration 34, loss = 1.34258810\n",
            "Iteration 35, loss = 1.34176681\n",
            "Iteration 36, loss = 1.34170395\n",
            "Iteration 37, loss = 1.33993765\n",
            "Iteration 38, loss = 1.33968613\n",
            "Iteration 39, loss = 1.33956715\n",
            "Iteration 40, loss = 1.33942945\n",
            "Iteration 41, loss = 1.33906967\n",
            "Iteration 42, loss = 1.33753560\n",
            "Iteration 43, loss = 1.33816721\n",
            "Iteration 44, loss = 1.33714791\n",
            "Iteration 45, loss = 1.33735220\n",
            "Iteration 46, loss = 1.33787071\n",
            "Iteration 47, loss = 1.33844737\n",
            "Iteration 48, loss = 1.33712975\n",
            "Iteration 49, loss = 1.33599630\n",
            "Iteration 50, loss = 1.33657164\n",
            "Iteration 51, loss = 1.33747940\n",
            "Iteration 52, loss = 1.33432319\n",
            "Iteration 53, loss = 1.33453700\n",
            "Iteration 54, loss = 1.33425949\n",
            "Iteration 55, loss = 1.33559873\n",
            "Iteration 56, loss = 1.33360558\n",
            "Iteration 57, loss = 1.33429331\n",
            "Iteration 58, loss = 1.33311329\n",
            "Iteration 59, loss = 1.33349631\n",
            "Iteration 60, loss = 1.33378273\n",
            "Iteration 61, loss = 1.33396736\n",
            "Iteration 62, loss = 1.33253342\n",
            "Iteration 63, loss = 1.33205540\n",
            "Iteration 64, loss = 1.33169009\n",
            "Iteration 65, loss = 1.33227348\n",
            "Iteration 66, loss = 1.33173425\n",
            "Iteration 67, loss = 1.33183563\n",
            "Iteration 68, loss = 1.33208146\n",
            "Iteration 69, loss = 1.33163992\n",
            "Iteration 70, loss = 1.33141012\n",
            "Iteration 71, loss = 1.33264254\n",
            "Iteration 72, loss = 1.33219801\n",
            "Iteration 73, loss = 1.33122303\n",
            "Iteration 74, loss = 1.33076180\n",
            "Iteration 75, loss = 1.33023080\n",
            "Iteration 76, loss = 1.33166112\n",
            "Iteration 77, loss = 1.33018567\n",
            "Iteration 78, loss = 1.33140898\n",
            "Iteration 79, loss = 1.33146692\n",
            "Iteration 80, loss = 1.33165528\n",
            "Iteration 81, loss = 1.33269486\n",
            "Iteration 82, loss = 1.33182268\n",
            "Iteration 83, loss = 1.33004692\n",
            "Iteration 84, loss = 1.33099597\n",
            "Iteration 85, loss = 1.33091518\n",
            "Iteration 86, loss = 1.33023022\n",
            "Iteration 87, loss = 1.32986538\n",
            "Iteration 88, loss = 1.32922605\n",
            "Iteration 89, loss = 1.32917234\n",
            "Iteration 90, loss = 1.32852243\n",
            "Iteration 91, loss = 1.32926642\n",
            "Iteration 92, loss = 1.32865816\n",
            "Iteration 93, loss = 1.32871009\n",
            "Iteration 94, loss = 1.32898788\n",
            "Iteration 95, loss = 1.32994681\n",
            "Iteration 96, loss = 1.33016826\n",
            "Iteration 97, loss = 1.33013164\n",
            "Iteration 98, loss = 1.33074650\n",
            "Iteration 99, loss = 1.33078461\n",
            "Iteration 100, loss = 1.33255374\n",
            "48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40962603\n",
            "Iteration 2, loss = 1.37456050\n",
            "Iteration 3, loss = 1.36005785\n",
            "Iteration 4, loss = 1.35072763\n",
            "Iteration 5, loss = 1.33723395\n",
            "Iteration 6, loss = 1.32283805\n",
            "Iteration 7, loss = 1.30520213\n",
            "Iteration 8, loss = 1.28486532\n",
            "Iteration 9, loss = 1.26083248\n",
            "Iteration 10, loss = 1.23636603\n",
            "Iteration 11, loss = 1.22343710\n",
            "Iteration 12, loss = 1.20642024\n",
            "Iteration 13, loss = 1.18731021\n",
            "Iteration 14, loss = 1.17427399\n",
            "Iteration 15, loss = 1.15736074\n",
            "Iteration 16, loss = 1.14807998\n",
            "Iteration 17, loss = 1.14607002\n",
            "Iteration 18, loss = 1.13094034\n",
            "Iteration 19, loss = 1.12460205\n",
            "Iteration 20, loss = 1.12219634\n",
            "Iteration 21, loss = 1.12147581\n",
            "Iteration 22, loss = 1.11463496\n",
            "Iteration 23, loss = 1.11240815\n",
            "Iteration 24, loss = 1.11039759\n",
            "Iteration 25, loss = 1.10923459\n",
            "Iteration 26, loss = 1.10973462\n",
            "Iteration 27, loss = 1.10401413\n",
            "Iteration 28, loss = 1.10834952\n",
            "Iteration 29, loss = 1.10293239\n",
            "Iteration 30, loss = 1.10368039\n",
            "Iteration 31, loss = 1.09942443\n",
            "Iteration 32, loss = 1.09483813\n",
            "Iteration 33, loss = 1.09535546\n",
            "Iteration 34, loss = 1.09347091\n",
            "Iteration 35, loss = 1.09461179\n",
            "Iteration 36, loss = 1.09797771\n",
            "Iteration 37, loss = 1.09249628\n",
            "Iteration 38, loss = 1.08677499\n",
            "Iteration 39, loss = 1.08831911\n",
            "Iteration 40, loss = 1.08342650\n",
            "Iteration 41, loss = 1.08573227\n",
            "Iteration 42, loss = 1.08231151\n",
            "Iteration 43, loss = 1.08147876\n",
            "Iteration 44, loss = 1.07916869\n",
            "Iteration 45, loss = 1.07918265\n",
            "Iteration 46, loss = 1.07563306\n",
            "Iteration 47, loss = 1.07769600\n",
            "Iteration 48, loss = 1.07417422\n",
            "Iteration 49, loss = 1.07481744\n",
            "Iteration 50, loss = 1.07249547\n",
            "Iteration 51, loss = 1.07206940\n",
            "Iteration 52, loss = 1.07461623\n",
            "Iteration 53, loss = 1.07126788\n",
            "Iteration 54, loss = 1.07417041\n",
            "Iteration 55, loss = 1.07009032\n",
            "Iteration 56, loss = 1.06755426\n",
            "Iteration 57, loss = 1.06429819\n",
            "Iteration 58, loss = 1.06174820\n",
            "Iteration 59, loss = 1.06167725\n",
            "Iteration 60, loss = 1.06084775\n",
            "Iteration 61, loss = 1.07034590\n",
            "Iteration 62, loss = 1.06942998\n",
            "Iteration 63, loss = 1.05842552\n",
            "Iteration 64, loss = 1.06076925\n",
            "Iteration 65, loss = 1.05740341\n",
            "Iteration 66, loss = 1.05778772\n",
            "Iteration 67, loss = 1.05690882\n",
            "Iteration 68, loss = 1.05365965\n",
            "Iteration 69, loss = 1.05132097\n",
            "Iteration 70, loss = 1.05256226\n",
            "Iteration 71, loss = 1.05232562\n",
            "Iteration 72, loss = 1.05558908\n",
            "Iteration 73, loss = 1.04749711\n",
            "Iteration 74, loss = 1.05054383\n",
            "Iteration 75, loss = 1.05879635\n",
            "Iteration 76, loss = 1.05544451\n",
            "Iteration 77, loss = 1.05004816\n",
            "Iteration 78, loss = 1.04719459\n",
            "Iteration 79, loss = 1.04581602\n",
            "Iteration 80, loss = 1.04337829\n",
            "Iteration 81, loss = 1.03892149\n",
            "Iteration 82, loss = 1.04120874\n",
            "Iteration 83, loss = 1.04063309\n",
            "Iteration 84, loss = 1.04272888\n",
            "Iteration 85, loss = 1.04280686\n",
            "Iteration 86, loss = 1.04332498\n",
            "Iteration 87, loss = 1.03636228\n",
            "Iteration 88, loss = 1.03702815\n",
            "Iteration 89, loss = 1.03709702\n",
            "Iteration 90, loss = 1.03569308\n",
            "Iteration 91, loss = 1.03096187\n",
            "Iteration 92, loss = 1.03410883\n",
            "Iteration 93, loss = 1.03080746\n",
            "Iteration 94, loss = 1.03096243\n",
            "Iteration 95, loss = 1.03221308\n",
            "Iteration 96, loss = 1.02760861\n",
            "Iteration 97, loss = 1.02755349\n",
            "Iteration 98, loss = 1.02653910\n",
            "Iteration 99, loss = 1.02318728\n",
            "Iteration 100, loss = 1.02336520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41500321\n",
            "Iteration 2, loss = 1.37590093\n",
            "Iteration 3, loss = 1.36066643\n",
            "Iteration 4, loss = 1.35037407\n",
            "Iteration 5, loss = 1.33587990\n",
            "Iteration 6, loss = 1.32213273\n",
            "Iteration 7, loss = 1.30248929\n",
            "Iteration 8, loss = 1.28128325\n",
            "Iteration 9, loss = 1.25713987\n",
            "Iteration 10, loss = 1.23374395\n",
            "Iteration 11, loss = 1.22104894\n",
            "Iteration 12, loss = 1.20323832\n",
            "Iteration 13, loss = 1.18192133\n",
            "Iteration 14, loss = 1.17100149\n",
            "Iteration 15, loss = 1.15421345\n",
            "Iteration 16, loss = 1.14418374\n",
            "Iteration 17, loss = 1.14734531\n",
            "Iteration 18, loss = 1.13308394\n",
            "Iteration 19, loss = 1.12247278\n",
            "Iteration 20, loss = 1.12080677\n",
            "Iteration 21, loss = 1.11759549\n",
            "Iteration 22, loss = 1.11022779\n",
            "Iteration 23, loss = 1.10984933\n",
            "Iteration 24, loss = 1.10965902\n",
            "Iteration 25, loss = 1.10908520\n",
            "Iteration 26, loss = 1.11452315\n",
            "Iteration 27, loss = 1.10685878\n",
            "Iteration 28, loss = 1.10690639\n",
            "Iteration 29, loss = 1.10252559\n",
            "Iteration 30, loss = 1.10411072\n",
            "Iteration 31, loss = 1.09764872\n",
            "Iteration 32, loss = 1.09264992\n",
            "Iteration 33, loss = 1.09231840\n",
            "Iteration 34, loss = 1.09331636\n",
            "Iteration 35, loss = 1.09246798\n",
            "Iteration 36, loss = 1.09033702\n",
            "Iteration 37, loss = 1.08681747\n",
            "Iteration 38, loss = 1.08721912\n",
            "Iteration 39, loss = 1.08523839\n",
            "Iteration 40, loss = 1.08601760\n",
            "Iteration 41, loss = 1.08473760\n",
            "Iteration 42, loss = 1.08293213\n",
            "Iteration 43, loss = 1.07964530\n",
            "Iteration 44, loss = 1.07464429\n",
            "Iteration 45, loss = 1.07338237\n",
            "Iteration 46, loss = 1.07235361\n",
            "Iteration 47, loss = 1.07448252\n",
            "Iteration 48, loss = 1.07256671\n",
            "Iteration 49, loss = 1.07253772\n",
            "Iteration 50, loss = 1.06922115\n",
            "Iteration 51, loss = 1.06957402\n",
            "Iteration 52, loss = 1.07015541\n",
            "Iteration 53, loss = 1.06700901\n",
            "Iteration 54, loss = 1.06705078\n",
            "Iteration 55, loss = 1.06461633\n",
            "Iteration 56, loss = 1.06263998\n",
            "Iteration 57, loss = 1.06090734\n",
            "Iteration 58, loss = 1.05852827\n",
            "Iteration 59, loss = 1.05812369\n",
            "Iteration 60, loss = 1.06021972\n",
            "Iteration 61, loss = 1.06225880\n",
            "Iteration 62, loss = 1.06130961\n",
            "Iteration 63, loss = 1.06146505\n",
            "Iteration 64, loss = 1.05998450\n",
            "Iteration 65, loss = 1.05656839\n",
            "Iteration 66, loss = 1.05318991\n",
            "Iteration 67, loss = 1.05265632\n",
            "Iteration 68, loss = 1.05363877\n",
            "Iteration 69, loss = 1.04723866\n",
            "Iteration 70, loss = 1.04698159\n",
            "Iteration 71, loss = 1.04504289\n",
            "Iteration 72, loss = 1.04910687\n",
            "Iteration 73, loss = 1.04693385\n",
            "Iteration 74, loss = 1.05178610\n",
            "Iteration 75, loss = 1.05814731\n",
            "Iteration 76, loss = 1.05172075\n",
            "Iteration 77, loss = 1.04767703\n",
            "Iteration 78, loss = 1.04135161\n",
            "Iteration 79, loss = 1.04077975\n",
            "Iteration 80, loss = 1.03935803\n",
            "Iteration 81, loss = 1.03417248\n",
            "Iteration 82, loss = 1.03577051\n",
            "Iteration 83, loss = 1.03520024\n",
            "Iteration 84, loss = 1.03422025\n",
            "Iteration 85, loss = 1.04116662\n",
            "Iteration 86, loss = 1.04426822\n",
            "Iteration 87, loss = 1.03422870\n",
            "Iteration 88, loss = 1.03392120\n",
            "Iteration 89, loss = 1.02815709\n",
            "Iteration 90, loss = 1.02884648\n",
            "Iteration 91, loss = 1.02917739\n",
            "Iteration 92, loss = 1.02897018\n",
            "Iteration 93, loss = 1.02565730\n",
            "Iteration 94, loss = 1.02555917\n",
            "Iteration 95, loss = 1.03042953\n",
            "Iteration 96, loss = 1.02542344\n",
            "Iteration 97, loss = 1.02346489\n",
            "Iteration 98, loss = 1.02142572\n",
            "Iteration 99, loss = 1.01991346\n",
            "Iteration 100, loss = 1.02299305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41585733\n",
            "Iteration 2, loss = 1.37615250\n",
            "Iteration 3, loss = 1.36235542\n",
            "Iteration 4, loss = 1.35276985\n",
            "Iteration 5, loss = 1.33839936\n",
            "Iteration 6, loss = 1.32229148\n",
            "Iteration 7, loss = 1.29937034\n",
            "Iteration 8, loss = 1.27680703\n",
            "Iteration 9, loss = 1.24930490\n",
            "Iteration 10, loss = 1.22226026\n",
            "Iteration 11, loss = 1.20360194\n",
            "Iteration 12, loss = 1.18136126\n",
            "Iteration 13, loss = 1.16268217\n",
            "Iteration 14, loss = 1.14934920\n",
            "Iteration 15, loss = 1.13885942\n",
            "Iteration 16, loss = 1.12705813\n",
            "Iteration 17, loss = 1.12640720\n",
            "Iteration 18, loss = 1.11863791\n",
            "Iteration 19, loss = 1.11303910\n",
            "Iteration 20, loss = 1.11428000\n",
            "Iteration 21, loss = 1.11199345\n",
            "Iteration 22, loss = 1.10332628\n",
            "Iteration 23, loss = 1.10205275\n",
            "Iteration 24, loss = 1.10077640\n",
            "Iteration 25, loss = 1.10365374\n",
            "Iteration 26, loss = 1.10820537\n",
            "Iteration 27, loss = 1.09603084\n",
            "Iteration 28, loss = 1.09530703\n",
            "Iteration 29, loss = 1.09233270\n",
            "Iteration 30, loss = 1.09629056\n",
            "Iteration 31, loss = 1.09222110\n",
            "Iteration 32, loss = 1.08821810\n",
            "Iteration 33, loss = 1.08773603\n",
            "Iteration 34, loss = 1.08793992\n",
            "Iteration 35, loss = 1.08673528\n",
            "Iteration 36, loss = 1.08521252\n",
            "Iteration 37, loss = 1.08092026\n",
            "Iteration 38, loss = 1.08585382\n",
            "Iteration 39, loss = 1.08174834\n",
            "Iteration 40, loss = 1.08074026\n",
            "Iteration 41, loss = 1.07571809\n",
            "Iteration 42, loss = 1.07778329\n",
            "Iteration 43, loss = 1.07303936\n",
            "Iteration 44, loss = 1.07139592\n",
            "Iteration 45, loss = 1.06882134\n",
            "Iteration 46, loss = 1.06749901\n",
            "Iteration 47, loss = 1.06698661\n",
            "Iteration 48, loss = 1.06737411\n",
            "Iteration 49, loss = 1.07035649\n",
            "Iteration 50, loss = 1.06831351\n",
            "Iteration 51, loss = 1.06698952\n",
            "Iteration 52, loss = 1.06377767\n",
            "Iteration 53, loss = 1.05935150\n",
            "Iteration 54, loss = 1.06100764\n",
            "Iteration 55, loss = 1.06142147\n",
            "Iteration 56, loss = 1.05992255\n",
            "Iteration 57, loss = 1.05573499\n",
            "Iteration 58, loss = 1.05338813\n",
            "Iteration 59, loss = 1.05360794\n",
            "Iteration 60, loss = 1.05434576\n",
            "Iteration 61, loss = 1.05414623\n",
            "Iteration 62, loss = 1.05547574\n",
            "Iteration 63, loss = 1.05523851\n",
            "Iteration 64, loss = 1.05718390\n",
            "Iteration 65, loss = 1.05494562\n",
            "Iteration 66, loss = 1.05362384\n",
            "Iteration 67, loss = 1.05005813\n",
            "Iteration 68, loss = 1.05026749\n",
            "Iteration 69, loss = 1.04306439\n",
            "Iteration 70, loss = 1.04184812\n",
            "Iteration 71, loss = 1.03984844\n",
            "Iteration 72, loss = 1.03973641\n",
            "Iteration 73, loss = 1.03958379\n",
            "Iteration 74, loss = 1.04430345\n",
            "Iteration 75, loss = 1.05068341\n",
            "Iteration 76, loss = 1.04002442\n",
            "Iteration 77, loss = 1.04014124\n",
            "Iteration 78, loss = 1.03107014\n",
            "Iteration 79, loss = 1.03236134\n",
            "Iteration 80, loss = 1.03152855\n",
            "Iteration 81, loss = 1.02701471\n",
            "Iteration 82, loss = 1.02732950\n",
            "Iteration 83, loss = 1.02511039\n",
            "Iteration 84, loss = 1.02498827\n",
            "Iteration 85, loss = 1.03363632\n",
            "Iteration 86, loss = 1.03934624\n",
            "Iteration 87, loss = 1.02834489\n",
            "Iteration 88, loss = 1.02665259\n",
            "Iteration 89, loss = 1.01967389\n",
            "Iteration 90, loss = 1.01927604\n",
            "Iteration 91, loss = 1.01946513\n",
            "Iteration 92, loss = 1.02067910\n",
            "Iteration 93, loss = 1.02293236\n",
            "Iteration 94, loss = 1.01979843\n",
            "Iteration 95, loss = 1.02158198\n",
            "Iteration 96, loss = 1.01569847\n",
            "Iteration 97, loss = 1.01375069\n",
            "Iteration 98, loss = 1.01255476\n",
            "Iteration 99, loss = 1.01185386\n",
            "Iteration 100, loss = 1.01406280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41716337\n",
            "Iteration 2, loss = 1.37609933\n",
            "Iteration 3, loss = 1.36296829\n",
            "Iteration 4, loss = 1.35157295\n",
            "Iteration 5, loss = 1.33466065\n",
            "Iteration 6, loss = 1.31685658\n",
            "Iteration 7, loss = 1.29133742\n",
            "Iteration 8, loss = 1.26706543\n",
            "Iteration 9, loss = 1.23927980\n",
            "Iteration 10, loss = 1.21374565\n",
            "Iteration 11, loss = 1.19727275\n",
            "Iteration 12, loss = 1.17430189\n",
            "Iteration 13, loss = 1.15836377\n",
            "Iteration 14, loss = 1.14191853\n",
            "Iteration 15, loss = 1.12921011\n",
            "Iteration 16, loss = 1.11812843\n",
            "Iteration 17, loss = 1.11883903\n",
            "Iteration 18, loss = 1.10904357\n",
            "Iteration 19, loss = 1.10425379\n",
            "Iteration 20, loss = 1.10130173\n",
            "Iteration 21, loss = 1.10244330\n",
            "Iteration 22, loss = 1.09474333\n",
            "Iteration 23, loss = 1.09361714\n",
            "Iteration 24, loss = 1.09368621\n",
            "Iteration 25, loss = 1.09884489\n",
            "Iteration 26, loss = 1.10283367\n",
            "Iteration 27, loss = 1.08921043\n",
            "Iteration 28, loss = 1.08984521\n",
            "Iteration 29, loss = 1.08448691\n",
            "Iteration 30, loss = 1.09070733\n",
            "Iteration 31, loss = 1.08463027\n",
            "Iteration 32, loss = 1.08101292\n",
            "Iteration 33, loss = 1.08178213\n",
            "Iteration 34, loss = 1.07711964\n",
            "Iteration 35, loss = 1.07864578\n",
            "Iteration 36, loss = 1.07913295\n",
            "Iteration 37, loss = 1.07376293\n",
            "Iteration 38, loss = 1.07655945\n",
            "Iteration 39, loss = 1.07172048\n",
            "Iteration 40, loss = 1.06904821\n",
            "Iteration 41, loss = 1.06557144\n",
            "Iteration 42, loss = 1.06634250\n",
            "Iteration 43, loss = 1.06410089\n",
            "Iteration 44, loss = 1.06348588\n",
            "Iteration 45, loss = 1.06490866\n",
            "Iteration 46, loss = 1.05842415\n",
            "Iteration 47, loss = 1.05741083\n",
            "Iteration 48, loss = 1.05650987\n",
            "Iteration 49, loss = 1.05727210\n",
            "Iteration 50, loss = 1.05689808\n",
            "Iteration 51, loss = 1.05720492\n",
            "Iteration 52, loss = 1.05534837\n",
            "Iteration 53, loss = 1.05179494\n",
            "Iteration 54, loss = 1.05121179\n",
            "Iteration 55, loss = 1.04922916\n",
            "Iteration 56, loss = 1.05212249\n",
            "Iteration 57, loss = 1.04523860\n",
            "Iteration 58, loss = 1.04095095\n",
            "Iteration 59, loss = 1.04199281\n",
            "Iteration 60, loss = 1.04193164\n",
            "Iteration 61, loss = 1.04563612\n",
            "Iteration 62, loss = 1.04321912\n",
            "Iteration 63, loss = 1.03857245\n",
            "Iteration 64, loss = 1.04220425\n",
            "Iteration 65, loss = 1.03912486\n",
            "Iteration 66, loss = 1.03913580\n",
            "Iteration 67, loss = 1.03620630\n",
            "Iteration 68, loss = 1.03597840\n",
            "Iteration 69, loss = 1.03218331\n",
            "Iteration 70, loss = 1.02698989\n",
            "Iteration 71, loss = 1.02657672\n",
            "Iteration 72, loss = 1.02585749\n",
            "Iteration 73, loss = 1.02491505\n",
            "Iteration 74, loss = 1.03167873\n",
            "Iteration 75, loss = 1.03493741\n",
            "Iteration 76, loss = 1.02636295\n",
            "Iteration 77, loss = 1.02484297\n",
            "Iteration 78, loss = 1.01831163\n",
            "Iteration 79, loss = 1.01674962\n",
            "Iteration 80, loss = 1.01743285\n",
            "Iteration 81, loss = 1.01430043\n",
            "Iteration 82, loss = 1.01600268\n",
            "Iteration 83, loss = 1.01141160\n",
            "Iteration 84, loss = 1.01267162\n",
            "Iteration 85, loss = 1.01658908\n",
            "Iteration 86, loss = 1.01750599\n",
            "Iteration 87, loss = 1.01436048\n",
            "Iteration 88, loss = 1.01534698\n",
            "Iteration 89, loss = 1.00749637\n",
            "Iteration 90, loss = 1.00872293\n",
            "Iteration 91, loss = 1.01070135\n",
            "Iteration 92, loss = 1.01384899\n",
            "Iteration 93, loss = 1.00817623\n",
            "Iteration 94, loss = 1.00757205\n",
            "Iteration 95, loss = 1.00634216\n",
            "Iteration 96, loss = 1.00512009\n",
            "Iteration 97, loss = 1.00187062\n",
            "Iteration 98, loss = 1.00208363\n",
            "Iteration 99, loss = 1.00227783\n",
            "Iteration 100, loss = 0.99749056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41525548\n",
            "Iteration 2, loss = 1.37742129\n",
            "Iteration 3, loss = 1.36294222\n",
            "Iteration 4, loss = 1.35161940\n",
            "Iteration 5, loss = 1.33752373\n",
            "Iteration 6, loss = 1.31954526\n",
            "Iteration 7, loss = 1.29701082\n",
            "Iteration 8, loss = 1.27284177\n",
            "Iteration 9, loss = 1.24629992\n",
            "Iteration 10, loss = 1.22005451\n",
            "Iteration 11, loss = 1.20427477\n",
            "Iteration 12, loss = 1.17915384\n",
            "Iteration 13, loss = 1.16948885\n",
            "Iteration 14, loss = 1.14516242\n",
            "Iteration 15, loss = 1.13428815\n",
            "Iteration 16, loss = 1.12641256\n",
            "Iteration 17, loss = 1.12307427\n",
            "Iteration 18, loss = 1.11571934\n",
            "Iteration 19, loss = 1.10915063\n",
            "Iteration 20, loss = 1.10823509\n",
            "Iteration 21, loss = 1.10540047\n",
            "Iteration 22, loss = 1.10193796\n",
            "Iteration 23, loss = 1.10019123\n",
            "Iteration 24, loss = 1.09920724\n",
            "Iteration 25, loss = 1.10208784\n",
            "Iteration 26, loss = 1.10351621\n",
            "Iteration 27, loss = 1.10097657\n",
            "Iteration 28, loss = 1.09408411\n",
            "Iteration 29, loss = 1.08875699\n",
            "Iteration 30, loss = 1.09110995\n",
            "Iteration 31, loss = 1.08753942\n",
            "Iteration 32, loss = 1.08837432\n",
            "Iteration 33, loss = 1.09162174\n",
            "Iteration 34, loss = 1.08861474\n",
            "Iteration 35, loss = 1.08835986\n",
            "Iteration 36, loss = 1.08196844\n",
            "Iteration 37, loss = 1.08368648\n",
            "Iteration 38, loss = 1.08729669\n",
            "Iteration 39, loss = 1.07757652\n",
            "Iteration 40, loss = 1.07275178\n",
            "Iteration 41, loss = 1.07232658\n",
            "Iteration 42, loss = 1.07009708\n",
            "Iteration 43, loss = 1.07237290\n",
            "Iteration 44, loss = 1.07519673\n",
            "Iteration 45, loss = 1.07529801\n",
            "Iteration 46, loss = 1.06761227\n",
            "Iteration 47, loss = 1.06905533\n",
            "Iteration 48, loss = 1.07031047\n",
            "Iteration 49, loss = 1.06649688\n",
            "Iteration 50, loss = 1.06488403\n",
            "Iteration 51, loss = 1.06777802\n",
            "Iteration 52, loss = 1.06811865\n",
            "Iteration 53, loss = 1.06280678\n",
            "Iteration 54, loss = 1.06250314\n",
            "Iteration 55, loss = 1.06180237\n",
            "Iteration 56, loss = 1.06112093\n",
            "Iteration 57, loss = 1.05768683\n",
            "Iteration 58, loss = 1.05481712\n",
            "Iteration 59, loss = 1.05283952\n",
            "Iteration 60, loss = 1.05501922\n",
            "Iteration 61, loss = 1.05812397\n",
            "Iteration 62, loss = 1.05695516\n",
            "Iteration 63, loss = 1.05107848\n",
            "Iteration 64, loss = 1.05758548\n",
            "Iteration 65, loss = 1.04971912\n",
            "Iteration 66, loss = 1.05663742\n",
            "Iteration 67, loss = 1.04797998\n",
            "Iteration 68, loss = 1.04622491\n",
            "Iteration 69, loss = 1.04748079\n",
            "Iteration 70, loss = 1.04565317\n",
            "Iteration 71, loss = 1.04606482\n",
            "Iteration 72, loss = 1.04857442\n",
            "Iteration 73, loss = 1.04291802\n",
            "Iteration 74, loss = 1.04687634\n",
            "Iteration 75, loss = 1.04737870\n",
            "Iteration 76, loss = 1.04489662\n",
            "Iteration 77, loss = 1.04320334\n",
            "Iteration 78, loss = 1.03534762\n",
            "Iteration 79, loss = 1.03544060\n",
            "Iteration 80, loss = 1.03800759\n",
            "Iteration 81, loss = 1.03312628\n",
            "Iteration 82, loss = 1.03517008\n",
            "Iteration 83, loss = 1.03136210\n",
            "Iteration 84, loss = 1.03042286\n",
            "Iteration 85, loss = 1.03450939\n",
            "Iteration 86, loss = 1.03327973\n",
            "Iteration 87, loss = 1.03460671\n",
            "Iteration 88, loss = 1.03396051\n",
            "Iteration 89, loss = 1.02821449\n",
            "Iteration 90, loss = 1.02695629\n",
            "Iteration 91, loss = 1.02767609\n",
            "Iteration 92, loss = 1.02724618\n",
            "Iteration 93, loss = 1.02361781\n",
            "Iteration 94, loss = 1.02752649\n",
            "Iteration 95, loss = 1.03020836\n",
            "Iteration 96, loss = 1.02292194\n",
            "Iteration 97, loss = 1.02164667\n",
            "Iteration 98, loss = 1.02643683\n",
            "Iteration 99, loss = 1.02597566\n",
            "Iteration 100, loss = 1.01825544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.42022149\n",
            "Iteration 2, loss = 1.37730654\n",
            "Iteration 3, loss = 1.36665032\n",
            "Iteration 4, loss = 1.35862706\n",
            "Iteration 5, loss = 1.34830812\n",
            "Iteration 6, loss = 1.33618187\n",
            "Iteration 7, loss = 1.32069488\n",
            "Iteration 8, loss = 1.30496183\n",
            "Iteration 9, loss = 1.28354124\n",
            "Iteration 10, loss = 1.25988376\n",
            "Iteration 11, loss = 1.24213219\n",
            "Iteration 12, loss = 1.21314412\n",
            "Iteration 13, loss = 1.19809400\n",
            "Iteration 14, loss = 1.18092741\n",
            "Iteration 15, loss = 1.16806394\n",
            "Iteration 16, loss = 1.15950515\n",
            "Iteration 17, loss = 1.15141795\n",
            "Iteration 18, loss = 1.14362121\n",
            "Iteration 19, loss = 1.13464444\n",
            "Iteration 20, loss = 1.12975787\n",
            "Iteration 21, loss = 1.13277653\n",
            "Iteration 22, loss = 1.13054314\n",
            "Iteration 23, loss = 1.12649773\n",
            "Iteration 24, loss = 1.12296430\n",
            "Iteration 25, loss = 1.12178861\n",
            "Iteration 26, loss = 1.12474438\n",
            "Iteration 27, loss = 1.12113760\n",
            "Iteration 28, loss = 1.11514128\n",
            "Iteration 29, loss = 1.11308129\n",
            "Iteration 30, loss = 1.11467052\n",
            "Iteration 31, loss = 1.10985654\n",
            "Iteration 32, loss = 1.11040928\n",
            "Iteration 33, loss = 1.10900416\n",
            "Iteration 34, loss = 1.10702453\n",
            "Iteration 35, loss = 1.11079860\n",
            "Iteration 36, loss = 1.10721819\n",
            "Iteration 37, loss = 1.10515079\n",
            "Iteration 38, loss = 1.10327938\n",
            "Iteration 39, loss = 1.09788097\n",
            "Iteration 40, loss = 1.09614168\n",
            "Iteration 41, loss = 1.09548965\n",
            "Iteration 42, loss = 1.09452171\n",
            "Iteration 43, loss = 1.09583927\n",
            "Iteration 44, loss = 1.09751222\n",
            "Iteration 45, loss = 1.09621443\n",
            "Iteration 46, loss = 1.08983916\n",
            "Iteration 47, loss = 1.09194112\n",
            "Iteration 48, loss = 1.09336870\n",
            "Iteration 49, loss = 1.08976180\n",
            "Iteration 50, loss = 1.09032763\n",
            "Iteration 51, loss = 1.09358767\n",
            "Iteration 52, loss = 1.09374125\n",
            "Iteration 53, loss = 1.09016951\n",
            "Iteration 54, loss = 1.08625330\n",
            "Iteration 55, loss = 1.08817454\n",
            "Iteration 56, loss = 1.08840419\n",
            "Iteration 57, loss = 1.08259035\n",
            "Iteration 58, loss = 1.07891754\n",
            "Iteration 59, loss = 1.08060141\n",
            "Iteration 60, loss = 1.08325620\n",
            "Iteration 61, loss = 1.08124931\n",
            "Iteration 62, loss = 1.07860666\n",
            "Iteration 63, loss = 1.07661797\n",
            "Iteration 64, loss = 1.07971356\n",
            "Iteration 65, loss = 1.07378623\n",
            "Iteration 66, loss = 1.07801582\n",
            "Iteration 67, loss = 1.07259836\n",
            "Iteration 68, loss = 1.07175108\n",
            "Iteration 69, loss = 1.07291323\n",
            "Iteration 70, loss = 1.06622730\n",
            "Iteration 71, loss = 1.06963792\n",
            "Iteration 72, loss = 1.07488696\n",
            "Iteration 73, loss = 1.07149440\n",
            "Iteration 74, loss = 1.07144769\n",
            "Iteration 75, loss = 1.06427129\n",
            "Iteration 76, loss = 1.06152269\n",
            "Iteration 77, loss = 1.06361507\n",
            "Iteration 78, loss = 1.06045108\n",
            "Iteration 79, loss = 1.06067861\n",
            "Iteration 80, loss = 1.06435418\n",
            "Iteration 81, loss = 1.06115390\n",
            "Iteration 82, loss = 1.06135665\n",
            "Iteration 83, loss = 1.05932405\n",
            "Iteration 84, loss = 1.05729163\n",
            "Iteration 85, loss = 1.05956478\n",
            "Iteration 86, loss = 1.05695433\n",
            "Iteration 87, loss = 1.05590286\n",
            "Iteration 88, loss = 1.05908136\n",
            "Iteration 89, loss = 1.05236826\n",
            "Iteration 90, loss = 1.05567688\n",
            "Iteration 91, loss = 1.05330564\n",
            "Iteration 92, loss = 1.05211789\n",
            "Iteration 93, loss = 1.05417958\n",
            "Iteration 94, loss = 1.06007985\n",
            "Iteration 95, loss = 1.05265912\n",
            "Iteration 96, loss = 1.04990085\n",
            "Iteration 97, loss = 1.05497872\n",
            "Iteration 98, loss = 1.04921241\n",
            "Iteration 99, loss = 1.05064593\n",
            "Iteration 100, loss = 1.04363230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41856186\n",
            "Iteration 2, loss = 1.36709213\n",
            "Iteration 3, loss = 1.35404682\n",
            "Iteration 4, loss = 1.34459430\n",
            "Iteration 5, loss = 1.33394407\n",
            "Iteration 6, loss = 1.32168074\n",
            "Iteration 7, loss = 1.30584473\n",
            "Iteration 8, loss = 1.29055200\n",
            "Iteration 9, loss = 1.26983688\n",
            "Iteration 10, loss = 1.25115729\n",
            "Iteration 11, loss = 1.23354206\n",
            "Iteration 12, loss = 1.21004206\n",
            "Iteration 13, loss = 1.19799469\n",
            "Iteration 14, loss = 1.17937717\n",
            "Iteration 15, loss = 1.17012632\n",
            "Iteration 16, loss = 1.16027996\n",
            "Iteration 17, loss = 1.15438566\n",
            "Iteration 18, loss = 1.14737805\n",
            "Iteration 19, loss = 1.13819323\n",
            "Iteration 20, loss = 1.13679184\n",
            "Iteration 21, loss = 1.13920050\n",
            "Iteration 22, loss = 1.13228455\n",
            "Iteration 23, loss = 1.13015697\n",
            "Iteration 24, loss = 1.12807934\n",
            "Iteration 25, loss = 1.12418556\n",
            "Iteration 26, loss = 1.13058505\n",
            "Iteration 27, loss = 1.12573322\n",
            "Iteration 28, loss = 1.12207896\n",
            "Iteration 29, loss = 1.11813029\n",
            "Iteration 30, loss = 1.12192171\n",
            "Iteration 31, loss = 1.11466524\n",
            "Iteration 32, loss = 1.11490085\n",
            "Iteration 33, loss = 1.11708963\n",
            "Iteration 34, loss = 1.11581390\n",
            "Iteration 35, loss = 1.11897795\n",
            "Iteration 36, loss = 1.11498577\n",
            "Iteration 37, loss = 1.11146211\n",
            "Iteration 38, loss = 1.11096650\n",
            "Iteration 39, loss = 1.10775662\n",
            "Iteration 40, loss = 1.10587860\n",
            "Iteration 41, loss = 1.10523302\n",
            "Iteration 42, loss = 1.10501375\n",
            "Iteration 43, loss = 1.10374227\n",
            "Iteration 44, loss = 1.10560824\n",
            "Iteration 45, loss = 1.10428153\n",
            "Iteration 46, loss = 1.10137386\n",
            "Iteration 47, loss = 1.10438921\n",
            "Iteration 48, loss = 1.10448559\n",
            "Iteration 49, loss = 1.10357469\n",
            "Iteration 50, loss = 1.09970889\n",
            "Iteration 51, loss = 1.09975093\n",
            "Iteration 52, loss = 1.10013375\n",
            "Iteration 53, loss = 1.09807049\n",
            "Iteration 54, loss = 1.09484453\n",
            "Iteration 55, loss = 1.09453926\n",
            "Iteration 56, loss = 1.09842550\n",
            "Iteration 57, loss = 1.09354707\n",
            "Iteration 58, loss = 1.09234254\n",
            "Iteration 59, loss = 1.08998502\n",
            "Iteration 60, loss = 1.09140329\n",
            "Iteration 61, loss = 1.08723646\n",
            "Iteration 62, loss = 1.08742575\n",
            "Iteration 63, loss = 1.08740020\n",
            "Iteration 64, loss = 1.08916059\n",
            "Iteration 65, loss = 1.08472127\n",
            "Iteration 66, loss = 1.08622636\n",
            "Iteration 67, loss = 1.08402376\n",
            "Iteration 68, loss = 1.08161425\n",
            "Iteration 69, loss = 1.08343926\n",
            "Iteration 70, loss = 1.07720410\n",
            "Iteration 71, loss = 1.08097729\n",
            "Iteration 72, loss = 1.08337610\n",
            "Iteration 73, loss = 1.09019693\n",
            "Iteration 74, loss = 1.08416793\n",
            "Iteration 75, loss = 1.07411840\n",
            "Iteration 76, loss = 1.07161523\n",
            "Iteration 77, loss = 1.07406623\n",
            "Iteration 78, loss = 1.07185568\n",
            "Iteration 79, loss = 1.07114714\n",
            "Iteration 80, loss = 1.07429653\n",
            "Iteration 81, loss = 1.07521853\n",
            "Iteration 82, loss = 1.07256310\n",
            "Iteration 83, loss = 1.07123467\n",
            "Iteration 84, loss = 1.07453306\n",
            "Iteration 85, loss = 1.07029869\n",
            "Iteration 86, loss = 1.06848432\n",
            "Iteration 87, loss = 1.06838198\n",
            "Iteration 88, loss = 1.06751218\n",
            "Iteration 89, loss = 1.06323554\n",
            "Iteration 90, loss = 1.06438227\n",
            "Iteration 91, loss = 1.06258676\n",
            "Iteration 92, loss = 1.06312269\n",
            "Iteration 93, loss = 1.07025523\n",
            "Iteration 94, loss = 1.06692629\n",
            "Iteration 95, loss = 1.06600751\n",
            "Iteration 96, loss = 1.06162815\n",
            "Iteration 97, loss = 1.06045795\n",
            "Iteration 98, loss = 1.05825983\n",
            "Iteration 99, loss = 1.06171294\n",
            "Iteration 100, loss = 1.05263405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39171904\n",
            "Iteration 2, loss = 1.36621884\n",
            "Iteration 3, loss = 1.35166616\n",
            "Iteration 4, loss = 1.33838871\n",
            "Iteration 5, loss = 1.32195056\n",
            "Iteration 6, loss = 1.30129461\n",
            "Iteration 7, loss = 1.27738171\n",
            "Iteration 8, loss = 1.25679056\n",
            "Iteration 9, loss = 1.23054788\n",
            "Iteration 10, loss = 1.21391613\n",
            "Iteration 11, loss = 1.19811348\n",
            "Iteration 12, loss = 1.17245855\n",
            "Iteration 13, loss = 1.16463225\n",
            "Iteration 14, loss = 1.14713210\n",
            "Iteration 15, loss = 1.13908517\n",
            "Iteration 16, loss = 1.13391968\n",
            "Iteration 17, loss = 1.12684166\n",
            "Iteration 18, loss = 1.11965346\n",
            "Iteration 19, loss = 1.11227124\n",
            "Iteration 20, loss = 1.11311554\n",
            "Iteration 21, loss = 1.11885942\n",
            "Iteration 22, loss = 1.11474517\n",
            "Iteration 23, loss = 1.10913943\n",
            "Iteration 24, loss = 1.10426498\n",
            "Iteration 25, loss = 1.10009433\n",
            "Iteration 26, loss = 1.10791528\n",
            "Iteration 27, loss = 1.10255272\n",
            "Iteration 28, loss = 1.09471310\n",
            "Iteration 29, loss = 1.09294524\n",
            "Iteration 30, loss = 1.09527053\n",
            "Iteration 31, loss = 1.09339743\n",
            "Iteration 32, loss = 1.08993025\n",
            "Iteration 33, loss = 1.08886858\n",
            "Iteration 34, loss = 1.08937809\n",
            "Iteration 35, loss = 1.09260094\n",
            "Iteration 36, loss = 1.09137330\n",
            "Iteration 37, loss = 1.09062636\n",
            "Iteration 38, loss = 1.09268154\n",
            "Iteration 39, loss = 1.08061321\n",
            "Iteration 40, loss = 1.07925514\n",
            "Iteration 41, loss = 1.07816542\n",
            "Iteration 42, loss = 1.07726691\n",
            "Iteration 43, loss = 1.07863676\n",
            "Iteration 44, loss = 1.07842657\n",
            "Iteration 45, loss = 1.07652910\n",
            "Iteration 46, loss = 1.07253984\n",
            "Iteration 47, loss = 1.07335450\n",
            "Iteration 48, loss = 1.07518229\n",
            "Iteration 49, loss = 1.07024352\n",
            "Iteration 50, loss = 1.06635323\n",
            "Iteration 51, loss = 1.06772563\n",
            "Iteration 52, loss = 1.06516065\n",
            "Iteration 53, loss = 1.06704153\n",
            "Iteration 54, loss = 1.06293461\n",
            "Iteration 55, loss = 1.06214702\n",
            "Iteration 56, loss = 1.06248724\n",
            "Iteration 57, loss = 1.05923244\n",
            "Iteration 58, loss = 1.06088052\n",
            "Iteration 59, loss = 1.06187928\n",
            "Iteration 60, loss = 1.05971242\n",
            "Iteration 61, loss = 1.05379889\n",
            "Iteration 62, loss = 1.05409254\n",
            "Iteration 63, loss = 1.05389599\n",
            "Iteration 64, loss = 1.05474534\n",
            "Iteration 65, loss = 1.04839807\n",
            "Iteration 66, loss = 1.04784170\n",
            "Iteration 67, loss = 1.04600905\n",
            "Iteration 68, loss = 1.04556508\n",
            "Iteration 69, loss = 1.04394848\n",
            "Iteration 70, loss = 1.04044801\n",
            "Iteration 71, loss = 1.04064044\n",
            "Iteration 72, loss = 1.04233720\n",
            "Iteration 73, loss = 1.04450312\n",
            "Iteration 74, loss = 1.04951962\n",
            "Iteration 75, loss = 1.03999649\n",
            "Iteration 76, loss = 1.03423363\n",
            "Iteration 77, loss = 1.03580053\n",
            "Iteration 78, loss = 1.03540557\n",
            "Iteration 79, loss = 1.03554422\n",
            "Iteration 80, loss = 1.03604197\n",
            "Iteration 81, loss = 1.03678033\n",
            "Iteration 82, loss = 1.03651697\n",
            "Iteration 83, loss = 1.02899292\n",
            "Iteration 84, loss = 1.03239668\n",
            "Iteration 85, loss = 1.03139245\n",
            "Iteration 86, loss = 1.02625184\n",
            "Iteration 87, loss = 1.02884213\n",
            "Iteration 88, loss = 1.02972571\n",
            "Iteration 89, loss = 1.02624894\n",
            "Iteration 90, loss = 1.02601416\n",
            "Iteration 91, loss = 1.02269096\n",
            "Iteration 92, loss = 1.02287943\n",
            "Iteration 93, loss = 1.02872712\n",
            "Iteration 94, loss = 1.03100576\n",
            "Iteration 95, loss = 1.02561358\n",
            "Iteration 96, loss = 1.02420644\n",
            "Iteration 97, loss = 1.02774733\n",
            "Iteration 98, loss = 1.02427080\n",
            "Iteration 99, loss = 1.02064003\n",
            "Iteration 100, loss = 1.01845742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38337841\n",
            "Iteration 2, loss = 1.36326879\n",
            "Iteration 3, loss = 1.34679213\n",
            "Iteration 4, loss = 1.33262891\n",
            "Iteration 5, loss = 1.31259823\n",
            "Iteration 6, loss = 1.29134510\n",
            "Iteration 7, loss = 1.26609029\n",
            "Iteration 8, loss = 1.24366040\n",
            "Iteration 9, loss = 1.21623270\n",
            "Iteration 10, loss = 1.19653268\n",
            "Iteration 11, loss = 1.18460374\n",
            "Iteration 12, loss = 1.15870074\n",
            "Iteration 13, loss = 1.15090935\n",
            "Iteration 14, loss = 1.13431684\n",
            "Iteration 15, loss = 1.12273750\n",
            "Iteration 16, loss = 1.11805764\n",
            "Iteration 17, loss = 1.10758474\n",
            "Iteration 18, loss = 1.10155738\n",
            "Iteration 19, loss = 1.09648653\n",
            "Iteration 20, loss = 1.09528864\n",
            "Iteration 21, loss = 1.09963942\n",
            "Iteration 22, loss = 1.09731927\n",
            "Iteration 23, loss = 1.09195484\n",
            "Iteration 24, loss = 1.08850874\n",
            "Iteration 25, loss = 1.08466404\n",
            "Iteration 26, loss = 1.09532841\n",
            "Iteration 27, loss = 1.09069455\n",
            "Iteration 28, loss = 1.07799703\n",
            "Iteration 29, loss = 1.07463458\n",
            "Iteration 30, loss = 1.07522481\n",
            "Iteration 31, loss = 1.07347637\n",
            "Iteration 32, loss = 1.06929105\n",
            "Iteration 33, loss = 1.06864463\n",
            "Iteration 34, loss = 1.07007503\n",
            "Iteration 35, loss = 1.06960993\n",
            "Iteration 36, loss = 1.06801866\n",
            "Iteration 37, loss = 1.06423456\n",
            "Iteration 38, loss = 1.06936820\n",
            "Iteration 39, loss = 1.06067201\n",
            "Iteration 40, loss = 1.06038048\n",
            "Iteration 41, loss = 1.05691470\n",
            "Iteration 42, loss = 1.05385644\n",
            "Iteration 43, loss = 1.05330826\n",
            "Iteration 44, loss = 1.05800055\n",
            "Iteration 45, loss = 1.05616742\n",
            "Iteration 46, loss = 1.05326191\n",
            "Iteration 47, loss = 1.05201119\n",
            "Iteration 48, loss = 1.05399581\n",
            "Iteration 49, loss = 1.04588183\n",
            "Iteration 50, loss = 1.04695247\n",
            "Iteration 51, loss = 1.04406890\n",
            "Iteration 52, loss = 1.04473037\n",
            "Iteration 53, loss = 1.04426544\n",
            "Iteration 54, loss = 1.04070711\n",
            "Iteration 55, loss = 1.04071301\n",
            "Iteration 56, loss = 1.04207054\n",
            "Iteration 57, loss = 1.04328501\n",
            "Iteration 58, loss = 1.03986627\n",
            "Iteration 59, loss = 1.04351564\n",
            "Iteration 60, loss = 1.04019563\n",
            "Iteration 61, loss = 1.03814604\n",
            "Iteration 62, loss = 1.03666766\n",
            "Iteration 63, loss = 1.04144393\n",
            "Iteration 64, loss = 1.03137604\n",
            "Iteration 65, loss = 1.02854328\n",
            "Iteration 66, loss = 1.03150347\n",
            "Iteration 67, loss = 1.02810584\n",
            "Iteration 68, loss = 1.02544940\n",
            "Iteration 69, loss = 1.02394061\n",
            "Iteration 70, loss = 1.02079523\n",
            "Iteration 71, loss = 1.02036611\n",
            "Iteration 72, loss = 1.02222286\n",
            "Iteration 73, loss = 1.02184220\n",
            "Iteration 74, loss = 1.02436795\n",
            "Iteration 75, loss = 1.01869577\n",
            "Iteration 76, loss = 1.01589269\n",
            "Iteration 77, loss = 1.01790313\n",
            "Iteration 78, loss = 1.01773130\n",
            "Iteration 79, loss = 1.01633124\n",
            "Iteration 80, loss = 1.01555639\n",
            "Iteration 81, loss = 1.01621904\n",
            "Iteration 82, loss = 1.01495897\n",
            "Iteration 83, loss = 1.01102905\n",
            "Iteration 84, loss = 1.01109384\n",
            "Iteration 85, loss = 1.01406104\n",
            "Iteration 86, loss = 1.00932016\n",
            "Iteration 87, loss = 1.01196589\n",
            "Iteration 88, loss = 1.01085167\n",
            "Iteration 89, loss = 1.00731898\n",
            "Iteration 90, loss = 1.01003171\n",
            "Iteration 91, loss = 1.00629995\n",
            "Iteration 92, loss = 1.00548644\n",
            "Iteration 93, loss = 1.00704109\n",
            "Iteration 94, loss = 1.00990158\n",
            "Iteration 95, loss = 1.00818635\n",
            "Iteration 96, loss = 1.00979497\n",
            "Iteration 97, loss = 1.01066549\n",
            "Iteration 98, loss = 1.01088755\n",
            "Iteration 99, loss = 1.00399995\n",
            "Iteration 100, loss = 0.99960807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.40724110\n",
            "Iteration 2, loss = 1.35932206\n",
            "Iteration 3, loss = 1.34809488\n",
            "Iteration 4, loss = 1.33408230\n",
            "Iteration 5, loss = 1.31826793\n",
            "Iteration 6, loss = 1.30037654\n",
            "Iteration 7, loss = 1.27630566\n",
            "Iteration 8, loss = 1.25360939\n",
            "Iteration 9, loss = 1.22622650\n",
            "Iteration 10, loss = 1.20475041\n",
            "Iteration 11, loss = 1.19177571\n",
            "Iteration 12, loss = 1.16730986\n",
            "Iteration 13, loss = 1.15946725\n",
            "Iteration 14, loss = 1.14128479\n",
            "Iteration 15, loss = 1.13072837\n",
            "Iteration 16, loss = 1.12379186\n",
            "Iteration 17, loss = 1.11602240\n",
            "Iteration 18, loss = 1.10990137\n",
            "Iteration 19, loss = 1.10356992\n",
            "Iteration 20, loss = 1.10084125\n",
            "Iteration 21, loss = 1.10383566\n",
            "Iteration 22, loss = 1.10278895\n",
            "Iteration 23, loss = 1.09824556\n",
            "Iteration 24, loss = 1.09589585\n",
            "Iteration 25, loss = 1.09163868\n",
            "Iteration 26, loss = 1.09708009\n",
            "Iteration 27, loss = 1.09512432\n",
            "Iteration 28, loss = 1.08674178\n",
            "Iteration 29, loss = 1.08300454\n",
            "Iteration 30, loss = 1.08335032\n",
            "Iteration 31, loss = 1.08136341\n",
            "Iteration 32, loss = 1.07794616\n",
            "Iteration 33, loss = 1.07451537\n",
            "Iteration 34, loss = 1.07629932\n",
            "Iteration 35, loss = 1.07639846\n",
            "Iteration 36, loss = 1.07534294\n",
            "Iteration 37, loss = 1.07329488\n",
            "Iteration 38, loss = 1.07529146\n",
            "Iteration 39, loss = 1.06787805\n",
            "Iteration 40, loss = 1.06515525\n",
            "Iteration 41, loss = 1.06348408\n",
            "Iteration 42, loss = 1.06178878\n",
            "Iteration 43, loss = 1.06357226\n",
            "Iteration 44, loss = 1.06495304\n",
            "Iteration 45, loss = 1.06460117\n",
            "Iteration 46, loss = 1.05976863\n",
            "Iteration 47, loss = 1.05750842\n",
            "Iteration 48, loss = 1.05547597\n",
            "Iteration 49, loss = 1.05187411\n",
            "Iteration 50, loss = 1.05104854\n",
            "Iteration 51, loss = 1.04915866\n",
            "Iteration 52, loss = 1.04789935\n",
            "Iteration 53, loss = 1.04448006\n",
            "Iteration 54, loss = 1.04491925\n",
            "Iteration 55, loss = 1.04355602\n",
            "Iteration 56, loss = 1.04266912\n",
            "Iteration 57, loss = 1.04420694\n",
            "Iteration 58, loss = 1.04323414\n",
            "Iteration 59, loss = 1.04241534\n",
            "Iteration 60, loss = 1.03983703\n",
            "Iteration 61, loss = 1.04203687\n",
            "Iteration 62, loss = 1.03800536\n",
            "Iteration 63, loss = 1.04301086\n",
            "Iteration 64, loss = 1.03252581\n",
            "Iteration 65, loss = 1.02876100\n",
            "Iteration 66, loss = 1.03174620\n",
            "Iteration 67, loss = 1.02977910\n",
            "Iteration 68, loss = 1.02838415\n",
            "Iteration 69, loss = 1.02458770\n",
            "Iteration 70, loss = 1.02213047\n",
            "Iteration 71, loss = 1.02233797\n",
            "Iteration 72, loss = 1.02523498\n",
            "Iteration 73, loss = 1.02231565\n",
            "Iteration 74, loss = 1.02460556\n",
            "Iteration 75, loss = 1.01849348\n",
            "Iteration 76, loss = 1.01730647\n",
            "Iteration 77, loss = 1.02268703\n",
            "Iteration 78, loss = 1.02268876\n",
            "Iteration 79, loss = 1.01896200\n",
            "Iteration 80, loss = 1.01748667\n",
            "Iteration 81, loss = 1.01954449\n",
            "Iteration 82, loss = 1.01890427\n",
            "Iteration 83, loss = 1.01628130\n",
            "Iteration 84, loss = 1.00942397\n",
            "Iteration 85, loss = 1.01130552\n",
            "Iteration 86, loss = 1.00626742\n",
            "Iteration 87, loss = 1.01300908\n",
            "Iteration 88, loss = 1.00881412\n",
            "Iteration 89, loss = 1.00785061\n",
            "Iteration 90, loss = 1.00828900\n",
            "Iteration 91, loss = 1.00440254\n",
            "Iteration 92, loss = 1.00398582\n",
            "Iteration 93, loss = 1.00204926\n",
            "Iteration 94, loss = 1.00902317\n",
            "Iteration 95, loss = 1.00326564\n",
            "Iteration 96, loss = 1.00303187\n",
            "Iteration 97, loss = 1.00182801\n",
            "Iteration 98, loss = 1.00352469\n",
            "Iteration 99, loss = 1.00104599\n",
            "Iteration 100, loss = 0.99595289\n",
            "49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38969159\n",
            "Iteration 2, loss = 1.37087159\n",
            "Iteration 3, loss = 1.35564068\n",
            "Iteration 4, loss = 1.33658839\n",
            "Iteration 5, loss = 1.31122354\n",
            "Iteration 6, loss = 1.28665885\n",
            "Iteration 7, loss = 1.26035165\n",
            "Iteration 8, loss = 1.23468246\n",
            "Iteration 9, loss = 1.21613587\n",
            "Iteration 10, loss = 1.22164113\n",
            "Iteration 11, loss = 1.18630334\n",
            "Iteration 12, loss = 1.17284680\n",
            "Iteration 13, loss = 1.15916895\n",
            "Iteration 14, loss = 1.14639557\n",
            "Iteration 15, loss = 1.13484676\n",
            "Iteration 16, loss = 1.13273662\n",
            "Iteration 17, loss = 1.11089591\n",
            "Iteration 18, loss = 1.11133803\n",
            "Iteration 19, loss = 1.10470684\n",
            "Iteration 20, loss = 1.10526680\n",
            "Iteration 21, loss = 1.09667783\n",
            "Iteration 22, loss = 1.08798877\n",
            "Iteration 23, loss = 1.07531583\n",
            "Iteration 24, loss = 1.06544579\n",
            "Iteration 25, loss = 1.07344401\n",
            "Iteration 26, loss = 1.05990310\n",
            "Iteration 27, loss = 1.03852506\n",
            "Iteration 28, loss = 1.02783068\n",
            "Iteration 29, loss = 1.02483092\n",
            "Iteration 30, loss = 1.02421808\n",
            "Iteration 31, loss = 1.01436598\n",
            "Iteration 32, loss = 1.00010136\n",
            "Iteration 33, loss = 0.99525764\n",
            "Iteration 34, loss = 1.00357151\n",
            "Iteration 35, loss = 0.99304588\n",
            "Iteration 36, loss = 0.98484149\n",
            "Iteration 37, loss = 0.96952054\n",
            "Iteration 38, loss = 0.95789311\n",
            "Iteration 39, loss = 0.95254810\n",
            "Iteration 40, loss = 0.95218553\n",
            "Iteration 41, loss = 0.94132415\n",
            "Iteration 42, loss = 0.95629697\n",
            "Iteration 43, loss = 0.94915185\n",
            "Iteration 44, loss = 0.92761497\n",
            "Iteration 45, loss = 0.94103911\n",
            "Iteration 46, loss = 0.93792682\n",
            "Iteration 47, loss = 0.93722522\n",
            "Iteration 48, loss = 0.91849594\n",
            "Iteration 49, loss = 0.91889041\n",
            "Iteration 50, loss = 0.89228090\n",
            "Iteration 51, loss = 0.89505861\n",
            "Iteration 52, loss = 0.89141185\n",
            "Iteration 53, loss = 0.87621791\n",
            "Iteration 54, loss = 0.85467072\n",
            "Iteration 55, loss = 0.85427842\n",
            "Iteration 56, loss = 0.86588978\n",
            "Iteration 57, loss = 0.86739833\n",
            "Iteration 58, loss = 0.88975999\n",
            "Iteration 59, loss = 0.84233263\n",
            "Iteration 60, loss = 0.83895657\n",
            "Iteration 61, loss = 0.82754675\n",
            "Iteration 62, loss = 0.80871146\n",
            "Iteration 63, loss = 0.79760804\n",
            "Iteration 64, loss = 0.79696916\n",
            "Iteration 65, loss = 0.79275735\n",
            "Iteration 66, loss = 0.79224928\n",
            "Iteration 67, loss = 0.77830359\n",
            "Iteration 68, loss = 0.77156499\n",
            "Iteration 69, loss = 0.76781706\n",
            "Iteration 70, loss = 0.76905467\n",
            "Iteration 71, loss = 0.75553179\n",
            "Iteration 72, loss = 0.75266947\n",
            "Iteration 73, loss = 0.75632073\n",
            "Iteration 74, loss = 0.74941597\n",
            "Iteration 75, loss = 0.76278395\n",
            "Iteration 76, loss = 0.75261553\n",
            "Iteration 77, loss = 0.73867926\n",
            "Iteration 78, loss = 0.71866578\n",
            "Iteration 79, loss = 0.72761760\n",
            "Iteration 80, loss = 0.74444594\n",
            "Iteration 81, loss = 0.76786986\n",
            "Iteration 82, loss = 0.74183703\n",
            "Iteration 83, loss = 0.74685702\n",
            "Iteration 84, loss = 0.72055868\n",
            "Iteration 85, loss = 0.69416544\n",
            "Iteration 86, loss = 0.68119590\n",
            "Iteration 87, loss = 0.68118627\n",
            "Iteration 88, loss = 0.67715931\n",
            "Iteration 89, loss = 0.68008582\n",
            "Iteration 90, loss = 0.67094200\n",
            "Iteration 91, loss = 0.67412650\n",
            "Iteration 92, loss = 0.66886783\n",
            "Iteration 93, loss = 0.66854027\n",
            "Iteration 94, loss = 0.65791423\n",
            "Iteration 95, loss = 0.63629421\n",
            "Iteration 96, loss = 0.63148881\n",
            "Iteration 97, loss = 0.63889130\n",
            "Iteration 98, loss = 0.64446171\n",
            "Iteration 99, loss = 0.64249027\n",
            "Iteration 100, loss = 0.62463598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38938397\n",
            "Iteration 2, loss = 1.37115462\n",
            "Iteration 3, loss = 1.35775706\n",
            "Iteration 4, loss = 1.34000296\n",
            "Iteration 5, loss = 1.32277810\n",
            "Iteration 6, loss = 1.30099606\n",
            "Iteration 7, loss = 1.27894825\n",
            "Iteration 8, loss = 1.25256829\n",
            "Iteration 9, loss = 1.23136052\n",
            "Iteration 10, loss = 1.23869596\n",
            "Iteration 11, loss = 1.20148909\n",
            "Iteration 12, loss = 1.18604732\n",
            "Iteration 13, loss = 1.16792161\n",
            "Iteration 14, loss = 1.15463106\n",
            "Iteration 15, loss = 1.14224247\n",
            "Iteration 16, loss = 1.14159975\n",
            "Iteration 17, loss = 1.11852834\n",
            "Iteration 18, loss = 1.11980457\n",
            "Iteration 19, loss = 1.10948597\n",
            "Iteration 20, loss = 1.10556245\n",
            "Iteration 21, loss = 1.09507285\n",
            "Iteration 22, loss = 1.07635455\n",
            "Iteration 23, loss = 1.06333103\n",
            "Iteration 24, loss = 1.05741410\n",
            "Iteration 25, loss = 1.07448326\n",
            "Iteration 26, loss = 1.05495929\n",
            "Iteration 27, loss = 1.03419059\n",
            "Iteration 28, loss = 1.02530040\n",
            "Iteration 29, loss = 1.02377311\n",
            "Iteration 30, loss = 1.02463551\n",
            "Iteration 31, loss = 1.01786704\n",
            "Iteration 32, loss = 0.99997645\n",
            "Iteration 33, loss = 0.98944316\n",
            "Iteration 34, loss = 1.01144760\n",
            "Iteration 35, loss = 0.99392804\n",
            "Iteration 36, loss = 0.98576657\n",
            "Iteration 37, loss = 0.97370718\n",
            "Iteration 38, loss = 0.95875742\n",
            "Iteration 39, loss = 0.94464017\n",
            "Iteration 40, loss = 0.94213951\n",
            "Iteration 41, loss = 0.93046550\n",
            "Iteration 42, loss = 0.93986528\n",
            "Iteration 43, loss = 0.94012763\n",
            "Iteration 44, loss = 0.92309171\n",
            "Iteration 45, loss = 0.92451860\n",
            "Iteration 46, loss = 0.91648388\n",
            "Iteration 47, loss = 0.92881650\n",
            "Iteration 48, loss = 0.91211230\n",
            "Iteration 49, loss = 0.90797213\n",
            "Iteration 50, loss = 0.88132568\n",
            "Iteration 51, loss = 0.87506856\n",
            "Iteration 52, loss = 0.87239788\n",
            "Iteration 53, loss = 0.87806851\n",
            "Iteration 54, loss = 0.85959171\n",
            "Iteration 55, loss = 0.84558804\n",
            "Iteration 56, loss = 0.85197221\n",
            "Iteration 57, loss = 0.84385154\n",
            "Iteration 58, loss = 0.87015513\n",
            "Iteration 59, loss = 0.83420022\n",
            "Iteration 60, loss = 0.82351513\n",
            "Iteration 61, loss = 0.81097833\n",
            "Iteration 62, loss = 0.80447522\n",
            "Iteration 63, loss = 0.78423481\n",
            "Iteration 64, loss = 0.80093843\n",
            "Iteration 65, loss = 0.78685899\n",
            "Iteration 66, loss = 0.77863215\n",
            "Iteration 67, loss = 0.76645254\n",
            "Iteration 68, loss = 0.76172265\n",
            "Iteration 69, loss = 0.75496366\n",
            "Iteration 70, loss = 0.75542046\n",
            "Iteration 71, loss = 0.74410058\n",
            "Iteration 72, loss = 0.74070380\n",
            "Iteration 73, loss = 0.73838495\n",
            "Iteration 74, loss = 0.72670735\n",
            "Iteration 75, loss = 0.73434861\n",
            "Iteration 76, loss = 0.74147949\n",
            "Iteration 77, loss = 0.73357205\n",
            "Iteration 78, loss = 0.70447767\n",
            "Iteration 79, loss = 0.70269071\n",
            "Iteration 80, loss = 0.69505581\n",
            "Iteration 81, loss = 0.72255062\n",
            "Iteration 82, loss = 0.70058722\n",
            "Iteration 83, loss = 0.67269269\n",
            "Iteration 84, loss = 0.67416538\n",
            "Iteration 85, loss = 0.68438872\n",
            "Iteration 86, loss = 0.68478802\n",
            "Iteration 87, loss = 0.67343579\n",
            "Iteration 88, loss = 0.65493087\n",
            "Iteration 89, loss = 0.67193020\n",
            "Iteration 90, loss = 0.66786961\n",
            "Iteration 91, loss = 0.65507933\n",
            "Iteration 92, loss = 0.64590034\n",
            "Iteration 93, loss = 0.63161762\n",
            "Iteration 94, loss = 0.63487264\n",
            "Iteration 95, loss = 0.63494518\n",
            "Iteration 96, loss = 0.62180848\n",
            "Iteration 97, loss = 0.62707636\n",
            "Iteration 98, loss = 0.65589970\n",
            "Iteration 99, loss = 0.65188274\n",
            "Iteration 100, loss = 0.62791762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39199365\n",
            "Iteration 2, loss = 1.37320607\n",
            "Iteration 3, loss = 1.36114496\n",
            "Iteration 4, loss = 1.34759815\n",
            "Iteration 5, loss = 1.33208846\n",
            "Iteration 6, loss = 1.30919129\n",
            "Iteration 7, loss = 1.28388630\n",
            "Iteration 8, loss = 1.25751957\n",
            "Iteration 9, loss = 1.23101742\n",
            "Iteration 10, loss = 1.22323625\n",
            "Iteration 11, loss = 1.20545219\n",
            "Iteration 12, loss = 1.17621833\n",
            "Iteration 13, loss = 1.17209537\n",
            "Iteration 14, loss = 1.16028489\n",
            "Iteration 15, loss = 1.14529953\n",
            "Iteration 16, loss = 1.13525191\n",
            "Iteration 17, loss = 1.12057222\n",
            "Iteration 18, loss = 1.11480341\n",
            "Iteration 19, loss = 1.10988468\n",
            "Iteration 20, loss = 1.10398344\n",
            "Iteration 21, loss = 1.09913392\n",
            "Iteration 22, loss = 1.08047998\n",
            "Iteration 23, loss = 1.06969154\n",
            "Iteration 24, loss = 1.06168714\n",
            "Iteration 25, loss = 1.08106903\n",
            "Iteration 26, loss = 1.05584208\n",
            "Iteration 27, loss = 1.03718113\n",
            "Iteration 28, loss = 1.03065190\n",
            "Iteration 29, loss = 1.02163606\n",
            "Iteration 30, loss = 1.02821195\n",
            "Iteration 31, loss = 1.03301263\n",
            "Iteration 32, loss = 1.00262128\n",
            "Iteration 33, loss = 1.00767365\n",
            "Iteration 34, loss = 1.03309171\n",
            "Iteration 35, loss = 1.01014115\n",
            "Iteration 36, loss = 0.97567701\n",
            "Iteration 37, loss = 0.95940116\n",
            "Iteration 38, loss = 0.95875533\n",
            "Iteration 39, loss = 0.94668452\n",
            "Iteration 40, loss = 0.94522383\n",
            "Iteration 41, loss = 0.93933457\n",
            "Iteration 42, loss = 0.95549785\n",
            "Iteration 43, loss = 0.95710784\n",
            "Iteration 44, loss = 0.94091311\n",
            "Iteration 45, loss = 0.93052966\n",
            "Iteration 46, loss = 0.91450264\n",
            "Iteration 47, loss = 0.92321802\n",
            "Iteration 48, loss = 0.89751545\n",
            "Iteration 49, loss = 0.89543275\n",
            "Iteration 50, loss = 0.88318004\n",
            "Iteration 51, loss = 0.87469208\n",
            "Iteration 52, loss = 0.88579396\n",
            "Iteration 53, loss = 0.87334453\n",
            "Iteration 54, loss = 0.85213217\n",
            "Iteration 55, loss = 0.85245401\n",
            "Iteration 56, loss = 0.86186511\n",
            "Iteration 57, loss = 0.84766263\n",
            "Iteration 58, loss = 0.87776869\n",
            "Iteration 59, loss = 0.83313047\n",
            "Iteration 60, loss = 0.82129030\n",
            "Iteration 61, loss = 0.81897155\n",
            "Iteration 62, loss = 0.81844271\n",
            "Iteration 63, loss = 0.79799814\n",
            "Iteration 64, loss = 0.80304494\n",
            "Iteration 65, loss = 0.79114282\n",
            "Iteration 66, loss = 0.79206137\n",
            "Iteration 67, loss = 0.77380398\n",
            "Iteration 68, loss = 0.76842020\n",
            "Iteration 69, loss = 0.76013237\n",
            "Iteration 70, loss = 0.77265893\n",
            "Iteration 71, loss = 0.76923512\n",
            "Iteration 72, loss = 0.76136184\n",
            "Iteration 73, loss = 0.77121816\n",
            "Iteration 74, loss = 0.74982410\n",
            "Iteration 75, loss = 0.75039259\n",
            "Iteration 76, loss = 0.74726823\n",
            "Iteration 77, loss = 0.72724686\n",
            "Iteration 78, loss = 0.71357151\n",
            "Iteration 79, loss = 0.72017242\n",
            "Iteration 80, loss = 0.71398570\n",
            "Iteration 81, loss = 0.75154858\n",
            "Iteration 82, loss = 0.72377083\n",
            "Iteration 83, loss = 0.72194289\n",
            "Iteration 84, loss = 0.70477336\n",
            "Iteration 85, loss = 0.69885603\n",
            "Iteration 86, loss = 0.69246554\n",
            "Iteration 87, loss = 0.67399628\n",
            "Iteration 88, loss = 0.66292410\n",
            "Iteration 89, loss = 0.68707227\n",
            "Iteration 90, loss = 0.67259405\n",
            "Iteration 91, loss = 0.66460342\n",
            "Iteration 92, loss = 0.66653197\n",
            "Iteration 93, loss = 0.67449346\n",
            "Iteration 94, loss = 0.66224059\n",
            "Iteration 95, loss = 0.64618501\n",
            "Iteration 96, loss = 0.63594173\n",
            "Iteration 97, loss = 0.64560879\n",
            "Iteration 98, loss = 0.64140646\n",
            "Iteration 99, loss = 0.64555265\n",
            "Iteration 100, loss = 0.64005683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.39262632\n",
            "Iteration 2, loss = 1.37390992\n",
            "Iteration 3, loss = 1.36190622\n",
            "Iteration 4, loss = 1.34755328\n",
            "Iteration 5, loss = 1.33152265\n",
            "Iteration 6, loss = 1.31058462\n",
            "Iteration 7, loss = 1.28596584\n",
            "Iteration 8, loss = 1.25854242\n",
            "Iteration 9, loss = 1.23473858\n",
            "Iteration 10, loss = 1.22140131\n",
            "Iteration 11, loss = 1.20149930\n",
            "Iteration 12, loss = 1.17783608\n",
            "Iteration 13, loss = 1.16984470\n",
            "Iteration 14, loss = 1.15888962\n",
            "Iteration 15, loss = 1.14872129\n",
            "Iteration 16, loss = 1.12879009\n",
            "Iteration 17, loss = 1.12309614\n",
            "Iteration 18, loss = 1.10601052\n",
            "Iteration 19, loss = 1.10136272\n",
            "Iteration 20, loss = 1.09635748\n",
            "Iteration 21, loss = 1.08713693\n",
            "Iteration 22, loss = 1.08135192\n",
            "Iteration 23, loss = 1.06479481\n",
            "Iteration 24, loss = 1.05791953\n",
            "Iteration 25, loss = 1.06910687\n",
            "Iteration 26, loss = 1.04586478\n",
            "Iteration 27, loss = 1.03854691\n",
            "Iteration 28, loss = 1.02791244\n",
            "Iteration 29, loss = 1.02877711\n",
            "Iteration 30, loss = 1.02532251\n",
            "Iteration 31, loss = 1.02446747\n",
            "Iteration 32, loss = 1.00435842\n",
            "Iteration 33, loss = 1.00309681\n",
            "Iteration 34, loss = 1.03341003\n",
            "Iteration 35, loss = 1.01411172\n",
            "Iteration 36, loss = 0.99299776\n",
            "Iteration 37, loss = 0.98075557\n",
            "Iteration 38, loss = 0.96952217\n",
            "Iteration 39, loss = 0.96207870\n",
            "Iteration 40, loss = 0.96568967\n",
            "Iteration 41, loss = 0.95753245\n",
            "Iteration 42, loss = 0.97295888\n",
            "Iteration 43, loss = 0.97073450\n",
            "Iteration 44, loss = 0.95063079\n",
            "Iteration 45, loss = 0.94593645\n",
            "Iteration 46, loss = 0.94539854\n",
            "Iteration 47, loss = 0.95809081\n",
            "Iteration 48, loss = 0.93211490\n",
            "Iteration 49, loss = 0.92633732\n",
            "Iteration 50, loss = 0.90923757\n",
            "Iteration 51, loss = 0.90875604\n",
            "Iteration 52, loss = 0.92691100\n",
            "Iteration 53, loss = 0.93107694\n",
            "Iteration 54, loss = 0.91348972\n",
            "Iteration 55, loss = 0.90505373\n",
            "Iteration 56, loss = 0.89973134\n",
            "Iteration 57, loss = 0.88304769\n",
            "Iteration 58, loss = 0.90536212\n",
            "Iteration 59, loss = 0.86896728\n",
            "Iteration 60, loss = 0.86506702\n",
            "Iteration 61, loss = 0.86103650\n",
            "Iteration 62, loss = 0.86312291\n",
            "Iteration 63, loss = 0.84520197\n",
            "Iteration 64, loss = 0.85824272\n",
            "Iteration 65, loss = 0.84575968\n",
            "Iteration 66, loss = 0.83549556\n",
            "Iteration 67, loss = 0.82585240\n",
            "Iteration 68, loss = 0.82015007\n",
            "Iteration 69, loss = 0.82552834\n",
            "Iteration 70, loss = 0.84025771\n",
            "Iteration 71, loss = 0.81848520\n",
            "Iteration 72, loss = 0.80001509\n",
            "Iteration 73, loss = 0.80290196\n",
            "Iteration 74, loss = 0.79734265\n",
            "Iteration 75, loss = 0.79258187\n",
            "Iteration 76, loss = 0.78276117\n",
            "Iteration 77, loss = 0.77353950\n",
            "Iteration 78, loss = 0.76768378\n",
            "Iteration 79, loss = 0.78291584\n",
            "Iteration 80, loss = 0.79107880\n",
            "Iteration 81, loss = 0.82453731\n",
            "Iteration 82, loss = 0.77755696\n",
            "Iteration 83, loss = 0.76824271\n",
            "Iteration 84, loss = 0.74730400\n",
            "Iteration 85, loss = 0.75894846\n",
            "Iteration 86, loss = 0.76159959\n",
            "Iteration 87, loss = 0.73646777\n",
            "Iteration 88, loss = 0.73128482\n",
            "Iteration 89, loss = 0.73962738\n",
            "Iteration 90, loss = 0.73728448\n",
            "Iteration 91, loss = 0.73471418\n",
            "Iteration 92, loss = 0.71377941\n",
            "Iteration 93, loss = 0.71632322\n",
            "Iteration 94, loss = 0.70297402\n",
            "Iteration 95, loss = 0.70623671\n",
            "Iteration 96, loss = 0.69578673\n",
            "Iteration 97, loss = 0.70597010\n",
            "Iteration 98, loss = 0.75877936\n",
            "Iteration 99, loss = 0.75531410\n",
            "Iteration 100, loss = 0.73624868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38995055\n",
            "Iteration 2, loss = 1.37313861\n",
            "Iteration 3, loss = 1.35819426\n",
            "Iteration 4, loss = 1.34316742\n",
            "Iteration 5, loss = 1.32730372\n",
            "Iteration 6, loss = 1.30163799\n",
            "Iteration 7, loss = 1.27678964\n",
            "Iteration 8, loss = 1.24867751\n",
            "Iteration 9, loss = 1.22766757\n",
            "Iteration 10, loss = 1.21132228\n",
            "Iteration 11, loss = 1.19077945\n",
            "Iteration 12, loss = 1.16393186\n",
            "Iteration 13, loss = 1.16073545\n",
            "Iteration 14, loss = 1.15128548\n",
            "Iteration 15, loss = 1.13769448\n",
            "Iteration 16, loss = 1.11813570\n",
            "Iteration 17, loss = 1.10199258\n",
            "Iteration 18, loss = 1.09341593\n",
            "Iteration 19, loss = 1.09170986\n",
            "Iteration 20, loss = 1.08883959\n",
            "Iteration 21, loss = 1.07926386\n",
            "Iteration 22, loss = 1.06096404\n",
            "Iteration 23, loss = 1.04605391\n",
            "Iteration 24, loss = 1.04253209\n",
            "Iteration 25, loss = 1.04820843\n",
            "Iteration 26, loss = 1.02442736\n",
            "Iteration 27, loss = 1.02520231\n",
            "Iteration 28, loss = 1.00329874\n",
            "Iteration 29, loss = 1.00129540\n",
            "Iteration 30, loss = 1.00740345\n",
            "Iteration 31, loss = 1.01791149\n",
            "Iteration 32, loss = 0.97991381\n",
            "Iteration 33, loss = 0.98498378\n",
            "Iteration 34, loss = 1.00989946\n",
            "Iteration 35, loss = 0.98068129\n",
            "Iteration 36, loss = 0.95859813\n",
            "Iteration 37, loss = 0.94501854\n",
            "Iteration 38, loss = 0.93892515\n",
            "Iteration 39, loss = 0.93820954\n",
            "Iteration 40, loss = 0.94762307\n",
            "Iteration 41, loss = 0.92751265\n",
            "Iteration 42, loss = 0.94878581\n",
            "Iteration 43, loss = 0.93212803\n",
            "Iteration 44, loss = 0.90752002\n",
            "Iteration 45, loss = 0.90023393\n",
            "Iteration 46, loss = 0.90170298\n",
            "Iteration 47, loss = 0.90674288\n",
            "Iteration 48, loss = 0.87941033\n",
            "Iteration 49, loss = 0.88375712\n",
            "Iteration 50, loss = 0.86902585\n",
            "Iteration 51, loss = 0.85149261\n",
            "Iteration 52, loss = 0.86465521\n",
            "Iteration 53, loss = 0.86357756\n",
            "Iteration 54, loss = 0.84313501\n",
            "Iteration 55, loss = 0.83451078\n",
            "Iteration 56, loss = 0.83989802\n",
            "Iteration 57, loss = 0.82296120\n",
            "Iteration 58, loss = 0.83300320\n",
            "Iteration 59, loss = 0.79998145\n",
            "Iteration 60, loss = 0.80085119\n",
            "Iteration 61, loss = 0.79584040\n",
            "Iteration 62, loss = 0.78170882\n",
            "Iteration 63, loss = 0.77321463\n",
            "Iteration 64, loss = 0.77328235\n",
            "Iteration 65, loss = 0.76449871\n",
            "Iteration 66, loss = 0.75962531\n",
            "Iteration 67, loss = 0.74794292\n",
            "Iteration 68, loss = 0.74770804\n",
            "Iteration 69, loss = 0.76190999\n",
            "Iteration 70, loss = 0.77483818\n",
            "Iteration 71, loss = 0.74969415\n",
            "Iteration 72, loss = 0.72908613\n",
            "Iteration 73, loss = 0.72420137\n",
            "Iteration 74, loss = 0.71947361\n",
            "Iteration 75, loss = 0.70754263\n",
            "Iteration 76, loss = 0.69907475\n",
            "Iteration 77, loss = 0.69091857\n",
            "Iteration 78, loss = 0.69176538\n",
            "Iteration 79, loss = 0.70792481\n",
            "Iteration 80, loss = 0.70727408\n",
            "Iteration 81, loss = 0.71763758\n",
            "Iteration 82, loss = 0.68504639\n",
            "Iteration 83, loss = 0.67922031\n",
            "Iteration 84, loss = 0.66671883\n",
            "Iteration 85, loss = 0.66407959\n",
            "Iteration 86, loss = 0.66830117\n",
            "Iteration 87, loss = 0.65693561\n",
            "Iteration 88, loss = 0.65996908\n",
            "Iteration 89, loss = 0.64488052\n",
            "Iteration 90, loss = 0.64858994\n",
            "Iteration 91, loss = 0.66986795\n",
            "Iteration 92, loss = 0.63556178\n",
            "Iteration 93, loss = 0.61761356\n",
            "Iteration 94, loss = 0.60593850\n",
            "Iteration 95, loss = 0.59865405\n",
            "Iteration 96, loss = 0.60201015\n",
            "Iteration 97, loss = 0.60649395\n",
            "Iteration 98, loss = 0.64965811\n",
            "Iteration 99, loss = 0.62154813\n",
            "Iteration 100, loss = 0.62412581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38782650\n",
            "Iteration 2, loss = 1.37078857\n",
            "Iteration 3, loss = 1.35157666\n",
            "Iteration 4, loss = 1.33650673\n",
            "Iteration 5, loss = 1.32028506\n",
            "Iteration 6, loss = 1.29976614\n",
            "Iteration 7, loss = 1.28390106\n",
            "Iteration 8, loss = 1.26201307\n",
            "Iteration 9, loss = 1.25299146\n",
            "Iteration 10, loss = 1.23730689\n",
            "Iteration 11, loss = 1.21224146\n",
            "Iteration 12, loss = 1.20147472\n",
            "Iteration 13, loss = 1.19304370\n",
            "Iteration 14, loss = 1.17708075\n",
            "Iteration 15, loss = 1.17276841\n",
            "Iteration 16, loss = 1.15816511\n",
            "Iteration 17, loss = 1.14274043\n",
            "Iteration 18, loss = 1.13567377\n",
            "Iteration 19, loss = 1.11957988\n",
            "Iteration 20, loss = 1.11579051\n",
            "Iteration 21, loss = 1.10165901\n",
            "Iteration 22, loss = 1.09663026\n",
            "Iteration 23, loss = 1.08686551\n",
            "Iteration 24, loss = 1.08078659\n",
            "Iteration 25, loss = 1.07958807\n",
            "Iteration 26, loss = 1.06289161\n",
            "Iteration 27, loss = 1.06514888\n",
            "Iteration 28, loss = 1.04442196\n",
            "Iteration 29, loss = 1.04061257\n",
            "Iteration 30, loss = 1.03225771\n",
            "Iteration 31, loss = 1.03727422\n",
            "Iteration 32, loss = 1.02714404\n",
            "Iteration 33, loss = 1.04585488\n",
            "Iteration 34, loss = 1.04599609\n",
            "Iteration 35, loss = 1.02802017\n",
            "Iteration 36, loss = 1.00392661\n",
            "Iteration 37, loss = 0.99222384\n",
            "Iteration 38, loss = 0.99241392\n",
            "Iteration 39, loss = 0.97729537\n",
            "Iteration 40, loss = 0.98464692\n",
            "Iteration 41, loss = 0.97371913\n",
            "Iteration 42, loss = 0.98463933\n",
            "Iteration 43, loss = 0.96212500\n",
            "Iteration 44, loss = 0.95260013\n",
            "Iteration 45, loss = 0.93925391\n",
            "Iteration 46, loss = 0.94477385\n",
            "Iteration 47, loss = 0.95097705\n",
            "Iteration 48, loss = 0.91930113\n",
            "Iteration 49, loss = 0.93194946\n",
            "Iteration 50, loss = 0.91515931\n",
            "Iteration 51, loss = 0.90659337\n",
            "Iteration 52, loss = 0.92384674\n",
            "Iteration 53, loss = 0.92020116\n",
            "Iteration 54, loss = 0.89961614\n",
            "Iteration 55, loss = 0.89074032\n",
            "Iteration 56, loss = 0.87743901\n",
            "Iteration 57, loss = 0.87481548\n",
            "Iteration 58, loss = 0.88250000\n",
            "Iteration 59, loss = 0.86035493\n",
            "Iteration 60, loss = 0.85173406\n",
            "Iteration 61, loss = 0.84871078\n",
            "Iteration 62, loss = 0.84199414\n",
            "Iteration 63, loss = 0.84250061\n",
            "Iteration 64, loss = 0.82785072\n",
            "Iteration 65, loss = 0.82321498\n",
            "Iteration 66, loss = 0.82297576\n",
            "Iteration 67, loss = 0.81914944\n",
            "Iteration 68, loss = 0.81113027\n",
            "Iteration 69, loss = 0.81125528\n",
            "Iteration 70, loss = 0.81795703\n",
            "Iteration 71, loss = 0.81453050\n",
            "Iteration 72, loss = 0.79577484\n",
            "Iteration 73, loss = 0.77655473\n",
            "Iteration 74, loss = 0.78970990\n",
            "Iteration 75, loss = 0.78118077\n",
            "Iteration 76, loss = 0.75492849\n",
            "Iteration 77, loss = 0.74935730\n",
            "Iteration 78, loss = 0.74829096\n",
            "Iteration 79, loss = 0.75647597\n",
            "Iteration 80, loss = 0.75378676\n",
            "Iteration 81, loss = 0.78299079\n",
            "Iteration 82, loss = 0.75499290\n",
            "Iteration 83, loss = 0.75107810\n",
            "Iteration 84, loss = 0.74022994\n",
            "Iteration 85, loss = 0.72826828\n",
            "Iteration 86, loss = 0.71812813\n",
            "Iteration 87, loss = 0.72187806\n",
            "Iteration 88, loss = 0.72881349\n",
            "Iteration 89, loss = 0.69991765\n",
            "Iteration 90, loss = 0.69731915\n",
            "Iteration 91, loss = 0.69551262\n",
            "Iteration 92, loss = 0.68637496\n",
            "Iteration 93, loss = 0.67831599\n",
            "Iteration 94, loss = 0.67129167\n",
            "Iteration 95, loss = 0.66528990\n",
            "Iteration 96, loss = 0.66295235\n",
            "Iteration 97, loss = 0.67006863\n",
            "Iteration 98, loss = 0.71012914\n",
            "Iteration 99, loss = 0.68084278\n",
            "Iteration 100, loss = 0.69384918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38300680\n",
            "Iteration 2, loss = 1.35990272\n",
            "Iteration 3, loss = 1.34328970\n",
            "Iteration 4, loss = 1.33043228\n",
            "Iteration 5, loss = 1.31763040\n",
            "Iteration 6, loss = 1.29971823\n",
            "Iteration 7, loss = 1.28596251\n",
            "Iteration 8, loss = 1.27229322\n",
            "Iteration 9, loss = 1.26503464\n",
            "Iteration 10, loss = 1.24685166\n",
            "Iteration 11, loss = 1.22452975\n",
            "Iteration 12, loss = 1.22385625\n",
            "Iteration 13, loss = 1.21955193\n",
            "Iteration 14, loss = 1.20961722\n",
            "Iteration 15, loss = 1.19411576\n",
            "Iteration 16, loss = 1.18141433\n",
            "Iteration 17, loss = 1.16492939\n",
            "Iteration 18, loss = 1.16039804\n",
            "Iteration 19, loss = 1.14252818\n",
            "Iteration 20, loss = 1.13108924\n",
            "Iteration 21, loss = 1.12667454\n",
            "Iteration 22, loss = 1.12288355\n",
            "Iteration 23, loss = 1.11072977\n",
            "Iteration 24, loss = 1.10474479\n",
            "Iteration 25, loss = 1.10644369\n",
            "Iteration 26, loss = 1.08803106\n",
            "Iteration 27, loss = 1.08035240\n",
            "Iteration 28, loss = 1.07386287\n",
            "Iteration 29, loss = 1.07394083\n",
            "Iteration 30, loss = 1.07646406\n",
            "Iteration 31, loss = 1.07079655\n",
            "Iteration 32, loss = 1.06431396\n",
            "Iteration 33, loss = 1.06798533\n",
            "Iteration 34, loss = 1.05535557\n",
            "Iteration 35, loss = 1.03591982\n",
            "Iteration 36, loss = 1.03634967\n",
            "Iteration 37, loss = 1.01678715\n",
            "Iteration 38, loss = 1.02236402\n",
            "Iteration 39, loss = 1.01326234\n",
            "Iteration 40, loss = 1.01657396\n",
            "Iteration 41, loss = 1.00965655\n",
            "Iteration 42, loss = 1.00482336\n",
            "Iteration 43, loss = 0.99688066\n",
            "Iteration 44, loss = 0.98666665\n",
            "Iteration 45, loss = 0.98146991\n",
            "Iteration 46, loss = 0.97418892\n",
            "Iteration 47, loss = 0.97861105\n",
            "Iteration 48, loss = 0.96087214\n",
            "Iteration 49, loss = 0.96637701\n",
            "Iteration 50, loss = 0.96024082\n",
            "Iteration 51, loss = 0.95388020\n",
            "Iteration 52, loss = 0.95482804\n",
            "Iteration 53, loss = 0.94807333\n",
            "Iteration 54, loss = 0.92979545\n",
            "Iteration 55, loss = 0.93259509\n",
            "Iteration 56, loss = 0.92153543\n",
            "Iteration 57, loss = 0.91744626\n",
            "Iteration 58, loss = 0.93728774\n",
            "Iteration 59, loss = 0.91780190\n",
            "Iteration 60, loss = 0.90444348\n",
            "Iteration 61, loss = 0.89621968\n",
            "Iteration 62, loss = 0.88558651\n",
            "Iteration 63, loss = 0.88784259\n",
            "Iteration 64, loss = 0.88211385\n",
            "Iteration 65, loss = 0.87890838\n",
            "Iteration 66, loss = 0.86839819\n",
            "Iteration 67, loss = 0.86768250\n",
            "Iteration 68, loss = 0.86254088\n",
            "Iteration 69, loss = 0.85496702\n",
            "Iteration 70, loss = 0.86156017\n",
            "Iteration 71, loss = 0.86024092\n",
            "Iteration 72, loss = 0.84120136\n",
            "Iteration 73, loss = 0.85531392\n",
            "Iteration 74, loss = 0.83561731\n",
            "Iteration 75, loss = 0.82329726\n",
            "Iteration 76, loss = 0.82132386\n",
            "Iteration 77, loss = 0.80657818\n",
            "Iteration 78, loss = 0.80003835\n",
            "Iteration 79, loss = 0.80999319\n",
            "Iteration 80, loss = 0.78611530\n",
            "Iteration 81, loss = 0.81560534\n",
            "Iteration 82, loss = 0.80428216\n",
            "Iteration 83, loss = 0.79388001\n",
            "Iteration 84, loss = 0.78126273\n",
            "Iteration 85, loss = 0.77934363\n",
            "Iteration 86, loss = 0.76774497\n",
            "Iteration 87, loss = 0.75212096\n",
            "Iteration 88, loss = 0.74901517\n",
            "Iteration 89, loss = 0.74698404\n",
            "Iteration 90, loss = 0.74211156\n",
            "Iteration 91, loss = 0.73704917\n",
            "Iteration 92, loss = 0.74328111\n",
            "Iteration 93, loss = 0.73415416\n",
            "Iteration 94, loss = 0.71855258\n",
            "Iteration 95, loss = 0.71505530\n",
            "Iteration 96, loss = 0.71650686\n",
            "Iteration 97, loss = 0.69976213\n",
            "Iteration 98, loss = 0.73961521\n",
            "Iteration 99, loss = 0.71607463\n",
            "Iteration 100, loss = 0.71216728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38076243\n",
            "Iteration 2, loss = 1.36906153\n",
            "Iteration 3, loss = 1.34973952\n",
            "Iteration 4, loss = 1.33062563\n",
            "Iteration 5, loss = 1.31689047\n",
            "Iteration 6, loss = 1.29881233\n",
            "Iteration 7, loss = 1.28090560\n",
            "Iteration 8, loss = 1.26473153\n",
            "Iteration 9, loss = 1.24871290\n",
            "Iteration 10, loss = 1.22762880\n",
            "Iteration 11, loss = 1.20632501\n",
            "Iteration 12, loss = 1.19135782\n",
            "Iteration 13, loss = 1.18635002\n",
            "Iteration 14, loss = 1.17357574\n",
            "Iteration 15, loss = 1.16048967\n",
            "Iteration 16, loss = 1.14600390\n",
            "Iteration 17, loss = 1.13233684\n",
            "Iteration 18, loss = 1.12068274\n",
            "Iteration 19, loss = 1.10965779\n",
            "Iteration 20, loss = 1.10350381\n",
            "Iteration 21, loss = 1.09627387\n",
            "Iteration 22, loss = 1.09682580\n",
            "Iteration 23, loss = 1.07967844\n",
            "Iteration 24, loss = 1.07671138\n",
            "Iteration 25, loss = 1.06817986\n",
            "Iteration 26, loss = 1.05266311\n",
            "Iteration 27, loss = 1.04764159\n",
            "Iteration 28, loss = 1.04278330\n",
            "Iteration 29, loss = 1.03725329\n",
            "Iteration 30, loss = 1.05184546\n",
            "Iteration 31, loss = 1.04222511\n",
            "Iteration 32, loss = 1.01615590\n",
            "Iteration 33, loss = 1.03011814\n",
            "Iteration 34, loss = 1.01198797\n",
            "Iteration 35, loss = 0.99423358\n",
            "Iteration 36, loss = 0.98614630\n",
            "Iteration 37, loss = 0.97017911\n",
            "Iteration 38, loss = 0.97459183\n",
            "Iteration 39, loss = 0.96523530\n",
            "Iteration 40, loss = 0.96623455\n",
            "Iteration 41, loss = 0.97859353\n",
            "Iteration 42, loss = 0.97214611\n",
            "Iteration 43, loss = 0.94249605\n",
            "Iteration 44, loss = 0.93356396\n",
            "Iteration 45, loss = 0.93224202\n",
            "Iteration 46, loss = 0.92056045\n",
            "Iteration 47, loss = 0.91847807\n",
            "Iteration 48, loss = 0.90733969\n",
            "Iteration 49, loss = 0.89585563\n",
            "Iteration 50, loss = 0.89179489\n",
            "Iteration 51, loss = 0.88129696\n",
            "Iteration 52, loss = 0.89917010\n",
            "Iteration 53, loss = 0.88758656\n",
            "Iteration 54, loss = 0.87648274\n",
            "Iteration 55, loss = 0.87820395\n",
            "Iteration 56, loss = 0.86952082\n",
            "Iteration 57, loss = 0.86181452\n",
            "Iteration 58, loss = 0.87205100\n",
            "Iteration 59, loss = 0.85772276\n",
            "Iteration 60, loss = 0.83581106\n",
            "Iteration 61, loss = 0.83550783\n",
            "Iteration 62, loss = 0.82454474\n",
            "Iteration 63, loss = 0.82109237\n",
            "Iteration 64, loss = 0.82357139\n",
            "Iteration 65, loss = 0.80693222\n",
            "Iteration 66, loss = 0.80196158\n",
            "Iteration 67, loss = 0.80528846\n",
            "Iteration 68, loss = 0.80088835\n",
            "Iteration 69, loss = 0.78602220\n",
            "Iteration 70, loss = 0.79481014\n",
            "Iteration 71, loss = 0.78060820\n",
            "Iteration 72, loss = 0.75785391\n",
            "Iteration 73, loss = 0.75841043\n",
            "Iteration 74, loss = 0.74565735\n",
            "Iteration 75, loss = 0.75505343\n",
            "Iteration 76, loss = 0.75163263\n",
            "Iteration 77, loss = 0.75340529\n",
            "Iteration 78, loss = 0.72332488\n",
            "Iteration 79, loss = 0.73564404\n",
            "Iteration 80, loss = 0.72224459\n",
            "Iteration 81, loss = 0.74783389\n",
            "Iteration 82, loss = 0.74097853\n",
            "Iteration 83, loss = 0.75095341\n",
            "Iteration 84, loss = 0.72579019\n",
            "Iteration 85, loss = 0.71238745\n",
            "Iteration 86, loss = 0.69240392\n",
            "Iteration 87, loss = 0.68096107\n",
            "Iteration 88, loss = 0.67838822\n",
            "Iteration 89, loss = 0.66809753\n",
            "Iteration 90, loss = 0.67235293\n",
            "Iteration 91, loss = 0.66743617\n",
            "Iteration 92, loss = 0.69172729\n",
            "Iteration 93, loss = 0.67306762\n",
            "Iteration 94, loss = 0.64653800\n",
            "Iteration 95, loss = 0.65633384\n",
            "Iteration 96, loss = 0.63998644\n",
            "Iteration 97, loss = 0.65200132\n",
            "Iteration 98, loss = 0.66216725\n",
            "Iteration 99, loss = 0.65333222\n",
            "Iteration 100, loss = 0.63963895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37525933\n",
            "Iteration 2, loss = 1.36032028\n",
            "Iteration 3, loss = 1.34374053\n",
            "Iteration 4, loss = 1.32479190\n",
            "Iteration 5, loss = 1.30589780\n",
            "Iteration 6, loss = 1.28564322\n",
            "Iteration 7, loss = 1.27070568\n",
            "Iteration 8, loss = 1.25011742\n",
            "Iteration 9, loss = 1.24042730\n",
            "Iteration 10, loss = 1.21731935\n",
            "Iteration 11, loss = 1.19728199\n",
            "Iteration 12, loss = 1.19003377\n",
            "Iteration 13, loss = 1.17731095\n",
            "Iteration 14, loss = 1.17121619\n",
            "Iteration 15, loss = 1.16733290\n",
            "Iteration 16, loss = 1.15456231\n",
            "Iteration 17, loss = 1.13070423\n",
            "Iteration 18, loss = 1.11844549\n",
            "Iteration 19, loss = 1.10776555\n",
            "Iteration 20, loss = 1.09818610\n",
            "Iteration 21, loss = 1.08730451\n",
            "Iteration 22, loss = 1.08624703\n",
            "Iteration 23, loss = 1.07728706\n",
            "Iteration 24, loss = 1.06264943\n",
            "Iteration 25, loss = 1.05604743\n",
            "Iteration 26, loss = 1.04794696\n",
            "Iteration 27, loss = 1.03403166\n",
            "Iteration 28, loss = 1.03720593\n",
            "Iteration 29, loss = 1.03273995\n",
            "Iteration 30, loss = 1.04646620\n",
            "Iteration 31, loss = 1.02917847\n",
            "Iteration 32, loss = 1.01004857\n",
            "Iteration 33, loss = 0.99861018\n",
            "Iteration 34, loss = 0.99090591\n",
            "Iteration 35, loss = 0.97529800\n",
            "Iteration 36, loss = 0.97487943\n",
            "Iteration 37, loss = 0.96161258\n",
            "Iteration 38, loss = 0.96104056\n",
            "Iteration 39, loss = 0.95145817\n",
            "Iteration 40, loss = 0.96312422\n",
            "Iteration 41, loss = 0.96763408\n",
            "Iteration 42, loss = 0.96559860\n",
            "Iteration 43, loss = 0.92730865\n",
            "Iteration 44, loss = 0.93053067\n",
            "Iteration 45, loss = 0.91577590\n",
            "Iteration 46, loss = 0.90909535\n",
            "Iteration 47, loss = 0.92049936\n",
            "Iteration 48, loss = 0.89477309\n",
            "Iteration 49, loss = 0.88991629\n",
            "Iteration 50, loss = 0.87936153\n",
            "Iteration 51, loss = 0.87258563\n",
            "Iteration 52, loss = 0.88264486\n",
            "Iteration 53, loss = 0.87287962\n",
            "Iteration 54, loss = 0.85474587\n",
            "Iteration 55, loss = 0.85321211\n",
            "Iteration 56, loss = 0.85406264\n",
            "Iteration 57, loss = 0.83725810\n",
            "Iteration 58, loss = 0.84546367\n",
            "Iteration 59, loss = 0.83602471\n",
            "Iteration 60, loss = 0.81815740\n",
            "Iteration 61, loss = 0.81450911\n",
            "Iteration 62, loss = 0.80393221\n",
            "Iteration 63, loss = 0.80769765\n",
            "Iteration 64, loss = 0.80127806\n",
            "Iteration 65, loss = 0.78492624\n",
            "Iteration 66, loss = 0.77953590\n",
            "Iteration 67, loss = 0.78078791\n",
            "Iteration 68, loss = 0.78805446\n",
            "Iteration 69, loss = 0.77086251\n",
            "Iteration 70, loss = 0.78136607\n",
            "Iteration 71, loss = 0.76771359\n",
            "Iteration 72, loss = 0.74472246\n",
            "Iteration 73, loss = 0.74118280\n",
            "Iteration 74, loss = 0.73291625\n",
            "Iteration 75, loss = 0.73699333\n",
            "Iteration 76, loss = 0.75073301\n",
            "Iteration 77, loss = 0.73391662\n",
            "Iteration 78, loss = 0.72655805\n",
            "Iteration 79, loss = 0.73619704\n",
            "Iteration 80, loss = 0.72819153\n",
            "Iteration 81, loss = 0.73136566\n",
            "Iteration 82, loss = 0.71778974\n",
            "Iteration 83, loss = 0.71894689\n",
            "Iteration 84, loss = 0.69968348\n",
            "Iteration 85, loss = 0.68293674\n",
            "Iteration 86, loss = 0.68027856\n",
            "Iteration 87, loss = 0.67840131\n",
            "Iteration 88, loss = 0.66367793\n",
            "Iteration 89, loss = 0.65996648\n",
            "Iteration 90, loss = 0.66707517\n",
            "Iteration 91, loss = 0.65795760\n",
            "Iteration 92, loss = 0.66715823\n",
            "Iteration 93, loss = 0.65901179\n",
            "Iteration 94, loss = 0.65365231\n",
            "Iteration 95, loss = 0.63857393\n",
            "Iteration 96, loss = 0.63563957\n",
            "Iteration 97, loss = 0.66048293\n",
            "Iteration 98, loss = 0.64715223\n",
            "Iteration 99, loss = 0.62865782\n",
            "Iteration 100, loss = 0.62817549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.38461788\n",
            "Iteration 2, loss = 1.36143346\n",
            "Iteration 3, loss = 1.34774357\n",
            "Iteration 4, loss = 1.33283576\n",
            "Iteration 5, loss = 1.31500829\n",
            "Iteration 6, loss = 1.29481775\n",
            "Iteration 7, loss = 1.27706690\n",
            "Iteration 8, loss = 1.26142370\n",
            "Iteration 9, loss = 1.24921316\n",
            "Iteration 10, loss = 1.22514373\n",
            "Iteration 11, loss = 1.20455702\n",
            "Iteration 12, loss = 1.19630248\n",
            "Iteration 13, loss = 1.18303853\n",
            "Iteration 14, loss = 1.17027298\n",
            "Iteration 15, loss = 1.15328206\n",
            "Iteration 16, loss = 1.14112329\n",
            "Iteration 17, loss = 1.12515318\n",
            "Iteration 18, loss = 1.12163838\n",
            "Iteration 19, loss = 1.10528801\n",
            "Iteration 20, loss = 1.09759926\n",
            "Iteration 21, loss = 1.09340063\n",
            "Iteration 22, loss = 1.08160440\n",
            "Iteration 23, loss = 1.07018530\n",
            "Iteration 24, loss = 1.06581398\n",
            "Iteration 25, loss = 1.07283421\n",
            "Iteration 26, loss = 1.04465479\n",
            "Iteration 27, loss = 1.03494433\n",
            "Iteration 28, loss = 1.03155958\n",
            "Iteration 29, loss = 1.03069898\n",
            "Iteration 30, loss = 1.02835784\n",
            "Iteration 31, loss = 1.01504953\n",
            "Iteration 32, loss = 1.02338045\n",
            "Iteration 33, loss = 0.99700658\n",
            "Iteration 34, loss = 0.98873877\n",
            "Iteration 35, loss = 0.97062798\n",
            "Iteration 36, loss = 0.96964943\n",
            "Iteration 37, loss = 0.95915835\n",
            "Iteration 38, loss = 0.95120897\n",
            "Iteration 39, loss = 0.94800755\n",
            "Iteration 40, loss = 0.95237288\n",
            "Iteration 41, loss = 0.95741611\n",
            "Iteration 42, loss = 0.94944218\n",
            "Iteration 43, loss = 0.91668737\n",
            "Iteration 44, loss = 0.91558881\n",
            "Iteration 45, loss = 0.90726590\n",
            "Iteration 46, loss = 0.90604033\n",
            "Iteration 47, loss = 0.90225105\n",
            "Iteration 48, loss = 0.88151130\n",
            "Iteration 49, loss = 0.88217825\n",
            "Iteration 50, loss = 0.87553739\n",
            "Iteration 51, loss = 0.86235617\n",
            "Iteration 52, loss = 0.86759362\n",
            "Iteration 53, loss = 0.86193159\n",
            "Iteration 54, loss = 0.84707324\n",
            "Iteration 55, loss = 0.84093671\n",
            "Iteration 56, loss = 0.85217564\n",
            "Iteration 57, loss = 0.82431812\n",
            "Iteration 58, loss = 0.82239079\n",
            "Iteration 59, loss = 0.81770411\n",
            "Iteration 60, loss = 0.80143534\n",
            "Iteration 61, loss = 0.80178425\n",
            "Iteration 62, loss = 0.79969880\n",
            "Iteration 63, loss = 0.80243563\n",
            "Iteration 64, loss = 0.78015890\n",
            "Iteration 65, loss = 0.77979190\n",
            "Iteration 66, loss = 0.77147720\n",
            "Iteration 67, loss = 0.76288733\n",
            "Iteration 68, loss = 0.76508854\n",
            "Iteration 69, loss = 0.76513435\n",
            "Iteration 70, loss = 0.75971234\n",
            "Iteration 71, loss = 0.76622886\n",
            "Iteration 72, loss = 0.75153005\n",
            "Iteration 73, loss = 0.74514630\n",
            "Iteration 74, loss = 0.73835975\n",
            "Iteration 75, loss = 0.73179744\n",
            "Iteration 76, loss = 0.72704137\n",
            "Iteration 77, loss = 0.71044636\n",
            "Iteration 78, loss = 0.70700915\n",
            "Iteration 79, loss = 0.70462270\n",
            "Iteration 80, loss = 0.70811884\n",
            "Iteration 81, loss = 0.69793068\n",
            "Iteration 82, loss = 0.69308044\n",
            "Iteration 83, loss = 0.68801925\n",
            "Iteration 84, loss = 0.67857898\n",
            "Iteration 85, loss = 0.68762027\n",
            "Iteration 86, loss = 0.70540765\n",
            "Iteration 87, loss = 0.71161834\n",
            "Iteration 88, loss = 0.67144675\n",
            "Iteration 89, loss = 0.66458277\n",
            "Iteration 90, loss = 0.66207263\n",
            "Iteration 91, loss = 0.65492550\n",
            "Iteration 92, loss = 0.64580100\n",
            "Iteration 93, loss = 0.64180574\n",
            "Iteration 94, loss = 0.63613624\n",
            "Iteration 95, loss = 0.64476704\n",
            "Iteration 96, loss = 0.63474230\n",
            "Iteration 97, loss = 0.66009012\n",
            "Iteration 98, loss = 0.64340423\n",
            "Iteration 99, loss = 0.63338142\n",
            "Iteration 100, loss = 0.61788966\n",
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37182307\n",
            "Iteration 2, loss = 1.34057331\n",
            "Iteration 3, loss = 1.31008068\n",
            "Iteration 4, loss = 1.27056984\n",
            "Iteration 5, loss = 1.22103812\n",
            "Iteration 6, loss = 1.16057633\n",
            "Iteration 7, loss = 1.09409588\n",
            "Iteration 8, loss = 1.02480724\n",
            "Iteration 9, loss = 0.96004384\n",
            "Iteration 10, loss = 0.90196054\n",
            "Iteration 11, loss = 0.85361663\n",
            "Iteration 12, loss = 0.81565050\n",
            "Iteration 13, loss = 0.77659048\n",
            "Iteration 14, loss = 0.74431719\n",
            "Iteration 15, loss = 0.72032727\n",
            "Iteration 16, loss = 0.69865477\n",
            "Iteration 17, loss = 0.68544381\n",
            "Iteration 18, loss = 0.67072439\n",
            "Iteration 19, loss = 0.65939811\n",
            "Iteration 20, loss = 0.65257234\n",
            "Iteration 21, loss = 0.64723886\n",
            "Iteration 22, loss = 0.63608064\n",
            "Iteration 23, loss = 0.62435352\n",
            "Iteration 24, loss = 0.61678438\n",
            "Iteration 25, loss = 0.60849697\n",
            "Iteration 26, loss = 0.59898280\n",
            "Iteration 27, loss = 0.58773417\n",
            "Iteration 28, loss = 0.58553031\n",
            "Iteration 29, loss = 0.57784205\n",
            "Iteration 30, loss = 0.56509634\n",
            "Iteration 31, loss = 0.55544003\n",
            "Iteration 32, loss = 0.54822869\n",
            "Iteration 33, loss = 0.53879910\n",
            "Iteration 34, loss = 0.52998778\n",
            "Iteration 35, loss = 0.52883406\n",
            "Iteration 36, loss = 0.52088419\n",
            "Iteration 37, loss = 0.51091554\n",
            "Iteration 38, loss = 0.50458810\n",
            "Iteration 39, loss = 0.50350387\n",
            "Iteration 40, loss = 0.49392335\n",
            "Iteration 41, loss = 0.48318301\n",
            "Iteration 42, loss = 0.48061875\n",
            "Iteration 43, loss = 0.47505095\n",
            "Iteration 44, loss = 0.47171049\n",
            "Iteration 45, loss = 0.47140259\n",
            "Iteration 46, loss = 0.46991930\n",
            "Iteration 47, loss = 0.46061359\n",
            "Iteration 48, loss = 0.45856494\n",
            "Iteration 49, loss = 0.45206287\n",
            "Iteration 50, loss = 0.44839536\n",
            "Iteration 51, loss = 0.44353660\n",
            "Iteration 52, loss = 0.43940592\n",
            "Iteration 53, loss = 0.43686226\n",
            "Iteration 54, loss = 0.43254098\n",
            "Iteration 55, loss = 0.42869521\n",
            "Iteration 56, loss = 0.42746813\n",
            "Iteration 57, loss = 0.42622694\n",
            "Iteration 58, loss = 0.42269548\n",
            "Iteration 59, loss = 0.42045240\n",
            "Iteration 60, loss = 0.41380278\n",
            "Iteration 61, loss = 0.41301775\n",
            "Iteration 62, loss = 0.41052386\n",
            "Iteration 63, loss = 0.41145623\n",
            "Iteration 64, loss = 0.40974464\n",
            "Iteration 65, loss = 0.40655574\n",
            "Iteration 66, loss = 0.41412165\n",
            "Iteration 67, loss = 0.40426866\n",
            "Iteration 68, loss = 0.39961847\n",
            "Iteration 69, loss = 0.40422984\n",
            "Iteration 70, loss = 0.40029330\n",
            "Iteration 71, loss = 0.39363797\n",
            "Iteration 72, loss = 0.38995295\n",
            "Iteration 73, loss = 0.39376750\n",
            "Iteration 74, loss = 0.39455598\n",
            "Iteration 75, loss = 0.39458190\n",
            "Iteration 76, loss = 0.38521814\n",
            "Iteration 77, loss = 0.38331116\n",
            "Iteration 78, loss = 0.38207889\n",
            "Iteration 79, loss = 0.37961104\n",
            "Iteration 80, loss = 0.38356081\n",
            "Iteration 81, loss = 0.38968194\n",
            "Iteration 82, loss = 0.38377645\n",
            "Iteration 83, loss = 0.37933996\n",
            "Iteration 84, loss = 0.38200911\n",
            "Iteration 85, loss = 0.37386654\n",
            "Iteration 86, loss = 0.37142118\n",
            "Iteration 87, loss = 0.37027269\n",
            "Iteration 88, loss = 0.36744662\n",
            "Iteration 89, loss = 0.36452500\n",
            "Iteration 90, loss = 0.36754890\n",
            "Iteration 91, loss = 0.37012936\n",
            "Iteration 92, loss = 0.36547159\n",
            "Iteration 93, loss = 0.36442661\n",
            "Iteration 94, loss = 0.36372524\n",
            "Iteration 95, loss = 0.36155715\n",
            "Iteration 96, loss = 0.35985939\n",
            "Iteration 97, loss = 0.36790842\n",
            "Iteration 98, loss = 0.36486466\n",
            "Iteration 99, loss = 0.36503756\n",
            "Iteration 100, loss = 0.36303354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37084349\n",
            "Iteration 2, loss = 1.34114283\n",
            "Iteration 3, loss = 1.31245802\n",
            "Iteration 4, loss = 1.27569284\n",
            "Iteration 5, loss = 1.22991768\n",
            "Iteration 6, loss = 1.17215565\n",
            "Iteration 7, loss = 1.10711915\n",
            "Iteration 8, loss = 1.03903004\n",
            "Iteration 9, loss = 0.97172218\n",
            "Iteration 10, loss = 0.91334388\n",
            "Iteration 11, loss = 0.86222910\n",
            "Iteration 12, loss = 0.82180719\n",
            "Iteration 13, loss = 0.78223554\n",
            "Iteration 14, loss = 0.74733775\n",
            "Iteration 15, loss = 0.72065591\n",
            "Iteration 16, loss = 0.69973732\n",
            "Iteration 17, loss = 0.68407992\n",
            "Iteration 18, loss = 0.66898895\n",
            "Iteration 19, loss = 0.65702870\n",
            "Iteration 20, loss = 0.64824439\n",
            "Iteration 21, loss = 0.64407256\n",
            "Iteration 22, loss = 0.63342355\n",
            "Iteration 23, loss = 0.61888149\n",
            "Iteration 24, loss = 0.60880939\n",
            "Iteration 25, loss = 0.60135048\n",
            "Iteration 26, loss = 0.59193424\n",
            "Iteration 27, loss = 0.58029602\n",
            "Iteration 28, loss = 0.57495358\n",
            "Iteration 29, loss = 0.56210547\n",
            "Iteration 30, loss = 0.55530433\n",
            "Iteration 31, loss = 0.54608203\n",
            "Iteration 32, loss = 0.53738523\n",
            "Iteration 33, loss = 0.52752058\n",
            "Iteration 34, loss = 0.51959877\n",
            "Iteration 35, loss = 0.51872744\n",
            "Iteration 36, loss = 0.51363691\n",
            "Iteration 37, loss = 0.50219960\n",
            "Iteration 38, loss = 0.49607511\n",
            "Iteration 39, loss = 0.49309823\n",
            "Iteration 40, loss = 0.48436589\n",
            "Iteration 41, loss = 0.47640447\n",
            "Iteration 42, loss = 0.47135407\n",
            "Iteration 43, loss = 0.46746395\n",
            "Iteration 44, loss = 0.46345819\n",
            "Iteration 45, loss = 0.46234705\n",
            "Iteration 46, loss = 0.46503523\n",
            "Iteration 47, loss = 0.45627632\n",
            "Iteration 48, loss = 0.45177019\n",
            "Iteration 49, loss = 0.44564201\n",
            "Iteration 50, loss = 0.44251483\n",
            "Iteration 51, loss = 0.43978963\n",
            "Iteration 52, loss = 0.43430532\n",
            "Iteration 53, loss = 0.42833548\n",
            "Iteration 54, loss = 0.42734291\n",
            "Iteration 55, loss = 0.42769532\n",
            "Iteration 56, loss = 0.42638615\n",
            "Iteration 57, loss = 0.42366681\n",
            "Iteration 58, loss = 0.42354702\n",
            "Iteration 59, loss = 0.41494092\n",
            "Iteration 60, loss = 0.40935332\n",
            "Iteration 61, loss = 0.41288607\n",
            "Iteration 62, loss = 0.40736050\n",
            "Iteration 63, loss = 0.41039641\n",
            "Iteration 64, loss = 0.41087952\n",
            "Iteration 65, loss = 0.40806468\n",
            "Iteration 66, loss = 0.41104016\n",
            "Iteration 67, loss = 0.40868815\n",
            "Iteration 68, loss = 0.39982034\n",
            "Iteration 69, loss = 0.40242932\n",
            "Iteration 70, loss = 0.40284034\n",
            "Iteration 71, loss = 0.39077952\n",
            "Iteration 72, loss = 0.38740058\n",
            "Iteration 73, loss = 0.39001324\n",
            "Iteration 74, loss = 0.38931449\n",
            "Iteration 75, loss = 0.38864386\n",
            "Iteration 76, loss = 0.38384318\n",
            "Iteration 77, loss = 0.38103081\n",
            "Iteration 78, loss = 0.37711279\n",
            "Iteration 79, loss = 0.37392796\n",
            "Iteration 80, loss = 0.38104100\n",
            "Iteration 81, loss = 0.39191665\n",
            "Iteration 82, loss = 0.38612658\n",
            "Iteration 83, loss = 0.37492113\n",
            "Iteration 84, loss = 0.37912826\n",
            "Iteration 85, loss = 0.37251648\n",
            "Iteration 86, loss = 0.36713561\n",
            "Iteration 87, loss = 0.36657784\n",
            "Iteration 88, loss = 0.36213964\n",
            "Iteration 89, loss = 0.36100697\n",
            "Iteration 90, loss = 0.36474151\n",
            "Iteration 91, loss = 0.37192659\n",
            "Iteration 92, loss = 0.36957010\n",
            "Iteration 93, loss = 0.36141834\n",
            "Iteration 94, loss = 0.36490663\n",
            "Iteration 95, loss = 0.35888311\n",
            "Iteration 96, loss = 0.35853101\n",
            "Iteration 97, loss = 0.36500972\n",
            "Iteration 98, loss = 0.35965013\n",
            "Iteration 99, loss = 0.36167517\n",
            "Iteration 100, loss = 0.36962084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36994105\n",
            "Iteration 2, loss = 1.33954817\n",
            "Iteration 3, loss = 1.30988955\n",
            "Iteration 4, loss = 1.27048725\n",
            "Iteration 5, loss = 1.22333366\n",
            "Iteration 6, loss = 1.16752357\n",
            "Iteration 7, loss = 1.10216712\n",
            "Iteration 8, loss = 1.03483885\n",
            "Iteration 9, loss = 0.97115605\n",
            "Iteration 10, loss = 0.91351462\n",
            "Iteration 11, loss = 0.86578394\n",
            "Iteration 12, loss = 0.82572008\n",
            "Iteration 13, loss = 0.78530872\n",
            "Iteration 14, loss = 0.75067616\n",
            "Iteration 15, loss = 0.72455005\n",
            "Iteration 16, loss = 0.70458620\n",
            "Iteration 17, loss = 0.68640438\n",
            "Iteration 18, loss = 0.67025559\n",
            "Iteration 19, loss = 0.65895431\n",
            "Iteration 20, loss = 0.65134133\n",
            "Iteration 21, loss = 0.64859365\n",
            "Iteration 22, loss = 0.63601432\n",
            "Iteration 23, loss = 0.62099880\n",
            "Iteration 24, loss = 0.61321610\n",
            "Iteration 25, loss = 0.60257524\n",
            "Iteration 26, loss = 0.59074859\n",
            "Iteration 27, loss = 0.57909879\n",
            "Iteration 28, loss = 0.57212819\n",
            "Iteration 29, loss = 0.55989281\n",
            "Iteration 30, loss = 0.55194004\n",
            "Iteration 31, loss = 0.54600019\n",
            "Iteration 32, loss = 0.53442398\n",
            "Iteration 33, loss = 0.52677517\n",
            "Iteration 34, loss = 0.51899756\n",
            "Iteration 35, loss = 0.51217852\n",
            "Iteration 36, loss = 0.50594171\n",
            "Iteration 37, loss = 0.49609462\n",
            "Iteration 38, loss = 0.49084007\n",
            "Iteration 39, loss = 0.49041492\n",
            "Iteration 40, loss = 0.47917252\n",
            "Iteration 41, loss = 0.47090939\n",
            "Iteration 42, loss = 0.46934364\n",
            "Iteration 43, loss = 0.46658077\n",
            "Iteration 44, loss = 0.46372153\n",
            "Iteration 45, loss = 0.45905630\n",
            "Iteration 46, loss = 0.45663095\n",
            "Iteration 47, loss = 0.45050149\n",
            "Iteration 48, loss = 0.44346067\n",
            "Iteration 49, loss = 0.44308232\n",
            "Iteration 50, loss = 0.43609860\n",
            "Iteration 51, loss = 0.43300306\n",
            "Iteration 52, loss = 0.42595678\n",
            "Iteration 53, loss = 0.42280760\n",
            "Iteration 54, loss = 0.42366472\n",
            "Iteration 55, loss = 0.42316095\n",
            "Iteration 56, loss = 0.42063730\n",
            "Iteration 57, loss = 0.41735939\n",
            "Iteration 58, loss = 0.41100370\n",
            "Iteration 59, loss = 0.40727791\n",
            "Iteration 60, loss = 0.40384368\n",
            "Iteration 61, loss = 0.40817974\n",
            "Iteration 62, loss = 0.40675484\n",
            "Iteration 63, loss = 0.40314294\n",
            "Iteration 64, loss = 0.40184942\n",
            "Iteration 65, loss = 0.39941793\n",
            "Iteration 66, loss = 0.39881505\n",
            "Iteration 67, loss = 0.40038417\n",
            "Iteration 68, loss = 0.39386849\n",
            "Iteration 69, loss = 0.39498813\n",
            "Iteration 70, loss = 0.39912834\n",
            "Iteration 71, loss = 0.39050906\n",
            "Iteration 72, loss = 0.38220895\n",
            "Iteration 73, loss = 0.38741279\n",
            "Iteration 74, loss = 0.38557345\n",
            "Iteration 75, loss = 0.38102827\n",
            "Iteration 76, loss = 0.37845639\n",
            "Iteration 77, loss = 0.37695187\n",
            "Iteration 78, loss = 0.37359582\n",
            "Iteration 79, loss = 0.36907511\n",
            "Iteration 80, loss = 0.37109427\n",
            "Iteration 81, loss = 0.37267368\n",
            "Iteration 82, loss = 0.37260997\n",
            "Iteration 83, loss = 0.36909976\n",
            "Iteration 84, loss = 0.36992669\n",
            "Iteration 85, loss = 0.36637790\n",
            "Iteration 86, loss = 0.36309617\n",
            "Iteration 87, loss = 0.36230159\n",
            "Iteration 88, loss = 0.35733555\n",
            "Iteration 89, loss = 0.35679837\n",
            "Iteration 90, loss = 0.35913965\n",
            "Iteration 91, loss = 0.36062447\n",
            "Iteration 92, loss = 0.35957638\n",
            "Iteration 93, loss = 0.36252255\n",
            "Iteration 94, loss = 0.35613726\n",
            "Iteration 95, loss = 0.35661309\n",
            "Iteration 96, loss = 0.35202411\n",
            "Iteration 97, loss = 0.35362595\n",
            "Iteration 98, loss = 0.35434316\n",
            "Iteration 99, loss = 0.36232575\n",
            "Iteration 100, loss = 0.36513767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37031719\n",
            "Iteration 2, loss = 1.34091927\n",
            "Iteration 3, loss = 1.31020369\n",
            "Iteration 4, loss = 1.27390930\n",
            "Iteration 5, loss = 1.22597261\n",
            "Iteration 6, loss = 1.17049667\n",
            "Iteration 7, loss = 1.10376757\n",
            "Iteration 8, loss = 1.03394322\n",
            "Iteration 9, loss = 0.96851114\n",
            "Iteration 10, loss = 0.90911769\n",
            "Iteration 11, loss = 0.87088121\n",
            "Iteration 12, loss = 0.82009762\n",
            "Iteration 13, loss = 0.77961281\n",
            "Iteration 14, loss = 0.74860135\n",
            "Iteration 15, loss = 0.72376000\n",
            "Iteration 16, loss = 0.70395769\n",
            "Iteration 17, loss = 0.68678673\n",
            "Iteration 18, loss = 0.67204565\n",
            "Iteration 19, loss = 0.66201459\n",
            "Iteration 20, loss = 0.65374612\n",
            "Iteration 21, loss = 0.64598433\n",
            "Iteration 22, loss = 0.63433525\n",
            "Iteration 23, loss = 0.62392747\n",
            "Iteration 24, loss = 0.61472853\n",
            "Iteration 25, loss = 0.60291774\n",
            "Iteration 26, loss = 0.59418754\n",
            "Iteration 27, loss = 0.58247435\n",
            "Iteration 28, loss = 0.57466915\n",
            "Iteration 29, loss = 0.56390287\n",
            "Iteration 30, loss = 0.55649153\n",
            "Iteration 31, loss = 0.54989090\n",
            "Iteration 32, loss = 0.53492581\n",
            "Iteration 33, loss = 0.52881795\n",
            "Iteration 34, loss = 0.52013574\n",
            "Iteration 35, loss = 0.51374293\n",
            "Iteration 36, loss = 0.50548134\n",
            "Iteration 37, loss = 0.49678077\n",
            "Iteration 38, loss = 0.49023271\n",
            "Iteration 39, loss = 0.48724834\n",
            "Iteration 40, loss = 0.47963108\n",
            "Iteration 41, loss = 0.46908700\n",
            "Iteration 42, loss = 0.46851638\n",
            "Iteration 43, loss = 0.46403998\n",
            "Iteration 44, loss = 0.46146964\n",
            "Iteration 45, loss = 0.45742135\n",
            "Iteration 46, loss = 0.45066517\n",
            "Iteration 47, loss = 0.44368049\n",
            "Iteration 48, loss = 0.43880953\n",
            "Iteration 49, loss = 0.43552037\n",
            "Iteration 50, loss = 0.43246001\n",
            "Iteration 51, loss = 0.42965021\n",
            "Iteration 52, loss = 0.42115425\n",
            "Iteration 53, loss = 0.42148318\n",
            "Iteration 54, loss = 0.42308803\n",
            "Iteration 55, loss = 0.42343100\n",
            "Iteration 56, loss = 0.41847949\n",
            "Iteration 57, loss = 0.41744765\n",
            "Iteration 58, loss = 0.40891045\n",
            "Iteration 59, loss = 0.40362866\n",
            "Iteration 60, loss = 0.39932520\n",
            "Iteration 61, loss = 0.40374037\n",
            "Iteration 62, loss = 0.40214634\n",
            "Iteration 63, loss = 0.39747886\n",
            "Iteration 64, loss = 0.39858391\n",
            "Iteration 65, loss = 0.39786562\n",
            "Iteration 66, loss = 0.39568555\n",
            "Iteration 67, loss = 0.39921382\n",
            "Iteration 68, loss = 0.38819747\n",
            "Iteration 69, loss = 0.38393477\n",
            "Iteration 70, loss = 0.38503761\n",
            "Iteration 71, loss = 0.38320884\n",
            "Iteration 72, loss = 0.37989279\n",
            "Iteration 73, loss = 0.38507556\n",
            "Iteration 74, loss = 0.37648684\n",
            "Iteration 75, loss = 0.37500676\n",
            "Iteration 76, loss = 0.36922879\n",
            "Iteration 77, loss = 0.36986678\n",
            "Iteration 78, loss = 0.36900311\n",
            "Iteration 79, loss = 0.36736135\n",
            "Iteration 80, loss = 0.36370219\n",
            "Iteration 81, loss = 0.36881066\n",
            "Iteration 82, loss = 0.37018797\n",
            "Iteration 83, loss = 0.36482712\n",
            "Iteration 84, loss = 0.36835067\n",
            "Iteration 85, loss = 0.36040201\n",
            "Iteration 86, loss = 0.36018560\n",
            "Iteration 87, loss = 0.36213737\n",
            "Iteration 88, loss = 0.35419056\n",
            "Iteration 89, loss = 0.35448555\n",
            "Iteration 90, loss = 0.35433246\n",
            "Iteration 91, loss = 0.35627826\n",
            "Iteration 92, loss = 0.35375887\n",
            "Iteration 93, loss = 0.35456385\n",
            "Iteration 94, loss = 0.35280718\n",
            "Iteration 95, loss = 0.35158860\n",
            "Iteration 96, loss = 0.34980019\n",
            "Iteration 97, loss = 0.35130312\n",
            "Iteration 98, loss = 0.35009577\n",
            "Iteration 99, loss = 0.35501808\n",
            "Iteration 100, loss = 0.35476938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37005178\n",
            "Iteration 2, loss = 1.33720952\n",
            "Iteration 3, loss = 1.30414373\n",
            "Iteration 4, loss = 1.26385190\n",
            "Iteration 5, loss = 1.21097860\n",
            "Iteration 6, loss = 1.14960082\n",
            "Iteration 7, loss = 1.07656100\n",
            "Iteration 8, loss = 0.99891159\n",
            "Iteration 9, loss = 0.92583496\n",
            "Iteration 10, loss = 0.85783867\n",
            "Iteration 11, loss = 0.80812114\n",
            "Iteration 12, loss = 0.76138903\n",
            "Iteration 13, loss = 0.71867111\n",
            "Iteration 14, loss = 0.68650565\n",
            "Iteration 15, loss = 0.65884826\n",
            "Iteration 16, loss = 0.64108555\n",
            "Iteration 17, loss = 0.62479724\n",
            "Iteration 18, loss = 0.61423250\n",
            "Iteration 19, loss = 0.60775490\n",
            "Iteration 20, loss = 0.59661194\n",
            "Iteration 21, loss = 0.59088940\n",
            "Iteration 22, loss = 0.58479085\n",
            "Iteration 23, loss = 0.57348321\n",
            "Iteration 24, loss = 0.57055192\n",
            "Iteration 25, loss = 0.56543967\n",
            "Iteration 26, loss = 0.56361169\n",
            "Iteration 27, loss = 0.55395900\n",
            "Iteration 28, loss = 0.54839800\n",
            "Iteration 29, loss = 0.53460016\n",
            "Iteration 30, loss = 0.53210893\n",
            "Iteration 31, loss = 0.52247777\n",
            "Iteration 32, loss = 0.51462099\n",
            "Iteration 33, loss = 0.50843468\n",
            "Iteration 34, loss = 0.50027006\n",
            "Iteration 35, loss = 0.49628171\n",
            "Iteration 36, loss = 0.48890027\n",
            "Iteration 37, loss = 0.48233296\n",
            "Iteration 38, loss = 0.47578224\n",
            "Iteration 39, loss = 0.47425202\n",
            "Iteration 40, loss = 0.46680804\n",
            "Iteration 41, loss = 0.45822413\n",
            "Iteration 42, loss = 0.45995911\n",
            "Iteration 43, loss = 0.45025828\n",
            "Iteration 44, loss = 0.44827517\n",
            "Iteration 45, loss = 0.44509908\n",
            "Iteration 46, loss = 0.43108297\n",
            "Iteration 47, loss = 0.42922416\n",
            "Iteration 48, loss = 0.42590611\n",
            "Iteration 49, loss = 0.42052977\n",
            "Iteration 50, loss = 0.41592021\n",
            "Iteration 51, loss = 0.41274618\n",
            "Iteration 52, loss = 0.41019811\n",
            "Iteration 53, loss = 0.41380369\n",
            "Iteration 54, loss = 0.40685531\n",
            "Iteration 55, loss = 0.40600595\n",
            "Iteration 56, loss = 0.40389437\n",
            "Iteration 57, loss = 0.39631958\n",
            "Iteration 58, loss = 0.38791611\n",
            "Iteration 59, loss = 0.38565257\n",
            "Iteration 60, loss = 0.38689411\n",
            "Iteration 61, loss = 0.38824860\n",
            "Iteration 62, loss = 0.38326862\n",
            "Iteration 63, loss = 0.38248933\n",
            "Iteration 64, loss = 0.37925979\n",
            "Iteration 65, loss = 0.37271815\n",
            "Iteration 66, loss = 0.37211266\n",
            "Iteration 67, loss = 0.37108738\n",
            "Iteration 68, loss = 0.36540874\n",
            "Iteration 69, loss = 0.36445730\n",
            "Iteration 70, loss = 0.36308770\n",
            "Iteration 71, loss = 0.36210694\n",
            "Iteration 72, loss = 0.35400945\n",
            "Iteration 73, loss = 0.35897935\n",
            "Iteration 74, loss = 0.35665503\n",
            "Iteration 75, loss = 0.35944290\n",
            "Iteration 76, loss = 0.35063399\n",
            "Iteration 77, loss = 0.35231200\n",
            "Iteration 78, loss = 0.34705318\n",
            "Iteration 79, loss = 0.34381476\n",
            "Iteration 80, loss = 0.34261960\n",
            "Iteration 81, loss = 0.34109977\n",
            "Iteration 82, loss = 0.34753990\n",
            "Iteration 83, loss = 0.35612174\n",
            "Iteration 84, loss = 0.35094153\n",
            "Iteration 85, loss = 0.34148109\n",
            "Iteration 86, loss = 0.34400203\n",
            "Iteration 87, loss = 0.33706449\n",
            "Iteration 88, loss = 0.33430251\n",
            "Iteration 89, loss = 0.33301440\n",
            "Iteration 90, loss = 0.33512349\n",
            "Iteration 91, loss = 0.33449825\n",
            "Iteration 92, loss = 0.33058919\n",
            "Iteration 93, loss = 0.33521092\n",
            "Iteration 94, loss = 0.33130835\n",
            "Iteration 95, loss = 0.32807541\n",
            "Iteration 96, loss = 0.32837178\n",
            "Iteration 97, loss = 0.32974286\n",
            "Iteration 98, loss = 0.32973245\n",
            "Iteration 99, loss = 0.33080596\n",
            "Iteration 100, loss = 0.32974063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36378269\n",
            "Iteration 2, loss = 1.33510360\n",
            "Iteration 3, loss = 1.30526587\n",
            "Iteration 4, loss = 1.26991712\n",
            "Iteration 5, loss = 1.22088717\n",
            "Iteration 6, loss = 1.16558163\n",
            "Iteration 7, loss = 1.09952873\n",
            "Iteration 8, loss = 1.02662193\n",
            "Iteration 9, loss = 0.95277170\n",
            "Iteration 10, loss = 0.88465826\n",
            "Iteration 11, loss = 0.82735865\n",
            "Iteration 12, loss = 0.78948813\n",
            "Iteration 13, loss = 0.74727338\n",
            "Iteration 14, loss = 0.71407415\n",
            "Iteration 15, loss = 0.68872273\n",
            "Iteration 16, loss = 0.67195537\n",
            "Iteration 17, loss = 0.65808904\n",
            "Iteration 18, loss = 0.64832781\n",
            "Iteration 19, loss = 0.64244564\n",
            "Iteration 20, loss = 0.62532196\n",
            "Iteration 21, loss = 0.61697588\n",
            "Iteration 22, loss = 0.60680309\n",
            "Iteration 23, loss = 0.60010961\n",
            "Iteration 24, loss = 0.59110874\n",
            "Iteration 25, loss = 0.58043336\n",
            "Iteration 26, loss = 0.57859465\n",
            "Iteration 27, loss = 0.56509610\n",
            "Iteration 28, loss = 0.55904144\n",
            "Iteration 29, loss = 0.54671975\n",
            "Iteration 30, loss = 0.53991905\n",
            "Iteration 31, loss = 0.52875235\n",
            "Iteration 32, loss = 0.51978022\n",
            "Iteration 33, loss = 0.51267958\n",
            "Iteration 34, loss = 0.50121182\n",
            "Iteration 35, loss = 0.49650924\n",
            "Iteration 36, loss = 0.49094909\n",
            "Iteration 37, loss = 0.48081588\n",
            "Iteration 38, loss = 0.47818552\n",
            "Iteration 39, loss = 0.46990571\n",
            "Iteration 40, loss = 0.46261469\n",
            "Iteration 41, loss = 0.45808259\n",
            "Iteration 42, loss = 0.46051126\n",
            "Iteration 43, loss = 0.45128395\n",
            "Iteration 44, loss = 0.44908834\n",
            "Iteration 45, loss = 0.44534834\n",
            "Iteration 46, loss = 0.43436253\n",
            "Iteration 47, loss = 0.43293622\n",
            "Iteration 48, loss = 0.43300546\n",
            "Iteration 49, loss = 0.42980479\n",
            "Iteration 50, loss = 0.42487557\n",
            "Iteration 51, loss = 0.41878283\n",
            "Iteration 52, loss = 0.41598658\n",
            "Iteration 53, loss = 0.41706975\n",
            "Iteration 54, loss = 0.41506163\n",
            "Iteration 55, loss = 0.41545387\n",
            "Iteration 56, loss = 0.41803294\n",
            "Iteration 57, loss = 0.40951058\n",
            "Iteration 58, loss = 0.40339241\n",
            "Iteration 59, loss = 0.40124722\n",
            "Iteration 60, loss = 0.40022161\n",
            "Iteration 61, loss = 0.40101473\n",
            "Iteration 62, loss = 0.39788581\n",
            "Iteration 63, loss = 0.39535968\n",
            "Iteration 64, loss = 0.39511517\n",
            "Iteration 65, loss = 0.39065558\n",
            "Iteration 66, loss = 0.39296527\n",
            "Iteration 67, loss = 0.39042556\n",
            "Iteration 68, loss = 0.38244285\n",
            "Iteration 69, loss = 0.38702580\n",
            "Iteration 70, loss = 0.38622524\n",
            "Iteration 71, loss = 0.38449746\n",
            "Iteration 72, loss = 0.38310771\n",
            "Iteration 73, loss = 0.37638495\n",
            "Iteration 74, loss = 0.37503315\n",
            "Iteration 75, loss = 0.37622346\n",
            "Iteration 76, loss = 0.37171719\n",
            "Iteration 77, loss = 0.37192873\n",
            "Iteration 78, loss = 0.37159622\n",
            "Iteration 79, loss = 0.36918041\n",
            "Iteration 80, loss = 0.36819556\n",
            "Iteration 81, loss = 0.37240216\n",
            "Iteration 82, loss = 0.37402919\n",
            "Iteration 83, loss = 0.37469209\n",
            "Iteration 84, loss = 0.37187928\n",
            "Iteration 85, loss = 0.36360239\n",
            "Iteration 86, loss = 0.37491915\n",
            "Iteration 87, loss = 0.37297268\n",
            "Iteration 88, loss = 0.37267425\n",
            "Iteration 89, loss = 0.36388125\n",
            "Iteration 90, loss = 0.35755599\n",
            "Iteration 91, loss = 0.36014092\n",
            "Iteration 92, loss = 0.35749148\n",
            "Iteration 93, loss = 0.35609793\n",
            "Iteration 94, loss = 0.35958494\n",
            "Iteration 95, loss = 0.35437906\n",
            "Iteration 96, loss = 0.35528842\n",
            "Iteration 97, loss = 0.36304851\n",
            "Iteration 98, loss = 0.35824436\n",
            "Iteration 99, loss = 0.35343380\n",
            "Iteration 100, loss = 0.35706217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35318572\n",
            "Iteration 2, loss = 1.32580826\n",
            "Iteration 3, loss = 1.29951490\n",
            "Iteration 4, loss = 1.26979052\n",
            "Iteration 5, loss = 1.22924020\n",
            "Iteration 6, loss = 1.18316526\n",
            "Iteration 7, loss = 1.12763627\n",
            "Iteration 8, loss = 1.06278239\n",
            "Iteration 9, loss = 0.98908286\n",
            "Iteration 10, loss = 0.91571614\n",
            "Iteration 11, loss = 0.85209553\n",
            "Iteration 12, loss = 0.80867266\n",
            "Iteration 13, loss = 0.76727660\n",
            "Iteration 14, loss = 0.73397081\n",
            "Iteration 15, loss = 0.70702144\n",
            "Iteration 16, loss = 0.68946062\n",
            "Iteration 17, loss = 0.67545544\n",
            "Iteration 18, loss = 0.66684960\n",
            "Iteration 19, loss = 0.66018143\n",
            "Iteration 20, loss = 0.64242679\n",
            "Iteration 21, loss = 0.63308258\n",
            "Iteration 22, loss = 0.62340197\n",
            "Iteration 23, loss = 0.61752447\n",
            "Iteration 24, loss = 0.60932594\n",
            "Iteration 25, loss = 0.59998480\n",
            "Iteration 26, loss = 0.59353568\n",
            "Iteration 27, loss = 0.57970872\n",
            "Iteration 28, loss = 0.57463229\n",
            "Iteration 29, loss = 0.56620632\n",
            "Iteration 30, loss = 0.55708934\n",
            "Iteration 31, loss = 0.54433391\n",
            "Iteration 32, loss = 0.53613392\n",
            "Iteration 33, loss = 0.52675555\n",
            "Iteration 34, loss = 0.51678368\n",
            "Iteration 35, loss = 0.51101513\n",
            "Iteration 36, loss = 0.50143815\n",
            "Iteration 37, loss = 0.49356071\n",
            "Iteration 38, loss = 0.48834283\n",
            "Iteration 39, loss = 0.48014847\n",
            "Iteration 40, loss = 0.47361745\n",
            "Iteration 41, loss = 0.47312632\n",
            "Iteration 42, loss = 0.47354625\n",
            "Iteration 43, loss = 0.46390287\n",
            "Iteration 44, loss = 0.46058481\n",
            "Iteration 45, loss = 0.45440486\n",
            "Iteration 46, loss = 0.44360832\n",
            "Iteration 47, loss = 0.43944653\n",
            "Iteration 48, loss = 0.43802340\n",
            "Iteration 49, loss = 0.43351790\n",
            "Iteration 50, loss = 0.43312819\n",
            "Iteration 51, loss = 0.42456287\n",
            "Iteration 52, loss = 0.42186096\n",
            "Iteration 53, loss = 0.42386162\n",
            "Iteration 54, loss = 0.42587433\n",
            "Iteration 55, loss = 0.42392941\n",
            "Iteration 56, loss = 0.43069206\n",
            "Iteration 57, loss = 0.41931964\n",
            "Iteration 58, loss = 0.40645397\n",
            "Iteration 59, loss = 0.40611520\n",
            "Iteration 60, loss = 0.39945332\n",
            "Iteration 61, loss = 0.40177435\n",
            "Iteration 62, loss = 0.40043582\n",
            "Iteration 63, loss = 0.39788970\n",
            "Iteration 64, loss = 0.39969128\n",
            "Iteration 65, loss = 0.39182568\n",
            "Iteration 66, loss = 0.39628820\n",
            "Iteration 67, loss = 0.40022506\n",
            "Iteration 68, loss = 0.39290602\n",
            "Iteration 69, loss = 0.38948397\n",
            "Iteration 70, loss = 0.38715462\n",
            "Iteration 71, loss = 0.38470254\n",
            "Iteration 72, loss = 0.38340765\n",
            "Iteration 73, loss = 0.37938418\n",
            "Iteration 74, loss = 0.38111107\n",
            "Iteration 75, loss = 0.38045766\n",
            "Iteration 76, loss = 0.37903839\n",
            "Iteration 77, loss = 0.37779682\n",
            "Iteration 78, loss = 0.37583869\n",
            "Iteration 79, loss = 0.37243321\n",
            "Iteration 80, loss = 0.37050522\n",
            "Iteration 81, loss = 0.37440521\n",
            "Iteration 82, loss = 0.37924662\n",
            "Iteration 83, loss = 0.38848917\n",
            "Iteration 84, loss = 0.38695942\n",
            "Iteration 85, loss = 0.37284482\n",
            "Iteration 86, loss = 0.37016742\n",
            "Iteration 87, loss = 0.37376121\n",
            "Iteration 88, loss = 0.37496538\n",
            "Iteration 89, loss = 0.36741243\n",
            "Iteration 90, loss = 0.36220153\n",
            "Iteration 91, loss = 0.36239758\n",
            "Iteration 92, loss = 0.35947631\n",
            "Iteration 93, loss = 0.36088816\n",
            "Iteration 94, loss = 0.36351433\n",
            "Iteration 95, loss = 0.36272749\n",
            "Iteration 96, loss = 0.36022051\n",
            "Iteration 97, loss = 0.36329230\n",
            "Iteration 98, loss = 0.35698649\n",
            "Iteration 99, loss = 0.35736810\n",
            "Iteration 100, loss = 0.35879046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36735153\n",
            "Iteration 2, loss = 1.33824681\n",
            "Iteration 3, loss = 1.30829805\n",
            "Iteration 4, loss = 1.27162836\n",
            "Iteration 5, loss = 1.22549882\n",
            "Iteration 6, loss = 1.17014349\n",
            "Iteration 7, loss = 1.10609323\n",
            "Iteration 8, loss = 1.03733234\n",
            "Iteration 9, loss = 0.96758336\n",
            "Iteration 10, loss = 0.90381500\n",
            "Iteration 11, loss = 0.84817423\n",
            "Iteration 12, loss = 0.80751070\n",
            "Iteration 13, loss = 0.76657879\n",
            "Iteration 14, loss = 0.73117524\n",
            "Iteration 15, loss = 0.70346288\n",
            "Iteration 16, loss = 0.68716835\n",
            "Iteration 17, loss = 0.66900539\n",
            "Iteration 18, loss = 0.66194667\n",
            "Iteration 19, loss = 0.65451082\n",
            "Iteration 20, loss = 0.63787153\n",
            "Iteration 21, loss = 0.63040419\n",
            "Iteration 22, loss = 0.61884523\n",
            "Iteration 23, loss = 0.61411478\n",
            "Iteration 24, loss = 0.60447686\n",
            "Iteration 25, loss = 0.59356875\n",
            "Iteration 26, loss = 0.58583967\n",
            "Iteration 27, loss = 0.57401532\n",
            "Iteration 28, loss = 0.56649558\n",
            "Iteration 29, loss = 0.55917549\n",
            "Iteration 30, loss = 0.55209376\n",
            "Iteration 31, loss = 0.53821981\n",
            "Iteration 32, loss = 0.53177438\n",
            "Iteration 33, loss = 0.52251271\n",
            "Iteration 34, loss = 0.51282260\n",
            "Iteration 35, loss = 0.50589455\n",
            "Iteration 36, loss = 0.49758553\n",
            "Iteration 37, loss = 0.49131770\n",
            "Iteration 38, loss = 0.48658371\n",
            "Iteration 39, loss = 0.47593617\n",
            "Iteration 40, loss = 0.46862733\n",
            "Iteration 41, loss = 0.46746932\n",
            "Iteration 42, loss = 0.46448516\n",
            "Iteration 43, loss = 0.45816200\n",
            "Iteration 44, loss = 0.44962580\n",
            "Iteration 45, loss = 0.44973901\n",
            "Iteration 46, loss = 0.43798903\n",
            "Iteration 47, loss = 0.43309351\n",
            "Iteration 48, loss = 0.43291346\n",
            "Iteration 49, loss = 0.42973620\n",
            "Iteration 50, loss = 0.42489126\n",
            "Iteration 51, loss = 0.41776847\n",
            "Iteration 52, loss = 0.41507538\n",
            "Iteration 53, loss = 0.42113783\n",
            "Iteration 54, loss = 0.41674780\n",
            "Iteration 55, loss = 0.41533253\n",
            "Iteration 56, loss = 0.41921539\n",
            "Iteration 57, loss = 0.40829644\n",
            "Iteration 58, loss = 0.39804777\n",
            "Iteration 59, loss = 0.39550480\n",
            "Iteration 60, loss = 0.39407057\n",
            "Iteration 61, loss = 0.39355351\n",
            "Iteration 62, loss = 0.39327930\n",
            "Iteration 63, loss = 0.38694729\n",
            "Iteration 64, loss = 0.38832661\n",
            "Iteration 65, loss = 0.38123927\n",
            "Iteration 66, loss = 0.39105053\n",
            "Iteration 67, loss = 0.38536690\n",
            "Iteration 68, loss = 0.38027969\n",
            "Iteration 69, loss = 0.37488502\n",
            "Iteration 70, loss = 0.37332716\n",
            "Iteration 71, loss = 0.37111219\n",
            "Iteration 72, loss = 0.37165989\n",
            "Iteration 73, loss = 0.36982985\n",
            "Iteration 74, loss = 0.36790766\n",
            "Iteration 75, loss = 0.36698842\n",
            "Iteration 76, loss = 0.36689644\n",
            "Iteration 77, loss = 0.36760293\n",
            "Iteration 78, loss = 0.36138259\n",
            "Iteration 79, loss = 0.35922531\n",
            "Iteration 80, loss = 0.35814079\n",
            "Iteration 81, loss = 0.36711428\n",
            "Iteration 82, loss = 0.37362825\n",
            "Iteration 83, loss = 0.37969066\n",
            "Iteration 84, loss = 0.36810922\n",
            "Iteration 85, loss = 0.35530618\n",
            "Iteration 86, loss = 0.37570982\n",
            "Iteration 87, loss = 0.36888519\n",
            "Iteration 88, loss = 0.37491513\n",
            "Iteration 89, loss = 0.35112384\n",
            "Iteration 90, loss = 0.35136750\n",
            "Iteration 91, loss = 0.34704442\n",
            "Iteration 92, loss = 0.34437858\n",
            "Iteration 93, loss = 0.34478117\n",
            "Iteration 94, loss = 0.34369988\n",
            "Iteration 95, loss = 0.34693078\n",
            "Iteration 96, loss = 0.34468255\n",
            "Iteration 97, loss = 0.34551099\n",
            "Iteration 98, loss = 0.34856635\n",
            "Iteration 99, loss = 0.34426208\n",
            "Iteration 100, loss = 0.34977431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36706070\n",
            "Iteration 2, loss = 1.33759960\n",
            "Iteration 3, loss = 1.30758095\n",
            "Iteration 4, loss = 1.26944910\n",
            "Iteration 5, loss = 1.22258892\n",
            "Iteration 6, loss = 1.16581783\n",
            "Iteration 7, loss = 1.10150036\n",
            "Iteration 8, loss = 1.03458981\n",
            "Iteration 9, loss = 0.96909023\n",
            "Iteration 10, loss = 0.90950211\n",
            "Iteration 11, loss = 0.85676471\n",
            "Iteration 12, loss = 0.81155198\n",
            "Iteration 13, loss = 0.77233456\n",
            "Iteration 14, loss = 0.73417697\n",
            "Iteration 15, loss = 0.70617914\n",
            "Iteration 16, loss = 0.69091915\n",
            "Iteration 17, loss = 0.67294715\n",
            "Iteration 18, loss = 0.66820127\n",
            "Iteration 19, loss = 0.65635241\n",
            "Iteration 20, loss = 0.64224070\n",
            "Iteration 21, loss = 0.63210451\n",
            "Iteration 22, loss = 0.62394359\n",
            "Iteration 23, loss = 0.61557263\n",
            "Iteration 24, loss = 0.60900115\n",
            "Iteration 25, loss = 0.59885958\n",
            "Iteration 26, loss = 0.59179928\n",
            "Iteration 27, loss = 0.58011624\n",
            "Iteration 28, loss = 0.57435180\n",
            "Iteration 29, loss = 0.56791314\n",
            "Iteration 30, loss = 0.55822594\n",
            "Iteration 31, loss = 0.54787844\n",
            "Iteration 32, loss = 0.53950135\n",
            "Iteration 33, loss = 0.53149586\n",
            "Iteration 34, loss = 0.52198618\n",
            "Iteration 35, loss = 0.51604585\n",
            "Iteration 36, loss = 0.50741855\n",
            "Iteration 37, loss = 0.49684322\n",
            "Iteration 38, loss = 0.49341194\n",
            "Iteration 39, loss = 0.48228773\n",
            "Iteration 40, loss = 0.47516155\n",
            "Iteration 41, loss = 0.47140655\n",
            "Iteration 42, loss = 0.46401771\n",
            "Iteration 43, loss = 0.46086349\n",
            "Iteration 44, loss = 0.45234552\n",
            "Iteration 45, loss = 0.45756736\n",
            "Iteration 46, loss = 0.44520874\n",
            "Iteration 47, loss = 0.43790675\n",
            "Iteration 48, loss = 0.43446919\n",
            "Iteration 49, loss = 0.42906158\n",
            "Iteration 50, loss = 0.42483176\n",
            "Iteration 51, loss = 0.42235425\n",
            "Iteration 52, loss = 0.42001900\n",
            "Iteration 53, loss = 0.43145367\n",
            "Iteration 54, loss = 0.42194289\n",
            "Iteration 55, loss = 0.41724211\n",
            "Iteration 56, loss = 0.41065885\n",
            "Iteration 57, loss = 0.40587858\n",
            "Iteration 58, loss = 0.39839489\n",
            "Iteration 59, loss = 0.39454224\n",
            "Iteration 60, loss = 0.39221987\n",
            "Iteration 61, loss = 0.39292467\n",
            "Iteration 62, loss = 0.39064098\n",
            "Iteration 63, loss = 0.38501157\n",
            "Iteration 64, loss = 0.38595729\n",
            "Iteration 65, loss = 0.37853665\n",
            "Iteration 66, loss = 0.38374433\n",
            "Iteration 67, loss = 0.38300113\n",
            "Iteration 68, loss = 0.37782702\n",
            "Iteration 69, loss = 0.37450654\n",
            "Iteration 70, loss = 0.37272637\n",
            "Iteration 71, loss = 0.36796396\n",
            "Iteration 72, loss = 0.36877049\n",
            "Iteration 73, loss = 0.36893369\n",
            "Iteration 74, loss = 0.36610758\n",
            "Iteration 75, loss = 0.36353960\n",
            "Iteration 76, loss = 0.36291373\n",
            "Iteration 77, loss = 0.36127605\n",
            "Iteration 78, loss = 0.36057203\n",
            "Iteration 79, loss = 0.35685371\n",
            "Iteration 80, loss = 0.35775963\n",
            "Iteration 81, loss = 0.35916526\n",
            "Iteration 82, loss = 0.36393777\n",
            "Iteration 83, loss = 0.36808479\n",
            "Iteration 84, loss = 0.36265312\n",
            "Iteration 85, loss = 0.35739648\n",
            "Iteration 86, loss = 0.37100439\n",
            "Iteration 87, loss = 0.35737767\n",
            "Iteration 88, loss = 0.36999169\n",
            "Iteration 89, loss = 0.34914832\n",
            "Iteration 90, loss = 0.34897234\n",
            "Iteration 91, loss = 0.34875746\n",
            "Iteration 92, loss = 0.34365081\n",
            "Iteration 93, loss = 0.34306411\n",
            "Iteration 94, loss = 0.34328976\n",
            "Iteration 95, loss = 0.34370051\n",
            "Iteration 96, loss = 0.34636328\n",
            "Iteration 97, loss = 0.34371627\n",
            "Iteration 98, loss = 0.34316496\n",
            "Iteration 99, loss = 0.33957235\n",
            "Iteration 100, loss = 0.34368312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35309441\n",
            "Iteration 2, loss = 1.32083741\n",
            "Iteration 3, loss = 1.28933114\n",
            "Iteration 4, loss = 1.24825855\n",
            "Iteration 5, loss = 1.19597616\n",
            "Iteration 6, loss = 1.13394584\n",
            "Iteration 7, loss = 1.06444225\n",
            "Iteration 8, loss = 0.99435747\n",
            "Iteration 9, loss = 0.93210599\n",
            "Iteration 10, loss = 0.87800258\n",
            "Iteration 11, loss = 0.83458334\n",
            "Iteration 12, loss = 0.80063700\n",
            "Iteration 13, loss = 0.76629931\n",
            "Iteration 14, loss = 0.73283318\n",
            "Iteration 15, loss = 0.70730731\n",
            "Iteration 16, loss = 0.69140415\n",
            "Iteration 17, loss = 0.67132128\n",
            "Iteration 18, loss = 0.66098519\n",
            "Iteration 19, loss = 0.64511369\n",
            "Iteration 20, loss = 0.62805441\n",
            "Iteration 21, loss = 0.61519972\n",
            "Iteration 22, loss = 0.60742168\n",
            "Iteration 23, loss = 0.59567480\n",
            "Iteration 24, loss = 0.58850227\n",
            "Iteration 25, loss = 0.57824675\n",
            "Iteration 26, loss = 0.57202395\n",
            "Iteration 27, loss = 0.56051450\n",
            "Iteration 28, loss = 0.55379447\n",
            "Iteration 29, loss = 0.54901922\n",
            "Iteration 30, loss = 0.54122711\n",
            "Iteration 31, loss = 0.53146472\n",
            "Iteration 32, loss = 0.52660932\n",
            "Iteration 33, loss = 0.51898986\n",
            "Iteration 34, loss = 0.50857449\n",
            "Iteration 35, loss = 0.50225163\n",
            "Iteration 36, loss = 0.49448460\n",
            "Iteration 37, loss = 0.48415915\n",
            "Iteration 38, loss = 0.48460118\n",
            "Iteration 39, loss = 0.47330288\n",
            "Iteration 40, loss = 0.46828108\n",
            "Iteration 41, loss = 0.46335114\n",
            "Iteration 42, loss = 0.45627226\n",
            "Iteration 43, loss = 0.45702594\n",
            "Iteration 44, loss = 0.44487621\n",
            "Iteration 45, loss = 0.44981540\n",
            "Iteration 46, loss = 0.44566720\n",
            "Iteration 47, loss = 0.43683154\n",
            "Iteration 48, loss = 0.42686914\n",
            "Iteration 49, loss = 0.42147913\n",
            "Iteration 50, loss = 0.41893445\n",
            "Iteration 51, loss = 0.41621968\n",
            "Iteration 52, loss = 0.41125752\n",
            "Iteration 53, loss = 0.41442566\n",
            "Iteration 54, loss = 0.41745286\n",
            "Iteration 55, loss = 0.41201174\n",
            "Iteration 56, loss = 0.40707235\n",
            "Iteration 57, loss = 0.39895144\n",
            "Iteration 58, loss = 0.38866940\n",
            "Iteration 59, loss = 0.38528667\n",
            "Iteration 60, loss = 0.38508943\n",
            "Iteration 61, loss = 0.38320617\n",
            "Iteration 62, loss = 0.38981703\n",
            "Iteration 63, loss = 0.37778549\n",
            "Iteration 64, loss = 0.37418776\n",
            "Iteration 65, loss = 0.36914300\n",
            "Iteration 66, loss = 0.37675243\n",
            "Iteration 67, loss = 0.37150852\n",
            "Iteration 68, loss = 0.37305620\n",
            "Iteration 69, loss = 0.36687891\n",
            "Iteration 70, loss = 0.36362890\n",
            "Iteration 71, loss = 0.35999563\n",
            "Iteration 72, loss = 0.36358274\n",
            "Iteration 73, loss = 0.35837475\n",
            "Iteration 74, loss = 0.35941235\n",
            "Iteration 75, loss = 0.35378978\n",
            "Iteration 76, loss = 0.35543504\n",
            "Iteration 77, loss = 0.35200412\n",
            "Iteration 78, loss = 0.35501042\n",
            "Iteration 79, loss = 0.34856603\n",
            "Iteration 80, loss = 0.35051914\n",
            "Iteration 81, loss = 0.35343096\n",
            "Iteration 82, loss = 0.35214065\n",
            "Iteration 83, loss = 0.35838198\n",
            "Iteration 84, loss = 0.35362702\n",
            "Iteration 85, loss = 0.34779362\n",
            "Iteration 86, loss = 0.34792622\n",
            "Iteration 87, loss = 0.34762201\n",
            "Iteration 88, loss = 0.35494001\n",
            "Iteration 89, loss = 0.34065993\n",
            "Iteration 90, loss = 0.33621802\n",
            "Iteration 91, loss = 0.34053950\n",
            "Iteration 92, loss = 0.33638210\n",
            "Iteration 93, loss = 0.33548313\n",
            "Iteration 94, loss = 0.33405884\n",
            "Iteration 95, loss = 0.33438524\n",
            "Iteration 96, loss = 0.33447056\n",
            "Iteration 97, loss = 0.33216345\n",
            "Iteration 98, loss = 0.33092667\n",
            "Iteration 99, loss = 0.32858623\n",
            "Iteration 100, loss = 0.33255315\n",
            "51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35580651\n",
            "Iteration 2, loss = 1.27634958\n",
            "Iteration 3, loss = 1.18261674\n",
            "Iteration 4, loss = 1.06374726\n",
            "Iteration 5, loss = 0.92311112\n",
            "Iteration 6, loss = 0.77651625\n",
            "Iteration 7, loss = 0.64512366\n",
            "Iteration 8, loss = 0.53998279\n",
            "Iteration 9, loss = 0.45715960\n",
            "Iteration 10, loss = 0.40268850\n",
            "Iteration 11, loss = 0.36803667\n",
            "Iteration 12, loss = 0.34830399\n",
            "Iteration 13, loss = 0.33117407\n",
            "Iteration 14, loss = 0.32399331\n",
            "Iteration 15, loss = 0.31883114\n",
            "Iteration 16, loss = 0.31767688\n",
            "Iteration 17, loss = 0.31267259\n",
            "Iteration 18, loss = 0.30925259\n",
            "Iteration 19, loss = 0.31108977\n",
            "Iteration 20, loss = 0.31301664\n",
            "Iteration 21, loss = 0.31151889\n",
            "Iteration 22, loss = 0.30795674\n",
            "Iteration 23, loss = 0.30470636\n",
            "Iteration 24, loss = 0.30388103\n",
            "Iteration 25, loss = 0.30663678\n",
            "Iteration 26, loss = 0.30257077\n",
            "Iteration 27, loss = 0.31057190\n",
            "Iteration 28, loss = 0.30260948\n",
            "Iteration 29, loss = 0.30673680\n",
            "Iteration 30, loss = 0.30591896\n",
            "Iteration 31, loss = 0.30496168\n",
            "Iteration 32, loss = 0.30426772\n",
            "Iteration 33, loss = 0.30247525\n",
            "Iteration 34, loss = 0.30598885\n",
            "Iteration 35, loss = 0.30667752\n",
            "Iteration 36, loss = 0.30022007\n",
            "Iteration 37, loss = 0.29889193\n",
            "Iteration 38, loss = 0.30180662\n",
            "Iteration 39, loss = 0.30281321\n",
            "Iteration 40, loss = 0.30303717\n",
            "Iteration 41, loss = 0.30653344\n",
            "Iteration 42, loss = 0.31029664\n",
            "Iteration 43, loss = 0.30409615\n",
            "Iteration 44, loss = 0.30355213\n",
            "Iteration 45, loss = 0.30319475\n",
            "Iteration 46, loss = 0.30149795\n",
            "Iteration 47, loss = 0.29794669\n",
            "Iteration 48, loss = 0.30137608\n",
            "Iteration 49, loss = 0.30121517\n",
            "Iteration 50, loss = 0.30192577\n",
            "Iteration 51, loss = 0.30253432\n",
            "Iteration 52, loss = 0.29664849\n",
            "Iteration 53, loss = 0.30050808\n",
            "Iteration 54, loss = 0.29998842\n",
            "Iteration 55, loss = 0.29548884\n",
            "Iteration 56, loss = 0.29520262\n",
            "Iteration 57, loss = 0.29581969\n",
            "Iteration 58, loss = 0.29817830\n",
            "Iteration 59, loss = 0.29757562\n",
            "Iteration 60, loss = 0.30157833\n",
            "Iteration 61, loss = 0.29906103\n",
            "Iteration 62, loss = 0.30309226\n",
            "Iteration 63, loss = 0.29761912\n",
            "Iteration 64, loss = 0.29849930\n",
            "Iteration 65, loss = 0.29470064\n",
            "Iteration 66, loss = 0.29514359\n",
            "Iteration 67, loss = 0.29597018\n",
            "Iteration 68, loss = 0.29543443\n",
            "Iteration 69, loss = 0.29590865\n",
            "Iteration 70, loss = 0.29992390\n",
            "Iteration 71, loss = 0.30034384\n",
            "Iteration 72, loss = 0.29861629\n",
            "Iteration 73, loss = 0.29591347\n",
            "Iteration 74, loss = 0.29435782\n",
            "Iteration 75, loss = 0.29762942\n",
            "Iteration 76, loss = 0.29775542\n",
            "Iteration 77, loss = 0.29737207\n",
            "Iteration 78, loss = 0.29369083\n",
            "Iteration 79, loss = 0.29478437\n",
            "Iteration 80, loss = 0.29809801\n",
            "Iteration 81, loss = 0.29334876\n",
            "Iteration 82, loss = 0.29836207\n",
            "Iteration 83, loss = 0.29467276\n",
            "Iteration 84, loss = 0.29848088\n",
            "Iteration 85, loss = 0.29723435\n",
            "Iteration 86, loss = 0.29741374\n",
            "Iteration 87, loss = 0.29384503\n",
            "Iteration 88, loss = 0.29828740\n",
            "Iteration 89, loss = 0.29533035\n",
            "Iteration 90, loss = 0.29354028\n",
            "Iteration 91, loss = 0.29389898\n",
            "Iteration 92, loss = 0.29400768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35473057\n",
            "Iteration 2, loss = 1.27691670\n",
            "Iteration 3, loss = 1.18524055\n",
            "Iteration 4, loss = 1.06891678\n",
            "Iteration 5, loss = 0.93013545\n",
            "Iteration 6, loss = 0.78462291\n",
            "Iteration 7, loss = 0.65155801\n",
            "Iteration 8, loss = 0.54850156\n",
            "Iteration 9, loss = 0.46544247\n",
            "Iteration 10, loss = 0.41377820\n",
            "Iteration 11, loss = 0.38137070\n",
            "Iteration 12, loss = 0.35886724\n",
            "Iteration 13, loss = 0.34138385\n",
            "Iteration 14, loss = 0.33291867\n",
            "Iteration 15, loss = 0.32779091\n",
            "Iteration 16, loss = 0.32500860\n",
            "Iteration 17, loss = 0.32477548\n",
            "Iteration 18, loss = 0.32237876\n",
            "Iteration 19, loss = 0.31958474\n",
            "Iteration 20, loss = 0.32575275\n",
            "Iteration 21, loss = 0.32162573\n",
            "Iteration 22, loss = 0.32391593\n",
            "Iteration 23, loss = 0.31256968\n",
            "Iteration 24, loss = 0.31841330\n",
            "Iteration 25, loss = 0.31803778\n",
            "Iteration 26, loss = 0.31387594\n",
            "Iteration 27, loss = 0.31704109\n",
            "Iteration 28, loss = 0.31451140\n",
            "Iteration 29, loss = 0.31543259\n",
            "Iteration 30, loss = 0.31874693\n",
            "Iteration 31, loss = 0.31227170\n",
            "Iteration 32, loss = 0.31575075\n",
            "Iteration 33, loss = 0.31145502\n",
            "Iteration 34, loss = 0.31452516\n",
            "Iteration 35, loss = 0.31403633\n",
            "Iteration 36, loss = 0.30873323\n",
            "Iteration 37, loss = 0.30923702\n",
            "Iteration 38, loss = 0.30907598\n",
            "Iteration 39, loss = 0.31317787\n",
            "Iteration 40, loss = 0.31482819\n",
            "Iteration 41, loss = 0.31518786\n",
            "Iteration 42, loss = 0.31116294\n",
            "Iteration 43, loss = 0.30926513\n",
            "Iteration 44, loss = 0.30968479\n",
            "Iteration 45, loss = 0.31225662\n",
            "Iteration 46, loss = 0.30963478\n",
            "Iteration 47, loss = 0.30749904\n",
            "Iteration 48, loss = 0.30860883\n",
            "Iteration 49, loss = 0.31024630\n",
            "Iteration 50, loss = 0.30729918\n",
            "Iteration 51, loss = 0.31066528\n",
            "Iteration 52, loss = 0.30947741\n",
            "Iteration 53, loss = 0.31429953\n",
            "Iteration 54, loss = 0.31716852\n",
            "Iteration 55, loss = 0.30487147\n",
            "Iteration 56, loss = 0.30446128\n",
            "Iteration 57, loss = 0.30428396\n",
            "Iteration 58, loss = 0.30665660\n",
            "Iteration 59, loss = 0.30791649\n",
            "Iteration 60, loss = 0.31236041\n",
            "Iteration 61, loss = 0.30966613\n",
            "Iteration 62, loss = 0.31196036\n",
            "Iteration 63, loss = 0.30518035\n",
            "Iteration 64, loss = 0.30494806\n",
            "Iteration 65, loss = 0.30456149\n",
            "Iteration 66, loss = 0.30340447\n",
            "Iteration 67, loss = 0.30655964\n",
            "Iteration 68, loss = 0.30391185\n",
            "Iteration 69, loss = 0.30644825\n",
            "Iteration 70, loss = 0.31101959\n",
            "Iteration 71, loss = 0.30462525\n",
            "Iteration 72, loss = 0.30433670\n",
            "Iteration 73, loss = 0.30754376\n",
            "Iteration 74, loss = 0.30179767\n",
            "Iteration 75, loss = 0.30890465\n",
            "Iteration 76, loss = 0.30426644\n",
            "Iteration 77, loss = 0.30491849\n",
            "Iteration 78, loss = 0.30282314\n",
            "Iteration 79, loss = 0.30509015\n",
            "Iteration 80, loss = 0.30400257\n",
            "Iteration 81, loss = 0.30309969\n",
            "Iteration 82, loss = 0.30394792\n",
            "Iteration 83, loss = 0.30076042\n",
            "Iteration 84, loss = 0.31044583\n",
            "Iteration 85, loss = 0.30578224\n",
            "Iteration 86, loss = 0.30638385\n",
            "Iteration 87, loss = 0.30100258\n",
            "Iteration 88, loss = 0.30254935\n",
            "Iteration 89, loss = 0.30054148\n",
            "Iteration 90, loss = 0.30048620\n",
            "Iteration 91, loss = 0.30147976\n",
            "Iteration 92, loss = 0.30135175\n",
            "Iteration 93, loss = 0.30416436\n",
            "Iteration 94, loss = 0.30119974\n",
            "Iteration 95, loss = 0.30094701\n",
            "Iteration 96, loss = 0.29993764\n",
            "Iteration 97, loss = 0.29987501\n",
            "Iteration 98, loss = 0.30031648\n",
            "Iteration 99, loss = 0.30095568\n",
            "Iteration 100, loss = 0.30647727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.35175449\n",
            "Iteration 2, loss = 1.26977373\n",
            "Iteration 3, loss = 1.17378650\n",
            "Iteration 4, loss = 1.05228323\n",
            "Iteration 5, loss = 0.91118814\n",
            "Iteration 6, loss = 0.76540066\n",
            "Iteration 7, loss = 0.63653977\n",
            "Iteration 8, loss = 0.53458496\n",
            "Iteration 9, loss = 0.45810363\n",
            "Iteration 10, loss = 0.40759711\n",
            "Iteration 11, loss = 0.37694216\n",
            "Iteration 12, loss = 0.36286195\n",
            "Iteration 13, loss = 0.34709126\n",
            "Iteration 14, loss = 0.34018256\n",
            "Iteration 15, loss = 0.33585625\n",
            "Iteration 16, loss = 0.33440597\n",
            "Iteration 17, loss = 0.33438449\n",
            "Iteration 18, loss = 0.33372170\n",
            "Iteration 19, loss = 0.32808489\n",
            "Iteration 20, loss = 0.33509612\n",
            "Iteration 21, loss = 0.33126974\n",
            "Iteration 22, loss = 0.33322709\n",
            "Iteration 23, loss = 0.32377875\n",
            "Iteration 24, loss = 0.32638700\n",
            "Iteration 25, loss = 0.32589911\n",
            "Iteration 26, loss = 0.32586220\n",
            "Iteration 27, loss = 0.32721653\n",
            "Iteration 28, loss = 0.32291021\n",
            "Iteration 29, loss = 0.32518257\n",
            "Iteration 30, loss = 0.33110245\n",
            "Iteration 31, loss = 0.32298159\n",
            "Iteration 32, loss = 0.32606369\n",
            "Iteration 33, loss = 0.32557348\n",
            "Iteration 34, loss = 0.32751616\n",
            "Iteration 35, loss = 0.32531382\n",
            "Iteration 36, loss = 0.32138023\n",
            "Iteration 37, loss = 0.32322372\n",
            "Iteration 38, loss = 0.32157216\n",
            "Iteration 39, loss = 0.32774602\n",
            "Iteration 40, loss = 0.32907513\n",
            "Iteration 41, loss = 0.33267924\n",
            "Iteration 42, loss = 0.32216279\n",
            "Iteration 43, loss = 0.32059178\n",
            "Iteration 44, loss = 0.32080650\n",
            "Iteration 45, loss = 0.32207607\n",
            "Iteration 46, loss = 0.32459352\n",
            "Iteration 47, loss = 0.32281317\n",
            "Iteration 48, loss = 0.32350876\n",
            "Iteration 49, loss = 0.32022000\n",
            "Iteration 50, loss = 0.32036937\n",
            "Iteration 51, loss = 0.31958536\n",
            "Iteration 52, loss = 0.31853519\n",
            "Iteration 53, loss = 0.32448564\n",
            "Iteration 54, loss = 0.32275128\n",
            "Iteration 55, loss = 0.31873203\n",
            "Iteration 56, loss = 0.31735161\n",
            "Iteration 57, loss = 0.31804718\n",
            "Iteration 58, loss = 0.31878176\n",
            "Iteration 59, loss = 0.31966848\n",
            "Iteration 60, loss = 0.32121906\n",
            "Iteration 61, loss = 0.31921946\n",
            "Iteration 62, loss = 0.32742731\n",
            "Iteration 63, loss = 0.31941423\n",
            "Iteration 64, loss = 0.32024471\n",
            "Iteration 65, loss = 0.31772766\n",
            "Iteration 66, loss = 0.31673840\n",
            "Iteration 67, loss = 0.31690085\n",
            "Iteration 68, loss = 0.31526672\n",
            "Iteration 69, loss = 0.31798268\n",
            "Iteration 70, loss = 0.32045323\n",
            "Iteration 71, loss = 0.31523192\n",
            "Iteration 72, loss = 0.31461005\n",
            "Iteration 73, loss = 0.31712940\n",
            "Iteration 74, loss = 0.31245138\n",
            "Iteration 75, loss = 0.31714350\n",
            "Iteration 76, loss = 0.31606365\n",
            "Iteration 77, loss = 0.32017490\n",
            "Iteration 78, loss = 0.31796354\n",
            "Iteration 79, loss = 0.31902677\n",
            "Iteration 80, loss = 0.31649888\n",
            "Iteration 81, loss = 0.31450340\n",
            "Iteration 82, loss = 0.31349491\n",
            "Iteration 83, loss = 0.31503824\n",
            "Iteration 84, loss = 0.32205497\n",
            "Iteration 85, loss = 0.31624019\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35217340\n",
            "Iteration 2, loss = 1.27090450\n",
            "Iteration 3, loss = 1.17577633\n",
            "Iteration 4, loss = 1.05526523\n",
            "Iteration 5, loss = 0.91348731\n",
            "Iteration 6, loss = 0.76705218\n",
            "Iteration 7, loss = 0.63448186\n",
            "Iteration 8, loss = 0.52784760\n",
            "Iteration 9, loss = 0.45063513\n",
            "Iteration 10, loss = 0.40239802\n",
            "Iteration 11, loss = 0.37280829\n",
            "Iteration 12, loss = 0.34987021\n",
            "Iteration 13, loss = 0.33514888\n",
            "Iteration 14, loss = 0.32762718\n",
            "Iteration 15, loss = 0.32196963\n",
            "Iteration 16, loss = 0.31826334\n",
            "Iteration 17, loss = 0.31773624\n",
            "Iteration 18, loss = 0.31994036\n",
            "Iteration 19, loss = 0.31452415\n",
            "Iteration 20, loss = 0.31932531\n",
            "Iteration 21, loss = 0.31204213\n",
            "Iteration 22, loss = 0.30919224\n",
            "Iteration 23, loss = 0.30716315\n",
            "Iteration 24, loss = 0.30554019\n",
            "Iteration 25, loss = 0.30828985\n",
            "Iteration 26, loss = 0.30839822\n",
            "Iteration 27, loss = 0.30779887\n",
            "Iteration 28, loss = 0.30449606\n",
            "Iteration 29, loss = 0.30507087\n",
            "Iteration 30, loss = 0.31228242\n",
            "Iteration 31, loss = 0.30198551\n",
            "Iteration 32, loss = 0.30460327\n",
            "Iteration 33, loss = 0.30762573\n",
            "Iteration 34, loss = 0.30967392\n",
            "Iteration 35, loss = 0.30647743\n",
            "Iteration 36, loss = 0.30017875\n",
            "Iteration 37, loss = 0.30378342\n",
            "Iteration 38, loss = 0.30074060\n",
            "Iteration 39, loss = 0.30635078\n",
            "Iteration 40, loss = 0.30720259\n",
            "Iteration 41, loss = 0.30965154\n",
            "Iteration 42, loss = 0.30226681\n",
            "Iteration 43, loss = 0.30275584\n",
            "Iteration 44, loss = 0.30128030\n",
            "Iteration 45, loss = 0.30337898\n",
            "Iteration 46, loss = 0.30357299\n",
            "Iteration 47, loss = 0.30085270\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35430673\n",
            "Iteration 2, loss = 1.27496606\n",
            "Iteration 3, loss = 1.18335213\n",
            "Iteration 4, loss = 1.06654824\n",
            "Iteration 5, loss = 0.92967606\n",
            "Iteration 6, loss = 0.78834464\n",
            "Iteration 7, loss = 0.65912350\n",
            "Iteration 8, loss = 0.55281118\n",
            "Iteration 9, loss = 0.47500273\n",
            "Iteration 10, loss = 0.42057697\n",
            "Iteration 11, loss = 0.39093564\n",
            "Iteration 12, loss = 0.37053265\n",
            "Iteration 13, loss = 0.35433834\n",
            "Iteration 14, loss = 0.34627913\n",
            "Iteration 15, loss = 0.34141422\n",
            "Iteration 16, loss = 0.33713427\n",
            "Iteration 17, loss = 0.33772074\n",
            "Iteration 18, loss = 0.34323611\n",
            "Iteration 19, loss = 0.33858616\n",
            "Iteration 20, loss = 0.34407725\n",
            "Iteration 21, loss = 0.33618270\n",
            "Iteration 22, loss = 0.33086344\n",
            "Iteration 23, loss = 0.32999433\n",
            "Iteration 24, loss = 0.32769449\n",
            "Iteration 25, loss = 0.32996603\n",
            "Iteration 26, loss = 0.33343585\n",
            "Iteration 27, loss = 0.32668007\n",
            "Iteration 28, loss = 0.32726629\n",
            "Iteration 29, loss = 0.32816579\n",
            "Iteration 30, loss = 0.33486296\n",
            "Iteration 31, loss = 0.32790439\n",
            "Iteration 32, loss = 0.33199072\n",
            "Iteration 33, loss = 0.32849904\n",
            "Iteration 34, loss = 0.33068428\n",
            "Iteration 35, loss = 0.32609705\n",
            "Iteration 36, loss = 0.32379349\n",
            "Iteration 37, loss = 0.32484853\n",
            "Iteration 38, loss = 0.32530599\n",
            "Iteration 39, loss = 0.32993367\n",
            "Iteration 40, loss = 0.33317758\n",
            "Iteration 41, loss = 0.33722332\n",
            "Iteration 42, loss = 0.32646248\n",
            "Iteration 43, loss = 0.32801247\n",
            "Iteration 44, loss = 0.32753395\n",
            "Iteration 45, loss = 0.32636029\n",
            "Iteration 46, loss = 0.32871278\n",
            "Iteration 47, loss = 0.32384495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34583641\n",
            "Iteration 2, loss = 1.27255728\n",
            "Iteration 3, loss = 1.18465428\n",
            "Iteration 4, loss = 1.07204580\n",
            "Iteration 5, loss = 0.93639813\n",
            "Iteration 6, loss = 0.78994618\n",
            "Iteration 7, loss = 0.65606641\n",
            "Iteration 8, loss = 0.54573910\n",
            "Iteration 9, loss = 0.46876798\n",
            "Iteration 10, loss = 0.41701718\n",
            "Iteration 11, loss = 0.39269236\n",
            "Iteration 12, loss = 0.37944065\n",
            "Iteration 13, loss = 0.36207541\n",
            "Iteration 14, loss = 0.35627027\n",
            "Iteration 15, loss = 0.35214799\n",
            "Iteration 16, loss = 0.34933165\n",
            "Iteration 17, loss = 0.34990002\n",
            "Iteration 18, loss = 0.35400181\n",
            "Iteration 19, loss = 0.34866289\n",
            "Iteration 20, loss = 0.35211599\n",
            "Iteration 21, loss = 0.34453702\n",
            "Iteration 22, loss = 0.34256444\n",
            "Iteration 23, loss = 0.34184462\n",
            "Iteration 24, loss = 0.34129226\n",
            "Iteration 25, loss = 0.34323569\n",
            "Iteration 26, loss = 0.34415818\n",
            "Iteration 27, loss = 0.34053919\n",
            "Iteration 28, loss = 0.33941110\n",
            "Iteration 29, loss = 0.34041385\n",
            "Iteration 30, loss = 0.34554857\n",
            "Iteration 31, loss = 0.34273276\n",
            "Iteration 32, loss = 0.34363447\n",
            "Iteration 33, loss = 0.34418232\n",
            "Iteration 34, loss = 0.34776873\n",
            "Iteration 35, loss = 0.34225184\n",
            "Iteration 36, loss = 0.33727171\n",
            "Iteration 37, loss = 0.33780057\n",
            "Iteration 38, loss = 0.33846712\n",
            "Iteration 39, loss = 0.34331257\n",
            "Iteration 40, loss = 0.34674912\n",
            "Iteration 41, loss = 0.35011048\n",
            "Iteration 42, loss = 0.33724052\n",
            "Iteration 43, loss = 0.34142448\n",
            "Iteration 44, loss = 0.34044527\n",
            "Iteration 45, loss = 0.33940071\n",
            "Iteration 46, loss = 0.34418373\n",
            "Iteration 47, loss = 0.33843055\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33504969\n",
            "Iteration 2, loss = 1.26272355\n",
            "Iteration 3, loss = 1.17526279\n",
            "Iteration 4, loss = 1.06398459\n",
            "Iteration 5, loss = 0.92597605\n",
            "Iteration 6, loss = 0.78219621\n",
            "Iteration 7, loss = 0.64890849\n",
            "Iteration 8, loss = 0.54262459\n",
            "Iteration 9, loss = 0.46517595\n",
            "Iteration 10, loss = 0.41463317\n",
            "Iteration 11, loss = 0.39234901\n",
            "Iteration 12, loss = 0.38208962\n",
            "Iteration 13, loss = 0.36465397\n",
            "Iteration 14, loss = 0.35866645\n",
            "Iteration 15, loss = 0.35438079\n",
            "Iteration 16, loss = 0.35143209\n",
            "Iteration 17, loss = 0.34973885\n",
            "Iteration 18, loss = 0.35341834\n",
            "Iteration 19, loss = 0.35048220\n",
            "Iteration 20, loss = 0.35391408\n",
            "Iteration 21, loss = 0.34646622\n",
            "Iteration 22, loss = 0.34503467\n",
            "Iteration 23, loss = 0.34471652\n",
            "Iteration 24, loss = 0.34502108\n",
            "Iteration 25, loss = 0.34538081\n",
            "Iteration 26, loss = 0.34844132\n",
            "Iteration 27, loss = 0.34119934\n",
            "Iteration 28, loss = 0.34318739\n",
            "Iteration 29, loss = 0.34454432\n",
            "Iteration 30, loss = 0.34939864\n",
            "Iteration 31, loss = 0.34560962\n",
            "Iteration 32, loss = 0.34707494\n",
            "Iteration 33, loss = 0.34618495\n",
            "Iteration 34, loss = 0.34770390\n",
            "Iteration 35, loss = 0.34688477\n",
            "Iteration 36, loss = 0.34001838\n",
            "Iteration 37, loss = 0.34097974\n",
            "Iteration 38, loss = 0.34245888\n",
            "Iteration 39, loss = 0.34650120\n",
            "Iteration 40, loss = 0.34840151\n",
            "Iteration 41, loss = 0.35457447\n",
            "Iteration 42, loss = 0.34044477\n",
            "Iteration 43, loss = 0.34529362\n",
            "Iteration 44, loss = 0.34340295\n",
            "Iteration 45, loss = 0.34369569\n",
            "Iteration 46, loss = 0.35082173\n",
            "Iteration 47, loss = 0.34453426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34838237\n",
            "Iteration 2, loss = 1.27379805\n",
            "Iteration 3, loss = 1.18446484\n",
            "Iteration 4, loss = 1.06736710\n",
            "Iteration 5, loss = 0.92700959\n",
            "Iteration 6, loss = 0.77764111\n",
            "Iteration 7, loss = 0.64066806\n",
            "Iteration 8, loss = 0.53168072\n",
            "Iteration 9, loss = 0.45623306\n",
            "Iteration 10, loss = 0.40494637\n",
            "Iteration 11, loss = 0.38180987\n",
            "Iteration 12, loss = 0.37358613\n",
            "Iteration 13, loss = 0.35782247\n",
            "Iteration 14, loss = 0.35103305\n",
            "Iteration 15, loss = 0.34793926\n",
            "Iteration 16, loss = 0.34182841\n",
            "Iteration 17, loss = 0.34102186\n",
            "Iteration 18, loss = 0.34552993\n",
            "Iteration 19, loss = 0.33985440\n",
            "Iteration 20, loss = 0.34410260\n",
            "Iteration 21, loss = 0.33875870\n",
            "Iteration 22, loss = 0.33755496\n",
            "Iteration 23, loss = 0.33643559\n",
            "Iteration 24, loss = 0.33555809\n",
            "Iteration 25, loss = 0.33649790\n",
            "Iteration 26, loss = 0.33905425\n",
            "Iteration 27, loss = 0.33281767\n",
            "Iteration 28, loss = 0.33675419\n",
            "Iteration 29, loss = 0.33891510\n",
            "Iteration 30, loss = 0.34128157\n",
            "Iteration 31, loss = 0.33655183\n",
            "Iteration 32, loss = 0.33758206\n",
            "Iteration 33, loss = 0.33792339\n",
            "Iteration 34, loss = 0.34115330\n",
            "Iteration 35, loss = 0.33857545\n",
            "Iteration 36, loss = 0.33258751\n",
            "Iteration 37, loss = 0.33282500\n",
            "Iteration 38, loss = 0.33465104\n",
            "Iteration 39, loss = 0.33782872\n",
            "Iteration 40, loss = 0.34184019\n",
            "Iteration 41, loss = 0.34951054\n",
            "Iteration 42, loss = 0.33428182\n",
            "Iteration 43, loss = 0.33695521\n",
            "Iteration 44, loss = 0.33419787\n",
            "Iteration 45, loss = 0.33531575\n",
            "Iteration 46, loss = 0.34267509\n",
            "Iteration 47, loss = 0.33743911\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34777001\n",
            "Iteration 2, loss = 1.26889748\n",
            "Iteration 3, loss = 1.17836870\n",
            "Iteration 4, loss = 1.06042462\n",
            "Iteration 5, loss = 0.92199662\n",
            "Iteration 6, loss = 0.77705047\n",
            "Iteration 7, loss = 0.64408752\n",
            "Iteration 8, loss = 0.53626967\n",
            "Iteration 9, loss = 0.45464569\n",
            "Iteration 10, loss = 0.39691423\n",
            "Iteration 11, loss = 0.36522490\n",
            "Iteration 12, loss = 0.34676641\n",
            "Iteration 13, loss = 0.33402668\n",
            "Iteration 14, loss = 0.32503679\n",
            "Iteration 15, loss = 0.32034014\n",
            "Iteration 16, loss = 0.31648468\n",
            "Iteration 17, loss = 0.31330700\n",
            "Iteration 18, loss = 0.31764645\n",
            "Iteration 19, loss = 0.31214546\n",
            "Iteration 20, loss = 0.31199276\n",
            "Iteration 21, loss = 0.31137550\n",
            "Iteration 22, loss = 0.30880077\n",
            "Iteration 23, loss = 0.30804009\n",
            "Iteration 24, loss = 0.30546253\n",
            "Iteration 25, loss = 0.30714535\n",
            "Iteration 26, loss = 0.31156732\n",
            "Iteration 27, loss = 0.30491019\n",
            "Iteration 28, loss = 0.30580753\n",
            "Iteration 29, loss = 0.30873138\n",
            "Iteration 30, loss = 0.30813636\n",
            "Iteration 31, loss = 0.30588089\n",
            "Iteration 32, loss = 0.30685703\n",
            "Iteration 33, loss = 0.30486737\n",
            "Iteration 34, loss = 0.30772769\n",
            "Iteration 35, loss = 0.30305584\n",
            "Iteration 36, loss = 0.30293556\n",
            "Iteration 37, loss = 0.30468840\n",
            "Iteration 38, loss = 0.30269200\n",
            "Iteration 39, loss = 0.30464274\n",
            "Iteration 40, loss = 0.31024639\n",
            "Iteration 41, loss = 0.31800108\n",
            "Iteration 42, loss = 0.30501003\n",
            "Iteration 43, loss = 0.30662871\n",
            "Iteration 44, loss = 0.30630445\n",
            "Iteration 45, loss = 0.30658241\n",
            "Iteration 46, loss = 0.31418871\n",
            "Iteration 47, loss = 0.30744115\n",
            "Iteration 48, loss = 0.30961784\n",
            "Iteration 49, loss = 0.29987158\n",
            "Iteration 50, loss = 0.30261724\n",
            "Iteration 51, loss = 0.30473790\n",
            "Iteration 52, loss = 0.30161283\n",
            "Iteration 53, loss = 0.30106557\n",
            "Iteration 54, loss = 0.29858176\n",
            "Iteration 55, loss = 0.30017989\n",
            "Iteration 56, loss = 0.30285992\n",
            "Iteration 57, loss = 0.29893576\n",
            "Iteration 58, loss = 0.29886920\n",
            "Iteration 59, loss = 0.30183607\n",
            "Iteration 60, loss = 0.30788389\n",
            "Iteration 61, loss = 0.30128901\n",
            "Iteration 62, loss = 0.30402093\n",
            "Iteration 63, loss = 0.30258634\n",
            "Iteration 64, loss = 0.30162233\n",
            "Iteration 65, loss = 0.29975032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.33757949\n",
            "Iteration 2, loss = 1.25798759\n",
            "Iteration 3, loss = 1.16546482\n",
            "Iteration 4, loss = 1.04836487\n",
            "Iteration 5, loss = 0.90952712\n",
            "Iteration 6, loss = 0.76854470\n",
            "Iteration 7, loss = 0.64248209\n",
            "Iteration 8, loss = 0.54185302\n",
            "Iteration 9, loss = 0.46870978\n",
            "Iteration 10, loss = 0.41585507\n",
            "Iteration 11, loss = 0.38393107\n",
            "Iteration 12, loss = 0.36559401\n",
            "Iteration 13, loss = 0.35037419\n",
            "Iteration 14, loss = 0.34255092\n",
            "Iteration 15, loss = 0.33668361\n",
            "Iteration 16, loss = 0.33489700\n",
            "Iteration 17, loss = 0.33419076\n",
            "Iteration 18, loss = 0.33317320\n",
            "Iteration 19, loss = 0.32570360\n",
            "Iteration 20, loss = 0.32310974\n",
            "Iteration 21, loss = 0.32298828\n",
            "Iteration 22, loss = 0.32174089\n",
            "Iteration 23, loss = 0.32462921\n",
            "Iteration 24, loss = 0.32018034\n",
            "Iteration 25, loss = 0.31951813\n",
            "Iteration 26, loss = 0.32346784\n",
            "Iteration 27, loss = 0.32211952\n",
            "Iteration 28, loss = 0.32182604\n",
            "Iteration 29, loss = 0.32091598\n",
            "Iteration 30, loss = 0.31756342\n",
            "Iteration 31, loss = 0.31816632\n",
            "Iteration 32, loss = 0.31991177\n",
            "Iteration 33, loss = 0.31857530\n",
            "Iteration 34, loss = 0.31714270\n",
            "Iteration 35, loss = 0.31551782\n",
            "Iteration 36, loss = 0.31634313\n",
            "Iteration 37, loss = 0.31619507\n",
            "Iteration 38, loss = 0.31862963\n",
            "Iteration 39, loss = 0.31529641\n",
            "Iteration 40, loss = 0.31517289\n",
            "Iteration 41, loss = 0.31581561\n",
            "Iteration 42, loss = 0.31325747\n",
            "Iteration 43, loss = 0.31661461\n",
            "Iteration 44, loss = 0.31700370\n",
            "Iteration 45, loss = 0.31895168\n",
            "Iteration 46, loss = 0.32503270\n",
            "Iteration 47, loss = 0.31989035\n",
            "Iteration 48, loss = 0.32237213\n",
            "Iteration 49, loss = 0.31497914\n",
            "Iteration 50, loss = 0.31450790\n",
            "Iteration 51, loss = 0.31512544\n",
            "Iteration 52, loss = 0.31421414\n",
            "Iteration 53, loss = 0.31299380\n",
            "Iteration 54, loss = 0.31138181\n",
            "Iteration 55, loss = 0.31198289\n",
            "Iteration 56, loss = 0.31389630\n",
            "Iteration 57, loss = 0.31213122\n",
            "Iteration 58, loss = 0.31311029\n",
            "Iteration 59, loss = 0.31561034\n",
            "Iteration 60, loss = 0.32778007\n",
            "Iteration 61, loss = 0.31724391\n",
            "Iteration 62, loss = 0.32189474\n",
            "Iteration 63, loss = 0.31648570\n",
            "Iteration 64, loss = 0.31165112\n",
            "Iteration 65, loss = 0.31099652\n",
            "Iteration 66, loss = 0.30930228\n",
            "Iteration 67, loss = 0.31053201\n",
            "Iteration 68, loss = 0.31173436\n",
            "Iteration 69, loss = 0.31127735\n",
            "Iteration 70, loss = 0.31012251\n",
            "Iteration 71, loss = 0.31103437\n",
            "Iteration 72, loss = 0.30882893\n",
            "Iteration 73, loss = 0.31052233\n",
            "Iteration 74, loss = 0.31439753\n",
            "Iteration 75, loss = 0.31259483\n",
            "Iteration 76, loss = 0.31284364\n",
            "Iteration 77, loss = 0.31091305\n",
            "Iteration 78, loss = 0.30872258\n",
            "Iteration 79, loss = 0.30885902\n",
            "Iteration 80, loss = 0.31083305\n",
            "Iteration 81, loss = 0.30979910\n",
            "Iteration 82, loss = 0.30777340\n",
            "Iteration 83, loss = 0.30743983\n",
            "Iteration 84, loss = 0.30792476\n",
            "Iteration 85, loss = 0.31011464\n",
            "Iteration 86, loss = 0.31155803\n",
            "Iteration 87, loss = 0.31380945\n",
            "Iteration 88, loss = 0.32304627\n",
            "Iteration 89, loss = 0.31480230\n",
            "Iteration 90, loss = 0.30996734\n",
            "Iteration 91, loss = 0.30769618\n",
            "Iteration 92, loss = 0.30796942\n",
            "Iteration 93, loss = 0.30747639\n",
            "Iteration 94, loss = 0.31100851\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "52\n",
            "Iteration 1, loss = 1.37192270\n",
            "Iteration 2, loss = 1.29770012\n",
            "Iteration 3, loss = 1.21854795\n",
            "Iteration 4, loss = 1.12136435\n",
            "Iteration 5, loss = 1.00168645\n",
            "Iteration 6, loss = 0.86839226\n",
            "Iteration 7, loss = 0.74041688\n",
            "Iteration 8, loss = 0.63373772\n",
            "Iteration 9, loss = 0.54506538\n",
            "Iteration 10, loss = 0.48689058\n",
            "Iteration 11, loss = 0.44237213\n",
            "Iteration 12, loss = 0.42010550\n",
            "Iteration 13, loss = 0.40387086\n",
            "Iteration 14, loss = 0.39056369\n",
            "Iteration 15, loss = 0.37837667\n",
            "Iteration 16, loss = 0.37484276\n",
            "Iteration 17, loss = 0.37327413\n",
            "Iteration 18, loss = 0.36716680\n",
            "Iteration 19, loss = 0.37707927\n",
            "Iteration 20, loss = 0.36845513\n",
            "Iteration 21, loss = 0.36799556\n",
            "Iteration 22, loss = 0.36749568\n",
            "Iteration 23, loss = 0.36296572\n",
            "Iteration 24, loss = 0.36111622\n",
            "Iteration 25, loss = 0.36058702\n",
            "Iteration 26, loss = 0.36245199\n",
            "Iteration 27, loss = 0.36405556\n",
            "Iteration 28, loss = 0.36831447\n",
            "Iteration 29, loss = 0.36457092\n",
            "Iteration 30, loss = 0.36279773\n",
            "Iteration 31, loss = 0.36229564\n",
            "Iteration 32, loss = 0.35993189\n",
            "Iteration 33, loss = 0.35976617\n",
            "Iteration 34, loss = 0.36081746\n",
            "Iteration 35, loss = 0.35997609\n",
            "Iteration 36, loss = 0.35924552\n",
            "Iteration 37, loss = 0.35525670\n",
            "Iteration 38, loss = 0.36693835\n",
            "Iteration 39, loss = 0.35852218\n",
            "Iteration 40, loss = 0.35787113\n",
            "Iteration 41, loss = 0.35966251\n",
            "Iteration 42, loss = 0.36524409\n",
            "Iteration 43, loss = 0.36043646\n",
            "Iteration 44, loss = 0.36571033\n",
            "Iteration 45, loss = 0.37199761\n",
            "Iteration 46, loss = 0.36677414\n",
            "Iteration 47, loss = 0.36244759\n",
            "Iteration 48, loss = 0.36037855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37163780\n",
            "Iteration 2, loss = 1.29867183\n",
            "Iteration 3, loss = 1.22109485\n",
            "Iteration 4, loss = 1.12514268\n",
            "Iteration 5, loss = 1.00697147\n",
            "Iteration 6, loss = 0.87182898\n",
            "Iteration 7, loss = 0.74210021\n",
            "Iteration 8, loss = 0.64207741\n",
            "Iteration 9, loss = 0.55240136\n",
            "Iteration 10, loss = 0.49449526\n",
            "Iteration 11, loss = 0.44980324\n",
            "Iteration 12, loss = 0.42404007\n",
            "Iteration 13, loss = 0.40209435\n",
            "Iteration 14, loss = 0.38873012\n",
            "Iteration 15, loss = 0.38242096\n",
            "Iteration 16, loss = 0.37275003\n",
            "Iteration 17, loss = 0.37377439\n",
            "Iteration 18, loss = 0.36948474\n",
            "Iteration 19, loss = 0.37656347\n",
            "Iteration 20, loss = 0.36750680\n",
            "Iteration 21, loss = 0.36747531\n",
            "Iteration 22, loss = 0.37566351\n",
            "Iteration 23, loss = 0.35933210\n",
            "Iteration 24, loss = 0.35897278\n",
            "Iteration 25, loss = 0.36279383\n",
            "Iteration 26, loss = 0.36110437\n",
            "Iteration 27, loss = 0.35754586\n",
            "Iteration 28, loss = 0.36131681\n",
            "Iteration 29, loss = 0.36132866\n",
            "Iteration 30, loss = 0.36110661\n",
            "Iteration 31, loss = 0.35737325\n",
            "Iteration 32, loss = 0.35854514\n",
            "Iteration 33, loss = 0.36024103\n",
            "Iteration 34, loss = 0.35928233\n",
            "Iteration 35, loss = 0.35721069\n",
            "Iteration 36, loss = 0.35601828\n",
            "Iteration 37, loss = 0.35608370\n",
            "Iteration 38, loss = 0.36001498\n",
            "Iteration 39, loss = 0.35715660\n",
            "Iteration 40, loss = 0.35835936\n",
            "Iteration 41, loss = 0.36055430\n",
            "Iteration 42, loss = 0.36446346\n",
            "Iteration 43, loss = 0.35960241\n",
            "Iteration 44, loss = 0.36408902\n",
            "Iteration 45, loss = 0.36314728\n",
            "Iteration 46, loss = 0.35921292\n",
            "Iteration 47, loss = 0.35873320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36864532\n",
            "Iteration 2, loss = 1.29223403\n",
            "Iteration 3, loss = 1.21336479\n",
            "Iteration 4, loss = 1.11341838\n",
            "Iteration 5, loss = 0.99306444\n",
            "Iteration 6, loss = 0.85651471\n",
            "Iteration 7, loss = 0.72692081\n",
            "Iteration 8, loss = 0.62398702\n",
            "Iteration 9, loss = 0.53953886\n",
            "Iteration 10, loss = 0.48054710\n",
            "Iteration 11, loss = 0.44036084\n",
            "Iteration 12, loss = 0.41879844\n",
            "Iteration 13, loss = 0.39832822\n",
            "Iteration 14, loss = 0.38659212\n",
            "Iteration 15, loss = 0.38481966\n",
            "Iteration 16, loss = 0.37686155\n",
            "Iteration 17, loss = 0.37558623\n",
            "Iteration 18, loss = 0.37581487\n",
            "Iteration 19, loss = 0.37679796\n",
            "Iteration 20, loss = 0.37295647\n",
            "Iteration 21, loss = 0.37774746\n",
            "Iteration 22, loss = 0.37476480\n",
            "Iteration 23, loss = 0.36701586\n",
            "Iteration 24, loss = 0.36636784\n",
            "Iteration 25, loss = 0.36486847\n",
            "Iteration 26, loss = 0.36783740\n",
            "Iteration 27, loss = 0.36310132\n",
            "Iteration 28, loss = 0.36657696\n",
            "Iteration 29, loss = 0.36432764\n",
            "Iteration 30, loss = 0.37316937\n",
            "Iteration 31, loss = 0.36458028\n",
            "Iteration 32, loss = 0.36373248\n",
            "Iteration 33, loss = 0.36590630\n",
            "Iteration 34, loss = 0.36687027\n",
            "Iteration 35, loss = 0.36376091\n",
            "Iteration 36, loss = 0.36483165\n",
            "Iteration 37, loss = 0.36262009\n",
            "Iteration 38, loss = 0.36546013\n",
            "Iteration 39, loss = 0.36571047\n",
            "Iteration 40, loss = 0.36854708\n",
            "Iteration 41, loss = 0.37215758\n",
            "Iteration 42, loss = 0.36649158\n",
            "Iteration 43, loss = 0.36474525\n",
            "Iteration 44, loss = 0.36983705\n",
            "Iteration 45, loss = 0.36846111\n",
            "Iteration 46, loss = 0.36207968\n",
            "Iteration 47, loss = 0.36657409\n",
            "Iteration 48, loss = 0.36027571\n",
            "Iteration 49, loss = 0.36149911\n",
            "Iteration 50, loss = 0.36079125\n",
            "Iteration 51, loss = 0.36307700\n",
            "Iteration 52, loss = 0.36232874\n",
            "Iteration 53, loss = 0.36711406\n",
            "Iteration 54, loss = 0.36295596\n",
            "Iteration 55, loss = 0.36044186\n",
            "Iteration 56, loss = 0.36111111\n",
            "Iteration 57, loss = 0.36340080\n",
            "Iteration 58, loss = 0.36500211\n",
            "Iteration 59, loss = 0.36347124\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36880371\n",
            "Iteration 2, loss = 1.29278384\n",
            "Iteration 3, loss = 1.21260285\n",
            "Iteration 4, loss = 1.11194653\n",
            "Iteration 5, loss = 0.98937413\n",
            "Iteration 6, loss = 0.84955476\n",
            "Iteration 7, loss = 0.71344802\n",
            "Iteration 8, loss = 0.60662606\n",
            "Iteration 9, loss = 0.52532033\n",
            "Iteration 10, loss = 0.46996814\n",
            "Iteration 11, loss = 0.43634255\n",
            "Iteration 12, loss = 0.40933692\n",
            "Iteration 13, loss = 0.39101320\n",
            "Iteration 14, loss = 0.38034178\n",
            "Iteration 15, loss = 0.38098916\n",
            "Iteration 16, loss = 0.36822851\n",
            "Iteration 17, loss = 0.36846686\n",
            "Iteration 18, loss = 0.37214213\n",
            "Iteration 19, loss = 0.37060977\n",
            "Iteration 20, loss = 0.37184614\n",
            "Iteration 21, loss = 0.36573915\n",
            "Iteration 22, loss = 0.35904358\n",
            "Iteration 23, loss = 0.35984380\n",
            "Iteration 24, loss = 0.36068364\n",
            "Iteration 25, loss = 0.35694272\n",
            "Iteration 26, loss = 0.35846480\n",
            "Iteration 27, loss = 0.35471740\n",
            "Iteration 28, loss = 0.35742890\n",
            "Iteration 29, loss = 0.35643288\n",
            "Iteration 30, loss = 0.36705866\n",
            "Iteration 31, loss = 0.35698837\n",
            "Iteration 32, loss = 0.35706700\n",
            "Iteration 33, loss = 0.35829906\n",
            "Iteration 34, loss = 0.35852504\n",
            "Iteration 35, loss = 0.35292675\n",
            "Iteration 36, loss = 0.35697186\n",
            "Iteration 37, loss = 0.35303124\n",
            "Iteration 38, loss = 0.35231918\n",
            "Iteration 39, loss = 0.35655172\n",
            "Iteration 40, loss = 0.35669825\n",
            "Iteration 41, loss = 0.35934053\n",
            "Iteration 42, loss = 0.35803446\n",
            "Iteration 43, loss = 0.36161612\n",
            "Iteration 44, loss = 0.36438613\n",
            "Iteration 45, loss = 0.35474513\n",
            "Iteration 46, loss = 0.35170116\n",
            "Iteration 47, loss = 0.35568852\n",
            "Iteration 48, loss = 0.35105236\n",
            "Iteration 49, loss = 0.35450324\n",
            "Iteration 50, loss = 0.35130719\n",
            "Iteration 51, loss = 0.35574152\n",
            "Iteration 52, loss = 0.35933772\n",
            "Iteration 53, loss = 0.36294883\n",
            "Iteration 54, loss = 0.35141472\n",
            "Iteration 55, loss = 0.35479296\n",
            "Iteration 56, loss = 0.35367544\n",
            "Iteration 57, loss = 0.35604231\n",
            "Iteration 58, loss = 0.36042695\n",
            "Iteration 59, loss = 0.35394707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37090173\n",
            "Iteration 2, loss = 1.29813517\n",
            "Iteration 3, loss = 1.22284478\n",
            "Iteration 4, loss = 1.12923873\n",
            "Iteration 5, loss = 1.01232901\n",
            "Iteration 6, loss = 0.88049117\n",
            "Iteration 7, loss = 0.75364210\n",
            "Iteration 8, loss = 0.64192161\n",
            "Iteration 9, loss = 0.55657870\n",
            "Iteration 10, loss = 0.49299929\n",
            "Iteration 11, loss = 0.45685039\n",
            "Iteration 12, loss = 0.42999770\n",
            "Iteration 13, loss = 0.41134158\n",
            "Iteration 14, loss = 0.40095109\n",
            "Iteration 15, loss = 0.39834826\n",
            "Iteration 16, loss = 0.38681442\n",
            "Iteration 17, loss = 0.38688281\n",
            "Iteration 18, loss = 0.38907200\n",
            "Iteration 19, loss = 0.38648953\n",
            "Iteration 20, loss = 0.39442289\n",
            "Iteration 21, loss = 0.38840958\n",
            "Iteration 22, loss = 0.38043852\n",
            "Iteration 23, loss = 0.38266663\n",
            "Iteration 24, loss = 0.37634460\n",
            "Iteration 25, loss = 0.37921490\n",
            "Iteration 26, loss = 0.38657137\n",
            "Iteration 27, loss = 0.37832157\n",
            "Iteration 28, loss = 0.37855415\n",
            "Iteration 29, loss = 0.37802951\n",
            "Iteration 30, loss = 0.38468585\n",
            "Iteration 31, loss = 0.38103629\n",
            "Iteration 32, loss = 0.38078130\n",
            "Iteration 33, loss = 0.37562415\n",
            "Iteration 34, loss = 0.37497582\n",
            "Iteration 35, loss = 0.37225203\n",
            "Iteration 36, loss = 0.37704624\n",
            "Iteration 37, loss = 0.37186403\n",
            "Iteration 38, loss = 0.37333634\n",
            "Iteration 39, loss = 0.37958863\n",
            "Iteration 40, loss = 0.37984659\n",
            "Iteration 41, loss = 0.38436143\n",
            "Iteration 42, loss = 0.37644708\n",
            "Iteration 43, loss = 0.38274751\n",
            "Iteration 44, loss = 0.38546662\n",
            "Iteration 45, loss = 0.37522790\n",
            "Iteration 46, loss = 0.37469189\n",
            "Iteration 47, loss = 0.37515434\n",
            "Iteration 48, loss = 0.36953731\n",
            "Iteration 49, loss = 0.37536975\n",
            "Iteration 50, loss = 0.37251416\n",
            "Iteration 51, loss = 0.37575797\n",
            "Iteration 52, loss = 0.37740781\n",
            "Iteration 53, loss = 0.37459058\n",
            "Iteration 54, loss = 0.37220004\n",
            "Iteration 55, loss = 0.37162765\n",
            "Iteration 56, loss = 0.37291300\n",
            "Iteration 57, loss = 0.37299032\n",
            "Iteration 58, loss = 0.37468072\n",
            "Iteration 59, loss = 0.37281070\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36159573\n",
            "Iteration 2, loss = 1.29858556\n",
            "Iteration 3, loss = 1.23128967\n",
            "Iteration 4, loss = 1.14255637\n",
            "Iteration 5, loss = 1.03359759\n",
            "Iteration 6, loss = 0.90224191\n",
            "Iteration 7, loss = 0.76293316\n",
            "Iteration 8, loss = 0.63917815\n",
            "Iteration 9, loss = 0.55158233\n",
            "Iteration 10, loss = 0.48700398\n",
            "Iteration 11, loss = 0.45693425\n",
            "Iteration 12, loss = 0.43538220\n",
            "Iteration 13, loss = 0.41608269\n",
            "Iteration 14, loss = 0.41044511\n",
            "Iteration 15, loss = 0.40517303\n",
            "Iteration 16, loss = 0.39712731\n",
            "Iteration 17, loss = 0.39903314\n",
            "Iteration 18, loss = 0.39944905\n",
            "Iteration 19, loss = 0.39600466\n",
            "Iteration 20, loss = 0.39961925\n",
            "Iteration 21, loss = 0.39522152\n",
            "Iteration 22, loss = 0.39004709\n",
            "Iteration 23, loss = 0.39579650\n",
            "Iteration 24, loss = 0.38862908\n",
            "Iteration 25, loss = 0.39111643\n",
            "Iteration 26, loss = 0.39863326\n",
            "Iteration 27, loss = 0.39389043\n",
            "Iteration 28, loss = 0.38902126\n",
            "Iteration 29, loss = 0.39083791\n",
            "Iteration 30, loss = 0.39549975\n",
            "Iteration 31, loss = 0.39269492\n",
            "Iteration 32, loss = 0.39128822\n",
            "Iteration 33, loss = 0.38956627\n",
            "Iteration 34, loss = 0.39066774\n",
            "Iteration 35, loss = 0.38633185\n",
            "Iteration 36, loss = 0.38973751\n",
            "Iteration 37, loss = 0.38757464\n",
            "Iteration 38, loss = 0.38576003\n",
            "Iteration 39, loss = 0.39063377\n",
            "Iteration 40, loss = 0.39287832\n",
            "Iteration 41, loss = 0.39737568\n",
            "Iteration 42, loss = 0.39008570\n",
            "Iteration 43, loss = 0.39736015\n",
            "Iteration 44, loss = 0.39531288\n",
            "Iteration 45, loss = 0.38935641\n",
            "Iteration 46, loss = 0.39422273\n",
            "Iteration 47, loss = 0.38952316\n",
            "Iteration 48, loss = 0.38430863\n",
            "Iteration 49, loss = 0.38604667\n",
            "Iteration 50, loss = 0.38557825\n",
            "Iteration 51, loss = 0.38763347\n",
            "Iteration 52, loss = 0.38997491\n",
            "Iteration 53, loss = 0.38690545\n",
            "Iteration 54, loss = 0.38660640\n",
            "Iteration 55, loss = 0.38621782\n",
            "Iteration 56, loss = 0.39029562\n",
            "Iteration 57, loss = 0.38393983\n",
            "Iteration 58, loss = 0.38769101\n",
            "Iteration 59, loss = 0.38547466\n",
            "Iteration 60, loss = 0.38963874\n",
            "Iteration 61, loss = 0.38690178\n",
            "Iteration 62, loss = 0.39096082\n",
            "Iteration 63, loss = 0.38214080\n",
            "Iteration 64, loss = 0.38995499\n",
            "Iteration 65, loss = 0.38430051\n",
            "Iteration 66, loss = 0.38441292\n",
            "Iteration 67, loss = 0.38351191\n",
            "Iteration 68, loss = 0.38690543\n",
            "Iteration 69, loss = 0.38625194\n",
            "Iteration 70, loss = 0.38612539\n",
            "Iteration 71, loss = 0.38252478\n",
            "Iteration 72, loss = 0.38224191\n",
            "Iteration 73, loss = 0.38477671\n",
            "Iteration 74, loss = 0.38639558\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35140769\n",
            "Iteration 2, loss = 1.28697004\n",
            "Iteration 3, loss = 1.22211674\n",
            "Iteration 4, loss = 1.14074560\n",
            "Iteration 5, loss = 1.03393601\n",
            "Iteration 6, loss = 0.90700555\n",
            "Iteration 7, loss = 0.77267004\n",
            "Iteration 8, loss = 0.65319327\n",
            "Iteration 9, loss = 0.56311893\n",
            "Iteration 10, loss = 0.49465678\n",
            "Iteration 11, loss = 0.46137710\n",
            "Iteration 12, loss = 0.43694254\n",
            "Iteration 13, loss = 0.41752590\n",
            "Iteration 14, loss = 0.41050023\n",
            "Iteration 15, loss = 0.40368693\n",
            "Iteration 16, loss = 0.39721674\n",
            "Iteration 17, loss = 0.39579024\n",
            "Iteration 18, loss = 0.39614387\n",
            "Iteration 19, loss = 0.39423733\n",
            "Iteration 20, loss = 0.39888031\n",
            "Iteration 21, loss = 0.39227611\n",
            "Iteration 22, loss = 0.38756341\n",
            "Iteration 23, loss = 0.39216361\n",
            "Iteration 24, loss = 0.38676308\n",
            "Iteration 25, loss = 0.38790304\n",
            "Iteration 26, loss = 0.39569797\n",
            "Iteration 27, loss = 0.39123392\n",
            "Iteration 28, loss = 0.38601722\n",
            "Iteration 29, loss = 0.38848918\n",
            "Iteration 30, loss = 0.39246253\n",
            "Iteration 31, loss = 0.38952146\n",
            "Iteration 32, loss = 0.38844182\n",
            "Iteration 33, loss = 0.38720123\n",
            "Iteration 34, loss = 0.38910397\n",
            "Iteration 35, loss = 0.38568808\n",
            "Iteration 36, loss = 0.38565782\n",
            "Iteration 37, loss = 0.38647327\n",
            "Iteration 38, loss = 0.38263222\n",
            "Iteration 39, loss = 0.38963335\n",
            "Iteration 40, loss = 0.39174683\n",
            "Iteration 41, loss = 0.39793343\n",
            "Iteration 42, loss = 0.39132132\n",
            "Iteration 43, loss = 0.39775185\n",
            "Iteration 44, loss = 0.39615013\n",
            "Iteration 45, loss = 0.38690448\n",
            "Iteration 46, loss = 0.38955904\n",
            "Iteration 47, loss = 0.38841493\n",
            "Iteration 48, loss = 0.38234441\n",
            "Iteration 49, loss = 0.38339403\n",
            "Iteration 50, loss = 0.38202036\n",
            "Iteration 51, loss = 0.38479221\n",
            "Iteration 52, loss = 0.38821211\n",
            "Iteration 53, loss = 0.38482292\n",
            "Iteration 54, loss = 0.38394049\n",
            "Iteration 55, loss = 0.38413281\n",
            "Iteration 56, loss = 0.38617298\n",
            "Iteration 57, loss = 0.38204416\n",
            "Iteration 58, loss = 0.38667418\n",
            "Iteration 59, loss = 0.38263557\n",
            "Iteration 60, loss = 0.38919788\n",
            "Iteration 61, loss = 0.38586997\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36257193\n",
            "Iteration 2, loss = 1.29788883\n",
            "Iteration 3, loss = 1.23168129\n",
            "Iteration 4, loss = 1.14833600\n",
            "Iteration 5, loss = 1.03900402\n",
            "Iteration 6, loss = 0.90974268\n",
            "Iteration 7, loss = 0.77840829\n",
            "Iteration 8, loss = 0.65834407\n",
            "Iteration 9, loss = 0.56881787\n",
            "Iteration 10, loss = 0.49866252\n",
            "Iteration 11, loss = 0.46039166\n",
            "Iteration 12, loss = 0.43674887\n",
            "Iteration 13, loss = 0.41944464\n",
            "Iteration 14, loss = 0.41096444\n",
            "Iteration 15, loss = 0.40421096\n",
            "Iteration 16, loss = 0.39565532\n",
            "Iteration 17, loss = 0.39400786\n",
            "Iteration 18, loss = 0.39469832\n",
            "Iteration 19, loss = 0.39290442\n",
            "Iteration 20, loss = 0.39742848\n",
            "Iteration 21, loss = 0.38727493\n",
            "Iteration 22, loss = 0.38697373\n",
            "Iteration 23, loss = 0.39032233\n",
            "Iteration 24, loss = 0.38679046\n",
            "Iteration 25, loss = 0.38714101\n",
            "Iteration 26, loss = 0.39336735\n",
            "Iteration 27, loss = 0.38897502\n",
            "Iteration 28, loss = 0.38654563\n",
            "Iteration 29, loss = 0.39098723\n",
            "Iteration 30, loss = 0.39133151\n",
            "Iteration 31, loss = 0.39000033\n",
            "Iteration 32, loss = 0.38885770\n",
            "Iteration 33, loss = 0.38824213\n",
            "Iteration 34, loss = 0.39031578\n",
            "Iteration 35, loss = 0.38507951\n",
            "Iteration 36, loss = 0.38274222\n",
            "Iteration 37, loss = 0.38628566\n",
            "Iteration 38, loss = 0.38325614\n",
            "Iteration 39, loss = 0.38867795\n",
            "Iteration 40, loss = 0.38844676\n",
            "Iteration 41, loss = 0.39713282\n",
            "Iteration 42, loss = 0.38948931\n",
            "Iteration 43, loss = 0.39660963\n",
            "Iteration 44, loss = 0.39535609\n",
            "Iteration 45, loss = 0.39041567\n",
            "Iteration 46, loss = 0.38966977\n",
            "Iteration 47, loss = 0.39107623\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36409341\n",
            "Iteration 2, loss = 1.29124031\n",
            "Iteration 3, loss = 1.21330433\n",
            "Iteration 4, loss = 1.11789037\n",
            "Iteration 5, loss = 1.00067822\n",
            "Iteration 6, loss = 0.86938877\n",
            "Iteration 7, loss = 0.74142842\n",
            "Iteration 8, loss = 0.62880163\n",
            "Iteration 9, loss = 0.54450859\n",
            "Iteration 10, loss = 0.47766599\n",
            "Iteration 11, loss = 0.43832188\n",
            "Iteration 12, loss = 0.40809651\n",
            "Iteration 13, loss = 0.39205632\n",
            "Iteration 14, loss = 0.38205535\n",
            "Iteration 15, loss = 0.37421694\n",
            "Iteration 16, loss = 0.36916758\n",
            "Iteration 17, loss = 0.36546770\n",
            "Iteration 18, loss = 0.36616634\n",
            "Iteration 19, loss = 0.35912195\n",
            "Iteration 20, loss = 0.36235448\n",
            "Iteration 21, loss = 0.36220956\n",
            "Iteration 22, loss = 0.35572323\n",
            "Iteration 23, loss = 0.36159837\n",
            "Iteration 24, loss = 0.35552698\n",
            "Iteration 25, loss = 0.35695446\n",
            "Iteration 26, loss = 0.36486731\n",
            "Iteration 27, loss = 0.35956826\n",
            "Iteration 28, loss = 0.35461170\n",
            "Iteration 29, loss = 0.35900795\n",
            "Iteration 30, loss = 0.35553797\n",
            "Iteration 31, loss = 0.35319530\n",
            "Iteration 32, loss = 0.35376974\n",
            "Iteration 33, loss = 0.35368206\n",
            "Iteration 34, loss = 0.35617557\n",
            "Iteration 35, loss = 0.35188596\n",
            "Iteration 36, loss = 0.35040494\n",
            "Iteration 37, loss = 0.35374620\n",
            "Iteration 38, loss = 0.34933168\n",
            "Iteration 39, loss = 0.35359702\n",
            "Iteration 40, loss = 0.35331311\n",
            "Iteration 41, loss = 0.35808223\n",
            "Iteration 42, loss = 0.35431436\n",
            "Iteration 43, loss = 0.36235458\n",
            "Iteration 44, loss = 0.36273229\n",
            "Iteration 45, loss = 0.36015645\n",
            "Iteration 46, loss = 0.35396458\n",
            "Iteration 47, loss = 0.36082406\n",
            "Iteration 48, loss = 0.34884882\n",
            "Iteration 49, loss = 0.34954416\n",
            "Iteration 50, loss = 0.34802500\n",
            "Iteration 51, loss = 0.35098358\n",
            "Iteration 52, loss = 0.36040127\n",
            "Iteration 53, loss = 0.35485021\n",
            "Iteration 54, loss = 0.34947792\n",
            "Iteration 55, loss = 0.35051990\n",
            "Iteration 56, loss = 0.35293811\n",
            "Iteration 57, loss = 0.35089190\n",
            "Iteration 58, loss = 0.35246299\n",
            "Iteration 59, loss = 0.34730402\n",
            "Iteration 60, loss = 0.35163639\n",
            "Iteration 61, loss = 0.34774431\n",
            "Iteration 62, loss = 0.35093648\n",
            "Iteration 63, loss = 0.34568719\n",
            "Iteration 64, loss = 0.35639387\n",
            "Iteration 65, loss = 0.35065073\n",
            "Iteration 66, loss = 0.35027168\n",
            "Iteration 67, loss = 0.35072965\n",
            "Iteration 68, loss = 0.34973913\n",
            "Iteration 69, loss = 0.34650728\n",
            "Iteration 70, loss = 0.34655689\n",
            "Iteration 71, loss = 0.34627692\n",
            "Iteration 72, loss = 0.34643776\n",
            "Iteration 73, loss = 0.34587790\n",
            "Iteration 74, loss = 0.34943192\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35537920\n",
            "Iteration 2, loss = 1.28196263\n",
            "Iteration 3, loss = 1.19849995\n",
            "Iteration 4, loss = 1.09136263\n",
            "Iteration 5, loss = 0.96173078\n",
            "Iteration 6, loss = 0.82494089\n",
            "Iteration 7, loss = 0.70068952\n",
            "Iteration 8, loss = 0.59827211\n",
            "Iteration 9, loss = 0.52593483\n",
            "Iteration 10, loss = 0.47221837\n",
            "Iteration 11, loss = 0.44304327\n",
            "Iteration 12, loss = 0.41752431\n",
            "Iteration 13, loss = 0.40250239\n",
            "Iteration 14, loss = 0.39163378\n",
            "Iteration 15, loss = 0.38454118\n",
            "Iteration 16, loss = 0.38298219\n",
            "Iteration 17, loss = 0.38040979\n",
            "Iteration 18, loss = 0.37731613\n",
            "Iteration 19, loss = 0.37104140\n",
            "Iteration 20, loss = 0.37291434\n",
            "Iteration 21, loss = 0.36724665\n",
            "Iteration 22, loss = 0.36379455\n",
            "Iteration 23, loss = 0.37087751\n",
            "Iteration 24, loss = 0.36438797\n",
            "Iteration 25, loss = 0.36454703\n",
            "Iteration 26, loss = 0.37220379\n",
            "Iteration 27, loss = 0.36639318\n",
            "Iteration 28, loss = 0.36546180\n",
            "Iteration 29, loss = 0.36737082\n",
            "Iteration 30, loss = 0.36181281\n",
            "Iteration 31, loss = 0.36195353\n",
            "Iteration 32, loss = 0.36754359\n",
            "Iteration 33, loss = 0.36313351\n",
            "Iteration 34, loss = 0.36407584\n",
            "Iteration 35, loss = 0.36111395\n",
            "Iteration 36, loss = 0.35991755\n",
            "Iteration 37, loss = 0.36168334\n",
            "Iteration 38, loss = 0.36052015\n",
            "Iteration 39, loss = 0.36130205\n",
            "Iteration 40, loss = 0.36076000\n",
            "Iteration 41, loss = 0.36150729\n",
            "Iteration 42, loss = 0.36437205\n",
            "Iteration 43, loss = 0.37299236\n",
            "Iteration 44, loss = 0.36907968\n",
            "Iteration 45, loss = 0.37305419\n",
            "Iteration 46, loss = 0.37146416\n",
            "Iteration 47, loss = 0.36838172\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "53\n",
            "Iteration 1, loss = 1.36855822\n",
            "Iteration 2, loss = 1.29580143\n",
            "Iteration 3, loss = 1.20568616\n",
            "Iteration 4, loss = 1.08619986\n",
            "Iteration 5, loss = 0.94049226\n",
            "Iteration 6, loss = 0.78471491\n",
            "Iteration 7, loss = 0.64374041\n",
            "Iteration 8, loss = 0.53575926\n",
            "Iteration 9, loss = 0.45533586\n",
            "Iteration 10, loss = 0.40646521\n",
            "Iteration 11, loss = 0.37979886\n",
            "Iteration 12, loss = 0.36329052\n",
            "Iteration 13, loss = 0.34546696\n",
            "Iteration 14, loss = 0.33987261\n",
            "Iteration 15, loss = 0.33564803\n",
            "Iteration 16, loss = 0.33670442\n",
            "Iteration 17, loss = 0.33079254\n",
            "Iteration 18, loss = 0.32919109\n",
            "Iteration 19, loss = 0.32534167\n",
            "Iteration 20, loss = 0.33330145\n",
            "Iteration 21, loss = 0.32749072\n",
            "Iteration 22, loss = 0.32819198\n",
            "Iteration 23, loss = 0.32857135\n",
            "Iteration 24, loss = 0.32499577\n",
            "Iteration 25, loss = 0.32125412\n",
            "Iteration 26, loss = 0.32368618\n",
            "Iteration 27, loss = 0.32281515\n",
            "Iteration 28, loss = 0.32704093\n",
            "Iteration 29, loss = 0.32017515\n",
            "Iteration 30, loss = 0.32194196\n",
            "Iteration 31, loss = 0.31802691\n",
            "Iteration 32, loss = 0.31788700\n",
            "Iteration 33, loss = 0.31729141\n",
            "Iteration 34, loss = 0.32133419\n",
            "Iteration 35, loss = 0.32399706\n",
            "Iteration 36, loss = 0.32309094\n",
            "Iteration 37, loss = 0.31912041\n",
            "Iteration 38, loss = 0.32010631\n",
            "Iteration 39, loss = 0.32018474\n",
            "Iteration 40, loss = 0.31920486\n",
            "Iteration 41, loss = 0.32337574\n",
            "Iteration 42, loss = 0.32103277\n",
            "Iteration 43, loss = 0.31721987\n",
            "Iteration 44, loss = 0.31608684\n",
            "Iteration 45, loss = 0.31691712\n",
            "Iteration 46, loss = 0.31672014\n",
            "Iteration 47, loss = 0.31464403\n",
            "Iteration 48, loss = 0.31689402\n",
            "Iteration 49, loss = 0.31802463\n",
            "Iteration 50, loss = 0.31874832\n",
            "Iteration 51, loss = 0.31967939\n",
            "Iteration 52, loss = 0.32478382\n",
            "Iteration 53, loss = 0.32055919\n",
            "Iteration 54, loss = 0.31641601\n",
            "Iteration 55, loss = 0.31632528\n",
            "Iteration 56, loss = 0.31451135\n",
            "Iteration 57, loss = 0.31450441\n",
            "Iteration 58, loss = 0.31282918\n",
            "Iteration 59, loss = 0.31393922\n",
            "Iteration 60, loss = 0.32062271\n",
            "Iteration 61, loss = 0.32216090\n",
            "Iteration 62, loss = 0.31894725\n",
            "Iteration 63, loss = 0.31878880\n",
            "Iteration 64, loss = 0.31587947\n",
            "Iteration 65, loss = 0.31293493\n",
            "Iteration 66, loss = 0.31175582\n",
            "Iteration 67, loss = 0.31184216\n",
            "Iteration 68, loss = 0.31115488\n",
            "Iteration 69, loss = 0.31124531\n",
            "Iteration 70, loss = 0.31130850\n",
            "Iteration 71, loss = 0.31125511\n",
            "Iteration 72, loss = 0.31336673\n",
            "Iteration 73, loss = 0.31897288\n",
            "Iteration 74, loss = 0.31363070\n",
            "Iteration 75, loss = 0.31813571\n",
            "Iteration 76, loss = 0.31532257\n",
            "Iteration 77, loss = 0.31254059\n",
            "Iteration 78, loss = 0.31457168\n",
            "Iteration 79, loss = 0.31453430\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36701936\n",
            "Iteration 2, loss = 1.29588503\n",
            "Iteration 3, loss = 1.20848463\n",
            "Iteration 4, loss = 1.09357396\n",
            "Iteration 5, loss = 0.95203448\n",
            "Iteration 6, loss = 0.79796983\n",
            "Iteration 7, loss = 0.65617012\n",
            "Iteration 8, loss = 0.54849660\n",
            "Iteration 9, loss = 0.46563972\n",
            "Iteration 10, loss = 0.41886958\n",
            "Iteration 11, loss = 0.39008050\n",
            "Iteration 12, loss = 0.37440409\n",
            "Iteration 13, loss = 0.35491163\n",
            "Iteration 14, loss = 0.34827834\n",
            "Iteration 15, loss = 0.34173573\n",
            "Iteration 16, loss = 0.33927287\n",
            "Iteration 17, loss = 0.33624601\n",
            "Iteration 18, loss = 0.33460955\n",
            "Iteration 19, loss = 0.33018123\n",
            "Iteration 20, loss = 0.33713136\n",
            "Iteration 21, loss = 0.34049635\n",
            "Iteration 22, loss = 0.33789261\n",
            "Iteration 23, loss = 0.32815618\n",
            "Iteration 24, loss = 0.32533328\n",
            "Iteration 25, loss = 0.32523310\n",
            "Iteration 26, loss = 0.32463139\n",
            "Iteration 27, loss = 0.32564418\n",
            "Iteration 28, loss = 0.33457260\n",
            "Iteration 29, loss = 0.32782657\n",
            "Iteration 30, loss = 0.33621703\n",
            "Iteration 31, loss = 0.32883027\n",
            "Iteration 32, loss = 0.32468543\n",
            "Iteration 33, loss = 0.32114297\n",
            "Iteration 34, loss = 0.32401989\n",
            "Iteration 35, loss = 0.32585870\n",
            "Iteration 36, loss = 0.32550967\n",
            "Iteration 37, loss = 0.32349470\n",
            "Iteration 38, loss = 0.32258249\n",
            "Iteration 39, loss = 0.32339563\n",
            "Iteration 40, loss = 0.32470533\n",
            "Iteration 41, loss = 0.32603226\n",
            "Iteration 42, loss = 0.32608177\n",
            "Iteration 43, loss = 0.32523875\n",
            "Iteration 44, loss = 0.32204048\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36502769\n",
            "Iteration 2, loss = 1.28908345\n",
            "Iteration 3, loss = 1.19626911\n",
            "Iteration 4, loss = 1.07413605\n",
            "Iteration 5, loss = 0.92942594\n",
            "Iteration 6, loss = 0.77670719\n",
            "Iteration 7, loss = 0.63855044\n",
            "Iteration 8, loss = 0.53285859\n",
            "Iteration 9, loss = 0.45474733\n",
            "Iteration 10, loss = 0.40627780\n",
            "Iteration 11, loss = 0.37419130\n",
            "Iteration 12, loss = 0.36133222\n",
            "Iteration 13, loss = 0.34273923\n",
            "Iteration 14, loss = 0.33383073\n",
            "Iteration 15, loss = 0.32724912\n",
            "Iteration 16, loss = 0.32783997\n",
            "Iteration 17, loss = 0.32176602\n",
            "Iteration 18, loss = 0.31727407\n",
            "Iteration 19, loss = 0.31468541\n",
            "Iteration 20, loss = 0.32218473\n",
            "Iteration 21, loss = 0.31808688\n",
            "Iteration 22, loss = 0.31919367\n",
            "Iteration 23, loss = 0.31093173\n",
            "Iteration 24, loss = 0.31057467\n",
            "Iteration 25, loss = 0.30971331\n",
            "Iteration 26, loss = 0.31082996\n",
            "Iteration 27, loss = 0.31423206\n",
            "Iteration 28, loss = 0.31599642\n",
            "Iteration 29, loss = 0.31057462\n",
            "Iteration 30, loss = 0.32384437\n",
            "Iteration 31, loss = 0.31315503\n",
            "Iteration 32, loss = 0.30945699\n",
            "Iteration 33, loss = 0.30750554\n",
            "Iteration 34, loss = 0.30560257\n",
            "Iteration 35, loss = 0.30873705\n",
            "Iteration 36, loss = 0.31044693\n",
            "Iteration 37, loss = 0.31068708\n",
            "Iteration 38, loss = 0.30618724\n",
            "Iteration 39, loss = 0.30788787\n",
            "Iteration 40, loss = 0.30549401\n",
            "Iteration 41, loss = 0.30825641\n",
            "Iteration 42, loss = 0.30410661\n",
            "Iteration 43, loss = 0.30739555\n",
            "Iteration 44, loss = 0.30726150\n",
            "Iteration 45, loss = 0.30432982\n",
            "Iteration 46, loss = 0.31190350\n",
            "Iteration 47, loss = 0.30377845\n",
            "Iteration 48, loss = 0.30409850\n",
            "Iteration 49, loss = 0.30310560\n",
            "Iteration 50, loss = 0.30180645\n",
            "Iteration 51, loss = 0.30227257\n",
            "Iteration 52, loss = 0.30805833\n",
            "Iteration 53, loss = 0.30581457\n",
            "Iteration 54, loss = 0.30326112\n",
            "Iteration 55, loss = 0.30301646\n",
            "Iteration 56, loss = 0.30284462\n",
            "Iteration 57, loss = 0.30244779\n",
            "Iteration 58, loss = 0.30247437\n",
            "Iteration 59, loss = 0.29961566\n",
            "Iteration 60, loss = 0.30495367\n",
            "Iteration 61, loss = 0.30303923\n",
            "Iteration 62, loss = 0.30516893\n",
            "Iteration 63, loss = 0.30642851\n",
            "Iteration 64, loss = 0.30236022\n",
            "Iteration 65, loss = 0.30275771\n",
            "Iteration 66, loss = 0.30099641\n",
            "Iteration 67, loss = 0.30075652\n",
            "Iteration 68, loss = 0.29820903\n",
            "Iteration 69, loss = 0.30497975\n",
            "Iteration 70, loss = 0.30105343\n",
            "Iteration 71, loss = 0.30071881\n",
            "Iteration 72, loss = 0.29941273\n",
            "Iteration 73, loss = 0.30110756\n",
            "Iteration 74, loss = 0.30275858\n",
            "Iteration 75, loss = 0.30170438\n",
            "Iteration 76, loss = 0.29786903\n",
            "Iteration 77, loss = 0.30004655\n",
            "Iteration 78, loss = 0.29854220\n",
            "Iteration 79, loss = 0.30353118\n",
            "Iteration 80, loss = 0.29813790\n",
            "Iteration 81, loss = 0.29763134\n",
            "Iteration 82, loss = 0.29801035\n",
            "Iteration 83, loss = 0.30051885\n",
            "Iteration 84, loss = 0.30562826\n",
            "Iteration 85, loss = 0.30001334\n",
            "Iteration 86, loss = 0.29858313\n",
            "Iteration 87, loss = 0.29737474\n",
            "Iteration 88, loss = 0.30257981\n",
            "Iteration 89, loss = 0.29780583\n",
            "Iteration 90, loss = 0.29526403\n",
            "Iteration 91, loss = 0.30001590\n",
            "Iteration 92, loss = 0.29658479\n",
            "Iteration 93, loss = 0.29677289\n",
            "Iteration 94, loss = 0.29319808\n",
            "Iteration 95, loss = 0.29947838\n",
            "Iteration 96, loss = 0.29601236\n",
            "Iteration 97, loss = 0.29724897\n",
            "Iteration 98, loss = 0.29356796\n",
            "Iteration 99, loss = 0.29449464\n",
            "Iteration 100, loss = 0.29714220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36488197\n",
            "Iteration 2, loss = 1.28980175\n",
            "Iteration 3, loss = 1.19743272\n",
            "Iteration 4, loss = 1.07740285\n",
            "Iteration 5, loss = 0.93308078\n",
            "Iteration 6, loss = 0.77968745\n",
            "Iteration 7, loss = 0.63956946\n",
            "Iteration 8, loss = 0.53235331\n",
            "Iteration 9, loss = 0.45652541\n",
            "Iteration 10, loss = 0.41057457\n",
            "Iteration 11, loss = 0.38614766\n",
            "Iteration 12, loss = 0.36992029\n",
            "Iteration 13, loss = 0.35476572\n",
            "Iteration 14, loss = 0.34655440\n",
            "Iteration 15, loss = 0.34301020\n",
            "Iteration 16, loss = 0.33916314\n",
            "Iteration 17, loss = 0.33618024\n",
            "Iteration 18, loss = 0.33293689\n",
            "Iteration 19, loss = 0.33020272\n",
            "Iteration 20, loss = 0.33650103\n",
            "Iteration 21, loss = 0.33672011\n",
            "Iteration 22, loss = 0.33186412\n",
            "Iteration 23, loss = 0.33030627\n",
            "Iteration 24, loss = 0.32771861\n",
            "Iteration 25, loss = 0.32672077\n",
            "Iteration 26, loss = 0.32611413\n",
            "Iteration 27, loss = 0.32870369\n",
            "Iteration 28, loss = 0.33183369\n",
            "Iteration 29, loss = 0.33212001\n",
            "Iteration 30, loss = 0.33941044\n",
            "Iteration 31, loss = 0.33489091\n",
            "Iteration 32, loss = 0.32841213\n",
            "Iteration 33, loss = 0.32545506\n",
            "Iteration 34, loss = 0.32306850\n",
            "Iteration 35, loss = 0.32495166\n",
            "Iteration 36, loss = 0.32949451\n",
            "Iteration 37, loss = 0.32982915\n",
            "Iteration 38, loss = 0.32717992\n",
            "Iteration 39, loss = 0.32730158\n",
            "Iteration 40, loss = 0.32959208\n",
            "Iteration 41, loss = 0.32795277\n",
            "Iteration 42, loss = 0.32660912\n",
            "Iteration 43, loss = 0.33007199\n",
            "Iteration 44, loss = 0.32982857\n",
            "Iteration 45, loss = 0.32334209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36672354\n",
            "Iteration 2, loss = 1.29240903\n",
            "Iteration 3, loss = 1.20193363\n",
            "Iteration 4, loss = 1.08438038\n",
            "Iteration 5, loss = 0.94318670\n",
            "Iteration 6, loss = 0.79378213\n",
            "Iteration 7, loss = 0.65496160\n",
            "Iteration 8, loss = 0.54417785\n",
            "Iteration 9, loss = 0.46990402\n",
            "Iteration 10, loss = 0.42159786\n",
            "Iteration 11, loss = 0.39813215\n",
            "Iteration 12, loss = 0.37783335\n",
            "Iteration 13, loss = 0.36586713\n",
            "Iteration 14, loss = 0.35952115\n",
            "Iteration 15, loss = 0.35431699\n",
            "Iteration 16, loss = 0.34925562\n",
            "Iteration 17, loss = 0.34526876\n",
            "Iteration 18, loss = 0.34372767\n",
            "Iteration 19, loss = 0.34456989\n",
            "Iteration 20, loss = 0.34897039\n",
            "Iteration 21, loss = 0.34697762\n",
            "Iteration 22, loss = 0.34334910\n",
            "Iteration 23, loss = 0.34043300\n",
            "Iteration 24, loss = 0.33977000\n",
            "Iteration 25, loss = 0.34064043\n",
            "Iteration 26, loss = 0.34235216\n",
            "Iteration 27, loss = 0.34171557\n",
            "Iteration 28, loss = 0.33895495\n",
            "Iteration 29, loss = 0.34068222\n",
            "Iteration 30, loss = 0.34103477\n",
            "Iteration 31, loss = 0.34108218\n",
            "Iteration 32, loss = 0.33784053\n",
            "Iteration 33, loss = 0.33606575\n",
            "Iteration 34, loss = 0.33669261\n",
            "Iteration 35, loss = 0.33589777\n",
            "Iteration 36, loss = 0.33867079\n",
            "Iteration 37, loss = 0.34043970\n",
            "Iteration 38, loss = 0.34038880\n",
            "Iteration 39, loss = 0.33953423\n",
            "Iteration 40, loss = 0.34363394\n",
            "Iteration 41, loss = 0.33992157\n",
            "Iteration 42, loss = 0.33842761\n",
            "Iteration 43, loss = 0.34063759\n",
            "Iteration 44, loss = 0.34278329\n",
            "Iteration 45, loss = 0.33503989\n",
            "Iteration 46, loss = 0.33997171\n",
            "Iteration 47, loss = 0.33521592\n",
            "Iteration 48, loss = 0.33418829\n",
            "Iteration 49, loss = 0.33477965\n",
            "Iteration 50, loss = 0.33352721\n",
            "Iteration 51, loss = 0.33588984\n",
            "Iteration 52, loss = 0.33472524\n",
            "Iteration 53, loss = 0.33429836\n",
            "Iteration 54, loss = 0.33401656\n",
            "Iteration 55, loss = 0.33507977\n",
            "Iteration 56, loss = 0.33424697\n",
            "Iteration 57, loss = 0.33240735\n",
            "Iteration 58, loss = 0.33340197\n",
            "Iteration 59, loss = 0.33345008\n",
            "Iteration 60, loss = 0.33876986\n",
            "Iteration 61, loss = 0.34227068\n",
            "Iteration 62, loss = 0.34018165\n",
            "Iteration 63, loss = 0.34044421\n",
            "Iteration 64, loss = 0.33464062\n",
            "Iteration 65, loss = 0.33321051\n",
            "Iteration 66, loss = 0.33352214\n",
            "Iteration 67, loss = 0.33086246\n",
            "Iteration 68, loss = 0.33114628\n",
            "Iteration 69, loss = 0.33381968\n",
            "Iteration 70, loss = 0.33341413\n",
            "Iteration 71, loss = 0.33328893\n",
            "Iteration 72, loss = 0.32991795\n",
            "Iteration 73, loss = 0.33423983\n",
            "Iteration 74, loss = 0.33639007\n",
            "Iteration 75, loss = 0.33170712\n",
            "Iteration 76, loss = 0.33065654\n",
            "Iteration 77, loss = 0.33190800\n",
            "Iteration 78, loss = 0.33057698\n",
            "Iteration 79, loss = 0.33146601\n",
            "Iteration 80, loss = 0.33310829\n",
            "Iteration 81, loss = 0.33346420\n",
            "Iteration 82, loss = 0.32854520\n",
            "Iteration 83, loss = 0.33498676\n",
            "Iteration 84, loss = 0.33751526\n",
            "Iteration 85, loss = 0.33282615\n",
            "Iteration 86, loss = 0.33534999\n",
            "Iteration 87, loss = 0.33045007\n",
            "Iteration 88, loss = 0.33422423\n",
            "Iteration 89, loss = 0.32865081\n",
            "Iteration 90, loss = 0.32867694\n",
            "Iteration 91, loss = 0.33132895\n",
            "Iteration 92, loss = 0.33242765\n",
            "Iteration 93, loss = 0.33528208\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35514687\n",
            "Iteration 2, loss = 1.28887530\n",
            "Iteration 3, loss = 1.20992819\n",
            "Iteration 4, loss = 1.10555557\n",
            "Iteration 5, loss = 0.97208524\n",
            "Iteration 6, loss = 0.82640774\n",
            "Iteration 7, loss = 0.68533628\n",
            "Iteration 8, loss = 0.56892506\n",
            "Iteration 9, loss = 0.48851274\n",
            "Iteration 10, loss = 0.43715805\n",
            "Iteration 11, loss = 0.41142786\n",
            "Iteration 12, loss = 0.39337340\n",
            "Iteration 13, loss = 0.37635719\n",
            "Iteration 14, loss = 0.37236560\n",
            "Iteration 15, loss = 0.36642877\n",
            "Iteration 16, loss = 0.36152908\n",
            "Iteration 17, loss = 0.35962836\n",
            "Iteration 18, loss = 0.35584541\n",
            "Iteration 19, loss = 0.35791335\n",
            "Iteration 20, loss = 0.36339383\n",
            "Iteration 21, loss = 0.36045069\n",
            "Iteration 22, loss = 0.35666433\n",
            "Iteration 23, loss = 0.35376522\n",
            "Iteration 24, loss = 0.35199466\n",
            "Iteration 25, loss = 0.35429923\n",
            "Iteration 26, loss = 0.35493450\n",
            "Iteration 27, loss = 0.35421583\n",
            "Iteration 28, loss = 0.35078127\n",
            "Iteration 29, loss = 0.35403143\n",
            "Iteration 30, loss = 0.35246337\n",
            "Iteration 31, loss = 0.35291951\n",
            "Iteration 32, loss = 0.35231053\n",
            "Iteration 33, loss = 0.35134712\n",
            "Iteration 34, loss = 0.35053916\n",
            "Iteration 35, loss = 0.34904789\n",
            "Iteration 36, loss = 0.35082255\n",
            "Iteration 37, loss = 0.35445015\n",
            "Iteration 38, loss = 0.35142138\n",
            "Iteration 39, loss = 0.35033565\n",
            "Iteration 40, loss = 0.35259637\n",
            "Iteration 41, loss = 0.35268743\n",
            "Iteration 42, loss = 0.34950460\n",
            "Iteration 43, loss = 0.35089441\n",
            "Iteration 44, loss = 0.35432736\n",
            "Iteration 45, loss = 0.34780452\n",
            "Iteration 46, loss = 0.35417521\n",
            "Iteration 47, loss = 0.34859214\n",
            "Iteration 48, loss = 0.34595536\n",
            "Iteration 49, loss = 0.34726937\n",
            "Iteration 50, loss = 0.34424490\n",
            "Iteration 51, loss = 0.34628703\n",
            "Iteration 52, loss = 0.34593084\n",
            "Iteration 53, loss = 0.34592419\n",
            "Iteration 54, loss = 0.34479483\n",
            "Iteration 55, loss = 0.34493753\n",
            "Iteration 56, loss = 0.34420275\n",
            "Iteration 57, loss = 0.34389100\n",
            "Iteration 58, loss = 0.34627593\n",
            "Iteration 59, loss = 0.34382344\n",
            "Iteration 60, loss = 0.34853742\n",
            "Iteration 61, loss = 0.35498859\n",
            "Iteration 62, loss = 0.35006528\n",
            "Iteration 63, loss = 0.35146156\n",
            "Iteration 64, loss = 0.34669772\n",
            "Iteration 65, loss = 0.34408202\n",
            "Iteration 66, loss = 0.34230679\n",
            "Iteration 67, loss = 0.34069827\n",
            "Iteration 68, loss = 0.34078086\n",
            "Iteration 69, loss = 0.34391976\n",
            "Iteration 70, loss = 0.34465503\n",
            "Iteration 71, loss = 0.34842019\n",
            "Iteration 72, loss = 0.34099516\n",
            "Iteration 73, loss = 0.34366739\n",
            "Iteration 74, loss = 0.34795558\n",
            "Iteration 75, loss = 0.34356963\n",
            "Iteration 76, loss = 0.34288637\n",
            "Iteration 77, loss = 0.34246768\n",
            "Iteration 78, loss = 0.33995376\n",
            "Iteration 79, loss = 0.34122963\n",
            "Iteration 80, loss = 0.34121272\n",
            "Iteration 81, loss = 0.34520107\n",
            "Iteration 82, loss = 0.34269915\n",
            "Iteration 83, loss = 0.34338232\n",
            "Iteration 84, loss = 0.34592549\n",
            "Iteration 85, loss = 0.34293987\n",
            "Iteration 86, loss = 0.34340833\n",
            "Iteration 87, loss = 0.34192492\n",
            "Iteration 88, loss = 0.34576214\n",
            "Iteration 89, loss = 0.34001473\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34279691\n",
            "Iteration 2, loss = 1.27946629\n",
            "Iteration 3, loss = 1.20666507\n",
            "Iteration 4, loss = 1.11126349\n",
            "Iteration 5, loss = 0.98915865\n",
            "Iteration 6, loss = 0.85073973\n",
            "Iteration 7, loss = 0.71000879\n",
            "Iteration 8, loss = 0.58951082\n",
            "Iteration 9, loss = 0.50287397\n",
            "Iteration 10, loss = 0.44653497\n",
            "Iteration 11, loss = 0.41812336\n",
            "Iteration 12, loss = 0.40118250\n",
            "Iteration 13, loss = 0.38313758\n",
            "Iteration 14, loss = 0.37892819\n",
            "Iteration 15, loss = 0.37265846\n",
            "Iteration 16, loss = 0.36764156\n",
            "Iteration 17, loss = 0.36590565\n",
            "Iteration 18, loss = 0.36328097\n",
            "Iteration 19, loss = 0.36563547\n",
            "Iteration 20, loss = 0.37047951\n",
            "Iteration 21, loss = 0.36614569\n",
            "Iteration 22, loss = 0.36290107\n",
            "Iteration 23, loss = 0.36069296\n",
            "Iteration 24, loss = 0.35842633\n",
            "Iteration 25, loss = 0.36086131\n",
            "Iteration 26, loss = 0.36044881\n",
            "Iteration 27, loss = 0.36014286\n",
            "Iteration 28, loss = 0.35759542\n",
            "Iteration 29, loss = 0.36293912\n",
            "Iteration 30, loss = 0.36033101\n",
            "Iteration 31, loss = 0.35996354\n",
            "Iteration 32, loss = 0.36031107\n",
            "Iteration 33, loss = 0.36101060\n",
            "Iteration 34, loss = 0.35709058\n",
            "Iteration 35, loss = 0.35931331\n",
            "Iteration 36, loss = 0.35893133\n",
            "Iteration 37, loss = 0.36494359\n",
            "Iteration 38, loss = 0.35988351\n",
            "Iteration 39, loss = 0.36006088\n",
            "Iteration 40, loss = 0.36209121\n",
            "Iteration 41, loss = 0.35958614\n",
            "Iteration 42, loss = 0.35791665\n",
            "Iteration 43, loss = 0.35982419\n",
            "Iteration 44, loss = 0.36168725\n",
            "Iteration 45, loss = 0.35585798\n",
            "Iteration 46, loss = 0.36295671\n",
            "Iteration 47, loss = 0.35652677\n",
            "Iteration 48, loss = 0.35612079\n",
            "Iteration 49, loss = 0.35648062\n",
            "Iteration 50, loss = 0.35375093\n",
            "Iteration 51, loss = 0.35658101\n",
            "Iteration 52, loss = 0.35530020\n",
            "Iteration 53, loss = 0.35544112\n",
            "Iteration 54, loss = 0.35351583\n",
            "Iteration 55, loss = 0.35529988\n",
            "Iteration 56, loss = 0.35386845\n",
            "Iteration 57, loss = 0.35392615\n",
            "Iteration 58, loss = 0.35438281\n",
            "Iteration 59, loss = 0.35280443\n",
            "Iteration 60, loss = 0.35789784\n",
            "Iteration 61, loss = 0.36096815\n",
            "Iteration 62, loss = 0.35932795\n",
            "Iteration 63, loss = 0.35963229\n",
            "Iteration 64, loss = 0.35491054\n",
            "Iteration 65, loss = 0.35198570\n",
            "Iteration 66, loss = 0.35097480\n",
            "Iteration 67, loss = 0.35080205\n",
            "Iteration 68, loss = 0.35025466\n",
            "Iteration 69, loss = 0.35388798\n",
            "Iteration 70, loss = 0.35268157\n",
            "Iteration 71, loss = 0.35815784\n",
            "Iteration 72, loss = 0.35048696\n",
            "Iteration 73, loss = 0.35414045\n",
            "Iteration 74, loss = 0.35973691\n",
            "Iteration 75, loss = 0.35631780\n",
            "Iteration 76, loss = 0.35346770\n",
            "Iteration 77, loss = 0.35137938\n",
            "Iteration 78, loss = 0.35054500\n",
            "Iteration 79, loss = 0.35145729\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36208892\n",
            "Iteration 2, loss = 1.29836671\n",
            "Iteration 3, loss = 1.21954959\n",
            "Iteration 4, loss = 1.11255821\n",
            "Iteration 5, loss = 0.97330892\n",
            "Iteration 6, loss = 0.81670819\n",
            "Iteration 7, loss = 0.66690113\n",
            "Iteration 8, loss = 0.54394264\n",
            "Iteration 9, loss = 0.46081015\n",
            "Iteration 10, loss = 0.40786998\n",
            "Iteration 11, loss = 0.38088195\n",
            "Iteration 12, loss = 0.36496505\n",
            "Iteration 13, loss = 0.35414818\n",
            "Iteration 14, loss = 0.34686860\n",
            "Iteration 15, loss = 0.34446702\n",
            "Iteration 16, loss = 0.34110900\n",
            "Iteration 17, loss = 0.34124438\n",
            "Iteration 18, loss = 0.33779993\n",
            "Iteration 19, loss = 0.34106173\n",
            "Iteration 20, loss = 0.34543614\n",
            "Iteration 21, loss = 0.34255666\n",
            "Iteration 22, loss = 0.33673483\n",
            "Iteration 23, loss = 0.33814802\n",
            "Iteration 24, loss = 0.33386911\n",
            "Iteration 25, loss = 0.33681946\n",
            "Iteration 26, loss = 0.33583440\n",
            "Iteration 27, loss = 0.33466635\n",
            "Iteration 28, loss = 0.33280341\n",
            "Iteration 29, loss = 0.34009874\n",
            "Iteration 30, loss = 0.33461434\n",
            "Iteration 31, loss = 0.33520941\n",
            "Iteration 32, loss = 0.33897286\n",
            "Iteration 33, loss = 0.33794146\n",
            "Iteration 34, loss = 0.33574528\n",
            "Iteration 35, loss = 0.33828311\n",
            "Iteration 36, loss = 0.33612043\n",
            "Iteration 37, loss = 0.34131035\n",
            "Iteration 38, loss = 0.33482730\n",
            "Iteration 39, loss = 0.33591251\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36447680\n",
            "Iteration 2, loss = 1.29638629\n",
            "Iteration 3, loss = 1.21218945\n",
            "Iteration 4, loss = 1.10071446\n",
            "Iteration 5, loss = 0.95868164\n",
            "Iteration 6, loss = 0.80479381\n",
            "Iteration 7, loss = 0.66027754\n",
            "Iteration 8, loss = 0.54484323\n",
            "Iteration 9, loss = 0.46318263\n",
            "Iteration 10, loss = 0.40958008\n",
            "Iteration 11, loss = 0.37758597\n",
            "Iteration 12, loss = 0.36164919\n",
            "Iteration 13, loss = 0.34983747\n",
            "Iteration 14, loss = 0.34082416\n",
            "Iteration 15, loss = 0.33644632\n",
            "Iteration 16, loss = 0.33355553\n",
            "Iteration 17, loss = 0.33333005\n",
            "Iteration 18, loss = 0.33150785\n",
            "Iteration 19, loss = 0.33012896\n",
            "Iteration 20, loss = 0.32979137\n",
            "Iteration 21, loss = 0.32703026\n",
            "Iteration 22, loss = 0.32647841\n",
            "Iteration 23, loss = 0.32900908\n",
            "Iteration 24, loss = 0.32618434\n",
            "Iteration 25, loss = 0.32920932\n",
            "Iteration 26, loss = 0.32859240\n",
            "Iteration 27, loss = 0.32643558\n",
            "Iteration 28, loss = 0.32343268\n",
            "Iteration 29, loss = 0.32778744\n",
            "Iteration 30, loss = 0.32580126\n",
            "Iteration 31, loss = 0.32693591\n",
            "Iteration 32, loss = 0.32992020\n",
            "Iteration 33, loss = 0.32758089\n",
            "Iteration 34, loss = 0.33332557\n",
            "Iteration 35, loss = 0.33241959\n",
            "Iteration 36, loss = 0.32778993\n",
            "Iteration 37, loss = 0.33086172\n",
            "Iteration 38, loss = 0.32622089\n",
            "Iteration 39, loss = 0.32723104\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35125561\n",
            "Iteration 2, loss = 1.27875872\n",
            "Iteration 3, loss = 1.18863750\n",
            "Iteration 4, loss = 1.07365072\n",
            "Iteration 5, loss = 0.93397056\n",
            "Iteration 6, loss = 0.78757272\n",
            "Iteration 7, loss = 0.65836331\n",
            "Iteration 8, loss = 0.55564892\n",
            "Iteration 9, loss = 0.48782919\n",
            "Iteration 10, loss = 0.43731416\n",
            "Iteration 11, loss = 0.40206602\n",
            "Iteration 12, loss = 0.38137611\n",
            "Iteration 13, loss = 0.36803660\n",
            "Iteration 14, loss = 0.35739631\n",
            "Iteration 15, loss = 0.35069131\n",
            "Iteration 16, loss = 0.34487054\n",
            "Iteration 17, loss = 0.34235525\n",
            "Iteration 18, loss = 0.34145186\n",
            "Iteration 19, loss = 0.33662156\n",
            "Iteration 20, loss = 0.33614534\n",
            "Iteration 21, loss = 0.33289437\n",
            "Iteration 22, loss = 0.33127118\n",
            "Iteration 23, loss = 0.33450955\n",
            "Iteration 24, loss = 0.33087020\n",
            "Iteration 25, loss = 0.32921631\n",
            "Iteration 26, loss = 0.33138838\n",
            "Iteration 27, loss = 0.33104744\n",
            "Iteration 28, loss = 0.32660283\n",
            "Iteration 29, loss = 0.32762329\n",
            "Iteration 30, loss = 0.32792026\n",
            "Iteration 31, loss = 0.32749808\n",
            "Iteration 32, loss = 0.33536315\n",
            "Iteration 33, loss = 0.33247010\n",
            "Iteration 34, loss = 0.33732303\n",
            "Iteration 35, loss = 0.33453063\n",
            "Iteration 36, loss = 0.32737111\n",
            "Iteration 37, loss = 0.32949426\n",
            "Iteration 38, loss = 0.32497292\n",
            "Iteration 39, loss = 0.32443227\n",
            "Iteration 40, loss = 0.32914632\n",
            "Iteration 41, loss = 0.32714427\n",
            "Iteration 42, loss = 0.32682817\n",
            "Iteration 43, loss = 0.32626740\n",
            "Iteration 44, loss = 0.32647531\n",
            "Iteration 45, loss = 0.32829150\n",
            "Iteration 46, loss = 0.33308673\n",
            "Iteration 47, loss = 0.32596793\n",
            "Iteration 48, loss = 0.32589726\n",
            "Iteration 49, loss = 0.32596189\n",
            "Iteration 50, loss = 0.32265916\n",
            "Iteration 51, loss = 0.32827730\n",
            "Iteration 52, loss = 0.32470923\n",
            "Iteration 53, loss = 0.32417350\n",
            "Iteration 54, loss = 0.32325513\n",
            "Iteration 55, loss = 0.32399243\n",
            "Iteration 56, loss = 0.32292624\n",
            "Iteration 57, loss = 0.32267618\n",
            "Iteration 58, loss = 0.32312616\n",
            "Iteration 59, loss = 0.31999549\n",
            "Iteration 60, loss = 0.32525396\n",
            "Iteration 61, loss = 0.32337733\n",
            "Iteration 62, loss = 0.32219455\n",
            "Iteration 63, loss = 0.32307218\n",
            "Iteration 64, loss = 0.31940236\n",
            "Iteration 65, loss = 0.31951552\n",
            "Iteration 66, loss = 0.32000741\n",
            "Iteration 67, loss = 0.32253356\n",
            "Iteration 68, loss = 0.31877977\n",
            "Iteration 69, loss = 0.32027375\n",
            "Iteration 70, loss = 0.32323559\n",
            "Iteration 71, loss = 0.32497198\n",
            "Iteration 72, loss = 0.32082258\n",
            "Iteration 73, loss = 0.32137284\n",
            "Iteration 74, loss = 0.32359126\n",
            "Iteration 75, loss = 0.31845479\n",
            "Iteration 76, loss = 0.31973823\n",
            "Iteration 77, loss = 0.31899133\n",
            "Iteration 78, loss = 0.32010064\n",
            "Iteration 79, loss = 0.31807374\n",
            "Iteration 80, loss = 0.31936458\n",
            "Iteration 81, loss = 0.32232203\n",
            "Iteration 82, loss = 0.32022621\n",
            "Iteration 83, loss = 0.31718488\n",
            "Iteration 84, loss = 0.31784255\n",
            "Iteration 85, loss = 0.31703501\n",
            "Iteration 86, loss = 0.31781691\n",
            "Iteration 87, loss = 0.31940485\n",
            "Iteration 88, loss = 0.32210002\n",
            "Iteration 89, loss = 0.32410379\n",
            "Iteration 90, loss = 0.32415787\n",
            "Iteration 91, loss = 0.31619279\n",
            "Iteration 92, loss = 0.31877400\n",
            "Iteration 93, loss = 0.32139887\n",
            "Iteration 94, loss = 0.32332845\n",
            "Iteration 95, loss = 0.32344271\n",
            "Iteration 96, loss = 0.32344810\n",
            "Iteration 97, loss = 0.31644067\n",
            "Iteration 98, loss = 0.31577115\n",
            "Iteration 99, loss = 0.31301034\n",
            "Iteration 100, loss = 0.31754350\n",
            "54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37974188\n",
            "Iteration 2, loss = 1.33394956\n",
            "Iteration 3, loss = 1.28803642\n",
            "Iteration 4, loss = 1.23408530\n",
            "Iteration 5, loss = 1.16903483\n",
            "Iteration 6, loss = 1.09496927\n",
            "Iteration 7, loss = 1.02517171\n",
            "Iteration 8, loss = 0.97225944\n",
            "Iteration 9, loss = 0.93247836\n",
            "Iteration 10, loss = 0.90764925\n",
            "Iteration 11, loss = 0.89498164\n",
            "Iteration 12, loss = 0.87332919\n",
            "Iteration 13, loss = 0.86961451\n",
            "Iteration 14, loss = 0.86197932\n",
            "Iteration 15, loss = 0.86095807\n",
            "Iteration 16, loss = 0.86056958\n",
            "Iteration 17, loss = 0.85381130\n",
            "Iteration 18, loss = 0.86037311\n",
            "Iteration 19, loss = 0.86029887\n",
            "Iteration 20, loss = 0.86260368\n",
            "Iteration 21, loss = 0.85753238\n",
            "Iteration 22, loss = 0.85781749\n",
            "Iteration 23, loss = 0.85594315\n",
            "Iteration 24, loss = 0.84988663\n",
            "Iteration 25, loss = 0.85232652\n",
            "Iteration 26, loss = 0.85088421\n",
            "Iteration 27, loss = 0.84762439\n",
            "Iteration 28, loss = 0.85080706\n",
            "Iteration 29, loss = 0.85451386\n",
            "Iteration 30, loss = 0.85315378\n",
            "Iteration 31, loss = 0.84662761\n",
            "Iteration 32, loss = 0.84431924\n",
            "Iteration 33, loss = 0.84514055\n",
            "Iteration 34, loss = 0.84530640\n",
            "Iteration 35, loss = 0.84289908\n",
            "Iteration 36, loss = 0.84211423\n",
            "Iteration 37, loss = 0.84233891\n",
            "Iteration 38, loss = 0.84068863\n",
            "Iteration 39, loss = 0.84261456\n",
            "Iteration 40, loss = 0.84675313\n",
            "Iteration 41, loss = 0.84042101\n",
            "Iteration 42, loss = 0.84194900\n",
            "Iteration 43, loss = 0.84117412\n",
            "Iteration 44, loss = 0.84563863\n",
            "Iteration 45, loss = 0.84199096\n",
            "Iteration 46, loss = 0.84124677\n",
            "Iteration 47, loss = 0.83786330\n",
            "Iteration 48, loss = 0.84055624\n",
            "Iteration 49, loss = 0.83963998\n",
            "Iteration 50, loss = 0.83993502\n",
            "Iteration 51, loss = 0.83954861\n",
            "Iteration 52, loss = 0.83960492\n",
            "Iteration 53, loss = 0.83695843\n",
            "Iteration 54, loss = 0.83833565\n",
            "Iteration 55, loss = 0.83853124\n",
            "Iteration 56, loss = 0.83496227\n",
            "Iteration 57, loss = 0.83541545\n",
            "Iteration 58, loss = 0.83476297\n",
            "Iteration 59, loss = 0.83386032\n",
            "Iteration 60, loss = 0.83688830\n",
            "Iteration 61, loss = 0.83938733\n",
            "Iteration 62, loss = 0.84502787\n",
            "Iteration 63, loss = 0.83243216\n",
            "Iteration 64, loss = 0.83379025\n",
            "Iteration 65, loss = 0.83277217\n",
            "Iteration 66, loss = 0.83194337\n",
            "Iteration 67, loss = 0.83210008\n",
            "Iteration 68, loss = 0.83455781\n",
            "Iteration 69, loss = 0.83360346\n",
            "Iteration 70, loss = 0.82891295\n",
            "Iteration 71, loss = 0.83277137\n",
            "Iteration 72, loss = 0.83049609\n",
            "Iteration 73, loss = 0.83294712\n",
            "Iteration 74, loss = 0.83150012\n",
            "Iteration 75, loss = 0.83873420\n",
            "Iteration 76, loss = 0.83648609\n",
            "Iteration 77, loss = 0.83475177\n",
            "Iteration 78, loss = 0.83805999\n",
            "Iteration 79, loss = 0.84660467\n",
            "Iteration 80, loss = 0.84335657\n",
            "Iteration 81, loss = 0.83390973\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37883237\n",
            "Iteration 2, loss = 1.33327311\n",
            "Iteration 3, loss = 1.28660380\n",
            "Iteration 4, loss = 1.23106943\n",
            "Iteration 5, loss = 1.16335246\n",
            "Iteration 6, loss = 1.08610997\n",
            "Iteration 7, loss = 1.01004162\n",
            "Iteration 8, loss = 0.95390902\n",
            "Iteration 9, loss = 0.90619419\n",
            "Iteration 10, loss = 0.88697653\n",
            "Iteration 11, loss = 0.87091112\n",
            "Iteration 12, loss = 0.85436077\n",
            "Iteration 13, loss = 0.85144657\n",
            "Iteration 14, loss = 0.84144644\n",
            "Iteration 15, loss = 0.83831104\n",
            "Iteration 16, loss = 0.83685939\n",
            "Iteration 17, loss = 0.83314057\n",
            "Iteration 18, loss = 0.83854513\n",
            "Iteration 19, loss = 0.83666143\n",
            "Iteration 20, loss = 0.83935876\n",
            "Iteration 21, loss = 0.84104235\n",
            "Iteration 22, loss = 0.83629782\n",
            "Iteration 23, loss = 0.83146915\n",
            "Iteration 24, loss = 0.82705945\n",
            "Iteration 25, loss = 0.82514414\n",
            "Iteration 26, loss = 0.82632734\n",
            "Iteration 27, loss = 0.82500639\n",
            "Iteration 28, loss = 0.82630990\n",
            "Iteration 29, loss = 0.82904785\n",
            "Iteration 30, loss = 0.82890077\n",
            "Iteration 31, loss = 0.82567171\n",
            "Iteration 32, loss = 0.82225408\n",
            "Iteration 33, loss = 0.82450146\n",
            "Iteration 34, loss = 0.82409916\n",
            "Iteration 35, loss = 0.82185683\n",
            "Iteration 36, loss = 0.82018191\n",
            "Iteration 37, loss = 0.82011062\n",
            "Iteration 38, loss = 0.81818039\n",
            "Iteration 39, loss = 0.82066191\n",
            "Iteration 40, loss = 0.82442908\n",
            "Iteration 41, loss = 0.81837760\n",
            "Iteration 42, loss = 0.82034242\n",
            "Iteration 43, loss = 0.82314958\n",
            "Iteration 44, loss = 0.82294782\n",
            "Iteration 45, loss = 0.81972370\n",
            "Iteration 46, loss = 0.81766730\n",
            "Iteration 47, loss = 0.81453301\n",
            "Iteration 48, loss = 0.81525232\n",
            "Iteration 49, loss = 0.81434514\n",
            "Iteration 50, loss = 0.81568646\n",
            "Iteration 51, loss = 0.81641211\n",
            "Iteration 52, loss = 0.81769086\n",
            "Iteration 53, loss = 0.81868106\n",
            "Iteration 54, loss = 0.81594616\n",
            "Iteration 55, loss = 0.81538784\n",
            "Iteration 56, loss = 0.81328140\n",
            "Iteration 57, loss = 0.81442982\n",
            "Iteration 58, loss = 0.81237357\n",
            "Iteration 59, loss = 0.81231546\n",
            "Iteration 60, loss = 0.81373059\n",
            "Iteration 61, loss = 0.81410525\n",
            "Iteration 62, loss = 0.82105720\n",
            "Iteration 63, loss = 0.81062325\n",
            "Iteration 64, loss = 0.81150398\n",
            "Iteration 65, loss = 0.80883368\n",
            "Iteration 66, loss = 0.80776405\n",
            "Iteration 67, loss = 0.80965686\n",
            "Iteration 68, loss = 0.80917878\n",
            "Iteration 69, loss = 0.81404922\n",
            "Iteration 70, loss = 0.80629530\n",
            "Iteration 71, loss = 0.81130561\n",
            "Iteration 72, loss = 0.80858179\n",
            "Iteration 73, loss = 0.81163373\n",
            "Iteration 74, loss = 0.81256338\n",
            "Iteration 75, loss = 0.81397618\n",
            "Iteration 76, loss = 0.81383029\n",
            "Iteration 77, loss = 0.81205078\n",
            "Iteration 78, loss = 0.81342690\n",
            "Iteration 79, loss = 0.82434294\n",
            "Iteration 80, loss = 0.82156415\n",
            "Iteration 81, loss = 0.81373342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37650626\n",
            "Iteration 2, loss = 1.32904437\n",
            "Iteration 3, loss = 1.28190196\n",
            "Iteration 4, loss = 1.22520535\n",
            "Iteration 5, loss = 1.15971620\n",
            "Iteration 6, loss = 1.08434680\n",
            "Iteration 7, loss = 1.01556904\n",
            "Iteration 8, loss = 0.96320569\n",
            "Iteration 9, loss = 0.91709768\n",
            "Iteration 10, loss = 0.89467136\n",
            "Iteration 11, loss = 0.87715098\n",
            "Iteration 12, loss = 0.85949319\n",
            "Iteration 13, loss = 0.85702087\n",
            "Iteration 14, loss = 0.84607839\n",
            "Iteration 15, loss = 0.84442127\n",
            "Iteration 16, loss = 0.84207178\n",
            "Iteration 17, loss = 0.83900280\n",
            "Iteration 18, loss = 0.84338722\n",
            "Iteration 19, loss = 0.84323021\n",
            "Iteration 20, loss = 0.84574531\n",
            "Iteration 21, loss = 0.84752561\n",
            "Iteration 22, loss = 0.84478524\n",
            "Iteration 23, loss = 0.83788236\n",
            "Iteration 24, loss = 0.83371667\n",
            "Iteration 25, loss = 0.83195535\n",
            "Iteration 26, loss = 0.83276767\n",
            "Iteration 27, loss = 0.83285125\n",
            "Iteration 28, loss = 0.83184598\n",
            "Iteration 29, loss = 0.83277724\n",
            "Iteration 30, loss = 0.83688591\n",
            "Iteration 31, loss = 0.83276939\n",
            "Iteration 32, loss = 0.82849624\n",
            "Iteration 33, loss = 0.83129950\n",
            "Iteration 34, loss = 0.82981356\n",
            "Iteration 35, loss = 0.83020083\n",
            "Iteration 36, loss = 0.82771671\n",
            "Iteration 37, loss = 0.82765192\n",
            "Iteration 38, loss = 0.82665932\n",
            "Iteration 39, loss = 0.83080859\n",
            "Iteration 40, loss = 0.82857432\n",
            "Iteration 41, loss = 0.82429702\n",
            "Iteration 42, loss = 0.82635371\n",
            "Iteration 43, loss = 0.83388131\n",
            "Iteration 44, loss = 0.82798278\n",
            "Iteration 45, loss = 0.82768482\n",
            "Iteration 46, loss = 0.82634995\n",
            "Iteration 47, loss = 0.82582901\n",
            "Iteration 48, loss = 0.82333328\n",
            "Iteration 49, loss = 0.82270878\n",
            "Iteration 50, loss = 0.82445273\n",
            "Iteration 51, loss = 0.82550500\n",
            "Iteration 52, loss = 0.82290100\n",
            "Iteration 53, loss = 0.82597771\n",
            "Iteration 54, loss = 0.82135651\n",
            "Iteration 55, loss = 0.82053673\n",
            "Iteration 56, loss = 0.82153997\n",
            "Iteration 57, loss = 0.82619025\n",
            "Iteration 58, loss = 0.82032287\n",
            "Iteration 59, loss = 0.81996114\n",
            "Iteration 60, loss = 0.82163609\n",
            "Iteration 61, loss = 0.82160076\n",
            "Iteration 62, loss = 0.82514015\n",
            "Iteration 63, loss = 0.81863210\n",
            "Iteration 64, loss = 0.82087019\n",
            "Iteration 65, loss = 0.81740282\n",
            "Iteration 66, loss = 0.81608491\n",
            "Iteration 67, loss = 0.81809978\n",
            "Iteration 68, loss = 0.81795929\n",
            "Iteration 69, loss = 0.81964938\n",
            "Iteration 70, loss = 0.81485884\n",
            "Iteration 71, loss = 0.81537297\n",
            "Iteration 72, loss = 0.81654278\n",
            "Iteration 73, loss = 0.81842703\n",
            "Iteration 74, loss = 0.81518976\n",
            "Iteration 75, loss = 0.81694818\n",
            "Iteration 76, loss = 0.81592930\n",
            "Iteration 77, loss = 0.81726926\n",
            "Iteration 78, loss = 0.81851566\n",
            "Iteration 79, loss = 0.83161797\n",
            "Iteration 80, loss = 0.82946581\n",
            "Iteration 81, loss = 0.81939568\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37718835\n",
            "Iteration 2, loss = 1.33032546\n",
            "Iteration 3, loss = 1.28304001\n",
            "Iteration 4, loss = 1.22719136\n",
            "Iteration 5, loss = 1.16017027\n",
            "Iteration 6, loss = 1.08519633\n",
            "Iteration 7, loss = 1.00660774\n",
            "Iteration 8, loss = 0.94694485\n",
            "Iteration 9, loss = 0.90240509\n",
            "Iteration 10, loss = 0.87989720\n",
            "Iteration 11, loss = 0.87218718\n",
            "Iteration 12, loss = 0.84308841\n",
            "Iteration 13, loss = 0.84634209\n",
            "Iteration 14, loss = 0.83224350\n",
            "Iteration 15, loss = 0.83341576\n",
            "Iteration 16, loss = 0.82797614\n",
            "Iteration 17, loss = 0.83098028\n",
            "Iteration 18, loss = 0.83025353\n",
            "Iteration 19, loss = 0.82846721\n",
            "Iteration 20, loss = 0.82977670\n",
            "Iteration 21, loss = 0.82655175\n",
            "Iteration 22, loss = 0.82512959\n",
            "Iteration 23, loss = 0.82082063\n",
            "Iteration 24, loss = 0.81812791\n",
            "Iteration 25, loss = 0.81701296\n",
            "Iteration 26, loss = 0.81742597\n",
            "Iteration 27, loss = 0.81924658\n",
            "Iteration 28, loss = 0.81760888\n",
            "Iteration 29, loss = 0.81592917\n",
            "Iteration 30, loss = 0.81988109\n",
            "Iteration 31, loss = 0.81912833\n",
            "Iteration 32, loss = 0.81599712\n",
            "Iteration 33, loss = 0.81634414\n",
            "Iteration 34, loss = 0.81370437\n",
            "Iteration 35, loss = 0.81425558\n",
            "Iteration 36, loss = 0.81455176\n",
            "Iteration 37, loss = 0.81577763\n",
            "Iteration 38, loss = 0.81389646\n",
            "Iteration 39, loss = 0.82188636\n",
            "Iteration 40, loss = 0.81796335\n",
            "Iteration 41, loss = 0.81408065\n",
            "Iteration 42, loss = 0.81851816\n",
            "Iteration 43, loss = 0.83572776\n",
            "Iteration 44, loss = 0.82049912\n",
            "Iteration 45, loss = 0.81528357\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37835145\n",
            "Iteration 2, loss = 1.33193193\n",
            "Iteration 3, loss = 1.28471607\n",
            "Iteration 4, loss = 1.22858355\n",
            "Iteration 5, loss = 1.15933159\n",
            "Iteration 6, loss = 1.08431777\n",
            "Iteration 7, loss = 1.00536561\n",
            "Iteration 8, loss = 0.94186678\n",
            "Iteration 9, loss = 0.89815583\n",
            "Iteration 10, loss = 0.87360726\n",
            "Iteration 11, loss = 0.87005868\n",
            "Iteration 12, loss = 0.84133675\n",
            "Iteration 13, loss = 0.84048679\n",
            "Iteration 14, loss = 0.83008490\n",
            "Iteration 15, loss = 0.82989653\n",
            "Iteration 16, loss = 0.82479244\n",
            "Iteration 17, loss = 0.82744558\n",
            "Iteration 18, loss = 0.82786694\n",
            "Iteration 19, loss = 0.82506805\n",
            "Iteration 20, loss = 0.82489897\n",
            "Iteration 21, loss = 0.81962074\n",
            "Iteration 22, loss = 0.81789090\n",
            "Iteration 23, loss = 0.81677445\n",
            "Iteration 24, loss = 0.81671711\n",
            "Iteration 25, loss = 0.81915936\n",
            "Iteration 26, loss = 0.82138143\n",
            "Iteration 27, loss = 0.81645937\n",
            "Iteration 28, loss = 0.81766444\n",
            "Iteration 29, loss = 0.81477821\n",
            "Iteration 30, loss = 0.81958326\n",
            "Iteration 31, loss = 0.82043922\n",
            "Iteration 32, loss = 0.81429485\n",
            "Iteration 33, loss = 0.81460286\n",
            "Iteration 34, loss = 0.81214345\n",
            "Iteration 35, loss = 0.81440955\n",
            "Iteration 36, loss = 0.81295204\n",
            "Iteration 37, loss = 0.81114972\n",
            "Iteration 38, loss = 0.81136443\n",
            "Iteration 39, loss = 0.81582948\n",
            "Iteration 40, loss = 0.81545121\n",
            "Iteration 41, loss = 0.81268189\n",
            "Iteration 42, loss = 0.81219130\n",
            "Iteration 43, loss = 0.82843673\n",
            "Iteration 44, loss = 0.81836819\n",
            "Iteration 45, loss = 0.81252456\n",
            "Iteration 46, loss = 0.81249556\n",
            "Iteration 47, loss = 0.81132306\n",
            "Iteration 48, loss = 0.81320130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.37075379\n",
            "Iteration 2, loss = 1.32763378\n",
            "Iteration 3, loss = 1.28490297\n",
            "Iteration 4, loss = 1.23385706\n",
            "Iteration 5, loss = 1.17121398\n",
            "Iteration 6, loss = 1.10325026\n",
            "Iteration 7, loss = 1.03134354\n",
            "Iteration 8, loss = 0.97470485\n",
            "Iteration 9, loss = 0.93497316\n",
            "Iteration 10, loss = 0.90785559\n",
            "Iteration 11, loss = 0.90299352\n",
            "Iteration 12, loss = 0.88183399\n",
            "Iteration 13, loss = 0.88165521\n",
            "Iteration 14, loss = 0.87240493\n",
            "Iteration 15, loss = 0.87624053\n",
            "Iteration 16, loss = 0.87455077\n",
            "Iteration 17, loss = 0.87714627\n",
            "Iteration 18, loss = 0.87327741\n",
            "Iteration 19, loss = 0.86921520\n",
            "Iteration 20, loss = 0.86916024\n",
            "Iteration 21, loss = 0.86650074\n",
            "Iteration 22, loss = 0.86472971\n",
            "Iteration 23, loss = 0.86566297\n",
            "Iteration 24, loss = 0.86241012\n",
            "Iteration 25, loss = 0.86191595\n",
            "Iteration 26, loss = 0.86365665\n",
            "Iteration 27, loss = 0.86289792\n",
            "Iteration 28, loss = 0.86180430\n",
            "Iteration 29, loss = 0.85976075\n",
            "Iteration 30, loss = 0.86306007\n",
            "Iteration 31, loss = 0.86160482\n",
            "Iteration 32, loss = 0.85830249\n",
            "Iteration 33, loss = 0.85989961\n",
            "Iteration 34, loss = 0.85650020\n",
            "Iteration 35, loss = 0.85796334\n",
            "Iteration 36, loss = 0.85795689\n",
            "Iteration 37, loss = 0.86014820\n",
            "Iteration 38, loss = 0.85620767\n",
            "Iteration 39, loss = 0.85911521\n",
            "Iteration 40, loss = 0.85813458\n",
            "Iteration 41, loss = 0.85776098\n",
            "Iteration 42, loss = 0.85442835\n",
            "Iteration 43, loss = 0.86649833\n",
            "Iteration 44, loss = 0.85510761\n",
            "Iteration 45, loss = 0.85386891\n",
            "Iteration 46, loss = 0.85446285\n",
            "Iteration 47, loss = 0.85232232\n",
            "Iteration 48, loss = 0.85529687\n",
            "Iteration 49, loss = 0.85346152\n",
            "Iteration 50, loss = 0.85333639\n",
            "Iteration 51, loss = 0.85144352\n",
            "Iteration 52, loss = 0.85260697\n",
            "Iteration 53, loss = 0.85314402\n",
            "Iteration 54, loss = 0.84997323\n",
            "Iteration 55, loss = 0.85083057\n",
            "Iteration 56, loss = 0.84924020\n",
            "Iteration 57, loss = 0.85174534\n",
            "Iteration 58, loss = 0.84999440\n",
            "Iteration 59, loss = 0.84798158\n",
            "Iteration 60, loss = 0.85092246\n",
            "Iteration 61, loss = 0.85081576\n",
            "Iteration 62, loss = 0.85658349\n",
            "Iteration 63, loss = 0.85026484\n",
            "Iteration 64, loss = 0.85194473\n",
            "Iteration 65, loss = 0.85310525\n",
            "Iteration 66, loss = 0.85110677\n",
            "Iteration 67, loss = 0.85064749\n",
            "Iteration 68, loss = 0.84469207\n",
            "Iteration 69, loss = 0.84757169\n",
            "Iteration 70, loss = 0.84595201\n",
            "Iteration 71, loss = 0.85057154\n",
            "Iteration 72, loss = 0.84837605\n",
            "Iteration 73, loss = 0.84757797\n",
            "Iteration 74, loss = 0.84681846\n",
            "Iteration 75, loss = 0.84614530\n",
            "Iteration 76, loss = 0.84437481\n",
            "Iteration 77, loss = 0.84899750\n",
            "Iteration 78, loss = 0.84770052\n",
            "Iteration 79, loss = 0.85518929\n",
            "Iteration 80, loss = 0.84696729\n",
            "Iteration 81, loss = 0.84738311\n",
            "Iteration 82, loss = 0.84105915\n",
            "Iteration 83, loss = 0.84368192\n",
            "Iteration 84, loss = 0.84187720\n",
            "Iteration 85, loss = 0.84265152\n",
            "Iteration 86, loss = 0.84051036\n",
            "Iteration 87, loss = 0.84044478\n",
            "Iteration 88, loss = 0.84139067\n",
            "Iteration 89, loss = 0.84053776\n",
            "Iteration 90, loss = 0.83783772\n",
            "Iteration 91, loss = 0.84292946\n",
            "Iteration 92, loss = 0.84068088\n",
            "Iteration 93, loss = 0.83860996\n",
            "Iteration 94, loss = 0.83781870\n",
            "Iteration 95, loss = 0.84250481\n",
            "Iteration 96, loss = 0.84273148\n",
            "Iteration 97, loss = 0.84385481\n",
            "Iteration 98, loss = 0.84232082\n",
            "Iteration 99, loss = 0.83871872\n",
            "Iteration 100, loss = 0.84153284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36057140\n",
            "Iteration 2, loss = 1.31754988\n",
            "Iteration 3, loss = 1.27770967\n",
            "Iteration 4, loss = 1.22907566\n",
            "Iteration 5, loss = 1.16819053\n",
            "Iteration 6, loss = 1.10191584\n",
            "Iteration 7, loss = 1.02967855\n",
            "Iteration 8, loss = 0.97350166\n",
            "Iteration 9, loss = 0.93162488\n",
            "Iteration 10, loss = 0.90167091\n",
            "Iteration 11, loss = 0.89288144\n",
            "Iteration 12, loss = 0.87692702\n",
            "Iteration 13, loss = 0.87553334\n",
            "Iteration 14, loss = 0.86819254\n",
            "Iteration 15, loss = 0.87386114\n",
            "Iteration 16, loss = 0.87001183\n",
            "Iteration 17, loss = 0.87146253\n",
            "Iteration 18, loss = 0.86958179\n",
            "Iteration 19, loss = 0.86365559\n",
            "Iteration 20, loss = 0.86165317\n",
            "Iteration 21, loss = 0.86012739\n",
            "Iteration 22, loss = 0.85937534\n",
            "Iteration 23, loss = 0.85933532\n",
            "Iteration 24, loss = 0.85724389\n",
            "Iteration 25, loss = 0.85728446\n",
            "Iteration 26, loss = 0.86065679\n",
            "Iteration 27, loss = 0.85784859\n",
            "Iteration 28, loss = 0.85672972\n",
            "Iteration 29, loss = 0.85497163\n",
            "Iteration 30, loss = 0.86082999\n",
            "Iteration 31, loss = 0.85859233\n",
            "Iteration 32, loss = 0.85614986\n",
            "Iteration 33, loss = 0.85693345\n",
            "Iteration 34, loss = 0.85489564\n",
            "Iteration 35, loss = 0.85601054\n",
            "Iteration 36, loss = 0.85674689\n",
            "Iteration 37, loss = 0.85937842\n",
            "Iteration 38, loss = 0.85265034\n",
            "Iteration 39, loss = 0.85659497\n",
            "Iteration 40, loss = 0.85684885\n",
            "Iteration 41, loss = 0.85517736\n",
            "Iteration 42, loss = 0.85217066\n",
            "Iteration 43, loss = 0.86655749\n",
            "Iteration 44, loss = 0.85483128\n",
            "Iteration 45, loss = 0.85312217\n",
            "Iteration 46, loss = 0.85189548\n",
            "Iteration 47, loss = 0.85173775\n",
            "Iteration 48, loss = 0.85287216\n",
            "Iteration 49, loss = 0.85111659\n",
            "Iteration 50, loss = 0.85254468\n",
            "Iteration 51, loss = 0.85170965\n",
            "Iteration 52, loss = 0.84961484\n",
            "Iteration 53, loss = 0.85249811\n",
            "Iteration 54, loss = 0.84942652\n",
            "Iteration 55, loss = 0.85074433\n",
            "Iteration 56, loss = 0.84804057\n",
            "Iteration 57, loss = 0.85045098\n",
            "Iteration 58, loss = 0.85018142\n",
            "Iteration 59, loss = 0.84709999\n",
            "Iteration 60, loss = 0.84976005\n",
            "Iteration 61, loss = 0.84764087\n",
            "Iteration 62, loss = 0.85406828\n",
            "Iteration 63, loss = 0.84815269\n",
            "Iteration 64, loss = 0.84984946\n",
            "Iteration 65, loss = 0.85340175\n",
            "Iteration 66, loss = 0.85143597\n",
            "Iteration 67, loss = 0.85136816\n",
            "Iteration 68, loss = 0.84390458\n",
            "Iteration 69, loss = 0.84739224\n",
            "Iteration 70, loss = 0.84449482\n",
            "Iteration 71, loss = 0.84920146\n",
            "Iteration 72, loss = 0.84590016\n",
            "Iteration 73, loss = 0.84547719\n",
            "Iteration 74, loss = 0.84564254\n",
            "Iteration 75, loss = 0.84643013\n",
            "Iteration 76, loss = 0.84358180\n",
            "Iteration 77, loss = 0.84831375\n",
            "Iteration 78, loss = 0.84783575\n",
            "Iteration 79, loss = 0.85375077\n",
            "Iteration 80, loss = 0.84535484\n",
            "Iteration 81, loss = 0.84497189\n",
            "Iteration 82, loss = 0.83964948\n",
            "Iteration 83, loss = 0.84347750\n",
            "Iteration 84, loss = 0.84293653\n",
            "Iteration 85, loss = 0.84319726\n",
            "Iteration 86, loss = 0.84080580\n",
            "Iteration 87, loss = 0.83950813\n",
            "Iteration 88, loss = 0.84123343\n",
            "Iteration 89, loss = 0.83877769\n",
            "Iteration 90, loss = 0.83738695\n",
            "Iteration 91, loss = 0.84239843\n",
            "Iteration 92, loss = 0.83931851\n",
            "Iteration 93, loss = 0.83860839\n",
            "Iteration 94, loss = 0.83665696\n",
            "Iteration 95, loss = 0.83884593\n",
            "Iteration 96, loss = 0.84260471\n",
            "Iteration 97, loss = 0.84311755\n",
            "Iteration 98, loss = 0.84431758\n",
            "Iteration 99, loss = 0.83923674\n",
            "Iteration 100, loss = 0.83938416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37157848\n",
            "Iteration 2, loss = 1.32687229\n",
            "Iteration 3, loss = 1.28329763\n",
            "Iteration 4, loss = 1.23170181\n",
            "Iteration 5, loss = 1.16625924\n",
            "Iteration 6, loss = 1.09566570\n",
            "Iteration 7, loss = 1.01987524\n",
            "Iteration 8, loss = 0.95887533\n",
            "Iteration 9, loss = 0.92027840\n",
            "Iteration 10, loss = 0.89124542\n",
            "Iteration 11, loss = 0.88371002\n",
            "Iteration 12, loss = 0.87003806\n",
            "Iteration 13, loss = 0.86730733\n",
            "Iteration 14, loss = 0.86441224\n",
            "Iteration 15, loss = 0.86860868\n",
            "Iteration 16, loss = 0.86236937\n",
            "Iteration 17, loss = 0.86437627\n",
            "Iteration 18, loss = 0.86597267\n",
            "Iteration 19, loss = 0.85990335\n",
            "Iteration 20, loss = 0.85874120\n",
            "Iteration 21, loss = 0.85691457\n",
            "Iteration 22, loss = 0.85640174\n",
            "Iteration 23, loss = 0.85614149\n",
            "Iteration 24, loss = 0.85168271\n",
            "Iteration 25, loss = 0.85456590\n",
            "Iteration 26, loss = 0.85424041\n",
            "Iteration 27, loss = 0.85341224\n",
            "Iteration 28, loss = 0.85138293\n",
            "Iteration 29, loss = 0.85082965\n",
            "Iteration 30, loss = 0.85425477\n",
            "Iteration 31, loss = 0.85064421\n",
            "Iteration 32, loss = 0.85222329\n",
            "Iteration 33, loss = 0.85143238\n",
            "Iteration 34, loss = 0.85220338\n",
            "Iteration 35, loss = 0.85073335\n",
            "Iteration 36, loss = 0.84968019\n",
            "Iteration 37, loss = 0.85207694\n",
            "Iteration 38, loss = 0.84636982\n",
            "Iteration 39, loss = 0.84962615\n",
            "Iteration 40, loss = 0.85093730\n",
            "Iteration 41, loss = 0.85131359\n",
            "Iteration 42, loss = 0.84701079\n",
            "Iteration 43, loss = 0.85847436\n",
            "Iteration 44, loss = 0.84718733\n",
            "Iteration 45, loss = 0.84801116\n",
            "Iteration 46, loss = 0.84458425\n",
            "Iteration 47, loss = 0.84365401\n",
            "Iteration 48, loss = 0.84522259\n",
            "Iteration 49, loss = 0.84393741\n",
            "Iteration 50, loss = 0.84462792\n",
            "Iteration 51, loss = 0.84488157\n",
            "Iteration 52, loss = 0.84270001\n",
            "Iteration 53, loss = 0.84325476\n",
            "Iteration 54, loss = 0.84094642\n",
            "Iteration 55, loss = 0.84184140\n",
            "Iteration 56, loss = 0.84015823\n",
            "Iteration 57, loss = 0.84152688\n",
            "Iteration 58, loss = 0.84166564\n",
            "Iteration 59, loss = 0.83897466\n",
            "Iteration 60, loss = 0.84320066\n",
            "Iteration 61, loss = 0.84035016\n",
            "Iteration 62, loss = 0.84529202\n",
            "Iteration 63, loss = 0.83944367\n",
            "Iteration 64, loss = 0.84247379\n",
            "Iteration 65, loss = 0.84798665\n",
            "Iteration 66, loss = 0.84806997\n",
            "Iteration 67, loss = 0.84387281\n",
            "Iteration 68, loss = 0.83709684\n",
            "Iteration 69, loss = 0.83870636\n",
            "Iteration 70, loss = 0.83743823\n",
            "Iteration 71, loss = 0.84087003\n",
            "Iteration 72, loss = 0.83785522\n",
            "Iteration 73, loss = 0.83969673\n",
            "Iteration 74, loss = 0.83949464\n",
            "Iteration 75, loss = 0.83757906\n",
            "Iteration 76, loss = 0.83657290\n",
            "Iteration 77, loss = 0.83807864\n",
            "Iteration 78, loss = 0.83626634\n",
            "Iteration 79, loss = 0.84238429\n",
            "Iteration 80, loss = 0.83685178\n",
            "Iteration 81, loss = 0.83985584\n",
            "Iteration 82, loss = 0.83329720\n",
            "Iteration 83, loss = 0.83493050\n",
            "Iteration 84, loss = 0.83748032\n",
            "Iteration 85, loss = 0.83901689\n",
            "Iteration 86, loss = 0.83352237\n",
            "Iteration 87, loss = 0.83203055\n",
            "Iteration 88, loss = 0.83249918\n",
            "Iteration 89, loss = 0.83163168\n",
            "Iteration 90, loss = 0.83062270\n",
            "Iteration 91, loss = 0.83420893\n",
            "Iteration 92, loss = 0.83189003\n",
            "Iteration 93, loss = 0.83119258\n",
            "Iteration 94, loss = 0.83062257\n",
            "Iteration 95, loss = 0.83400232\n",
            "Iteration 96, loss = 0.83443442\n",
            "Iteration 97, loss = 0.83290320\n",
            "Iteration 98, loss = 0.83532175\n",
            "Iteration 99, loss = 0.83268581\n",
            "Iteration 100, loss = 0.83394264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37206247\n",
            "Iteration 2, loss = 1.32631943\n",
            "Iteration 3, loss = 1.28074646\n",
            "Iteration 4, loss = 1.22520281\n",
            "Iteration 5, loss = 1.15871842\n",
            "Iteration 6, loss = 1.08767616\n",
            "Iteration 7, loss = 1.01844974\n",
            "Iteration 8, loss = 0.96133188\n",
            "Iteration 9, loss = 0.91899461\n",
            "Iteration 10, loss = 0.88789049\n",
            "Iteration 11, loss = 0.87328199\n",
            "Iteration 12, loss = 0.85628735\n",
            "Iteration 13, loss = 0.85322755\n",
            "Iteration 14, loss = 0.84717248\n",
            "Iteration 15, loss = 0.84817611\n",
            "Iteration 16, loss = 0.84340195\n",
            "Iteration 17, loss = 0.84559270\n",
            "Iteration 18, loss = 0.84747623\n",
            "Iteration 19, loss = 0.83901895\n",
            "Iteration 20, loss = 0.83745462\n",
            "Iteration 21, loss = 0.83597081\n",
            "Iteration 22, loss = 0.83629715\n",
            "Iteration 23, loss = 0.83710058\n",
            "Iteration 24, loss = 0.83293753\n",
            "Iteration 25, loss = 0.83549350\n",
            "Iteration 26, loss = 0.83642687\n",
            "Iteration 27, loss = 0.83385843\n",
            "Iteration 28, loss = 0.83218562\n",
            "Iteration 29, loss = 0.83166431\n",
            "Iteration 30, loss = 0.83214895\n",
            "Iteration 31, loss = 0.83094213\n",
            "Iteration 32, loss = 0.83408585\n",
            "Iteration 33, loss = 0.82904732\n",
            "Iteration 34, loss = 0.83264114\n",
            "Iteration 35, loss = 0.83111366\n",
            "Iteration 36, loss = 0.83023500\n",
            "Iteration 37, loss = 0.83201043\n",
            "Iteration 38, loss = 0.82693353\n",
            "Iteration 39, loss = 0.82717814\n",
            "Iteration 40, loss = 0.83113346\n",
            "Iteration 41, loss = 0.82980221\n",
            "Iteration 42, loss = 0.82881249\n",
            "Iteration 43, loss = 0.83503379\n",
            "Iteration 44, loss = 0.82855307\n",
            "Iteration 45, loss = 0.82717377\n",
            "Iteration 46, loss = 0.82561857\n",
            "Iteration 47, loss = 0.82784212\n",
            "Iteration 48, loss = 0.82992937\n",
            "Iteration 49, loss = 0.82522564\n",
            "Iteration 50, loss = 0.82474756\n",
            "Iteration 51, loss = 0.82445415\n",
            "Iteration 52, loss = 0.82275553\n",
            "Iteration 53, loss = 0.82660495\n",
            "Iteration 54, loss = 0.82411296\n",
            "Iteration 55, loss = 0.82425658\n",
            "Iteration 56, loss = 0.82097637\n",
            "Iteration 57, loss = 0.82237888\n",
            "Iteration 58, loss = 0.82476115\n",
            "Iteration 59, loss = 0.82276799\n",
            "Iteration 60, loss = 0.82774609\n",
            "Iteration 61, loss = 0.82307191\n",
            "Iteration 62, loss = 0.82540235\n",
            "Iteration 63, loss = 0.81972377\n",
            "Iteration 64, loss = 0.82016256\n",
            "Iteration 65, loss = 0.82738136\n",
            "Iteration 66, loss = 0.82760918\n",
            "Iteration 67, loss = 0.82720756\n",
            "Iteration 68, loss = 0.81852848\n",
            "Iteration 69, loss = 0.81966603\n",
            "Iteration 70, loss = 0.81920822\n",
            "Iteration 71, loss = 0.82101812\n",
            "Iteration 72, loss = 0.81892577\n",
            "Iteration 73, loss = 0.82111905\n",
            "Iteration 74, loss = 0.82003958\n",
            "Iteration 75, loss = 0.82047335\n",
            "Iteration 76, loss = 0.81943648\n",
            "Iteration 77, loss = 0.82242080\n",
            "Iteration 78, loss = 0.81772359\n",
            "Iteration 79, loss = 0.82228788\n",
            "Iteration 80, loss = 0.81840676\n",
            "Iteration 81, loss = 0.82103400\n",
            "Iteration 82, loss = 0.81568908\n",
            "Iteration 83, loss = 0.81578503\n",
            "Iteration 84, loss = 0.81993862\n",
            "Iteration 85, loss = 0.82165377\n",
            "Iteration 86, loss = 0.82039679\n",
            "Iteration 87, loss = 0.81860724\n",
            "Iteration 88, loss = 0.81955179\n",
            "Iteration 89, loss = 0.81601002\n",
            "Iteration 90, loss = 0.81614718\n",
            "Iteration 91, loss = 0.81639231\n",
            "Iteration 92, loss = 0.81513824\n",
            "Iteration 93, loss = 0.81369597\n",
            "Iteration 94, loss = 0.81292244\n",
            "Iteration 95, loss = 0.81452561\n",
            "Iteration 96, loss = 0.81745386\n",
            "Iteration 97, loss = 0.81451591\n",
            "Iteration 98, loss = 0.81663646\n",
            "Iteration 99, loss = 0.81580356\n",
            "Iteration 100, loss = 0.81518717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36398996\n",
            "Iteration 2, loss = 1.31895248\n",
            "Iteration 3, loss = 1.27340457\n",
            "Iteration 4, loss = 1.21625116\n",
            "Iteration 5, loss = 1.14923821\n",
            "Iteration 6, loss = 1.07654685\n",
            "Iteration 7, loss = 1.00456494\n",
            "Iteration 8, loss = 0.94689254\n",
            "Iteration 9, loss = 0.91138914\n",
            "Iteration 10, loss = 0.88187355\n",
            "Iteration 11, loss = 0.86428982\n",
            "Iteration 12, loss = 0.85300645\n",
            "Iteration 13, loss = 0.84600918\n",
            "Iteration 14, loss = 0.83812873\n",
            "Iteration 15, loss = 0.83780224\n",
            "Iteration 16, loss = 0.83508245\n",
            "Iteration 17, loss = 0.83118282\n",
            "Iteration 18, loss = 0.82963003\n",
            "Iteration 19, loss = 0.82843223\n",
            "Iteration 20, loss = 0.82588933\n",
            "Iteration 21, loss = 0.82363371\n",
            "Iteration 22, loss = 0.82539817\n",
            "Iteration 23, loss = 0.82525205\n",
            "Iteration 24, loss = 0.82152501\n",
            "Iteration 25, loss = 0.82214335\n",
            "Iteration 26, loss = 0.82570332\n",
            "Iteration 27, loss = 0.82355960\n",
            "Iteration 28, loss = 0.81939153\n",
            "Iteration 29, loss = 0.81946044\n",
            "Iteration 30, loss = 0.81713623\n",
            "Iteration 31, loss = 0.81844999\n",
            "Iteration 32, loss = 0.82232334\n",
            "Iteration 33, loss = 0.81705372\n",
            "Iteration 34, loss = 0.81963824\n",
            "Iteration 35, loss = 0.81815835\n",
            "Iteration 36, loss = 0.81839332\n",
            "Iteration 37, loss = 0.81927914\n",
            "Iteration 38, loss = 0.81685676\n",
            "Iteration 39, loss = 0.81485042\n",
            "Iteration 40, loss = 0.81781983\n",
            "Iteration 41, loss = 0.81773645\n",
            "Iteration 42, loss = 0.81837707\n",
            "Iteration 43, loss = 0.82243832\n",
            "Iteration 44, loss = 0.81656096\n",
            "Iteration 45, loss = 0.81430835\n",
            "Iteration 46, loss = 0.81362762\n",
            "Iteration 47, loss = 0.81449773\n",
            "Iteration 48, loss = 0.81690251\n",
            "Iteration 49, loss = 0.81759828\n",
            "Iteration 50, loss = 0.81129869\n",
            "Iteration 51, loss = 0.81141067\n",
            "Iteration 52, loss = 0.80893336\n",
            "Iteration 53, loss = 0.81219400\n",
            "Iteration 54, loss = 0.81066009\n",
            "Iteration 55, loss = 0.81031063\n",
            "Iteration 56, loss = 0.80732910\n",
            "Iteration 57, loss = 0.80980490\n",
            "Iteration 58, loss = 0.81370652\n",
            "Iteration 59, loss = 0.81056029\n",
            "Iteration 60, loss = 0.81486944\n",
            "Iteration 61, loss = 0.80987893\n",
            "Iteration 62, loss = 0.80825087\n",
            "Iteration 63, loss = 0.80619803\n",
            "Iteration 64, loss = 0.80649177\n",
            "Iteration 65, loss = 0.80931205\n",
            "Iteration 66, loss = 0.80961870\n",
            "Iteration 67, loss = 0.81010324\n",
            "Iteration 68, loss = 0.80467919\n",
            "Iteration 69, loss = 0.80774512\n",
            "Iteration 70, loss = 0.80488245\n",
            "Iteration 71, loss = 0.80800447\n",
            "Iteration 72, loss = 0.80584398\n",
            "Iteration 73, loss = 0.80560552\n",
            "Iteration 74, loss = 0.80394166\n",
            "Iteration 75, loss = 0.80160619\n",
            "Iteration 76, loss = 0.80280840\n",
            "Iteration 77, loss = 0.80668924\n",
            "Iteration 78, loss = 0.80281647\n",
            "Iteration 79, loss = 0.80770252\n",
            "Iteration 80, loss = 0.80442800\n",
            "Iteration 81, loss = 0.80591450\n",
            "Iteration 82, loss = 0.80065544\n",
            "Iteration 83, loss = 0.80054949\n",
            "Iteration 84, loss = 0.80213336\n",
            "Iteration 85, loss = 0.80541034\n",
            "Iteration 86, loss = 0.80468083\n",
            "Iteration 87, loss = 0.79955043\n",
            "Iteration 88, loss = 0.79940338\n",
            "Iteration 89, loss = 0.79968911\n",
            "Iteration 90, loss = 0.79989220\n",
            "Iteration 91, loss = 0.79973423\n",
            "Iteration 92, loss = 0.79838798\n",
            "Iteration 93, loss = 0.79861916\n",
            "Iteration 94, loss = 0.80096298\n",
            "Iteration 95, loss = 0.80037807\n",
            "Iteration 96, loss = 0.80383138\n",
            "Iteration 97, loss = 0.79950895\n",
            "Iteration 98, loss = 0.79897974\n",
            "Iteration 99, loss = 0.79796379\n",
            "Iteration 100, loss = 0.79850017\n",
            "55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.36263814\n",
            "Iteration 2, loss = 1.27944582\n",
            "Iteration 3, loss = 1.17796488\n",
            "Iteration 4, loss = 1.04430788\n",
            "Iteration 5, loss = 0.89173142\n",
            "Iteration 6, loss = 0.74122439\n",
            "Iteration 7, loss = 0.61532854\n",
            "Iteration 8, loss = 0.51905427\n",
            "Iteration 9, loss = 0.44739557\n",
            "Iteration 10, loss = 0.39996116\n",
            "Iteration 11, loss = 0.37221880\n",
            "Iteration 12, loss = 0.35323307\n",
            "Iteration 13, loss = 0.33534398\n",
            "Iteration 14, loss = 0.32618611\n",
            "Iteration 15, loss = 0.32159578\n",
            "Iteration 16, loss = 0.31886636\n",
            "Iteration 17, loss = 0.31298110\n",
            "Iteration 18, loss = 0.31058271\n",
            "Iteration 19, loss = 0.30644846\n",
            "Iteration 20, loss = 0.31311444\n",
            "Iteration 21, loss = 0.30786022\n",
            "Iteration 22, loss = 0.31174354\n",
            "Iteration 23, loss = 0.30760870\n",
            "Iteration 24, loss = 0.30836380\n",
            "Iteration 25, loss = 0.30607702\n",
            "Iteration 26, loss = 0.30552421\n",
            "Iteration 27, loss = 0.30082100\n",
            "Iteration 28, loss = 0.30524551\n",
            "Iteration 29, loss = 0.30678950\n",
            "Iteration 30, loss = 0.30332116\n",
            "Iteration 31, loss = 0.30464649\n",
            "Iteration 32, loss = 0.30339571\n",
            "Iteration 33, loss = 0.30195983\n",
            "Iteration 34, loss = 0.30238646\n",
            "Iteration 35, loss = 0.30206080\n",
            "Iteration 36, loss = 0.30667062\n",
            "Iteration 37, loss = 0.30018710\n",
            "Iteration 38, loss = 0.30247780\n",
            "Iteration 39, loss = 0.30259295\n",
            "Iteration 40, loss = 0.30506564\n",
            "Iteration 41, loss = 0.30364877\n",
            "Iteration 42, loss = 0.29946878\n",
            "Iteration 43, loss = 0.30177724\n",
            "Iteration 44, loss = 0.29770776\n",
            "Iteration 45, loss = 0.30039177\n",
            "Iteration 46, loss = 0.29868726\n",
            "Iteration 47, loss = 0.30041802\n",
            "Iteration 48, loss = 0.30339008\n",
            "Iteration 49, loss = 0.30291493\n",
            "Iteration 50, loss = 0.29780928\n",
            "Iteration 51, loss = 0.30262746\n",
            "Iteration 52, loss = 0.30953516\n",
            "Iteration 53, loss = 0.30590169\n",
            "Iteration 54, loss = 0.30086997\n",
            "Iteration 55, loss = 0.30582121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36202066\n",
            "Iteration 2, loss = 1.28016489\n",
            "Iteration 3, loss = 1.18111970\n",
            "Iteration 4, loss = 1.05087420\n",
            "Iteration 5, loss = 0.90246793\n",
            "Iteration 6, loss = 0.75398926\n",
            "Iteration 7, loss = 0.62774436\n",
            "Iteration 8, loss = 0.53469086\n",
            "Iteration 9, loss = 0.46172091\n",
            "Iteration 10, loss = 0.41700782\n",
            "Iteration 11, loss = 0.38796535\n",
            "Iteration 12, loss = 0.36858328\n",
            "Iteration 13, loss = 0.35048259\n",
            "Iteration 14, loss = 0.34130549\n",
            "Iteration 15, loss = 0.33409620\n",
            "Iteration 16, loss = 0.33208475\n",
            "Iteration 17, loss = 0.32821801\n",
            "Iteration 18, loss = 0.32509144\n",
            "Iteration 19, loss = 0.32134323\n",
            "Iteration 20, loss = 0.32753289\n",
            "Iteration 21, loss = 0.32492687\n",
            "Iteration 22, loss = 0.33664068\n",
            "Iteration 23, loss = 0.31915374\n",
            "Iteration 24, loss = 0.32212566\n",
            "Iteration 25, loss = 0.32131639\n",
            "Iteration 26, loss = 0.31991376\n",
            "Iteration 27, loss = 0.31691742\n",
            "Iteration 28, loss = 0.32663369\n",
            "Iteration 29, loss = 0.32253392\n",
            "Iteration 30, loss = 0.32411284\n",
            "Iteration 31, loss = 0.31857708\n",
            "Iteration 32, loss = 0.31583214\n",
            "Iteration 33, loss = 0.31609616\n",
            "Iteration 34, loss = 0.31763837\n",
            "Iteration 35, loss = 0.31693629\n",
            "Iteration 36, loss = 0.31979880\n",
            "Iteration 37, loss = 0.31743322\n",
            "Iteration 38, loss = 0.31911495\n",
            "Iteration 39, loss = 0.32033962\n",
            "Iteration 40, loss = 0.31673288\n",
            "Iteration 41, loss = 0.31566266\n",
            "Iteration 42, loss = 0.31599993\n",
            "Iteration 43, loss = 0.32001896\n",
            "Iteration 44, loss = 0.31567900\n",
            "Iteration 45, loss = 0.31816297\n",
            "Iteration 46, loss = 0.31465415\n",
            "Iteration 47, loss = 0.31594161\n",
            "Iteration 48, loss = 0.31437488\n",
            "Iteration 49, loss = 0.31642979\n",
            "Iteration 50, loss = 0.31310603\n",
            "Iteration 51, loss = 0.32022806\n",
            "Iteration 52, loss = 0.33157909\n",
            "Iteration 53, loss = 0.32410268\n",
            "Iteration 54, loss = 0.32321662\n",
            "Iteration 55, loss = 0.32306585\n",
            "Iteration 56, loss = 0.32252706\n",
            "Iteration 57, loss = 0.31078437\n",
            "Iteration 58, loss = 0.31400562\n",
            "Iteration 59, loss = 0.31285267\n",
            "Iteration 60, loss = 0.31561766\n",
            "Iteration 61, loss = 0.31439242\n",
            "Iteration 62, loss = 0.31458630\n",
            "Iteration 63, loss = 0.31407649\n",
            "Iteration 64, loss = 0.31369025\n",
            "Iteration 65, loss = 0.31187525\n",
            "Iteration 66, loss = 0.31271553\n",
            "Iteration 67, loss = 0.31235744\n",
            "Iteration 68, loss = 0.31100535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35828335\n",
            "Iteration 2, loss = 1.27155336\n",
            "Iteration 3, loss = 1.16943134\n",
            "Iteration 4, loss = 1.03655182\n",
            "Iteration 5, loss = 0.88598412\n",
            "Iteration 6, loss = 0.73988319\n",
            "Iteration 7, loss = 0.61768074\n",
            "Iteration 8, loss = 0.52711405\n",
            "Iteration 9, loss = 0.45874660\n",
            "Iteration 10, loss = 0.41251035\n",
            "Iteration 11, loss = 0.38146540\n",
            "Iteration 12, loss = 0.36325443\n",
            "Iteration 13, loss = 0.34722173\n",
            "Iteration 14, loss = 0.33722849\n",
            "Iteration 15, loss = 0.33037386\n",
            "Iteration 16, loss = 0.33494769\n",
            "Iteration 17, loss = 0.32850003\n",
            "Iteration 18, loss = 0.32102997\n",
            "Iteration 19, loss = 0.31968426\n",
            "Iteration 20, loss = 0.32387261\n",
            "Iteration 21, loss = 0.31773430\n",
            "Iteration 22, loss = 0.32598464\n",
            "Iteration 23, loss = 0.31419230\n",
            "Iteration 24, loss = 0.31372343\n",
            "Iteration 25, loss = 0.31378087\n",
            "Iteration 26, loss = 0.31305635\n",
            "Iteration 27, loss = 0.31218559\n",
            "Iteration 28, loss = 0.32298316\n",
            "Iteration 29, loss = 0.31690305\n",
            "Iteration 30, loss = 0.32294406\n",
            "Iteration 31, loss = 0.31593527\n",
            "Iteration 32, loss = 0.31211918\n",
            "Iteration 33, loss = 0.31214555\n",
            "Iteration 34, loss = 0.31293259\n",
            "Iteration 35, loss = 0.31280579\n",
            "Iteration 36, loss = 0.31722258\n",
            "Iteration 37, loss = 0.31537563\n",
            "Iteration 38, loss = 0.31599847\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35872045\n",
            "Iteration 2, loss = 1.27425811\n",
            "Iteration 3, loss = 1.17368727\n",
            "Iteration 4, loss = 1.04525857\n",
            "Iteration 5, loss = 0.89457155\n",
            "Iteration 6, loss = 0.74754991\n",
            "Iteration 7, loss = 0.62289723\n",
            "Iteration 8, loss = 0.52983518\n",
            "Iteration 9, loss = 0.46244180\n",
            "Iteration 10, loss = 0.41851870\n",
            "Iteration 11, loss = 0.39140075\n",
            "Iteration 12, loss = 0.36843332\n",
            "Iteration 13, loss = 0.35261649\n",
            "Iteration 14, loss = 0.34133365\n",
            "Iteration 15, loss = 0.33726547\n",
            "Iteration 16, loss = 0.33863452\n",
            "Iteration 17, loss = 0.33005435\n",
            "Iteration 18, loss = 0.32485570\n",
            "Iteration 19, loss = 0.32264431\n",
            "Iteration 20, loss = 0.32948684\n",
            "Iteration 21, loss = 0.32148431\n",
            "Iteration 22, loss = 0.32630334\n",
            "Iteration 23, loss = 0.31908097\n",
            "Iteration 24, loss = 0.32221282\n",
            "Iteration 25, loss = 0.32065607\n",
            "Iteration 26, loss = 0.31536323\n",
            "Iteration 27, loss = 0.31538611\n",
            "Iteration 28, loss = 0.32675992\n",
            "Iteration 29, loss = 0.32143469\n",
            "Iteration 30, loss = 0.32852956\n",
            "Iteration 31, loss = 0.31934261\n",
            "Iteration 32, loss = 0.31741369\n",
            "Iteration 33, loss = 0.31671699\n",
            "Iteration 34, loss = 0.31458766\n",
            "Iteration 35, loss = 0.31277859\n",
            "Iteration 36, loss = 0.31913461\n",
            "Iteration 37, loss = 0.31530166\n",
            "Iteration 38, loss = 0.32167623\n",
            "Iteration 39, loss = 0.31947115\n",
            "Iteration 40, loss = 0.31823875\n",
            "Iteration 41, loss = 0.31547155\n",
            "Iteration 42, loss = 0.31279367\n",
            "Iteration 43, loss = 0.31779490\n",
            "Iteration 44, loss = 0.32374623\n",
            "Iteration 45, loss = 0.31769309\n",
            "Iteration 46, loss = 0.31664398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.36008720\n",
            "Iteration 2, loss = 1.27535601\n",
            "Iteration 3, loss = 1.17455072\n",
            "Iteration 4, loss = 1.04681177\n",
            "Iteration 5, loss = 0.89920275\n",
            "Iteration 6, loss = 0.75612086\n",
            "Iteration 7, loss = 0.63367481\n",
            "Iteration 8, loss = 0.53755953\n",
            "Iteration 9, loss = 0.46720525\n",
            "Iteration 10, loss = 0.41881203\n",
            "Iteration 11, loss = 0.38907264\n",
            "Iteration 12, loss = 0.36722454\n",
            "Iteration 13, loss = 0.35036226\n",
            "Iteration 14, loss = 0.33994746\n",
            "Iteration 15, loss = 0.33709893\n",
            "Iteration 16, loss = 0.33166886\n",
            "Iteration 17, loss = 0.32546091\n",
            "Iteration 18, loss = 0.32356439\n",
            "Iteration 19, loss = 0.32380962\n",
            "Iteration 20, loss = 0.32856254\n",
            "Iteration 21, loss = 0.32104821\n",
            "Iteration 22, loss = 0.32504935\n",
            "Iteration 23, loss = 0.31663907\n",
            "Iteration 24, loss = 0.31889966\n",
            "Iteration 25, loss = 0.31744142\n",
            "Iteration 26, loss = 0.31617068\n",
            "Iteration 27, loss = 0.31876385\n",
            "Iteration 28, loss = 0.32744989\n",
            "Iteration 29, loss = 0.32504352\n",
            "Iteration 30, loss = 0.32810282\n",
            "Iteration 31, loss = 0.32017736\n",
            "Iteration 32, loss = 0.32154502\n",
            "Iteration 33, loss = 0.31950659\n",
            "Iteration 34, loss = 0.31476655\n",
            "Iteration 35, loss = 0.31327270\n",
            "Iteration 36, loss = 0.31699715\n",
            "Iteration 37, loss = 0.31303508\n",
            "Iteration 38, loss = 0.31890667\n",
            "Iteration 39, loss = 0.31786596\n",
            "Iteration 40, loss = 0.31664382\n",
            "Iteration 41, loss = 0.31339229\n",
            "Iteration 42, loss = 0.31156382\n",
            "Iteration 43, loss = 0.31563202\n",
            "Iteration 44, loss = 0.32395609\n",
            "Iteration 45, loss = 0.31606557\n",
            "Iteration 46, loss = 0.32076114\n",
            "Iteration 47, loss = 0.31536373\n",
            "Iteration 48, loss = 0.31030420\n",
            "Iteration 49, loss = 0.31412313\n",
            "Iteration 50, loss = 0.31104981\n",
            "Iteration 51, loss = 0.31142141\n",
            "Iteration 52, loss = 0.31130719\n",
            "Iteration 53, loss = 0.31221195\n",
            "Iteration 54, loss = 0.31460146\n",
            "Iteration 55, loss = 0.31599905\n",
            "Iteration 56, loss = 0.31291316\n",
            "Iteration 57, loss = 0.31080780\n",
            "Iteration 58, loss = 0.31268757\n",
            "Iteration 59, loss = 0.31306459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35129388\n",
            "Iteration 2, loss = 1.27693409\n",
            "Iteration 3, loss = 1.18618203\n",
            "Iteration 4, loss = 1.06679791\n",
            "Iteration 5, loss = 0.92397826\n",
            "Iteration 6, loss = 0.77762428\n",
            "Iteration 7, loss = 0.64536366\n",
            "Iteration 8, loss = 0.54085184\n",
            "Iteration 9, loss = 0.47005769\n",
            "Iteration 10, loss = 0.42205318\n",
            "Iteration 11, loss = 0.39706311\n",
            "Iteration 12, loss = 0.38421229\n",
            "Iteration 13, loss = 0.36811116\n",
            "Iteration 14, loss = 0.35822270\n",
            "Iteration 15, loss = 0.35811353\n",
            "Iteration 16, loss = 0.35504164\n",
            "Iteration 17, loss = 0.34902415\n",
            "Iteration 18, loss = 0.34573940\n",
            "Iteration 19, loss = 0.34620100\n",
            "Iteration 20, loss = 0.35098884\n",
            "Iteration 21, loss = 0.34287226\n",
            "Iteration 22, loss = 0.34931647\n",
            "Iteration 23, loss = 0.34024822\n",
            "Iteration 24, loss = 0.34085606\n",
            "Iteration 25, loss = 0.34198258\n",
            "Iteration 26, loss = 0.34239199\n",
            "Iteration 27, loss = 0.34297247\n",
            "Iteration 28, loss = 0.34819621\n",
            "Iteration 29, loss = 0.34598204\n",
            "Iteration 30, loss = 0.34896052\n",
            "Iteration 31, loss = 0.34284832\n",
            "Iteration 32, loss = 0.34322062\n",
            "Iteration 33, loss = 0.34297685\n",
            "Iteration 34, loss = 0.33712302\n",
            "Iteration 35, loss = 0.33583462\n",
            "Iteration 36, loss = 0.33898963\n",
            "Iteration 37, loss = 0.33889493\n",
            "Iteration 38, loss = 0.33855077\n",
            "Iteration 39, loss = 0.33830698\n",
            "Iteration 40, loss = 0.33599153\n",
            "Iteration 41, loss = 0.33655097\n",
            "Iteration 42, loss = 0.33452908\n",
            "Iteration 43, loss = 0.33695686\n",
            "Iteration 44, loss = 0.34303246\n",
            "Iteration 45, loss = 0.33780213\n",
            "Iteration 46, loss = 0.35122099\n",
            "Iteration 47, loss = 0.34278040\n",
            "Iteration 48, loss = 0.33426180\n",
            "Iteration 49, loss = 0.33692868\n",
            "Iteration 50, loss = 0.33434382\n",
            "Iteration 51, loss = 0.33426161\n",
            "Iteration 52, loss = 0.33385010\n",
            "Iteration 53, loss = 0.33467937\n",
            "Iteration 54, loss = 0.33429398\n",
            "Iteration 55, loss = 0.33656385\n",
            "Iteration 56, loss = 0.33461503\n",
            "Iteration 57, loss = 0.33504700\n",
            "Iteration 58, loss = 0.33496360\n",
            "Iteration 59, loss = 0.33516954\n",
            "Iteration 60, loss = 0.33712772\n",
            "Iteration 61, loss = 0.34181958\n",
            "Iteration 62, loss = 0.33662677\n",
            "Iteration 63, loss = 0.33605027\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34029589\n",
            "Iteration 2, loss = 1.26948666\n",
            "Iteration 3, loss = 1.18550949\n",
            "Iteration 4, loss = 1.07631500\n",
            "Iteration 5, loss = 0.94066117\n",
            "Iteration 6, loss = 0.79682344\n",
            "Iteration 7, loss = 0.65844833\n",
            "Iteration 8, loss = 0.55114349\n",
            "Iteration 9, loss = 0.47597627\n",
            "Iteration 10, loss = 0.42604030\n",
            "Iteration 11, loss = 0.40006754\n",
            "Iteration 12, loss = 0.38806428\n",
            "Iteration 13, loss = 0.37324802\n",
            "Iteration 14, loss = 0.36445699\n",
            "Iteration 15, loss = 0.36269842\n",
            "Iteration 16, loss = 0.35846626\n",
            "Iteration 17, loss = 0.35282478\n",
            "Iteration 18, loss = 0.35214756\n",
            "Iteration 19, loss = 0.35520641\n",
            "Iteration 20, loss = 0.35689883\n",
            "Iteration 21, loss = 0.34929892\n",
            "Iteration 22, loss = 0.35420910\n",
            "Iteration 23, loss = 0.34709975\n",
            "Iteration 24, loss = 0.34722308\n",
            "Iteration 25, loss = 0.34846763\n",
            "Iteration 26, loss = 0.34914385\n",
            "Iteration 27, loss = 0.34815891\n",
            "Iteration 28, loss = 0.35372543\n",
            "Iteration 29, loss = 0.35201862\n",
            "Iteration 30, loss = 0.35702235\n",
            "Iteration 31, loss = 0.34836421\n",
            "Iteration 32, loss = 0.34824948\n",
            "Iteration 33, loss = 0.34923230\n",
            "Iteration 34, loss = 0.34308411\n",
            "Iteration 35, loss = 0.34242988\n",
            "Iteration 36, loss = 0.34424821\n",
            "Iteration 37, loss = 0.34592063\n",
            "Iteration 38, loss = 0.34540521\n",
            "Iteration 39, loss = 0.34679879\n",
            "Iteration 40, loss = 0.34354045\n",
            "Iteration 41, loss = 0.34351572\n",
            "Iteration 42, loss = 0.34202274\n",
            "Iteration 43, loss = 0.34496297\n",
            "Iteration 44, loss = 0.34992253\n",
            "Iteration 45, loss = 0.34526451\n",
            "Iteration 46, loss = 0.36114492\n",
            "Iteration 47, loss = 0.34951321\n",
            "Iteration 48, loss = 0.34160635\n",
            "Iteration 49, loss = 0.34317428\n",
            "Iteration 50, loss = 0.34088566\n",
            "Iteration 51, loss = 0.34071902\n",
            "Iteration 52, loss = 0.34077016\n",
            "Iteration 53, loss = 0.34170727\n",
            "Iteration 54, loss = 0.34133393\n",
            "Iteration 55, loss = 0.34365590\n",
            "Iteration 56, loss = 0.34169696\n",
            "Iteration 57, loss = 0.34251192\n",
            "Iteration 58, loss = 0.34210194\n",
            "Iteration 59, loss = 0.34136011\n",
            "Iteration 60, loss = 0.34480998\n",
            "Iteration 61, loss = 0.34930109\n",
            "Iteration 62, loss = 0.34314617\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35636451\n",
            "Iteration 2, loss = 1.28211117\n",
            "Iteration 3, loss = 1.18958189\n",
            "Iteration 4, loss = 1.06690263\n",
            "Iteration 5, loss = 0.92240579\n",
            "Iteration 6, loss = 0.77348465\n",
            "Iteration 7, loss = 0.63834536\n",
            "Iteration 8, loss = 0.53170957\n",
            "Iteration 9, loss = 0.45770006\n",
            "Iteration 10, loss = 0.40771414\n",
            "Iteration 11, loss = 0.38159309\n",
            "Iteration 12, loss = 0.36415361\n",
            "Iteration 13, loss = 0.35332606\n",
            "Iteration 14, loss = 0.34148575\n",
            "Iteration 15, loss = 0.34198597\n",
            "Iteration 16, loss = 0.34059800\n",
            "Iteration 17, loss = 0.33358038\n",
            "Iteration 18, loss = 0.33191550\n",
            "Iteration 19, loss = 0.33521218\n",
            "Iteration 20, loss = 0.33476047\n",
            "Iteration 21, loss = 0.32747955\n",
            "Iteration 22, loss = 0.33067008\n",
            "Iteration 23, loss = 0.32737262\n",
            "Iteration 24, loss = 0.32580295\n",
            "Iteration 25, loss = 0.32540831\n",
            "Iteration 26, loss = 0.32685534\n",
            "Iteration 27, loss = 0.32485069\n",
            "Iteration 28, loss = 0.32497373\n",
            "Iteration 29, loss = 0.32783367\n",
            "Iteration 30, loss = 0.33228046\n",
            "Iteration 31, loss = 0.32686510\n",
            "Iteration 32, loss = 0.33088336\n",
            "Iteration 33, loss = 0.32838061\n",
            "Iteration 34, loss = 0.32191950\n",
            "Iteration 35, loss = 0.32419975\n",
            "Iteration 36, loss = 0.32436466\n",
            "Iteration 37, loss = 0.32565236\n",
            "Iteration 38, loss = 0.32428907\n",
            "Iteration 39, loss = 0.32669186\n",
            "Iteration 40, loss = 0.32277858\n",
            "Iteration 41, loss = 0.32797396\n",
            "Iteration 42, loss = 0.32099873\n",
            "Iteration 43, loss = 0.32261744\n",
            "Iteration 44, loss = 0.32556223\n",
            "Iteration 45, loss = 0.32562652\n",
            "Iteration 46, loss = 0.33713833\n",
            "Iteration 47, loss = 0.32838432\n",
            "Iteration 48, loss = 0.32125018\n",
            "Iteration 49, loss = 0.32450389\n",
            "Iteration 50, loss = 0.32116797\n",
            "Iteration 51, loss = 0.32171595\n",
            "Iteration 52, loss = 0.32087415\n",
            "Iteration 53, loss = 0.32141995\n",
            "Iteration 54, loss = 0.32119129\n",
            "Iteration 55, loss = 0.32371168\n",
            "Iteration 56, loss = 0.32056797\n",
            "Iteration 57, loss = 0.32254136\n",
            "Iteration 58, loss = 0.32048770\n",
            "Iteration 59, loss = 0.32052200\n",
            "Iteration 60, loss = 0.32447656\n",
            "Iteration 61, loss = 0.32423244\n",
            "Iteration 62, loss = 0.32229977\n",
            "Iteration 63, loss = 0.32282502\n",
            "Iteration 64, loss = 0.32406112\n",
            "Iteration 65, loss = 0.32151609\n",
            "Iteration 66, loss = 0.32233415\n",
            "Iteration 67, loss = 0.32157732\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.35733764\n",
            "Iteration 2, loss = 1.27865848\n",
            "Iteration 3, loss = 1.17923312\n",
            "Iteration 4, loss = 1.05157027\n",
            "Iteration 5, loss = 0.90449452\n",
            "Iteration 6, loss = 0.75960826\n",
            "Iteration 7, loss = 0.63216590\n",
            "Iteration 8, loss = 0.53311171\n",
            "Iteration 9, loss = 0.46027989\n",
            "Iteration 10, loss = 0.40772614\n",
            "Iteration 11, loss = 0.37641969\n",
            "Iteration 12, loss = 0.35671603\n",
            "Iteration 13, loss = 0.34368958\n",
            "Iteration 14, loss = 0.33242124\n",
            "Iteration 15, loss = 0.32834544\n",
            "Iteration 16, loss = 0.32333294\n",
            "Iteration 17, loss = 0.32086263\n",
            "Iteration 18, loss = 0.31903568\n",
            "Iteration 19, loss = 0.31732894\n",
            "Iteration 20, loss = 0.31779026\n",
            "Iteration 21, loss = 0.31494253\n",
            "Iteration 22, loss = 0.31817165\n",
            "Iteration 23, loss = 0.31473810\n",
            "Iteration 24, loss = 0.31705937\n",
            "Iteration 25, loss = 0.31678119\n",
            "Iteration 26, loss = 0.31487977\n",
            "Iteration 27, loss = 0.31385820\n",
            "Iteration 28, loss = 0.31470185\n",
            "Iteration 29, loss = 0.31735948\n",
            "Iteration 30, loss = 0.31652364\n",
            "Iteration 31, loss = 0.31085549\n",
            "Iteration 32, loss = 0.31394236\n",
            "Iteration 33, loss = 0.31378277\n",
            "Iteration 34, loss = 0.31026243\n",
            "Iteration 35, loss = 0.31267198\n",
            "Iteration 36, loss = 0.31195415\n",
            "Iteration 37, loss = 0.31384810\n",
            "Iteration 38, loss = 0.31113076\n",
            "Iteration 39, loss = 0.31376795\n",
            "Iteration 40, loss = 0.30953400\n",
            "Iteration 41, loss = 0.31149615\n",
            "Iteration 42, loss = 0.30812085\n",
            "Iteration 43, loss = 0.31190500\n",
            "Iteration 44, loss = 0.31445290\n",
            "Iteration 45, loss = 0.31451198\n",
            "Iteration 46, loss = 0.32799789\n",
            "Iteration 47, loss = 0.31653196\n",
            "Iteration 48, loss = 0.31143255\n",
            "Iteration 49, loss = 0.31237707\n",
            "Iteration 50, loss = 0.30846943\n",
            "Iteration 51, loss = 0.30958741\n",
            "Iteration 52, loss = 0.30842385\n",
            "Iteration 53, loss = 0.30923772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34506423\n",
            "Iteration 2, loss = 1.26208097\n",
            "Iteration 3, loss = 1.15889452\n",
            "Iteration 4, loss = 1.02896595\n",
            "Iteration 5, loss = 0.87940883\n",
            "Iteration 6, loss = 0.73515615\n",
            "Iteration 7, loss = 0.61605383\n",
            "Iteration 8, loss = 0.52759800\n",
            "Iteration 9, loss = 0.46740416\n",
            "Iteration 10, loss = 0.41915577\n",
            "Iteration 11, loss = 0.38809623\n",
            "Iteration 12, loss = 0.37056665\n",
            "Iteration 13, loss = 0.35290457\n",
            "Iteration 14, loss = 0.34239046\n",
            "Iteration 15, loss = 0.33592886\n",
            "Iteration 16, loss = 0.33013744\n",
            "Iteration 17, loss = 0.33107682\n",
            "Iteration 18, loss = 0.32404668\n",
            "Iteration 19, loss = 0.32093337\n",
            "Iteration 20, loss = 0.31679740\n",
            "Iteration 21, loss = 0.31690427\n",
            "Iteration 22, loss = 0.31920025\n",
            "Iteration 23, loss = 0.31821225\n",
            "Iteration 24, loss = 0.31736924\n",
            "Iteration 25, loss = 0.31422068\n",
            "Iteration 26, loss = 0.31732942\n",
            "Iteration 27, loss = 0.31818446\n",
            "Iteration 28, loss = 0.31732893\n",
            "Iteration 29, loss = 0.31531381\n",
            "Iteration 30, loss = 0.31732837\n",
            "Iteration 31, loss = 0.31132284\n",
            "Iteration 32, loss = 0.31497284\n",
            "Iteration 33, loss = 0.31507360\n",
            "Iteration 34, loss = 0.31155424\n",
            "Iteration 35, loss = 0.31356222\n",
            "Iteration 36, loss = 0.31099698\n",
            "Iteration 37, loss = 0.31298512\n",
            "Iteration 38, loss = 0.31134020\n",
            "Iteration 39, loss = 0.31123514\n",
            "Iteration 40, loss = 0.31446758\n",
            "Iteration 41, loss = 0.31145909\n",
            "Iteration 42, loss = 0.31097992\n",
            "Iteration 43, loss = 0.31361272\n",
            "Iteration 44, loss = 0.31396099\n",
            "Iteration 45, loss = 0.31516273\n",
            "Iteration 46, loss = 0.32793180\n",
            "Iteration 47, loss = 0.31707396\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "56\n",
            "Iteration 1, loss = 1.34255403\n",
            "Iteration 2, loss = 1.21288332\n",
            "Iteration 3, loss = 1.05594138\n",
            "Iteration 4, loss = 0.86114073\n",
            "Iteration 5, loss = 0.64636248\n",
            "Iteration 6, loss = 0.44748336\n",
            "Iteration 7, loss = 0.29557606\n",
            "Iteration 8, loss = 0.19310862\n",
            "Iteration 9, loss = 0.12975842\n",
            "Iteration 10, loss = 0.09299544\n",
            "Iteration 11, loss = 0.07122606\n",
            "Iteration 12, loss = 0.05933318\n",
            "Iteration 13, loss = 0.05124439\n",
            "Iteration 14, loss = 0.04479123\n",
            "Iteration 15, loss = 0.03981905\n",
            "Iteration 16, loss = 0.03746849\n",
            "Iteration 17, loss = 0.03482429\n",
            "Iteration 18, loss = 0.03401088\n",
            "Iteration 19, loss = 0.03134852\n",
            "Iteration 20, loss = 0.02934203\n",
            "Iteration 21, loss = 0.02911142\n",
            "Iteration 22, loss = 0.02712613\n",
            "Iteration 23, loss = 0.02597723\n",
            "Iteration 24, loss = 0.02515470\n",
            "Iteration 25, loss = 0.02458235\n",
            "Iteration 26, loss = 0.02355333\n",
            "Iteration 27, loss = 0.02389240\n",
            "Iteration 28, loss = 0.02374406\n",
            "Iteration 29, loss = 0.02150984\n",
            "Iteration 30, loss = 0.02219010\n",
            "Iteration 31, loss = 0.02288692\n",
            "Iteration 32, loss = 0.02127789\n",
            "Iteration 33, loss = 0.02116052\n",
            "Iteration 34, loss = 0.01985419\n",
            "Iteration 35, loss = 0.01996254\n",
            "Iteration 36, loss = 0.02019445\n",
            "Iteration 37, loss = 0.01918383\n",
            "Iteration 38, loss = 0.01798310\n",
            "Iteration 39, loss = 0.01841907\n",
            "Iteration 40, loss = 0.01759506\n",
            "Iteration 41, loss = 0.01810894\n",
            "Iteration 42, loss = 0.01812327\n",
            "Iteration 43, loss = 0.01778129\n",
            "Iteration 44, loss = 0.01694287\n",
            "Iteration 45, loss = 0.01661160\n",
            "Iteration 46, loss = 0.01667028\n",
            "Iteration 47, loss = 0.01655237\n",
            "Iteration 48, loss = 0.01703890\n",
            "Iteration 49, loss = 0.01690733\n",
            "Iteration 50, loss = 0.01575057\n",
            "Iteration 51, loss = 0.01748607\n",
            "Iteration 52, loss = 0.01625391\n",
            "Iteration 53, loss = 0.01530827\n",
            "Iteration 54, loss = 0.01533189\n",
            "Iteration 55, loss = 0.01524308\n",
            "Iteration 56, loss = 0.01602394\n",
            "Iteration 57, loss = 0.01664013\n",
            "Iteration 58, loss = 0.01496027\n",
            "Iteration 59, loss = 0.01542315\n",
            "Iteration 60, loss = 0.01509955\n",
            "Iteration 61, loss = 0.01516883\n",
            "Iteration 62, loss = 0.01486889\n",
            "Iteration 63, loss = 0.01645424\n",
            "Iteration 64, loss = 0.01502405\n",
            "Iteration 65, loss = 0.01578041\n",
            "Iteration 66, loss = 0.01594023\n",
            "Iteration 67, loss = 0.01500479\n",
            "Iteration 68, loss = 0.01377221\n",
            "Iteration 69, loss = 0.01420053\n",
            "Iteration 70, loss = 0.01425023\n",
            "Iteration 71, loss = 0.01408114\n",
            "Iteration 72, loss = 0.01318386\n",
            "Iteration 73, loss = 0.01334015\n",
            "Iteration 74, loss = 0.01410129\n",
            "Iteration 75, loss = 0.01466125\n",
            "Iteration 76, loss = 0.01595658\n",
            "Iteration 77, loss = 0.01373630\n",
            "Iteration 78, loss = 0.01476631\n",
            "Iteration 79, loss = 0.01453391\n",
            "Iteration 80, loss = 0.01350140\n",
            "Iteration 81, loss = 0.01285917\n",
            "Iteration 82, loss = 0.01272364\n",
            "Iteration 83, loss = 0.01750746\n",
            "Iteration 84, loss = 0.01273234\n",
            "Iteration 85, loss = 0.01386438\n",
            "Iteration 86, loss = 0.01298583\n",
            "Iteration 87, loss = 0.01296810\n",
            "Iteration 88, loss = 0.01287406\n",
            "Iteration 89, loss = 0.01243044\n",
            "Iteration 90, loss = 0.01226094\n",
            "Iteration 91, loss = 0.01285084\n",
            "Iteration 92, loss = 0.01331999\n",
            "Iteration 93, loss = 0.01251605\n",
            "Iteration 94, loss = 0.01387654\n",
            "Iteration 95, loss = 0.01252911\n",
            "Iteration 96, loss = 0.01211344\n",
            "Iteration 97, loss = 0.01247965\n",
            "Iteration 98, loss = 0.01374944\n",
            "Iteration 99, loss = 0.01254239\n",
            "Iteration 100, loss = 0.01810316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34153345\n",
            "Iteration 2, loss = 1.21409212\n",
            "Iteration 3, loss = 1.06069592\n",
            "Iteration 4, loss = 0.86902068\n",
            "Iteration 5, loss = 0.65679652\n",
            "Iteration 6, loss = 0.46059142\n",
            "Iteration 7, loss = 0.30652656\n",
            "Iteration 8, loss = 0.20268824\n",
            "Iteration 9, loss = 0.13844790\n",
            "Iteration 10, loss = 0.09927744\n",
            "Iteration 11, loss = 0.07638373\n",
            "Iteration 12, loss = 0.06345598\n",
            "Iteration 13, loss = 0.05450432\n",
            "Iteration 14, loss = 0.04812373\n",
            "Iteration 15, loss = 0.04333915\n",
            "Iteration 16, loss = 0.04036640\n",
            "Iteration 17, loss = 0.03799974\n",
            "Iteration 18, loss = 0.03645414\n",
            "Iteration 19, loss = 0.03313857\n",
            "Iteration 20, loss = 0.03301045\n",
            "Iteration 21, loss = 0.03268200\n",
            "Iteration 22, loss = 0.03052975\n",
            "Iteration 23, loss = 0.02864345\n",
            "Iteration 24, loss = 0.02794727\n",
            "Iteration 25, loss = 0.02694979\n",
            "Iteration 26, loss = 0.02586846\n",
            "Iteration 27, loss = 0.02646640\n",
            "Iteration 28, loss = 0.02686116\n",
            "Iteration 29, loss = 0.02439302\n",
            "Iteration 30, loss = 0.02496337\n",
            "Iteration 31, loss = 0.02658158\n",
            "Iteration 32, loss = 0.02539823\n",
            "Iteration 33, loss = 0.02345237\n",
            "Iteration 34, loss = 0.02256497\n",
            "Iteration 35, loss = 0.02248418\n",
            "Iteration 36, loss = 0.02277165\n",
            "Iteration 37, loss = 0.02170868\n",
            "Iteration 38, loss = 0.02077447\n",
            "Iteration 39, loss = 0.02213565\n",
            "Iteration 40, loss = 0.02040449\n",
            "Iteration 41, loss = 0.02050927\n",
            "Iteration 42, loss = 0.02126211\n",
            "Iteration 43, loss = 0.02083721\n",
            "Iteration 44, loss = 0.01971792\n",
            "Iteration 45, loss = 0.01916243\n",
            "Iteration 46, loss = 0.01949378\n",
            "Iteration 47, loss = 0.02088399\n",
            "Iteration 48, loss = 0.01977933\n",
            "Iteration 49, loss = 0.02012783\n",
            "Iteration 50, loss = 0.01901679\n",
            "Iteration 51, loss = 0.02168217\n",
            "Iteration 52, loss = 0.01949100\n",
            "Iteration 53, loss = 0.01867628\n",
            "Iteration 54, loss = 0.01811626\n",
            "Iteration 55, loss = 0.01785723\n",
            "Iteration 56, loss = 0.01829321\n",
            "Iteration 57, loss = 0.01935270\n",
            "Iteration 58, loss = 0.01754747\n",
            "Iteration 59, loss = 0.01813623\n",
            "Iteration 60, loss = 0.01846586\n",
            "Iteration 61, loss = 0.01740287\n",
            "Iteration 62, loss = 0.01805919\n",
            "Iteration 63, loss = 0.01825939\n",
            "Iteration 64, loss = 0.01715884\n",
            "Iteration 65, loss = 0.01782584\n",
            "Iteration 66, loss = 0.01827629\n",
            "Iteration 67, loss = 0.01859502\n",
            "Iteration 68, loss = 0.01836226\n",
            "Iteration 69, loss = 0.01686028\n",
            "Iteration 70, loss = 0.01835356\n",
            "Iteration 71, loss = 0.01781142\n",
            "Iteration 72, loss = 0.01662379\n",
            "Iteration 73, loss = 0.01626019\n",
            "Iteration 74, loss = 0.01658280\n",
            "Iteration 75, loss = 0.01882470\n",
            "Iteration 76, loss = 0.01715704\n",
            "Iteration 77, loss = 0.01781509\n",
            "Iteration 78, loss = 0.01772010\n",
            "Iteration 79, loss = 0.01679658\n",
            "Iteration 80, loss = 0.01703323\n",
            "Iteration 81, loss = 0.01641325\n",
            "Iteration 82, loss = 0.01635704\n",
            "Iteration 83, loss = 0.01762701\n",
            "Iteration 84, loss = 0.01533142\n",
            "Iteration 85, loss = 0.01629524\n",
            "Iteration 86, loss = 0.01611810\n",
            "Iteration 87, loss = 0.01549252\n",
            "Iteration 88, loss = 0.01569797\n",
            "Iteration 89, loss = 0.01551321\n",
            "Iteration 90, loss = 0.01527491\n",
            "Iteration 91, loss = 0.01574532\n",
            "Iteration 92, loss = 0.01547075\n",
            "Iteration 93, loss = 0.01559672\n",
            "Iteration 94, loss = 0.01838442\n",
            "Iteration 95, loss = 0.01454823\n",
            "Iteration 96, loss = 0.01631340\n",
            "Iteration 97, loss = 0.01640193\n",
            "Iteration 98, loss = 0.01529845\n",
            "Iteration 99, loss = 0.01643650\n",
            "Iteration 100, loss = 0.01796467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33835425\n",
            "Iteration 2, loss = 1.20706991\n",
            "Iteration 3, loss = 1.04910648\n",
            "Iteration 4, loss = 0.85328059\n",
            "Iteration 5, loss = 0.64069185\n",
            "Iteration 6, loss = 0.44924039\n",
            "Iteration 7, loss = 0.30307072\n",
            "Iteration 8, loss = 0.20355942\n",
            "Iteration 9, loss = 0.14191962\n",
            "Iteration 10, loss = 0.10389048\n",
            "Iteration 11, loss = 0.08055015\n",
            "Iteration 12, loss = 0.06659587\n",
            "Iteration 13, loss = 0.05743906\n",
            "Iteration 14, loss = 0.05148606\n",
            "Iteration 15, loss = 0.04638966\n",
            "Iteration 16, loss = 0.04313435\n",
            "Iteration 17, loss = 0.04052439\n",
            "Iteration 18, loss = 0.03880745\n",
            "Iteration 19, loss = 0.03584926\n",
            "Iteration 20, loss = 0.03538996\n",
            "Iteration 21, loss = 0.03566628\n",
            "Iteration 22, loss = 0.03277595\n",
            "Iteration 23, loss = 0.03045045\n",
            "Iteration 24, loss = 0.03029006\n",
            "Iteration 25, loss = 0.02896623\n",
            "Iteration 26, loss = 0.02785754\n",
            "Iteration 27, loss = 0.02830908\n",
            "Iteration 28, loss = 0.02849965\n",
            "Iteration 29, loss = 0.02604911\n",
            "Iteration 30, loss = 0.02675797\n",
            "Iteration 31, loss = 0.02755657\n",
            "Iteration 32, loss = 0.02638984\n",
            "Iteration 33, loss = 0.02545231\n",
            "Iteration 34, loss = 0.02434806\n",
            "Iteration 35, loss = 0.02369534\n",
            "Iteration 36, loss = 0.02357413\n",
            "Iteration 37, loss = 0.02303810\n",
            "Iteration 38, loss = 0.02240648\n",
            "Iteration 39, loss = 0.02323293\n",
            "Iteration 40, loss = 0.02187178\n",
            "Iteration 41, loss = 0.02205006\n",
            "Iteration 42, loss = 0.02264189\n",
            "Iteration 43, loss = 0.02198142\n",
            "Iteration 44, loss = 0.02124501\n",
            "Iteration 45, loss = 0.02079930\n",
            "Iteration 46, loss = 0.02116943\n",
            "Iteration 47, loss = 0.02157748\n",
            "Iteration 48, loss = 0.02130458\n",
            "Iteration 49, loss = 0.02150751\n",
            "Iteration 50, loss = 0.01984745\n",
            "Iteration 51, loss = 0.02196201\n",
            "Iteration 52, loss = 0.02079202\n",
            "Iteration 53, loss = 0.01998497\n",
            "Iteration 54, loss = 0.01920749\n",
            "Iteration 55, loss = 0.01886542\n",
            "Iteration 56, loss = 0.01927209\n",
            "Iteration 57, loss = 0.02042043\n",
            "Iteration 58, loss = 0.01885149\n",
            "Iteration 59, loss = 0.01971184\n",
            "Iteration 60, loss = 0.01965607\n",
            "Iteration 61, loss = 0.01846645\n",
            "Iteration 62, loss = 0.01887325\n",
            "Iteration 63, loss = 0.01942876\n",
            "Iteration 64, loss = 0.01839136\n",
            "Iteration 65, loss = 0.01891999\n",
            "Iteration 66, loss = 0.01923063\n",
            "Iteration 67, loss = 0.01965563\n",
            "Iteration 68, loss = 0.01931057\n",
            "Iteration 69, loss = 0.01774685\n",
            "Iteration 70, loss = 0.01925753\n",
            "Iteration 71, loss = 0.01830992\n",
            "Iteration 72, loss = 0.01729288\n",
            "Iteration 73, loss = 0.01716249\n",
            "Iteration 74, loss = 0.01733859\n",
            "Iteration 75, loss = 0.01978318\n",
            "Iteration 76, loss = 0.01950633\n",
            "Iteration 77, loss = 0.01905834\n",
            "Iteration 78, loss = 0.01845744\n",
            "Iteration 79, loss = 0.01761119\n",
            "Iteration 80, loss = 0.01782657\n",
            "Iteration 81, loss = 0.01725584\n",
            "Iteration 82, loss = 0.01723245\n",
            "Iteration 83, loss = 0.01879036\n",
            "Iteration 84, loss = 0.01644324\n",
            "Iteration 85, loss = 0.01701495\n",
            "Iteration 86, loss = 0.01667716\n",
            "Iteration 87, loss = 0.01648918\n",
            "Iteration 88, loss = 0.01709944\n",
            "Iteration 89, loss = 0.01603417\n",
            "Iteration 90, loss = 0.01589981\n",
            "Iteration 91, loss = 0.01681600\n",
            "Iteration 92, loss = 0.01633384\n",
            "Iteration 93, loss = 0.01617688\n",
            "Iteration 94, loss = 0.01913890\n",
            "Iteration 95, loss = 0.01604953\n",
            "Iteration 96, loss = 0.01648792\n",
            "Iteration 97, loss = 0.01761236\n",
            "Iteration 98, loss = 0.01660305\n",
            "Iteration 99, loss = 0.01660578\n",
            "Iteration 100, loss = 0.02030388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33815565\n",
            "Iteration 2, loss = 1.20712932\n",
            "Iteration 3, loss = 1.04915164\n",
            "Iteration 4, loss = 0.85426626\n",
            "Iteration 5, loss = 0.64109073\n",
            "Iteration 6, loss = 0.44704892\n",
            "Iteration 7, loss = 0.29906559\n",
            "Iteration 8, loss = 0.19871596\n",
            "Iteration 9, loss = 0.13691697\n",
            "Iteration 10, loss = 0.09961201\n",
            "Iteration 11, loss = 0.07736408\n",
            "Iteration 12, loss = 0.06409869\n",
            "Iteration 13, loss = 0.05519575\n",
            "Iteration 14, loss = 0.04905746\n",
            "Iteration 15, loss = 0.04438682\n",
            "Iteration 16, loss = 0.04097183\n",
            "Iteration 17, loss = 0.03872872\n",
            "Iteration 18, loss = 0.03663283\n",
            "Iteration 19, loss = 0.03365890\n",
            "Iteration 20, loss = 0.03389107\n",
            "Iteration 21, loss = 0.03323715\n",
            "Iteration 22, loss = 0.03068442\n",
            "Iteration 23, loss = 0.02862633\n",
            "Iteration 24, loss = 0.02823598\n",
            "Iteration 25, loss = 0.02705384\n",
            "Iteration 26, loss = 0.02611690\n",
            "Iteration 27, loss = 0.02645140\n",
            "Iteration 28, loss = 0.02593030\n",
            "Iteration 29, loss = 0.02438004\n",
            "Iteration 30, loss = 0.02464116\n",
            "Iteration 31, loss = 0.02529786\n",
            "Iteration 32, loss = 0.02499233\n",
            "Iteration 33, loss = 0.02396035\n",
            "Iteration 34, loss = 0.02265158\n",
            "Iteration 35, loss = 0.02196298\n",
            "Iteration 36, loss = 0.02209299\n",
            "Iteration 37, loss = 0.02110596\n",
            "Iteration 38, loss = 0.02091891\n",
            "Iteration 39, loss = 0.02197442\n",
            "Iteration 40, loss = 0.02077496\n",
            "Iteration 41, loss = 0.02072402\n",
            "Iteration 42, loss = 0.02153895\n",
            "Iteration 43, loss = 0.02053834\n",
            "Iteration 44, loss = 0.01976300\n",
            "Iteration 45, loss = 0.01984489\n",
            "Iteration 46, loss = 0.02092178\n",
            "Iteration 47, loss = 0.02060894\n",
            "Iteration 48, loss = 0.02013560\n",
            "Iteration 49, loss = 0.01992560\n",
            "Iteration 50, loss = 0.01948949\n",
            "Iteration 51, loss = 0.02078216\n",
            "Iteration 52, loss = 0.02018621\n",
            "Iteration 53, loss = 0.01966547\n",
            "Iteration 54, loss = 0.01826791\n",
            "Iteration 55, loss = 0.01798137\n",
            "Iteration 56, loss = 0.01814102\n",
            "Iteration 57, loss = 0.01914227\n",
            "Iteration 58, loss = 0.01790863\n",
            "Iteration 59, loss = 0.01914834\n",
            "Iteration 60, loss = 0.01883292\n",
            "Iteration 61, loss = 0.01737160\n",
            "Iteration 62, loss = 0.01797777\n",
            "Iteration 63, loss = 0.01820065\n",
            "Iteration 64, loss = 0.01761559\n",
            "Iteration 65, loss = 0.01753394\n",
            "Iteration 66, loss = 0.01798076\n",
            "Iteration 67, loss = 0.01872213\n",
            "Iteration 68, loss = 0.01871238\n",
            "Iteration 69, loss = 0.01718258\n",
            "Iteration 70, loss = 0.01821948\n",
            "Iteration 71, loss = 0.01707807\n",
            "Iteration 72, loss = 0.01701613\n",
            "Iteration 73, loss = 0.01611533\n",
            "Iteration 74, loss = 0.01651396\n",
            "Iteration 75, loss = 0.01889733\n",
            "Iteration 76, loss = 0.01736700\n",
            "Iteration 77, loss = 0.01895204\n",
            "Iteration 78, loss = 0.01803316\n",
            "Iteration 79, loss = 0.01695741\n",
            "Iteration 80, loss = 0.01697646\n",
            "Iteration 81, loss = 0.01638513\n",
            "Iteration 82, loss = 0.01691793\n",
            "Iteration 83, loss = 0.01877986\n",
            "Iteration 84, loss = 0.01571143\n",
            "Iteration 85, loss = 0.01655965\n",
            "Iteration 86, loss = 0.01610355\n",
            "Iteration 87, loss = 0.01593382\n",
            "Iteration 88, loss = 0.01640182\n",
            "Iteration 89, loss = 0.01540039\n",
            "Iteration 90, loss = 0.01526797\n",
            "Iteration 91, loss = 0.01655752\n",
            "Iteration 92, loss = 0.01587484\n",
            "Iteration 93, loss = 0.01576529\n",
            "Iteration 94, loss = 0.01877255\n",
            "Iteration 95, loss = 0.01562868\n",
            "Iteration 96, loss = 0.01620363\n",
            "Iteration 97, loss = 0.01701500\n",
            "Iteration 98, loss = 0.01613980\n",
            "Iteration 99, loss = 0.01675861\n",
            "Iteration 100, loss = 0.01811417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33865201\n",
            "Iteration 2, loss = 1.20746545\n",
            "Iteration 3, loss = 1.04857481\n",
            "Iteration 4, loss = 0.85237199\n",
            "Iteration 5, loss = 0.63898500\n",
            "Iteration 6, loss = 0.44439607\n",
            "Iteration 7, loss = 0.29293723\n",
            "Iteration 8, loss = 0.18997351\n",
            "Iteration 9, loss = 0.12606029\n",
            "Iteration 10, loss = 0.08796462\n",
            "Iteration 11, loss = 0.06611360\n",
            "Iteration 12, loss = 0.05307191\n",
            "Iteration 13, loss = 0.04427103\n",
            "Iteration 14, loss = 0.03802193\n",
            "Iteration 15, loss = 0.03404064\n",
            "Iteration 16, loss = 0.03089046\n",
            "Iteration 17, loss = 0.02822396\n",
            "Iteration 18, loss = 0.02670431\n",
            "Iteration 19, loss = 0.02465041\n",
            "Iteration 20, loss = 0.02363598\n",
            "Iteration 21, loss = 0.02185839\n",
            "Iteration 22, loss = 0.02203391\n",
            "Iteration 23, loss = 0.01943043\n",
            "Iteration 24, loss = 0.02016161\n",
            "Iteration 25, loss = 0.01884081\n",
            "Iteration 26, loss = 0.01777371\n",
            "Iteration 27, loss = 0.01780484\n",
            "Iteration 28, loss = 0.01723155\n",
            "Iteration 29, loss = 0.01609635\n",
            "Iteration 30, loss = 0.01493296\n",
            "Iteration 31, loss = 0.01573813\n",
            "Iteration 32, loss = 0.01459798\n",
            "Iteration 33, loss = 0.01524815\n",
            "Iteration 34, loss = 0.01490689\n",
            "Iteration 35, loss = 0.01349089\n",
            "Iteration 36, loss = 0.01368970\n",
            "Iteration 37, loss = 0.01311788\n",
            "Iteration 38, loss = 0.01330145\n",
            "Iteration 39, loss = 0.01258413\n",
            "Iteration 40, loss = 0.01189993\n",
            "Iteration 41, loss = 0.01190281\n",
            "Iteration 42, loss = 0.01161394\n",
            "Iteration 43, loss = 0.01101743\n",
            "Iteration 44, loss = 0.01341967\n",
            "Iteration 45, loss = 0.01418379\n",
            "Iteration 46, loss = 0.01203904\n",
            "Iteration 47, loss = 0.01236488\n",
            "Iteration 48, loss = 0.01104793\n",
            "Iteration 49, loss = 0.01073809\n",
            "Iteration 50, loss = 0.01044967\n",
            "Iteration 51, loss = 0.01040927\n",
            "Iteration 52, loss = 0.01042192\n",
            "Iteration 53, loss = 0.01001143\n",
            "Iteration 54, loss = 0.00982560\n",
            "Iteration 55, loss = 0.00970554\n",
            "Iteration 56, loss = 0.00968322\n",
            "Iteration 57, loss = 0.01033321\n",
            "Iteration 58, loss = 0.00925366\n",
            "Iteration 59, loss = 0.00946311\n",
            "Iteration 60, loss = 0.00947122\n",
            "Iteration 61, loss = 0.00919705\n",
            "Iteration 62, loss = 0.00921921\n",
            "Iteration 63, loss = 0.00939135\n",
            "Iteration 64, loss = 0.00912202\n",
            "Iteration 65, loss = 0.00903134\n",
            "Iteration 66, loss = 0.00923661\n",
            "Iteration 67, loss = 0.00938733\n",
            "Iteration 68, loss = 0.00998899\n",
            "Iteration 69, loss = 0.00931436\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32942221\n",
            "Iteration 2, loss = 1.21084924\n",
            "Iteration 3, loss = 1.06059941\n",
            "Iteration 4, loss = 0.87251406\n",
            "Iteration 5, loss = 0.65354678\n",
            "Iteration 6, loss = 0.44765281\n",
            "Iteration 7, loss = 0.28792662\n",
            "Iteration 8, loss = 0.18533418\n",
            "Iteration 9, loss = 0.12353499\n",
            "Iteration 10, loss = 0.08879352\n",
            "Iteration 11, loss = 0.06930804\n",
            "Iteration 12, loss = 0.05772060\n",
            "Iteration 13, loss = 0.05087481\n",
            "Iteration 14, loss = 0.04511579\n",
            "Iteration 15, loss = 0.04129483\n",
            "Iteration 16, loss = 0.03901885\n",
            "Iteration 17, loss = 0.03621908\n",
            "Iteration 18, loss = 0.03420490\n",
            "Iteration 19, loss = 0.03275868\n",
            "Iteration 20, loss = 0.03185211\n",
            "Iteration 21, loss = 0.03000998\n",
            "Iteration 22, loss = 0.02984039\n",
            "Iteration 23, loss = 0.02938562\n",
            "Iteration 24, loss = 0.03008587\n",
            "Iteration 25, loss = 0.02710219\n",
            "Iteration 26, loss = 0.02626954\n",
            "Iteration 27, loss = 0.02634802\n",
            "Iteration 28, loss = 0.02494950\n",
            "Iteration 29, loss = 0.02438524\n",
            "Iteration 30, loss = 0.02377424\n",
            "Iteration 31, loss = 0.02374489\n",
            "Iteration 32, loss = 0.02239992\n",
            "Iteration 33, loss = 0.02330935\n",
            "Iteration 34, loss = 0.02246496\n",
            "Iteration 35, loss = 0.02181779\n",
            "Iteration 36, loss = 0.02190415\n",
            "Iteration 37, loss = 0.02254017\n",
            "Iteration 38, loss = 0.02414392\n",
            "Iteration 39, loss = 0.02592465\n",
            "Iteration 40, loss = 0.02089035\n",
            "Iteration 41, loss = 0.02046816\n",
            "Iteration 42, loss = 0.02079822\n",
            "Iteration 43, loss = 0.01933152\n",
            "Iteration 44, loss = 0.02182412\n",
            "Iteration 45, loss = 0.02327840\n",
            "Iteration 46, loss = 0.01940401\n",
            "Iteration 47, loss = 0.01891768\n",
            "Iteration 48, loss = 0.01864889\n",
            "Iteration 49, loss = 0.01844130\n",
            "Iteration 50, loss = 0.01902434\n",
            "Iteration 51, loss = 0.01850002\n",
            "Iteration 52, loss = 0.01785670\n",
            "Iteration 53, loss = 0.01853565\n",
            "Iteration 54, loss = 0.01805655\n",
            "Iteration 55, loss = 0.01859763\n",
            "Iteration 56, loss = 0.01766006\n",
            "Iteration 57, loss = 0.02059431\n",
            "Iteration 58, loss = 0.02062957\n",
            "Iteration 59, loss = 0.01678757\n",
            "Iteration 60, loss = 0.01734788\n",
            "Iteration 61, loss = 0.01823585\n",
            "Iteration 62, loss = 0.01687511\n",
            "Iteration 63, loss = 0.01791997\n",
            "Iteration 64, loss = 0.01826082\n",
            "Iteration 65, loss = 0.01691924\n",
            "Iteration 66, loss = 0.01724525\n",
            "Iteration 67, loss = 0.01823113\n",
            "Iteration 68, loss = 0.01920459\n",
            "Iteration 69, loss = 0.01769041\n",
            "Iteration 70, loss = 0.01724155\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31755736\n",
            "Iteration 2, loss = 1.20071369\n",
            "Iteration 3, loss = 1.05487581\n",
            "Iteration 4, loss = 0.87123297\n",
            "Iteration 5, loss = 0.65806017\n",
            "Iteration 6, loss = 0.45282849\n",
            "Iteration 7, loss = 0.29124508\n",
            "Iteration 8, loss = 0.18775071\n",
            "Iteration 9, loss = 0.12579004\n",
            "Iteration 10, loss = 0.09089883\n",
            "Iteration 11, loss = 0.07212995\n",
            "Iteration 12, loss = 0.06052784\n",
            "Iteration 13, loss = 0.05365321\n",
            "Iteration 14, loss = 0.04788520\n",
            "Iteration 15, loss = 0.04421911\n",
            "Iteration 16, loss = 0.04219907\n",
            "Iteration 17, loss = 0.03873659\n",
            "Iteration 18, loss = 0.03649406\n",
            "Iteration 19, loss = 0.03493556\n",
            "Iteration 20, loss = 0.03441956\n",
            "Iteration 21, loss = 0.03186552\n",
            "Iteration 22, loss = 0.03123947\n",
            "Iteration 23, loss = 0.03057291\n",
            "Iteration 24, loss = 0.03263285\n",
            "Iteration 25, loss = 0.02871493\n",
            "Iteration 26, loss = 0.02785742\n",
            "Iteration 27, loss = 0.02785365\n",
            "Iteration 28, loss = 0.02657566\n",
            "Iteration 29, loss = 0.02590077\n",
            "Iteration 30, loss = 0.02508394\n",
            "Iteration 31, loss = 0.02527627\n",
            "Iteration 32, loss = 0.02418998\n",
            "Iteration 33, loss = 0.02463834\n",
            "Iteration 34, loss = 0.02355974\n",
            "Iteration 35, loss = 0.02321647\n",
            "Iteration 36, loss = 0.02322266\n",
            "Iteration 37, loss = 0.02353088\n",
            "Iteration 38, loss = 0.02514841\n",
            "Iteration 39, loss = 0.02747710\n",
            "Iteration 40, loss = 0.02306177\n",
            "Iteration 41, loss = 0.02224543\n",
            "Iteration 42, loss = 0.02225783\n",
            "Iteration 43, loss = 0.02021443\n",
            "Iteration 44, loss = 0.02240147\n",
            "Iteration 45, loss = 0.02395131\n",
            "Iteration 46, loss = 0.02063611\n",
            "Iteration 47, loss = 0.02008751\n",
            "Iteration 48, loss = 0.02050458\n",
            "Iteration 49, loss = 0.01977695\n",
            "Iteration 50, loss = 0.02001021\n",
            "Iteration 51, loss = 0.01970905\n",
            "Iteration 52, loss = 0.01946764\n",
            "Iteration 53, loss = 0.02043066\n",
            "Iteration 54, loss = 0.02044841\n",
            "Iteration 55, loss = 0.01994016\n",
            "Iteration 56, loss = 0.01914012\n",
            "Iteration 57, loss = 0.02171909\n",
            "Iteration 58, loss = 0.02307014\n",
            "Iteration 59, loss = 0.01824213\n",
            "Iteration 60, loss = 0.01916231\n",
            "Iteration 61, loss = 0.01935962\n",
            "Iteration 62, loss = 0.01871617\n",
            "Iteration 63, loss = 0.02022271\n",
            "Iteration 64, loss = 0.02081052\n",
            "Iteration 65, loss = 0.01888844\n",
            "Iteration 66, loss = 0.01880406\n",
            "Iteration 67, loss = 0.01954180\n",
            "Iteration 68, loss = 0.01888582\n",
            "Iteration 69, loss = 0.01809255\n",
            "Iteration 70, loss = 0.01825803\n",
            "Iteration 71, loss = 0.01784215\n",
            "Iteration 72, loss = 0.02124635\n",
            "Iteration 73, loss = 0.02088767\n",
            "Iteration 74, loss = 0.01990859\n",
            "Iteration 75, loss = 0.02007899\n",
            "Iteration 76, loss = 0.02095412\n",
            "Iteration 77, loss = 0.02013490\n",
            "Iteration 78, loss = 0.01944876\n",
            "Iteration 79, loss = 0.02020109\n",
            "Iteration 80, loss = 0.01864318\n",
            "Iteration 81, loss = 0.01733741\n",
            "Iteration 82, loss = 0.01698770\n",
            "Iteration 83, loss = 0.01755429\n",
            "Iteration 84, loss = 0.01789018\n",
            "Iteration 85, loss = 0.01825937\n",
            "Iteration 86, loss = 0.01780120\n",
            "Iteration 87, loss = 0.01784378\n",
            "Iteration 88, loss = 0.01704047\n",
            "Iteration 89, loss = 0.01754482\n",
            "Iteration 90, loss = 0.01659018\n",
            "Iteration 91, loss = 0.01902754\n",
            "Iteration 92, loss = 0.01814052\n",
            "Iteration 93, loss = 0.01753399\n",
            "Iteration 94, loss = 0.01634456\n",
            "Iteration 95, loss = 0.01758994\n",
            "Iteration 96, loss = 0.01864161\n",
            "Iteration 97, loss = 0.01792600\n",
            "Iteration 98, loss = 0.02620301\n",
            "Iteration 99, loss = 0.02173469\n",
            "Iteration 100, loss = 0.01941574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33624631\n",
            "Iteration 2, loss = 1.22070335\n",
            "Iteration 3, loss = 1.07334924\n",
            "Iteration 4, loss = 0.88872701\n",
            "Iteration 5, loss = 0.67537230\n",
            "Iteration 6, loss = 0.47082478\n",
            "Iteration 7, loss = 0.30585379\n",
            "Iteration 8, loss = 0.19410951\n",
            "Iteration 9, loss = 0.12792425\n",
            "Iteration 10, loss = 0.09079354\n",
            "Iteration 11, loss = 0.07077990\n",
            "Iteration 12, loss = 0.05850182\n",
            "Iteration 13, loss = 0.05088568\n",
            "Iteration 14, loss = 0.04493158\n",
            "Iteration 15, loss = 0.04136626\n",
            "Iteration 16, loss = 0.03871425\n",
            "Iteration 17, loss = 0.03576492\n",
            "Iteration 18, loss = 0.03388413\n",
            "Iteration 19, loss = 0.03239883\n",
            "Iteration 20, loss = 0.03165161\n",
            "Iteration 21, loss = 0.02987580\n",
            "Iteration 22, loss = 0.02858038\n",
            "Iteration 23, loss = 0.02780300\n",
            "Iteration 24, loss = 0.02875973\n",
            "Iteration 25, loss = 0.02609035\n",
            "Iteration 26, loss = 0.02504923\n",
            "Iteration 27, loss = 0.02547742\n",
            "Iteration 28, loss = 0.02392019\n",
            "Iteration 29, loss = 0.02351014\n",
            "Iteration 30, loss = 0.02305461\n",
            "Iteration 31, loss = 0.02254757\n",
            "Iteration 32, loss = 0.02221698\n",
            "Iteration 33, loss = 0.02340406\n",
            "Iteration 34, loss = 0.02181753\n",
            "Iteration 35, loss = 0.02173644\n",
            "Iteration 36, loss = 0.02087968\n",
            "Iteration 37, loss = 0.02171127\n",
            "Iteration 38, loss = 0.02279227\n",
            "Iteration 39, loss = 0.02365861\n",
            "Iteration 40, loss = 0.02079213\n",
            "Iteration 41, loss = 0.02027232\n",
            "Iteration 42, loss = 0.02072999\n",
            "Iteration 43, loss = 0.01861137\n",
            "Iteration 44, loss = 0.02023698\n",
            "Iteration 45, loss = 0.02157295\n",
            "Iteration 46, loss = 0.01913143\n",
            "Iteration 47, loss = 0.01853431\n",
            "Iteration 48, loss = 0.01883155\n",
            "Iteration 49, loss = 0.01864238\n",
            "Iteration 50, loss = 0.01832410\n",
            "Iteration 51, loss = 0.01817716\n",
            "Iteration 52, loss = 0.01811998\n",
            "Iteration 53, loss = 0.01996783\n",
            "Iteration 54, loss = 0.01990235\n",
            "Iteration 55, loss = 0.01878444\n",
            "Iteration 56, loss = 0.01769836\n",
            "Iteration 57, loss = 0.02013586\n",
            "Iteration 58, loss = 0.02057894\n",
            "Iteration 59, loss = 0.01717309\n",
            "Iteration 60, loss = 0.01799835\n",
            "Iteration 61, loss = 0.01762949\n",
            "Iteration 62, loss = 0.01751142\n",
            "Iteration 63, loss = 0.01834681\n",
            "Iteration 64, loss = 0.01892896\n",
            "Iteration 65, loss = 0.01796545\n",
            "Iteration 66, loss = 0.01804396\n",
            "Iteration 67, loss = 0.01794958\n",
            "Iteration 68, loss = 0.01720668\n",
            "Iteration 69, loss = 0.01653595\n",
            "Iteration 70, loss = 0.01709618\n",
            "Iteration 71, loss = 0.01643057\n",
            "Iteration 72, loss = 0.01916485\n",
            "Iteration 73, loss = 0.01782445\n",
            "Iteration 74, loss = 0.01716387\n",
            "Iteration 75, loss = 0.01805514\n",
            "Iteration 76, loss = 0.01853728\n",
            "Iteration 77, loss = 0.01742879\n",
            "Iteration 78, loss = 0.01722794\n",
            "Iteration 79, loss = 0.01887232\n",
            "Iteration 80, loss = 0.01735460\n",
            "Iteration 81, loss = 0.01631770\n",
            "Iteration 82, loss = 0.01601320\n",
            "Iteration 83, loss = 0.01638653\n",
            "Iteration 84, loss = 0.01630979\n",
            "Iteration 85, loss = 0.01773682\n",
            "Iteration 86, loss = 0.01713028\n",
            "Iteration 87, loss = 0.01773039\n",
            "Iteration 88, loss = 0.01640956\n",
            "Iteration 89, loss = 0.01632133\n",
            "Iteration 90, loss = 0.01577427\n",
            "Iteration 91, loss = 0.01816882\n",
            "Iteration 92, loss = 0.01672900\n",
            "Iteration 93, loss = 0.01636547\n",
            "Iteration 94, loss = 0.01551709\n",
            "Iteration 95, loss = 0.01674742\n",
            "Iteration 96, loss = 0.01786018\n",
            "Iteration 97, loss = 0.01616154\n",
            "Iteration 98, loss = 0.02261254\n",
            "Iteration 99, loss = 0.01852134\n",
            "Iteration 100, loss = 0.01722487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.33805010\n",
            "Iteration 2, loss = 1.21741848\n",
            "Iteration 3, loss = 1.06383638\n",
            "Iteration 4, loss = 0.87474894\n",
            "Iteration 5, loss = 0.66591283\n",
            "Iteration 6, loss = 0.47203150\n",
            "Iteration 7, loss = 0.31503900\n",
            "Iteration 8, loss = 0.20824062\n",
            "Iteration 9, loss = 0.14042557\n",
            "Iteration 10, loss = 0.10049595\n",
            "Iteration 11, loss = 0.07789990\n",
            "Iteration 12, loss = 0.06408397\n",
            "Iteration 13, loss = 0.05520005\n",
            "Iteration 14, loss = 0.04790426\n",
            "Iteration 15, loss = 0.04397335\n",
            "Iteration 16, loss = 0.04148959\n",
            "Iteration 17, loss = 0.03753435\n",
            "Iteration 18, loss = 0.03541436\n",
            "Iteration 19, loss = 0.03339130\n",
            "Iteration 20, loss = 0.03285976\n",
            "Iteration 21, loss = 0.03041426\n",
            "Iteration 22, loss = 0.03012309\n",
            "Iteration 23, loss = 0.02971397\n",
            "Iteration 24, loss = 0.02920243\n",
            "Iteration 25, loss = 0.02761898\n",
            "Iteration 26, loss = 0.02662356\n",
            "Iteration 27, loss = 0.02665730\n",
            "Iteration 28, loss = 0.02519670\n",
            "Iteration 29, loss = 0.02459008\n",
            "Iteration 30, loss = 0.02405162\n",
            "Iteration 31, loss = 0.02352450\n",
            "Iteration 32, loss = 0.02309152\n",
            "Iteration 33, loss = 0.02372734\n",
            "Iteration 34, loss = 0.02239353\n",
            "Iteration 35, loss = 0.02294539\n",
            "Iteration 36, loss = 0.02279110\n",
            "Iteration 37, loss = 0.02314548\n",
            "Iteration 38, loss = 0.02475404\n",
            "Iteration 39, loss = 0.02492566\n",
            "Iteration 40, loss = 0.02357100\n",
            "Iteration 41, loss = 0.02105788\n",
            "Iteration 42, loss = 0.02120323\n",
            "Iteration 43, loss = 0.01949179\n",
            "Iteration 44, loss = 0.02111855\n",
            "Iteration 45, loss = 0.02260269\n",
            "Iteration 46, loss = 0.01996205\n",
            "Iteration 47, loss = 0.01908014\n",
            "Iteration 48, loss = 0.01946182\n",
            "Iteration 49, loss = 0.01920182\n",
            "Iteration 50, loss = 0.01930671\n",
            "Iteration 51, loss = 0.01955234\n",
            "Iteration 52, loss = 0.01834793\n",
            "Iteration 53, loss = 0.01903986\n",
            "Iteration 54, loss = 0.01900468\n",
            "Iteration 55, loss = 0.01860774\n",
            "Iteration 56, loss = 0.01899201\n",
            "Iteration 57, loss = 0.02076622\n",
            "Iteration 58, loss = 0.02497641\n",
            "Iteration 59, loss = 0.01843294\n",
            "Iteration 60, loss = 0.01871659\n",
            "Iteration 61, loss = 0.01823771\n",
            "Iteration 62, loss = 0.01782662\n",
            "Iteration 63, loss = 0.01848380\n",
            "Iteration 64, loss = 0.01855049\n",
            "Iteration 65, loss = 0.01809416\n",
            "Iteration 66, loss = 0.01820077\n",
            "Iteration 67, loss = 0.01811786\n",
            "Iteration 68, loss = 0.01724678\n",
            "Iteration 69, loss = 0.01657614\n",
            "Iteration 70, loss = 0.01724607\n",
            "Iteration 71, loss = 0.01747942\n",
            "Iteration 72, loss = 0.02081781\n",
            "Iteration 73, loss = 0.02084567\n",
            "Iteration 74, loss = 0.01828804\n",
            "Iteration 75, loss = 0.01958479\n",
            "Iteration 76, loss = 0.02030751\n",
            "Iteration 77, loss = 0.01829538\n",
            "Iteration 78, loss = 0.01667056\n",
            "Iteration 79, loss = 0.01729666\n",
            "Iteration 80, loss = 0.01604344\n",
            "Iteration 81, loss = 0.01587988\n",
            "Iteration 82, loss = 0.01580677\n",
            "Iteration 83, loss = 0.01646673\n",
            "Iteration 84, loss = 0.01749846\n",
            "Iteration 85, loss = 0.01863677\n",
            "Iteration 86, loss = 0.01762250\n",
            "Iteration 87, loss = 0.01805399\n",
            "Iteration 88, loss = 0.01632894\n",
            "Iteration 89, loss = 0.01624461\n",
            "Iteration 90, loss = 0.01571565\n",
            "Iteration 91, loss = 0.01624421\n",
            "Iteration 92, loss = 0.01616832\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.32394921\n",
            "Iteration 2, loss = 1.19390023\n",
            "Iteration 3, loss = 1.03630306\n",
            "Iteration 4, loss = 0.84393272\n",
            "Iteration 5, loss = 0.63668678\n",
            "Iteration 6, loss = 0.44944486\n",
            "Iteration 7, loss = 0.31069689\n",
            "Iteration 8, loss = 0.22086869\n",
            "Iteration 9, loss = 0.15874700\n",
            "Iteration 10, loss = 0.11695429\n",
            "Iteration 11, loss = 0.09079451\n",
            "Iteration 12, loss = 0.07454794\n",
            "Iteration 13, loss = 0.06225984\n",
            "Iteration 14, loss = 0.05310580\n",
            "Iteration 15, loss = 0.04814003\n",
            "Iteration 16, loss = 0.04511850\n",
            "Iteration 17, loss = 0.04073184\n",
            "Iteration 18, loss = 0.03821347\n",
            "Iteration 19, loss = 0.03604380\n",
            "Iteration 20, loss = 0.03530635\n",
            "Iteration 21, loss = 0.03246717\n",
            "Iteration 22, loss = 0.03168396\n",
            "Iteration 23, loss = 0.03068617\n",
            "Iteration 24, loss = 0.03033718\n",
            "Iteration 25, loss = 0.02857625\n",
            "Iteration 26, loss = 0.02816055\n",
            "Iteration 27, loss = 0.02798111\n",
            "Iteration 28, loss = 0.02595640\n",
            "Iteration 29, loss = 0.02546700\n",
            "Iteration 30, loss = 0.02477322\n",
            "Iteration 31, loss = 0.02440491\n",
            "Iteration 32, loss = 0.02412383\n",
            "Iteration 33, loss = 0.02528300\n",
            "Iteration 34, loss = 0.02355193\n",
            "Iteration 35, loss = 0.02391503\n",
            "Iteration 36, loss = 0.02494625\n",
            "Iteration 37, loss = 0.02548636\n",
            "Iteration 38, loss = 0.02690123\n",
            "Iteration 39, loss = 0.02788028\n",
            "Iteration 40, loss = 0.02718626\n",
            "Iteration 41, loss = 0.02270802\n",
            "Iteration 42, loss = 0.02139662\n",
            "Iteration 43, loss = 0.01992240\n",
            "Iteration 44, loss = 0.02216760\n",
            "Iteration 45, loss = 0.02222817\n",
            "Iteration 46, loss = 0.02045101\n",
            "Iteration 47, loss = 0.02018203\n",
            "Iteration 48, loss = 0.01981524\n",
            "Iteration 49, loss = 0.01935505\n",
            "Iteration 50, loss = 0.01971989\n",
            "Iteration 51, loss = 0.02026262\n",
            "Iteration 52, loss = 0.01895924\n",
            "Iteration 53, loss = 0.01944232\n",
            "Iteration 54, loss = 0.01939337\n",
            "Iteration 55, loss = 0.01878950\n",
            "Iteration 56, loss = 0.01883281\n",
            "Iteration 57, loss = 0.02073078\n",
            "Iteration 58, loss = 0.02368362\n",
            "Iteration 59, loss = 0.01845989\n",
            "Iteration 60, loss = 0.01836896\n",
            "Iteration 61, loss = 0.01800833\n",
            "Iteration 62, loss = 0.01799999\n",
            "Iteration 63, loss = 0.01840287\n",
            "Iteration 64, loss = 0.01865436\n",
            "Iteration 65, loss = 0.01833905\n",
            "Iteration 66, loss = 0.01819719\n",
            "Iteration 67, loss = 0.01915119\n",
            "Iteration 68, loss = 0.01824347\n",
            "Iteration 69, loss = 0.01705507\n",
            "Iteration 70, loss = 0.01836120\n",
            "Iteration 71, loss = 0.01921148\n",
            "Iteration 72, loss = 0.02234235\n",
            "Iteration 73, loss = 0.02145825\n",
            "Iteration 74, loss = 0.01805525\n",
            "Iteration 75, loss = 0.01949165\n",
            "Iteration 76, loss = 0.01966225\n",
            "Iteration 77, loss = 0.01782571\n",
            "Iteration 78, loss = 0.01714812\n",
            "Iteration 79, loss = 0.01793653\n",
            "Iteration 80, loss = 0.01736342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy Prediction"
      ],
      "metadata": {
        "id": "_3qWo3n5hR9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import (f1_score, precision_score, recall_score, accuracy_score)"
      ],
      "metadata": {
        "id": "XMgKOuAxhUEE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Ensemble Data/\"\n",
        "rocv=np.zeros((56,5))\n",
        "\n",
        "for i in range(0, 56):\n",
        "    fname = path + str(i+1) + 'twop.csv'\n",
        "    data = np.genfromtxt(fname,delimiter=',')\n",
        "    y1 = data[:,20:24]\n",
        "    for j in range(0,5):\n",
        "        b = data[:,4*j:4*(j+1)]\n",
        "        r = np.zeros((4))\n",
        "        for k in range(0,4):\n",
        "            fpr, tpr, _ = roc_curve(y1[:, k], b[:, k])\n",
        "            r[k] = auc(fpr, tpr)\n",
        "        rocv[i,j] = np.mean(r)"
      ],
      "metadata": {
        "id": "f3mFkfgxhYK7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fval=np.zeros((56,5))\n",
        "acv=np.zeros((56,5))\n",
        "\n",
        "for i in range(0,56):\n",
        "    fname = path + str(i+1) + 'two.csv'\n",
        "    data1 = np.genfromtxt(fname,delimiter=',')\n",
        "    y1 = data1[:,-1]\n",
        "    for j in range(0,5):\n",
        "         fval[i,j] = f1_score(y1, data1[:,j], average=\"weighted\")\n",
        "         acv[i,j] = accuracy_score(y1, data1[:,j])"
      ],
      "metadata": {
        "id": "DyU7pxTCh2NB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname='acc.csv'    \n",
        "np.savetxt(fname, acv, delimiter=',', fmt='%f')  \n",
        "files.download(fname)\n",
        "\n",
        "fname='fmea.csv'    \n",
        "np.savetxt(fname, acv, delimiter=',', fmt='%f')  \n",
        "files.download(fname)\n",
        "\n",
        "fname='auc.csv'    \n",
        "np.savetxt(fname, acv, delimiter=',', fmt='%f')  \n",
        "files.download(fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zzuUzwUdiSER",
        "outputId": "27fd04b5-96c9-444f-d532-222f7db9c621"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf633f8e-4e07-4db4-a5c1-e77c37b26015\", \"acc.csv\", 2520)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f5e0581a-a077-4904-8c68-8d974b4ca59e\", \"fmea.csv\", 2520)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a0f601b4-9807-49ba-8830-38395a36609b\", \"auc.csv\", 2520)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}